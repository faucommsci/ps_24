---
title: "Unsupervised Machine Learning II"
subtitle: "Session 10 - Exercise"
date: last-modified
date-format: "DD.MM.YYYY"
format:
    html: 
        code-fold: true
        code-summary: "L√∂sung anzeigen"
---

::: {.callout-tip icon="false"}
[![Quarto Document](https://raw.githubusercontent.com/faucommsci/teaching_materials/main/images/badges/badge-quarto_document.svg)](https://github.com/faucommsci/ps_24/blob/main/exercises/ms-exercise-10_solution.qmd) Link to source file
:::

::: callout-note
## Fokus der √úbung: stm-Topicmodeling mit R kennenlernen

-   Typische Schritte der Auswertung eines `stm`-Topicmodels mit Hilfe von `tidytext` [@silge2016] reproduzieren
-   Verst√§ndnis f√ºr die Interpretation von Themenmodellen sch√§rfen.
-   Einfluss von Metadaten untersuchen und interpretieren.
:::

## Background

::: callout-tip
## Todays's data basis: [OpenAlex](https://openalex.org/)

-   Via API bzw. openalexR [@aria2024] gesammelte "works" der Datenbank [OpenAlex](https://openalex.org/) mit Bezug zu Literaturriews in den Sozialwissenschaften zwischen 2013 und 2023

-   Detaillierte Informationen und Ergebnisse zur Suchquery finden Sie [hier](https://openalex.org/works?page=1&filter=display_name.search%3A%28literature%20OR%20systematic%29%20AND%20review,primary_topic.domain.id%3Adomains%2F2,publication_year%3A2014%20-%202024&group_by=publication_year,open_access.is_oa,primary_topic.field.id).
:::

## Preparation

::: callout-important
## Wichtige Information

-   Bitte stellen Sie sicher, dass Sie das jeweilige R-Studio Projekt zur √úbung ge√∂ffnet haben. Nur so funktionieren alle Dependencies korrekt.
-   Um den einwandfreien Ablauf der √úbung zu gew√§hrleisten, wird f√ºr die Aufgaben auf eine eigenst√§ndige Datenerhebung verzichtet und ein √úbungsdatensatz zu verf√ºgung gestelt.
:::

### Packages

```{r load-packages}
#| code-fold: false

if (!require("pacman")) install.packages("pacman")
pacman::p_load(
    here, qs, # file management
    magrittr, janitor, # data wrangling
    easystats, sjmisc, # data analysis
    gt, gtExtras, # table visualization
    ggpubr, ggwordcloud, # visualization
    # text analysis    
    tidytext, widyr, # based on tidytext
    quanteda, # based on quanteda
    quanteda.textmodels, quanteda.textplots, quanteda.textstats, 
    stm, # structural topic modeling
    openalexR, pushoverr, tictoc, 
    tidyverse # load last to avoid masking issues
  )
```

### Import und Vorverarbeitung der Daten

```{r import-data}
#| echo: true
#| eval: false
#| code-fold: false

review_works <- qs::qread(here("data/session-07/openalex-review_works-2013_2023.qs"))

# Create correct data
review_subsample <- review_works %>% 
    # Create additional factor variables
    mutate(
        publication_year_fct = as.factor(publication_year), 
        type_fct = as.factor(type)
        ) %>%
    # Eingrenzung: Sprache und Typ
    filter(language == "en") %>% 
    filter(type == "article") %>%
    # Datentranformation
    unnest(topics, names_sep = "_") %>%
    filter(topics_name == "field") %>% 
    filter(topics_i == "1") %>% 
    # Eingrenzung: Forschungsfeldes
    filter(
    topics_display_name == "Social Sciences"|
    topics_display_name == "Psychology"
    ) %>% 
    mutate(
        field = as.factor(topics_display_name)
    ) %>% 
    # Eingrenzung: Keine Eintr√§ge ohne Abstract
    filter(!is.na(ab))
```

```{r create-dfm-quanteda}
#| echo: true
#| eval: false
#| code-fold: false
#| output: false

# Create corpus
quanteda_corpus <- review_subsample %>% 
  quanteda::corpus(
    docid_field = "id", 
    text_field = "ab"
  )

# Tokenize
quanteda_token <- quanteda_corpus %>% 
  quanteda::tokens(
    remove_punct = TRUE,
    remove_symbols = TRUE, 
    remove_numbers = TRUE, 
    remove_url = TRUE, 
    split_tags = FALSE # keep hashtags and mentions
  ) %>% 
  quanteda::tokens_tolower() %>% 
  quanteda::tokens_remove(
    pattern = stopwords("en")
    )

# Convert to Document-Feature-Matrix (DFM)
quanteda_dfm <- quanteda_token %>% 
  quanteda::dfm()

# Pruning
quanteda_dfm_trim <- quanteda_dfm %>% 
  dfm_trim( 
    min_docfreq = 10/nrow(review_subsample),
    max_docfreq = 0.99, 
    docfreq_type = "prop")

# Convert for stm topic modeling
quanteda_stm <- quanteda_dfm_trim %>% 
   convert(to = "stm")
```

```{r import-data-backend}
#| echo: false
#| eval: true

review_subsample <- qs::qread(here("data/session-10/review_subsample.qs"))
quanteda_stm <- qs::qread(here("data/session-10/quanteda_stm.qs"))
stm_mdl_k0 <- qs::qread(here("data/session-10/stm_mdl_k0.qs"))
stm_mdl_k20 <- qs::qread(here("data/session-10/stm_mdl_k20.qs"))
stm_search <- qs::qread(here("data/session-10/stm_search.qs"))
```

## üõ†Ô∏è Praktische Anwendung

::: callout-important
## Achtung, bitte lesen!

-   Bevor Sie mit der Arbeit an den folgenden üìã **Exercises** beginnen, stellen Sie bitte sicher, dass Sie alle Chunks des Abschnitts [Preparation] gerendert haben. Das k√∂nnen Sie tun, indem Sie den "*Run all chunks above*"-Knopf ![](https://raw.githubusercontent.com/faucommsci/teaching_materials/main/images/buttons/rstudio-button-render_all_chunks_above.png)des n√§chsten Chunks benutzen.
-   Bei Fragen zum Code lohnt sich ein Blick in den **Showcase** (.qmd oder .html). Beim Showcase handelt es sich um eine kompakte Darstellung des in der Pr√§sentation verwenden R-Codes. Sie k√∂nnen das Showcase also nutzen, um sich die Code-Bausteine anzusehen, die f√ºr die R-Outputs auf den Slides benutzt wurden.
:::

### üìã Exercise 1: Visualisierung der Themenpr√§valenz

#### 1.1. Auswahl des passenden Models

1.  Erstelen Sie einen neuen Datensatz `stm_mdl_k40`
    -   basierend auf dem Datensatz `stm_serach`
        1.  Verwenden Sie `filter(k == 40)`, um das Modell mit 40 Themen zu auszuw√§hlen.
        2.  Verwenden Sie `pull(mdl) %>% .[[1]]` um die Spalte und das Element zu extrahieren, die das Modell enth√§lt.
        3.  Speichern Sie diese Umwandlung, indem Sie einen neuen Datensatz mit dem Namen `stm_mdl_k40` erstellen.
2.  √úberpr√ºfen Sie die Transformation indem Sie `stm_mdl_k40` in die Konsole eingeben.

```{r exercise-1-1}
# Pull tpm with 40 topics
stm_mdl_k40 <- stm_search %>% 
  filter(k == 40) %>% 
  pull(mdl) %>% 
  .[[1]]

# Check
stm_mdl_k40
```

#### 1.2. Identifikation der Top-Terms f√ºr jedes Thema

1.  Erstellen Sie einen neuen Datensatz `td_beta`
    -   basierend auf dem Datensatz `stm_mdl_k40`,
    -   Verwenden Sie `tidy(method = "frex")`, um die Beta-Matrix zu erstellen.
2.  Erstellen Sie einen neuen Datensatz `top_terms`
    -   basierend auf dem Datenastz `td_beta`,
        1.  Verwenden Sie `arrange(beta)`, um die Begriffe nach Beta zu sortieren.
        2.  Gruppieren Sie die Begriffe nach `topic` mit `group_by(topic)`.
        3.  Extrahieren Sie die 7 h√§ufigsten Begriffe mit `top_n(7, beta)`.
        4.  Sortieren Sie die Begriffe absteigend mit `arrange(-beta)`.
        5.  W√§hlen Sie die Variablen `topic` und `term` mit `select(topic, term)` aus.
        6.  Extrahieren Sie die Top-Begriffe pro Thema mit `summarise(terms = list(term))`.
        7.  Transformieren Sie die extrahierten Begriffe pro Thema mit `map(terms, paste, collapse = ", ")` zu einem String.
        8.  "Entpacken" Sie die Begriffe aus der Liste (unnesten) mit `unnest(cols = c(terms))`.
3.  √úberpr√ºfen Sie die Transformation indem Sie `top_terms` in die Konsole eingeben.

```{r exercise-1-2}
# Create tidy beta matrix
td_beta <- tidy(stm_mdl_k40, method = "frex")

# Create top terms
top_terms <- td_beta %>%
  arrange(beta) %>%
  group_by(topic) %>%
  top_n(7, beta) %>%
  arrange(-beta) %>%
  select(topic, term) %>%
  summarise(terms = list(term)) %>%
  mutate(terms = map(terms, paste, collapse = ", ")) %>% 
  unnest(cols = c(terms))

# Output
top_terms
```

#### 1.3 Erstellung der Pr√§valenz-Tabelle f√ºr die Themen

1.  Erstellen Sie einen neuen Datensatz `td_gamma`
    -   basierend auf dem Datensatz `stm_mdl_k40`,
    -   Verwenden Sie `tidy()`, um die Gamma-Matrix zu erstellen.
    -   Verwenden Sie `document_names = names(quanteda_stm$documents)` um die Dokumentennamen zu speichern
2.  Erstellen Sie einen neuen Datensatz `prevalence`
    -   basierend auf dem Datensatz `td_gamma`,
        1.  Gruppieren Sie die Themen nach `topic` mit `group_by(topic)`.
        2.  Berechnen Sie den Durchschnitt der `gamma`-Werte pro Thema mit `summarise(gamma = mean(gamma))`.
        3.  Sortieren Sie die Themen (absteigend) nach `gamma` mit `arrange(desc(gamma))`.
        4.  Verkn√ºpfen Sie die Top-Begriffe mit den Themen mit `left_join(top_terms, by = "topic")`.
        5.  √úberarbeiten Sie die Variable `topic` mit dem `mutate`-Befehl:
            1.  Erstellen Sie eine neue Variable `topic` mit `paste0("Topic ",sprintf("%02d", topic))`.
            2.  Ordnen Sie die Themen nach `gamma` mit `reorder(topic, gamma)`.
3.  Erstellung Sie eine Tabelle als Output
    -   basierend auf dem Datenastz `prevalence`,
        1.  Verwenden Sie `gt()` um eine Tabelle zu erstellen.
        2.  Formatieren Sie die Spalte `gamma` mit `fmt_number(columns = vars(gamma), decimals = 2)` um nur zwei Nachkommastellen anzuzeigen.
        3.  Verwenden Sie `gtExtras::gt_theme_538()` um das Design der Tabelle anzupassen.
4.  ‚úçÔ∏è Auf Basis des Outputs von `prevalence` Notieren Sie, welche Themen Sie als problematisch sehen und warum.

```{r exercise-1-3}
# Create tidy gamma matrix
td_gamma <- tidy(
  stm_mdl_k40, 
  matrix = "gamma", 
  document_names = names(quanteda_stm$documents)
  )

# Create prevalence
prevalence <- td_gamma %>%
  group_by(topic) %>%
  summarise(gamma = mean(gamma)) %>%
  arrange(desc(gamma)) %>%
  left_join(top_terms, by = "topic") %>%
  mutate(topic = paste0("Topic ",sprintf("%02d", topic)),
         topic = reorder(topic, gamma))

# Output
prevalence %>% 
  gt() %>% 
  fmt_number(
    columns = vars(gamma), 
    decimals = 2) %>% 
  gtExtras::gt_theme_538()
```

### üìã Exercise 2: Einfluss der Metadaten

#### 2.1. Sch√§tzung der Meta-Effekte

1.  Erstellen Sie einen neuen Datensatz `effects`:
    -   Verwenden Sie die Funktion `estimateEffect()`, um die Effekte zu sch√§tzen.
    -   Verwenden Sie f√ºr das `formular`-Argument `1:40 ~ publication_year_fct + field`, um die Effekte der Ver√∂ffentlichungsjahre und Fachbereiche zu sch√§tzen.
    -   Verwenden Sie `stm_mdl_40` als das zu analysierende Modell.
    -   Verwenden Sie `meta = quanteda_stm$meta`, um die Metadaten f√ºr die Sch√§tzung zu verwenden.

```{r exercise-2-1}
# Create data
effects <- estimateEffect(
    1:40 ~ publication_year_fct + field,
    stm_mdl_k40,
    meta = quanteda_stm$meta)
```

#### 2.2. Untersuchung der Effekte

1.  Erstellen Sie einen neuen Datensatz `effects_tidy` eine bereinigte Tabelle der Effekte:

    -   Basierend auf dem Datensatz `effects`

    1.  Verwenden Sie die `tidy()` Funktion, um die Effekte in ein aufbereitetes Format zu bringen.
    2.  Filtern Sie die Daten:
        1.  Entfernen Sie Zeilen, bei denen term den Wert `(Intercept)` hat.
        2.  Behalten Sie nur die Zeilen, bei denen `term == "fieldSocial Sciences"` ist.
    3.  Entfernen Sie die Spalte term mit `select(-term)`

2.  Erstellen Sie ein Tabelle zur √úberpr√ºfung der Effekte

    -   Basierend auf dem Datensatz `effects_tidy`:

    1.  Verwenden Sie die Funktion `gt()`, um eine Tabelle zu erstellen.
    2.  Formatieren Sie alle numerischen Variablen mit `fmt_number(columns = -c(topic), decimals = 3)`, um lediglich drei Dezimalstellen darzustellen.
    3.  Verwenden Sie `data_color(columns = estimate, method = "numeric", palette = "viridis")`, um die Sch√§tzwerte farblich zu kennzeichnen.
    4.  Wenden Sie das Design `gtExtras::gt_theme_538()` an.

3.  ‚úçÔ∏è Notieren Sie, welches Thema am st√§rksten im Forschungsfeld "Social Science" vertreten ist.

```{r exercise-2-2}
# Filter effect data
effects_tidy <- effects %>% 
  tidy() %>% 
  filter(
    term != "(Intercept)",
    term == "fieldSocial Sciences") %>% 
    select(-term)


# Explore effects (table outpu)
effects_tidy %>% 
    gt() %>% 
    fmt_number(
      columns = -c(topic),
      decimals = 3
    ) %>% 
    data_color(
       columns = estimate,
    method = "numeric",
    palette = "viridis"
  ) %>% 
  gtExtras::gt_theme_538()

#### Notes:
# 

```

### üìã Exercise 3: Einzelthema im Fokus

#### 3.1. Benennung des Themas k = 20

1.  Benennen Sie das Thema `k = 20` aus dem Modell `stm_mdl_40`:
    -   Verwenden Sie die Funktion `labelTopics()`.
    -   Geben Sie das Thema 20 als Parameter mit `topic = 20` an.
2.  ‚úçÔ∏è Notieren Sie die Themennamen. Begr√ºnden Sie kurz Ihre Entscheidung.

```{r exercise-3-1}
# Create topic label
stm_mdl_k40 %>% labelTopics(topic = 20)

# Themenname:
```

#### 3.2. Zusammenf√ºhrung mit OpenAlex-Daten

1.  Erstellen Sie einen neuen Datensatz `gamma_export`
    -   basierend auf dem Datensatz `stm_mdl_k40`:
        1.  Verwenden Sie `tidy()` um die Gamma-Matrix zu erstellen. Geben Sie `matrix = "gamma"` und `document_names = names(quanteda_stm$documents)` als Parameter an.
        2.  Gruppieren Sie die Dokumente nach `document` mit `group_by(document)`.
        3.  W√§hlen Sie die Dokumente mit dem h√∂chsten `gamma`-Wert mit `slice_max(gamma)`.
        4.  L√∂sen Sie die Gruppierung mit `dplyr::ungroup()`.
        5.  Verkn√ºpfen Sie die Daten mit `review_subsample` mittels `left_join(review_subsample, by = c("document" = "id"))`.
    -   Benennen Sie die Spalte `document` in `id` um mit `dplyr::rename(id = document)`
    -   Erstellen Sie eine neue Variable `stm_topic` mit Hilfe des `mutate()`-Befehls. Verwenden Sie `as.factor(paste("Topic", sprintf("%02d", topic)))` um die Themen zu benennen und als Faktor zu speichern.
2.  √úberpr√ºfen Sie Transformation mit Hilfe der `glimpse()`-Funktion, um sicherzustellen, dass die Daten korrekt erstellt wurden.

```{r exercise-3-2}
# Create gamma export
gamma_export <- stm_mdl_k40 %>% 
  tidytext::tidy(
    matrix = "gamma", 
    document_names = names(quanteda_stm$documents)) %>%
  dplyr::group_by(document) %>% 
  dplyr::slice_max(gamma) %>% 
  dplyr::ungroup() %>% 
  dplyr::left_join(review_subsample, by = c("document" = "id")) %>% 
  dplyr::rename(id = document) %>% 
  dplyr::mutate(
    stm_topic = as.factor(paste("Topic", sprintf("%02d", topic)))
  )

# Check
glimpse(gamma_export)
```

#### 3.3 Verteilungsparameter von Thema 20

1.  Erstellung eines Outputs zur √úberpr√ºfung der Lageparameter

    -   Basierend auf dem Datensatz `gamma_export`:
        1.  Filtern Sie die Daten nach `topic == 20`.
        2.  W√§hlen Sie mit Hilfe der select()-Funktion die Variablen `gamma`, `relevance_score` und `cited_by_count` aus.
        3.  Verwenden Sie die Funktion `datawizard::describe_distribution()` um die Verteilungsparameter zu berechnen.

2.  ‚úçÔ∏è Identifizieren und notieren Sie folgende Informationen:

    -   Wie viele Abstracts haben Thema 20 als Hauptthema?
    -   Wie hoch ist der durschnittliche Relevance Score?
    -   Wie viele Zitationen haben die Dokumente im Durchschnitt?
    -   Wie viel Zitate hat das hochzitierteste Dokument?

```{r exercise-3-3}
# Create distribution parameters
gamma_export %>% 
  filter(topic == 20) %>%
  select(gamma, relevance_score, cited_by_count) %>% 
  datawizard::describe_distribution()

#### Notes
# Anzahl der Abstrats von Thema 20
# Durchschnittlicher Relevace Score: 
# Durchschnittliche Zitationen:
# Anzahl der Zitationen des am meisten zitierten Dokuments:
```

#### 3.4. Top-Dokumente des Themas

1.  Identifizierung der Top-Dokumente
    -   Basierend auf dem Datensatz `gamma_export`:
        1.  Filtern Sie den Datensatz nach `stm_topic == "Topic 20"`.
        2.  Sortieren Sie die Daten absteigend nach `gamma` mit `arrange(-gamma)`.
        3.  W√§hlen Sie die Variablen `title`, `so`, `gamma`, `type`, und `ab` mit `select()` aus.
        4.  W√§hlen Sie die obersten 5 Zeilen mit `slice_head(n = 5)`.
2.  Erstellung eines Outputs zur √úberpr√ºfung der Top-Dokumente
    -   Basierend auf dem Datensatz `top_docs_k20`:
        1.  Verwenden Sie `gt()` um eine Tabelle zu erstellen.
        2.  Formatieren Sie die Spalte `gamma` mit `fmt_number(columns = vars(gamma), decimals = 2)` um nur zwei Nachkommastellen anzuzeigen.
        3.  Verwenden Sie `gtExtras::gt_theme_538()` um das Design der Tabelle anzupassen.\
3.  ‚úçÔ∏è Basierend auf den den Abstracts und den Titeln der Top-Dokumente:
    -   Welche Themenbereiche decken die Dokumente ab?
    -   W√ºrden Sie den im Abschnitt 3.1. gew√§hlten Themennamen beibehalten oder ab√§ndern?

```{r exercise-3-4}
# Identify top documents for topic 20
top_docs_k20 <- gamma_export %>% 
  filter(stm_topic == "Topic 20") %>%
  arrange(-gamma) %>%
  select(title, so, gamma, type, ab) %>%
  slice_head(n = 5) 

# Creae output
top_docs_k20 %>% 
  gt() %>% 
  fmt_number(
    columns = c(gamma), 
    decimals = 2) %>% 
  gtExtras::gt_theme_538()
```