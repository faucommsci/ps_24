[
  {
    "objectID": "computing/computing-textbooks.html",
    "href": "computing/computing-textbooks.html",
    "title": "R textbooks",
    "section": "",
    "text": "While there is no official R textbook for the course, there are a few to look at:\n\n🔗 R for Data Science, 2nd Edition\n🔗 Data Visualization: A Practical Introduction\n🔗 Tidy modeling with R\n🔗 Text Mining with R",
    "crumbs": [
      "Computing",
      "R Textbooks"
    ]
  },
  {
    "objectID": "computing/computing-useful_links.html",
    "href": "computing/computing-useful_links.html",
    "title": "Useful sources",
    "section": "",
    "text": "This is selection of useful R sources:\n\n Quarto tutorials by Andy Field\n Automatisierte Inhaltsanalyse mit R by Kornelius Puschmann",
    "crumbs": [
      "Computing",
      "Useful R sources"
    ]
  },
  {
    "objectID": "exercises/ms-showcase-07.html",
    "href": "exercises/ms-showcase-07.html",
    "title": "API mining and data wrangling with R",
    "section": "",
    "text": "Link to slides"
  },
  {
    "objectID": "exercises/ms-showcase-07.html#packages",
    "href": "exercises/ms-showcase-07.html#packages",
    "title": "API mining and data wrangling with R",
    "section": "Packages",
    "text": "Packages\n\nZum Laden der Pakete wird das Paket pacman::pload() genutzt, dass gegenüber der herkömmlichen Methode mit library() eine Reihe an Vorteile hat:\n\nPrägnante Syntax\nAutomatische Installation (wenn Paket noch nicht vorhanden)\nLaden mehrerer Pakete auf einmal\nAutomatische Suche nach dependencies\n\n\n\npacman::p_load(\n  here, qs, \n  magrittr, janitor,\n  easystats, sjmisc,\n  ggpubr, \n  openalexR, \n  tidyverse\n)"
  },
  {
    "objectID": "exercises/ms-showcase-07.html#codechunks-aus-der-sitzung",
    "href": "exercises/ms-showcase-07.html#codechunks-aus-der-sitzung",
    "title": "API mining and data wrangling with R",
    "section": "Codechunks aus der Sitzung",
    "text": "Codechunks aus der Sitzung\n\nDatenerhebung via API\n\n# Set openalexR.mailto option so that your requests go to the polite pool for faster response times\noptions(openalexR.mailto = \"christoph.adrian@fau.de\")\n\n\n# Download data via API\nreview_works &lt;- openalexR::oa_fetch(\n  entity = \"works\",\n  title.search = \"(literature OR systematic) AND review\",\n  primary_topic.domain.id = \"domains/2\", # Social Science\n  publication_year = \"2013 - 2023\",\n  verbose = TRUE\n)\n\n\n# Overview\nreview_works \n\n# A tibble: 93,655 × 39\n   id     title display_name author ab    publication_date relevance_score so   \n   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;        &lt;list&gt; &lt;chr&gt; &lt;chr&gt;                      &lt;dbl&gt; &lt;chr&gt;\n 1 https… The … The PRISMA … &lt;df&gt;   The … 2021-03-29                 1625. BMJ  \n 2 https… Pref… Preferred r… &lt;df&gt;   Syst… 2015-01-01                 1340. Syst…\n 3 https… Rayy… Rayyan—a we… &lt;df&gt;   Synt… 2016-12-01                 1314. Syst…\n 4 https… Syst… Systematic … &lt;df&gt;   Scop… 2018-11-19                  990. BMC …\n 5 https… Upda… Updated gui… &lt;df&gt;   On a… 2019-10-03                  963. Coch…\n 6 https… The … The WHO-5 W… &lt;df&gt;   The … 2015-01-01                  939. Psyc…\n 7 https… Pref… Preferred r… &lt;df&gt;   Prot… 2015-01-02                  914. BMJ  \n 8 https… Guid… Guidance on… &lt;df&gt;   Lite… 2017-08-28                  752. Jour…\n 9 https… Lite… Literature … &lt;df&gt;   Know… 2019-11-01                  705. Jour…\n10 https… The … The PRISMA … &lt;df&gt;   The … 2021-04-01                  653. Inte…\n# ℹ 93,645 more rows\n# ℹ 31 more variables: so_id &lt;chr&gt;, host_organization &lt;chr&gt;, issn_l &lt;chr&gt;,\n#   url &lt;chr&gt;, pdf_url &lt;chr&gt;, license &lt;chr&gt;, version &lt;chr&gt;, first_page &lt;chr&gt;,\n#   last_page &lt;chr&gt;, volume &lt;chr&gt;, issue &lt;chr&gt;, is_oa &lt;lgl&gt;,\n#   is_oa_anywhere &lt;lgl&gt;, oa_status &lt;chr&gt;, oa_url &lt;chr&gt;,\n#   any_repository_has_fulltext &lt;lgl&gt;, language &lt;chr&gt;, grants &lt;list&gt;,\n#   cited_by_count &lt;int&gt;, counts_by_year &lt;list&gt;, publication_year &lt;int&gt;, …\n\n\n\n\nInitiale Sichtung und Überprüfung der Datem\n\n\n\n\n\n\nTypische Bestandteile\n\n\n\n\nWie viele Fälle sind enthalten? Wie viele Variablen? Sind die Variablennamen aussagekräftig?\nWelchen Typ hat jede Variable, z. B. numerisch, kategorisch, logisch?\n\n\n\n\nreview_works %&gt;% glimpse()\n\nRows: 93,655\nColumns: 39\n$ id                          &lt;chr&gt; \"https://openalex.org/W3118615836\", \"https…\n$ title                       &lt;chr&gt; \"The PRISMA 2020 statement: an updated gui…\n$ display_name                &lt;chr&gt; \"The PRISMA 2020 statement: an updated gui…\n$ author                      &lt;list&gt; [&lt;data.frame[26 x 12]&gt;], [&lt;data.frame[8 x…\n$ ab                          &lt;chr&gt; \"The Preferred Reporting Items for Systema…\n$ publication_date            &lt;chr&gt; \"2021-03-29\", \"2015-01-01\", \"2016-12-01\", …\n$ relevance_score             &lt;dbl&gt; 1625.1708, 1340.1902, 1314.3904, 990.4521,…\n$ so                          &lt;chr&gt; \"BMJ\", \"Systematic reviews\", \"Systematic r…\n$ so_id                       &lt;chr&gt; \"https://openalex.org/S4393917726\", \"https…\n$ host_organization           &lt;chr&gt; NA, \"BioMed Central\", \"BioMed Central\", \"B…\n$ issn_l                      &lt;chr&gt; \"1756-1833\", \"2046-4053\", \"2046-4053\", \"14…\n$ url                         &lt;chr&gt; \"https://doi.org/10.1136/bmj.n71\", \"https:…\n$ pdf_url                     &lt;chr&gt; \"https://www.bmj.com/content/bmj/372/bmj.n…\n$ license                     &lt;chr&gt; \"cc-by\", \"cc-by\", \"cc-by\", \"cc-by\", NA, \"c…\n$ version                     &lt;chr&gt; \"publishedVersion\", \"publishedVersion\", \"p…\n$ first_page                  &lt;chr&gt; \"n71\", NA, NA, NA, NA, \"167\", \"g7647\", \"93…\n$ last_page                   &lt;chr&gt; \"n71\", NA, NA, NA, NA, \"176\", \"g7647\", \"11…\n$ volume                      &lt;chr&gt; NA, \"4\", \"5\", \"18\", NA, \"84\", \"349\", \"39\",…\n$ issue                       &lt;chr&gt; NA, \"1\", \"1\", \"1\", NA, \"3\", \"jan02 1\", \"1\"…\n$ is_oa                       &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, FALSE, TRUE, TRUE,…\n$ is_oa_anywhere              &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, …\n$ oa_status                   &lt;chr&gt; \"hybrid\", \"gold\", \"gold\", \"gold\", \"green\",…\n$ oa_url                      &lt;chr&gt; \"https://www.bmj.com/content/bmj/372/bmj.n…\n$ any_repository_has_fulltext &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE,…\n$ language                    &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", …\n$ grants                      &lt;list&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, &lt;\"htt…\n$ cited_by_count              &lt;int&gt; 30303, 17347, 10540, 5298, 5664, 2657, 909…\n$ counts_by_year              &lt;list&gt; [&lt;data.frame[5 x 2]&gt;], [&lt;data.frame[11 x …\n$ publication_year            &lt;int&gt; 2021, 2015, 2016, 2018, 2019, 2015, 2015, …\n$ cited_by_api_url            &lt;chr&gt; \"https://api.openalex.org/works?filter=cit…\n$ ids                         &lt;list&gt; &lt;\"https://openalex.org/W3118615836\", \"htt…\n$ doi                         &lt;chr&gt; \"https://doi.org/10.1136/bmj.n71\", \"https:…\n$ type                        &lt;chr&gt; \"article\", \"article\", \"article\", \"article\"…\n$ referenced_works            &lt;list&gt; &lt;\"https://openalex.org/W1528251861\", \"htt…\n$ related_works               &lt;list&gt; &lt;\"https://openalex.org/W4234875088\", \"htt…\n$ is_paratext                 &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ is_retracted                &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ concepts                    &lt;list&gt; [&lt;data.frame[20 x 5]&gt;], [&lt;data.frame[18 x…\n$ topics                      &lt;list&gt; [&lt;tbl_df[12 x 5]&gt;], [&lt;tbl_df[12 x 5]&gt;], […\n\n\n\n\nDatentransformationen\n\nKorrektur der Rohdaten\n\nreview_works_correct &lt;- review_works %&gt;% \n    mutate(\n        # Create additional factor variables\n        publication_year_fct = as.factor(publication_year), \n        type_fct = as.factor(type)\n        )\n\n\n\nUnnest topics\n\nreview_works_correct %&gt;% \n    unnest(topics, names_sep = \"_\") %&gt;%\n    glimpse()\n\nRows: 942,560\nColumns: 45\n$ id                          &lt;chr&gt; \"https://openalex.org/W3118615836\", \"https…\n$ title                       &lt;chr&gt; \"The PRISMA 2020 statement: an updated gui…\n$ display_name                &lt;chr&gt; \"The PRISMA 2020 statement: an updated gui…\n$ author                      &lt;list&gt; [&lt;data.frame[26 x 12]&gt;], [&lt;data.frame[26 …\n$ ab                          &lt;chr&gt; \"The Preferred Reporting Items for Systema…\n$ publication_date            &lt;chr&gt; \"2021-03-29\", \"2021-03-29\", \"2021-03-29\", …\n$ relevance_score             &lt;dbl&gt; 1625.171, 1625.171, 1625.171, 1625.171, 16…\n$ so                          &lt;chr&gt; \"BMJ\", \"BMJ\", \"BMJ\", \"BMJ\", \"BMJ\", \"BMJ\", …\n$ so_id                       &lt;chr&gt; \"https://openalex.org/S4393917726\", \"https…\n$ host_organization           &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ issn_l                      &lt;chr&gt; \"1756-1833\", \"1756-1833\", \"1756-1833\", \"17…\n$ url                         &lt;chr&gt; \"https://doi.org/10.1136/bmj.n71\", \"https:…\n$ pdf_url                     &lt;chr&gt; \"https://www.bmj.com/content/bmj/372/bmj.n…\n$ license                     &lt;chr&gt; \"cc-by\", \"cc-by\", \"cc-by\", \"cc-by\", \"cc-by…\n$ version                     &lt;chr&gt; \"publishedVersion\", \"publishedVersion\", \"p…\n$ first_page                  &lt;chr&gt; \"n71\", \"n71\", \"n71\", \"n71\", \"n71\", \"n71\", …\n$ last_page                   &lt;chr&gt; \"n71\", \"n71\", \"n71\", \"n71\", \"n71\", \"n71\", …\n$ volume                      &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ issue                       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ is_oa                       &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, …\n$ is_oa_anywhere              &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, …\n$ oa_status                   &lt;chr&gt; \"hybrid\", \"hybrid\", \"hybrid\", \"hybrid\", \"h…\n$ oa_url                      &lt;chr&gt; \"https://www.bmj.com/content/bmj/372/bmj.n…\n$ any_repository_has_fulltext &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, …\n$ language                    &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", …\n$ grants                      &lt;list&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ cited_by_count              &lt;int&gt; 30303, 30303, 30303, 30303, 30303, 30303, …\n$ counts_by_year              &lt;list&gt; [&lt;data.frame[5 x 2]&gt;], [&lt;data.frame[5 x 2…\n$ publication_year            &lt;int&gt; 2021, 2021, 2021, 2021, 2021, 2021, 2021, …\n$ cited_by_api_url            &lt;chr&gt; \"https://api.openalex.org/works?filter=cit…\n$ ids                         &lt;list&gt; &lt;\"https://openalex.org/W3118615836\", \"htt…\n$ doi                         &lt;chr&gt; \"https://doi.org/10.1136/bmj.n71\", \"https:…\n$ type                        &lt;chr&gt; \"article\", \"article\", \"article\", \"article\"…\n$ referenced_works            &lt;list&gt; &lt;\"https://openalex.org/W1528251861\", \"htt…\n$ related_works               &lt;list&gt; &lt;\"https://openalex.org/W4234875088\", \"htt…\n$ is_paratext                 &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ is_retracted                &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ concepts                    &lt;list&gt; [&lt;data.frame[20 x 5]&gt;], [&lt;data.frame[20 x…\n$ topics_i                    &lt;int&gt; 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 1, 1, …\n$ topics_score                &lt;dbl&gt; 0.9993, 0.9993, 0.9993, 0.9993, 0.9832, 0.…\n$ topics_name                 &lt;chr&gt; \"topic\", \"subfield\", \"field\", \"domain\", \"t…\n$ topics_id                   &lt;chr&gt; \"https://openalex.org/T10206\", \"https://op…\n$ topics_display_name         &lt;chr&gt; \"Methods for Evidence Synthesis in Researc…\n$ publication_year_fct        &lt;fct&gt; 2021, 2021, 2021, 2021, 2021, 2021, 2021, …\n$ type_fct                    &lt;fct&gt; article, article, article, article, articl…\n\n\n\n\n\nRekonstruktion OpexAlex Dashboard\n\nPublikationen im Zeitverlauf\n\nreview_works_correct %&gt;% \n    ggplot(aes(publication_year)) +\n    geom_bar() +\n    theme_pubr()\n\n\n\n\n\n\n\n\n\n\nHäufigkeit Forschungsfelder\n\nreview_works_correct %&gt;% \n    unnest(topics, names_sep = \"_\") %&gt;% \n    filter(topics_name == \"field\") %&gt;% \n    filter(topics_i == 1) %&gt;% \n    sjmisc::frq(topics_display_name, sort.frq = \"desc\")\n\ntopics_display_name &lt;character&gt; \n# total N=93655 valid N=93655 mean=4.41 sd=1.62\n\nValue                               |     N | Raw % | Valid % | Cum. %\n----------------------------------------------------------------------\nSocial Sciences                     | 30580 | 32.65 |   32.65 |  32.65\nPsychology                          | 29054 | 31.02 |   31.02 |  63.67\nBusiness, Management and Accounting | 15558 | 16.61 |   16.61 |  80.29\nDecision Sciences                   |  7261 |  7.75 |    7.75 |  88.04\nEconomics, Econometrics and Finance |  6796 |  7.26 |    7.26 |  95.30\nArts and Humanities                 |  4406 |  4.70 |    4.70 | 100.00\n&lt;NA&gt;                                |     0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\n\n\n\nRelevanteste Publikationen\n\nreview_works_correct %&gt;% \n    arrange(desc(relevance_score)) %&gt;%\n    select(publication_year_fct, relevance_score, title) %&gt;% \n    head(5) %&gt;% \n    gt::gt()\n\n\n\n\n\n\n\npublication_year_fct\nrelevance_score\ntitle\n\n\n\n\n2021\n1625.1708\nThe PRISMA 2020 statement: an updated guideline for reporting systematic reviews\n\n\n2015\n1340.1902\nPreferred reporting items for systematic review and meta-analysis protocols (PRISMA-P) 2015 statement\n\n\n2016\n1314.3904\nRayyan—a web and mobile app for systematic reviews\n\n\n2018\n990.4521\nSystematic review or scoping review? Guidance for authors when choosing between a systematic or scoping review approach\n\n\n2019\n962.6738\nUpdated guidance for trusted systematic reviews: a new edition of the Cochrane Handbook for Systematic Reviews of Interventions\n\n\n\n\n\n\n\n\n\nLageparameter\n\nreview_works_correct %&gt;% \n  select(where(is.numeric)) %&gt;% \n  datawizard::describe_distribution() %&gt;% \n  print_html()\n\n\n\n\n\n\n\nVariable\nMean\nSD\nIQR\nMin\nMax\nSkewness\nKurtosis\nn\nn_Missing\n\n\n\n\nrelevance_score\n31.73\n42.51\n36.48\n1.17\n1625.17\n4.75\n67.87\n93655\n0\n\n\ncited_by_count\n18.33\n146.36\n10.00\n0.00\n30303.00\n123.44\n22236.55\n93655\n0\n\n\npublication_year\n2019.40\n2.97\n5.00\n2013.00\n2023.00\n-0.58\n-0.77\n93655\n0"
  },
  {
    "objectID": "exercises/ms-exercise-09_solution.html",
    "href": "exercises/ms-exercise-09_solution.html",
    "title": "Unsupervised Machine Learning I",
    "section": "",
    "text": "Link to source file"
  },
  {
    "objectID": "exercises/ms-exercise-09_solution.html#background",
    "href": "exercises/ms-exercise-09_solution.html#background",
    "title": "Unsupervised Machine Learning I",
    "section": "Background",
    "text": "Background\n\n\n\n\n\n\nTodays’s data basis: OpenAlex\n\n\n\n\nVia API bzw. openalexR (Aria et al. 2024) gesammelte “works” der Datenbank OpenAlex mit Bezug zu Literaturriews in den Sozialwissenschaften zwischen 2013 und 2023\nDetaillierte Informationen und Ergebnisse zur Suchquery finden Sie hier."
  },
  {
    "objectID": "exercises/ms-exercise-09_solution.html#preparation",
    "href": "exercises/ms-exercise-09_solution.html#preparation",
    "title": "Unsupervised Machine Learning I",
    "section": "Preparation",
    "text": "Preparation\n\n\n\n\n\n\nWichtige Information\n\n\n\n\nBitte stellen Sie sicher, dass Sie das jeweilige R-Studio Projekt zur Übung geöffnet haben. Nur so funktionieren alle Dependencies korrekt.\nUm den einwandfreien Ablauf der Übung zu gewährleisten, wird für die Aufgaben auf eine eigenständige Datenerhebung verzichtet und ein Übungsdatensatz zu verfügung gestelt.\n\n\n\n\nPackages\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n    here, qs, # file management\n    magrittr, janitor, # data wrangling\n    easystats, sjmisc, # data analysis\n    gt, gtExtras, # table visualization\n    ggpubr, ggwordcloud, # visualization\n    # text analysis    \n    tidytext, widyr, # based on tidytext\n    quanteda, # based on quanteda\n    quanteda.textmodels, quanteda.textplots, quanteda.textstats, \n    openalexR, \n    tidyverse # load last to avoid masking issues\n  )\n\n\n\nImport und Vorverarbeitung der Daten\n\n# Import from local\nreview_works &lt;- qs::qread(here(\"data/session-07/openalex-review_works-2013_2023.qs\"))\n\n# Create correct data\nreview_works_correct &lt;- review_works %&gt;% \n    mutate(\n        # Create additional factor variables\n        publication_year_fct = as.factor(publication_year), \n        type_fct = as.factor(type)\n        )\n\n# Create subsample\nreview_subsample &lt;- review_works_correct %&gt;%\n    # Eingrenzung: Sprache und Typ\n    filter(language == \"en\") %&gt;% \n    filter(type == \"article\") %&gt;%\n    # Eingrenzung: Keine Einträge ohne Abstract\n    filter(!is.na(ab)) %&gt;% \n    # Datentranformation\n    unnest(topics, names_sep = \"_\") %&gt;%\n    filter(topics_name == \"field\" ) %&gt;% \n    filter(topics_i == \"1\") %&gt;% \n    # Eingrenzung: Forschungsfeldes\n    filter(\n    topics_display_name == \"Social Sciences\"|\n    topics_display_name == \"Psychology\"\n    ) %&gt;% \n    # Eingrenzung: Keine Einträge ohne Abstract\n    filter(!is.na(ab))   \n\n\n\nErstellung Korpus & DFM\n\n\nLösung anzeigen\n# Create corpus\nquanteda_corpus &lt;- review_subsample %&gt;% \n  quanteda::corpus(\n    docid_field = \"id\", \n    text_field = \"ab\"\n  )\n\n# Tokenize\nquanteda_token &lt;- quanteda_corpus %&gt;% \n  quanteda::tokens(\n    remove_punct = TRUE,\n    remove_symbols = TRUE, \n    remove_numbers = TRUE, \n    remove_url = TRUE, \n    split_tags = FALSE # keep hashtags and mentions\n  ) %&gt;% \n  quanteda::tokens_tolower() %&gt;% \n  quanteda::tokens_remove(\n    pattern = stopwords(\"en\")\n    )\n\n# Convert to Document-Feature-Matrix (DFM)\nquanteda_dfm &lt;- quanteda_token %&gt;% \n  quanteda::dfm()"
  },
  {
    "objectID": "exercises/ms-exercise-09_solution.html#praktische-anwendung",
    "href": "exercises/ms-exercise-09_solution.html#praktische-anwendung",
    "title": "Unsupervised Machine Learning I",
    "section": "🛠️ Praktische Anwendung",
    "text": "🛠️ Praktische Anwendung\n\n\n\n\n\n\nAchtung, bitte lesen!\n\n\n\n\nBevor Sie mit der Arbeit an den folgenden 📋 Exercises beginnen, stellen Sie bitte sicher, dass Sie alle Chunks des Abschnitts Preparation gerendert haben. Das können Sie tun, indem Sie den “Run all chunks above”-Knopf des nächsten Chunks benutzen.\nBei Fragen zum Code lohnt sich ein Blick in den Showcase (.qmd oder .html). Beim Showcase handelt es sich um eine kompakte Darstellung des in der Präsentation verwenden R-Codes. Sie können das Showcase also nutzen, um sich die Code-Bausteine anzusehen, die für die R-Outputs auf den Slides benutzt wurden.\n\n\n\n\n📋 Exercise 1: Cleaned DFM\n\n\n\n\n\n\nZiel der Aufgabe\n\n\n\n\n\n\n\nErstelen Sie einen neuen Datensatz quanteda_dfm_cleaned\n\nbasierend auf dem Datensatz quanteda_dfm\n\nVerwenden Sie quanteda::dfm_remove(pattern = c(\"systematic\", \"literature\", \"review\"), um die Suchquery zu entfernen.\nSpeichern Sie diese Umwandlung, indem Sie einen neuen Datensatz mit dem Namen quanteda_dfm_cleaned erstellen.\n\n\nÜberprüfen Sie die Transformation indem Sie quanteda_dfm_cleaned in die Konsole eingeben.\n✍️ Notieren Sie, wie viele Dokumente & Features in quanteda_dfm_cleaned enthalten sind.\n\n\n\nLösung anzeigen\n# `quanteda_dfm_cleaned` erstellen\nquanteda_dfm_cleaned &lt;- quanteda_dfm %&gt;% \n  quanteda::dfm_remove(pattern = c(\"systematic\", \"literature\", \"review\"))\n\n# Überprüfung\nquanteda_dfm_cleaned\n\n\nDocument-feature matrix of: 36,680 documents, 135,074 features (99.93% sparse) and 43 docvars.\n                                  features\ndocs                               5-item world health organization well-being\n  https://openalex.org/W4293003987      1     2      1            1          3\n  https://openalex.org/W2750168540      0     0      0            0          0\n  https://openalex.org/W1998933811      0     0      0            0          0\n  https://openalex.org/W2547134104      0     0      7            0          4\n  https://openalex.org/W3047898105      0     0      4            0          0\n  https://openalex.org/W2149640470      0     0      0            0          0\n                                  features\ndocs                               index who-5 among widely used\n  https://openalex.org/W4293003987     1    10     1      1    3\n  https://openalex.org/W2750168540     0     0     0      0    0\n  https://openalex.org/W1998933811     0     0     0      0    0\n  https://openalex.org/W2547134104     0     0     0      0    1\n  https://openalex.org/W3047898105     0     0     0      0    0\n  https://openalex.org/W2149640470     0     0     0      0    1\n[ reached max_ndoc ... 36,674 more documents, reached max_nfeat ... 135,064 more features ]\n\n\nLösung anzeigen\n# Notiz:\n# `quanteda_dfm_cleaned` enthält 36680 Dokumente und 135074 Features.\n\n\n\n\n📋 Exercise 2: Neues Netzwerk der Top-Begriffe\n\n\n\n\n\n\nZiel der Aufgabe\n\n\n\n\nErstellung eines neuen Datensatzes review_subsample_new, der sich auf englischsprachig Bücher bzw. Buchrartikel beschränkt.\n\n\n\n\nNeues Dataset top_features_quanteda erstellen\n\nBasierend auf dem Dataset quanteda_dfm_cleaned,\nVerwenden Sie quanteda::topfeatures(20), um die 20 häufigsten Begriffe zu extrahieren.\nVerwenden Sie names(), um nur die Namen (nicht die Werte) zu speichern.\nSpeichern Sie diese Transformation, indem Sie einen neuen Datensatz mit dem Namen top_features_quanteda erstellen.\n\nVisualisierung des Netzwerks an Top-Begriffen\n\nBasierend auf dem Dataset quanteda_dfm_cleaned,\nTransformieren Sie die Daten mit quanteda::fcm() in eine Feature-Co-Occurrence-Matrix [FCM].\nAuswahl relevanter Hashtags mit quanteda::fcm_select(pattern = top_hashtags_quanteda, case_insensitive = FALSE).\nVisualisierung mit quanteda.textplots::textplot_network().\n\nErgebnisse interpretieren und vergleichen\n\nAnalysieren Sie die Beziehungen zwischen den Top-Begriffen.\nVergleichen Sie die Ergebnisse mit den Auswertungen der Folien. Welche Unterschiede gibt es?\n\n\n\n\nLösung anzeigen\n# Create top features\ntop_features_quanteda &lt;- quanteda_dfm_cleaned %&gt;% \n  topfeatures(20) %&gt;% \n  names()\n\n# Construct feature-occurrence matrix of features\nquanteda_dfm_cleaned %&gt;% \n  fcm() %&gt;% \n  fcm_select(pattern = top_features_quanteda) %&gt;% \n  textplot_network(\n    edge_color = \"#C50F3C\"\n  )"
  },
  {
    "objectID": "exercises/ms-exercise-09.html",
    "href": "exercises/ms-exercise-09.html",
    "title": "Unsupervised Machine Learning I",
    "section": "",
    "text": "Link to source file"
  },
  {
    "objectID": "exercises/ms-exercise-09.html#background",
    "href": "exercises/ms-exercise-09.html#background",
    "title": "Unsupervised Machine Learning I",
    "section": "Background",
    "text": "Background\n\n\n\n\n\n\nTodays’s data basis: OpenAlex\n\n\n\n\nVia API bzw. openalexR (Aria et al. 2024) gesammelte “works” der Datenbank OpenAlex mit Bezug zu Literaturriews in den Sozialwissenschaften zwischen 2013 und 2023\nDetaillierte Informationen und Ergebnisse zur Suchquery finden Sie hier."
  },
  {
    "objectID": "exercises/ms-exercise-09.html#preparation",
    "href": "exercises/ms-exercise-09.html#preparation",
    "title": "Unsupervised Machine Learning I",
    "section": "Preparation",
    "text": "Preparation\n\n\n\n\n\n\nWichtige Information\n\n\n\n\nBitte stellen Sie sicher, dass Sie das jeweilige R-Studio Projekt zur Übung geöffnet haben. Nur so funktionieren alle Dependencies korrekt.\nUm den einwandfreien Ablauf der Übung zu gewährleisten, wird für die Aufgaben auf eine eigenständige Datenerhebung verzichtet und ein Übungsdatensatz zu verfügung gestelt.\n\n\n\n\nPackages\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n    here, qs, # file management\n    magrittr, janitor, # data wrangling\n    easystats, sjmisc, # data analysis\n    gt, gtExtras, # table visualization\n    ggpubr, ggwordcloud, # visualization\n    # text analysis    \n    tidytext, widyr, # based on tidytext\n    quanteda, # based on quanteda\n    quanteda.textmodels, quanteda.textplots, quanteda.textstats, \n    openalexR, \n    tidyverse # load last to avoid masking issues\n  )\n\n\n\nImport und Vorverarbeitung der Daten\n\n# Import from local\nreview_works &lt;- qs::qread(here(\"data/session-07/openalex-review_works-2013_2023.qs\"))\n\n# Create correct data\nreview_works_correct &lt;- review_works %&gt;% \n    mutate(\n        # Create additional factor variables\n        publication_year_fct = as.factor(publication_year), \n        type_fct = as.factor(type)\n        )\n\n# Create subsample\nreview_subsample &lt;- review_works_correct %&gt;%\n    # Eingrenzung: Sprache und Typ\n    filter(language == \"en\") %&gt;% \n    filter(type == \"article\") %&gt;%\n    # Eingrenzung: Keine Einträge ohne Abstract\n    filter(!is.na(ab)) %&gt;% \n    # Datentranformation\n    unnest(topics, names_sep = \"_\") %&gt;%\n    filter(topics_name == \"field\" ) %&gt;% \n    filter(topics_i == \"1\") %&gt;% \n    # Eingrenzung: Forschungsfeldes\n    filter(\n    topics_display_name == \"Social Sciences\"|\n    topics_display_name == \"Psychology\"\n    ) %&gt;% \n    # Eingrenzung: Keine Einträge ohne Abstract\n    filter(!is.na(ab))   \n\n\n\nErstellung Korpus & DFM\n\n\nLösung anzeigen\n# Create corpus\nquanteda_corpus &lt;- review_subsample %&gt;% \n  quanteda::corpus(\n    docid_field = \"id\", \n    text_field = \"ab\"\n  )\n\n# Tokenize\nquanteda_token &lt;- quanteda_corpus %&gt;% \n  quanteda::tokens(\n    remove_punct = TRUE,\n    remove_symbols = TRUE, \n    remove_numbers = TRUE, \n    remove_url = TRUE, \n    split_tags = FALSE # keep hashtags and mentions\n  ) %&gt;% \n  quanteda::tokens_tolower() %&gt;% \n  quanteda::tokens_remove(\n    pattern = stopwords(\"en\")\n    )\n\n# Convert to Document-Feature-Matrix (DFM)\nquanteda_dfm &lt;- quanteda_token %&gt;% \n  quanteda::dfm()"
  },
  {
    "objectID": "exercises/ms-exercise-09.html#praktische-anwendung",
    "href": "exercises/ms-exercise-09.html#praktische-anwendung",
    "title": "Unsupervised Machine Learning I",
    "section": "🛠️ Praktische Anwendung",
    "text": "🛠️ Praktische Anwendung\n\n\n\n\n\n\nAchtung, bitte lesen!\n\n\n\n\nBevor Sie mit der Arbeit an den folgenden 📋 Exercises beginnen, stellen Sie bitte sicher, dass Sie alle Chunks des Abschnitts Preparation gerendert haben. Das können Sie tun, indem Sie den “Run all chunks above”-Knopf des nächsten Chunks benutzen.\nBei Fragen zum Code lohnt sich ein Blick in den Showcase (.qmd oder .html). Beim Showcase handelt es sich um eine kompakte Darstellung des in der Präsentation verwenden R-Codes. Sie können das Showcase also nutzen, um sich die Code-Bausteine anzusehen, die für die R-Outputs auf den Slides benutzt wurden.\n\n\n\n\n📋 Exercise 1: Cleaned DFM\n\n\n\n\n\n\nZiel der Aufgabe\n\n\n\n\n\n\n\nErstelen Sie einen neuen Datensatz quanteda_dfm_cleaned\n\nbasierend auf dem Datensatz quanteda_dfm\n\nVerwenden Sie quanteda::dfm_remove(pattern = c(\"systematic\", \"literature\", \"review\"), um die Suchquery zu entfernen.\nSpeichern Sie diese Umwandlung, indem Sie einen neuen Datensatz mit dem Namen quanteda_dfm_cleaned erstellen.\n\n\nÜberprüfen Sie die Transformation indem Sie quanteda_dfm_cleaned in die Konsole eingeben.\n✍️ Notieren Sie, wie viele Dokumente & Features in quanteda_dfm_cleaned enthalten sind.\n\n\n\nLösung anzeigen\n# `quanteda_dfm_cleaned` erstellen\n\n# Überprüfung\n\n# Notiz:\n# `quanteda_dfm_cleaned` enthält 36680 Dokumente und 135074 Features.\n\n\n\n\n📋 Exercise 2: Neues Netzwerk der Top-Begriffe\n\n\n\n\n\n\nZiel der Aufgabe\n\n\n\n\nErstellung eines neuen Datensatzes review_subsample_new, der sich auf englischsprachig Bücher bzw. Buchrartikel beschränkt.\n\n\n\n\nNeues Dataset top_features_quanteda erstellen\n\nBasierend auf dem Dataset quanteda_dfm_cleaned,\nVerwenden Sie quanteda::topfeatures(20), um die 20 häufigsten Begriffe zu extrahieren.\nVerwenden Sie names(), um nur die Namen (nicht die Werte) zu speichern.\nSpeichern Sie diese Transformation, indem Sie einen neuen Datensatz mit dem Namen top_features_quanteda erstellen.\n\nVisualisierung des Netzwerks an Top-Begriffen\n\nBasierend auf dem Dataset quanteda_dfm_cleaned,\nTransformieren Sie die Daten mit quanteda::fcm() in eine Feature-Co-Occurrence-Matrix [FCM].\nAuswahl relevanter Hashtags mit quanteda::fcm_select(pattern = top_hashtags_quanteda, case_insensitive = FALSE).\nVisualisierung mit quanteda.textplots::textplot_network().\n\nErgebnisse interpretieren und vergleichen\n\nAnalysieren Sie die Beziehungen zwischen den Top-Begriffen.\nVergleichen Sie die Ergebnisse mit den Auswertungen der Folien. Welche Unterschiede gibt es?\n\n\n\n\nLösung anzeigen\n# Create top features\n\n# Visualisierung des Netzwerks an Top-Begriffen"
  },
  {
    "objectID": "exercises/ms-showcase-09.html",
    "href": "exercises/ms-showcase-09.html",
    "title": "Unsupervised Machine Learning I",
    "section": "",
    "text": "Link to slides"
  },
  {
    "objectID": "exercises/ms-showcase-09.html#preparation",
    "href": "exercises/ms-showcase-09.html#preparation",
    "title": "Unsupervised Machine Learning I",
    "section": "Preparation",
    "text": "Preparation\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n    here, qs, # file management\n    magrittr, janitor, # data wrangling\n    easystats, sjmisc, # data analysis\n    gt, gtExtras, # table visualization\n    ggpubr, ggwordcloud, # visualization\n    # text analysis    \n    tidytext, widyr, # based on tidytext\n    quanteda, # based on quanteda\n    quanteda.textmodels, quanteda.textplots, quanteda.textstats, \n    stm, # structural topic modeling\n    openalexR, \n    tidyverse # load last to avoid masking issues\n  )"
  },
  {
    "objectID": "exercises/ms-showcase-09.html#codechunks-aus-der-sitzung",
    "href": "exercises/ms-showcase-09.html#codechunks-aus-der-sitzung",
    "title": "Unsupervised Machine Learning I",
    "section": "Codechunks aus der Sitzung",
    "text": "Codechunks aus der Sitzung\n\nErstellung und Bearbeitung der Subsample\n\nreview_subsample &lt;- review_works_correct %&gt;% \n  # Eingrenzung: Sprache und Typ\n  filter(language == \"en\") %&gt;% \n  filter(type == \"article\") %&gt;%\n  # Datentranformation\n  unnest(topics, names_sep = \"_\") %&gt;%\n  filter(topics_name == \"field\") %&gt;% \n  filter(topics_i == \"1\") %&gt;% \n  # Eingrenzung: Forschungsfeldes\n  filter(\n   topics_display_name == \"Social Sciences\"|\n   topics_display_name == \"Psychology\"\n    )\n\n# Überblick \nreview_subsample %&gt;% glimpse\n\nRows: 45,221\nColumns: 45\n$ id                          &lt;chr&gt; \"https://openalex.org/W4293003987\", \"https…\n$ title                       &lt;chr&gt; \"The WHO-5 Well-Being Index: A Systematic …\n$ display_name                &lt;chr&gt; \"The WHO-5 Well-Being Index: A Systematic …\n$ author                      &lt;list&gt; [&lt;data.frame[4 x 12]&gt;], [&lt;data.frame[2 x …\n$ ab                          &lt;chr&gt; \"The 5-item World Health Organization Well…\n$ publication_date            &lt;chr&gt; \"2015-01-01\", \"2017-08-28\", \"2014-01-01\", …\n$ relevance_score             &lt;dbl&gt; 938.7603, 752.3500, 591.2553, 576.1210, 56…\n$ so                          &lt;chr&gt; \"Psychotherapy and psychosomatics\", \"Journ…\n$ so_id                       &lt;chr&gt; \"https://openalex.org/S184803288\", \"https:…\n$ host_organization           &lt;chr&gt; \"Karger Publishers\", \"SAGE Publishing\", NA…\n$ issn_l                      &lt;chr&gt; \"0033-3190\", \"0739-456X\", NA, \"2214-7829\",…\n$ url                         &lt;chr&gt; \"https://doi.org/10.1159/000376585\", \"http…\n$ pdf_url                     &lt;chr&gt; \"https://www.karger.com/Article/Pdf/376585…\n$ license                     &lt;chr&gt; \"cc-by-nc\", NA, NA, \"cc-by\", NA, NA, \"cc-b…\n$ version                     &lt;chr&gt; \"publishedVersion\", NA, \"publishedVersion\"…\n$ first_page                  &lt;chr&gt; \"167\", \"93\", NA, \"89\", \"55\", \"2150\", \"e356…\n$ last_page                   &lt;chr&gt; \"176\", \"112\", NA, \"106\", \"64\", \"2159\", \"e3…\n$ volume                      &lt;chr&gt; \"84\", \"39\", NA, \"6\", \"277\", \"32\", \"2\", \"24…\n$ issue                       &lt;chr&gt; \"3\", \"1\", NA, NA, NA, \"19\", \"8\", NA, \"9\", …\n$ is_oa                       &lt;lgl&gt; TRUE, FALSE, TRUE, TRUE, TRUE, FALSE, TRUE…\n$ is_oa_anywhere              &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE,…\n$ oa_status                   &lt;chr&gt; \"hybrid\", \"green\", \"bronze\", \"gold\", \"bron…\n$ oa_url                      &lt;chr&gt; \"https://www.karger.com/Article/Pdf/376585…\n$ any_repository_has_fulltext &lt;lgl&gt; FALSE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE…\n$ language                    &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", …\n$ grants                      &lt;list&gt; NA, NA, NA, &lt;\"https://openalex.org/F43203…\n$ cited_by_count              &lt;int&gt; 2657, 1375, 2568, 803, 3664, 1553, 2895, 9…\n$ counts_by_year              &lt;list&gt; [&lt;data.frame[11 x 2]&gt;], [&lt;data.frame[7 x …\n$ publication_year            &lt;int&gt; 2015, 2017, 2014, 2016, 2020, 2014, 2017, …\n$ cited_by_api_url            &lt;chr&gt; \"https://api.openalex.org/works?filter=cit…\n$ ids                         &lt;list&gt; &lt;\"https://openalex.org/W4293003987\", \"htt…\n$ doi                         &lt;chr&gt; \"https://doi.org/10.1159/000376585\", \"http…\n$ type                        &lt;chr&gt; \"article\", \"article\", \"article\", \"article\"…\n$ referenced_works            &lt;list&gt; &lt;\"https://openalex.org/W1492518593\", \"htt…\n$ related_works               &lt;list&gt; &lt;\"https://openalex.org/W3020194755\", \"htt…\n$ is_paratext                 &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ is_retracted                &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ concepts                    &lt;list&gt; [&lt;data.frame[7 x 5]&gt;], [&lt;data.frame[18 x …\n$ topics_i                    &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ topics_score                &lt;dbl&gt; 0.9926, 0.9050, 0.9995, 0.9987, 0.9999, 1.…\n$ topics_name                 &lt;chr&gt; \"field\", \"field\", \"field\", \"field\", \"field…\n$ topics_id                   &lt;chr&gt; \"https://openalex.org/fields/32\", \"https:/…\n$ topics_display_name         &lt;chr&gt; \"Psychology\", \"Social Sciences\", \"Psycholo…\n$ publication_year_fct        &lt;fct&gt; 2015, 2017, 2014, 2016, 2020, 2014, 2017, …\n$ type_fct                    &lt;fct&gt; article, article, article, article, articl…\n\n\n\n\nExkurs: Identfikation von fehlenden Werten\n\nvisdat::vis_miss(review_subsample, warn_large_data = FALSE)\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\n\n\nAnpassung der Subsample\n\nreview_subsample &lt;- review_works_correct %&gt;%\n  # Eingrenzung: Sprache und Typ\n  filter(language == \"en\") %&gt;% \n  filter(type == \"article\") %&gt;%\n  # Datentranformation\n  unnest(topics, names_sep = \"_\") %&gt;%\n  filter(topics_name == \"field\") %&gt;% \n  filter(topics_i == \"1\") %&gt;% \n  # Eingrenzung: Forschungsfeldes\n  filter(\n   topics_display_name == \"Social Sciences\"|\n   topics_display_name == \"Psychology\"\n   ) %&gt;% \n  # Eingrenzung: Keine Einträge ohne Abstract\n  filter(!is.na(ab))\n\n# Überblick \nreview_subsample %&gt;% glimpse\n\nRows: 36,680\nColumns: 45\n$ id                          &lt;chr&gt; \"https://openalex.org/W4293003987\", \"https…\n$ title                       &lt;chr&gt; \"The WHO-5 Well-Being Index: A Systematic …\n$ display_name                &lt;chr&gt; \"The WHO-5 Well-Being Index: A Systematic …\n$ author                      &lt;list&gt; [&lt;data.frame[4 x 12]&gt;], [&lt;data.frame[2 x …\n$ ab                          &lt;chr&gt; \"The 5-item World Health Organization Well…\n$ publication_date            &lt;chr&gt; \"2015-01-01\", \"2017-08-28\", \"2014-01-01\", …\n$ relevance_score             &lt;dbl&gt; 938.7603, 752.3500, 591.2553, 576.1210, 56…\n$ so                          &lt;chr&gt; \"Psychotherapy and psychosomatics\", \"Journ…\n$ so_id                       &lt;chr&gt; \"https://openalex.org/S184803288\", \"https:…\n$ host_organization           &lt;chr&gt; \"Karger Publishers\", \"SAGE Publishing\", NA…\n$ issn_l                      &lt;chr&gt; \"0033-3190\", \"0739-456X\", NA, \"2214-7829\",…\n$ url                         &lt;chr&gt; \"https://doi.org/10.1159/000376585\", \"http…\n$ pdf_url                     &lt;chr&gt; \"https://www.karger.com/Article/Pdf/376585…\n$ license                     &lt;chr&gt; \"cc-by-nc\", NA, NA, \"cc-by\", NA, NA, \"cc-b…\n$ version                     &lt;chr&gt; \"publishedVersion\", NA, \"publishedVersion\"…\n$ first_page                  &lt;chr&gt; \"167\", \"93\", NA, \"89\", \"55\", \"2150\", \"e356…\n$ last_page                   &lt;chr&gt; \"176\", \"112\", NA, \"106\", \"64\", \"2159\", \"e3…\n$ volume                      &lt;chr&gt; \"84\", \"39\", NA, \"6\", \"277\", \"32\", \"2\", \"24…\n$ issue                       &lt;chr&gt; \"3\", \"1\", NA, NA, NA, \"19\", \"8\", NA, \"9\", …\n$ is_oa                       &lt;lgl&gt; TRUE, FALSE, TRUE, TRUE, TRUE, FALSE, TRUE…\n$ is_oa_anywhere              &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE,…\n$ oa_status                   &lt;chr&gt; \"hybrid\", \"green\", \"bronze\", \"gold\", \"bron…\n$ oa_url                      &lt;chr&gt; \"https://www.karger.com/Article/Pdf/376585…\n$ any_repository_has_fulltext &lt;lgl&gt; FALSE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE…\n$ language                    &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", …\n$ grants                      &lt;list&gt; NA, NA, NA, &lt;\"https://openalex.org/F43203…\n$ cited_by_count              &lt;int&gt; 2657, 1375, 2568, 803, 3664, 1553, 2895, 9…\n$ counts_by_year              &lt;list&gt; [&lt;data.frame[11 x 2]&gt;], [&lt;data.frame[7 x …\n$ publication_year            &lt;int&gt; 2015, 2017, 2014, 2016, 2020, 2014, 2017, …\n$ cited_by_api_url            &lt;chr&gt; \"https://api.openalex.org/works?filter=cit…\n$ ids                         &lt;list&gt; &lt;\"https://openalex.org/W4293003987\", \"htt…\n$ doi                         &lt;chr&gt; \"https://doi.org/10.1159/000376585\", \"http…\n$ type                        &lt;chr&gt; \"article\", \"article\", \"article\", \"article\"…\n$ referenced_works            &lt;list&gt; &lt;\"https://openalex.org/W1492518593\", \"htt…\n$ related_works               &lt;list&gt; &lt;\"https://openalex.org/W3020194755\", \"htt…\n$ is_paratext                 &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ is_retracted                &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ concepts                    &lt;list&gt; [&lt;data.frame[7 x 5]&gt;], [&lt;data.frame[18 x …\n$ topics_i                    &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ topics_score                &lt;dbl&gt; 0.9926, 0.9050, 0.9995, 0.9987, 0.9999, 1.…\n$ topics_name                 &lt;chr&gt; \"field\", \"field\", \"field\", \"field\", \"field…\n$ topics_id                   &lt;chr&gt; \"https://openalex.org/fields/32\", \"https:/…\n$ topics_display_name         &lt;chr&gt; \"Psychology\", \"Social Sciences\", \"Psycholo…\n$ publication_year_fct        &lt;fct&gt; 2015, 2017, 2014, 2016, 2020, 2014, 2017, …\n$ type_fct                    &lt;fct&gt; article, article, article, article, articl…\n\n\n\n\nDTM/DFM Erstellung\n\n`tidytext``\n\n# Create tidy data\nsubsample_tidy &lt;- review_subsample %&gt;% \n    tidytext::unnest_tokens(\"text\", ab) %&gt;% \n    filter(!text %in% tidytext::stop_words$word)\n\n# Create summarized data\nsubsample_summarized &lt;- subsample_tidy %&gt;% \n  count(id, text) \n\n# Create DTM\nsubsample_dtm &lt;- subsample_summarized %&gt;% \n  cast_dtm(id, text, n)\n\n# Preview\nsubsample_dtm\n\n&lt;&lt;DocumentTermMatrix (documents: 36654, terms: 122147)&gt;&gt;\nNon-/sparse entries: 3280664/4473895474\nSparsity           : 100%\nMaximal term length: 188\nWeighting          : term frequency (tf)\n\n\n\n\nquanteda\n\n# Create corpus\nquanteda_corpus &lt;- review_subsample %&gt;% \n  quanteda::corpus(\n    docid_field = \"id\", \n    text_field = \"ab\"\n  )\n\n# Tokenize\nquanteda_token &lt;- quanteda_corpus %&gt;% \n  quanteda::tokens(\n    remove_punct = TRUE,\n    remove_symbols = TRUE, \n    remove_numbers = TRUE, \n    remove_url = TRUE, \n    split_tags = FALSE # keep hashtags and mentions\n  ) %&gt;% \n  quanteda::tokens_tolower() %&gt;% \n  quanteda::tokens_remove(\n    pattern = stopwords(\"en\")\n    )\n\n# Convert to Document-Feature-Matrix (DFM)\nquanteda_dfm &lt;- quanteda_token %&gt;% \n  quanteda::dfm()\n\n# Preview\nquanteda_dfm\n\n\n\n\nNetzwerk der Top-Begriffe\n\ntidytext\n\n# Extract most common hashtags\ntop50_features_tidy &lt;- subsample_tidy %&gt;% \n  count(text, sort = TRUE) %&gt;%\n  slice_head(n = 50) %&gt;% \n  pull(text)\n\n# Visualize\nsubsample_tidy %&gt;% \n  count(id, text, sort = TRUE) %&gt;% \n  filter(!is.na(text)) %&gt;% \n  cast_dfm(id, text, n) %&gt;% \n  quanteda::fcm() %&gt;% \n  quanteda::fcm_select(\n    pattern = top50_features_tidy,\n    case_insensitive = FALSE\n  ) %&gt;%  \n  quanteda.textplots::textplot_network(\n    edge_color = \"#04316A\"\n  )\n\n\n\n\n\n\n\n\n\n\nquanteda\n\n# Extract most common features \ntop50_features_quanteda &lt;- quanteda_dfm %&gt;% \n  topfeatures(50) %&gt;% \n  names()\n\n# Construct feature-occurrence matrix of features\nquanteda_dfm %&gt;% \n  fcm() %&gt;% \n  fcm_select(pattern = top50_features_quanteda) %&gt;% \n  textplot_network(\n    edge_color = \"#C50F3C\"\n  ) \n\n\n\n\n\n\n\n\n\n\n\nPreparation for STM\n\n# Pruning\nquanteda_dfm_trim &lt;- quanteda_dfm %&gt;% \n  dfm_trim( \n    min_docfreq = 10/nrow(review_subsample),\n    max_docfreq = 0.99, \n    docfreq_type = \"prop\")\n\n# Convert for stm topic modeling\nquanteda_stm &lt;- quanteda_dfm_trim %&gt;% \n   convert(to = \"stm\")\n\n\n\nStructural Topic Model\n\nSchätzung\n\ntictoc::tic()\nstm_mdl &lt;- stm::stm(\n  documents = quanteda_stm$documents,\n  vocab = quanteda_stm$vocab, \n  K = 20, \n  seed = 42,\n  max.em.its = 10,\n  init.type = \"Spectral\",\n  verbose = TRUE)\ntictoc::toc(log = TRUE)\n\n\n\nModelinformationen\n\n# Überblick über STM\nstm_mdl\n\nA topic model with 20 topics, 36650 documents and a 14322 word dictionary.\n\n\n\n\nÜberblick über die Themen\n\n# Simple\nplot(stm_mdl, type = \"summary\")\n\n\n\n\n\n\n\n\n\n# Komplex\ntop_gamma &lt;- stm_mdl %&gt;%\n  tidy(matrix = \"gamma\") %&gt;% \n  dplyr::group_by(topic) %&gt;%\n  dplyr::summarise(gamma = mean(gamma), .groups = \"drop\") %&gt;%\n  dplyr::arrange(desc(gamma))\n\ntop_beta &lt;- stm_mdl %&gt;%\n  tidytext::tidy(.) %&gt;% \n  dplyr::group_by(topic) %&gt;%\n  dplyr::arrange(-beta) %&gt;%\n  dplyr::top_n(10, wt = beta) %&gt;% \n  dplyr::select(topic, term) %&gt;%\n  dplyr::summarise(terms_beta = toString(term), .groups = \"drop\")\n\ntop_topics_terms &lt;- top_beta %&gt;% \n  dplyr::left_join(top_gamma, by = \"topic\") %&gt;%\n  dplyr::mutate(\n          topic = reorder(topic, gamma)\n      )\n\n# Preview\ntop_topics_terms %&gt;%\n  mutate(across(gamma, ~round(.,3))) %&gt;% \n  dplyr::arrange(-gamma) %&gt;% \n  gt() %&gt;% \n  cols_label(\n    topic = \"Topic\", \n    terms_beta = \"Top Terms (based on beta)\",\n    gamma = \"Gamma\"\n  ) %&gt;% \n  gtExtras::gt_theme_538()\n\n\n\n\n\n\n\nTopic\nTop Terms (based on beta)\nGamma\n\n\n\n\n16\nresearch, literature, review, paper, systematic, future, study, analysis, findings, knowledge\n0.142\n\n\n19\nstudies, interventions, review, systematic, evidence, outcomes, included, quality, intervention, health\n0.096\n\n\n13\nhealth, mental, care, review, support, family, children, social, factors, studies\n0.079\n\n\n9\nlearning, students, education, school, review, skills, educational, teachers, teaching, study\n0.072\n\n\n11\nstudy, literature, research, can, work, development, review, also, human, economic\n0.068\n\n\n15\nstudies, ci, effect, meta-analysis, depression, p, trials, anxiety, effects, interventions\n0.061\n\n\n14\nsocial, media, information, use, digital, data, technology, communication, review, research\n0.060\n\n\n17\nstudies, children, relationship, review, language, variables, research, factors, results, effects\n0.052\n\n\n6\nprevalence, studies, covid-19, suicide, among, risk, pandemic, countries, ci, vaccine\n0.050\n\n\n1\nsleep, studies, eating, cognitive, review, associated, weight, body, may, association\n0.044\n\n\n2\nhealth, studies, women, gender, review, care, social, services, cultural, access\n0.043\n\n\n18\nstudies, health, used, measures, review, tools, instruments, assessment, training, n\n0.039\n\n\n3\ntreatment, disorder, disorders, patients, ptsd, symptoms, clinical, studies, therapy, anxiety\n0.036\n\n\n7\narticles, review, adolescents, studies, literature, search, use, results, systematic, databases\n0.036\n\n\n4\npatients, articles, review, music, therapy, cancer, can, study, pain, life\n0.032\n\n\n12\nviolence, studies, use, sexual, risk, h3, ipv, substance, alcohol, review\n0.031\n\n\n5\net, al, university, review, gt, literature, lt, author, p, search\n0.030\n\n\n8\nphysical, training, studies, disability, exercise, disabilities, employment, ed, review, strength\n0.015\n\n\n20\nattachment, studies, scholar, google, science, review, social, styles, welfare, research\n0.009\n\n\n10\nde, la, y, en, los, e, 的, el, se, que\n0.004\n\n\n\n\n\n\n\n\n\nThemenkorrelation\n\nstm_corr &lt;- stm::topicCorr(stm_mdl)\nplot(stm_corr)\n\n\n\n\n\n\n\n\n\n\nFokus auf einzele Themen\n\nProminente Wörter\n\n# Fokus auf Themas 16\nstm::labelTopics(stm_mdl, topic=16)\n\nTopic 16 Top Words:\n     Highest Prob: research, literature, review, paper, systematic, future, study \n     FREX: tourism, sustainable, sustainability, originality, innovation, conceptual, agenda \n     Lift: paradigmatic, hrd, edlm, positivist, internationalisation, tccm, wom \n     Score: tourism, research, literature, paper, leadership, themes, sustainable \n\n# Fokus auf Thema 10\nstm::labelTopics(stm_mdl, topic=10)\n\nTopic 10 Top Words:\n     Highest Prob: de, la, y, en, los, e, 的 \n     FREX: resultados, foram, que, sobre, intervenciones, riesgo, uma \n     Lift: criterios, efecto, así, comparación, cumplieron, debido, depresión \n     Score: de, la, 的, en, los, y, que"
  },
  {
    "objectID": "sessions/ms-session-07.html",
    "href": "sessions/ms-session-07.html",
    "title": "Introduction to Text as Data",
    "section": "",
    "text": "🖥️ Slides\n📋 Showcase"
  },
  {
    "objectID": "sessions/ms-session-07.html#participate",
    "href": "sessions/ms-session-07.html#participate",
    "title": "Introduction to Text as Data",
    "section": "",
    "text": "🖥️ Slides\n📋 Showcase"
  },
  {
    "objectID": "sessions/ms-session-07.html#practice",
    "href": "sessions/ms-session-07.html#practice",
    "title": "Introduction to Text as Data",
    "section": "Practice",
    "text": "Practice\n✍️ Exercise"
  },
  {
    "objectID": "sessions/ms-session-07.html#suggested-readings",
    "href": "sessions/ms-session-07.html#suggested-readings",
    "title": "Introduction to Text as Data",
    "section": "Suggested readings",
    "text": "Suggested readings\n\nBenoit, K. (2020). Text as Data: An Overview (L. Curini & R. Franzese, Eds.). SAGE Publications Ltd. https://methods.sagepub.com/book/research-methods-in-political-science-and-international-relations\nGrimmer, J., Roberts, M. E., & Stewart, B. M. (2022). Text as data: A new framework for machine learning and the social sciences. Princeton University Press.\nGrimmer, J., & Stewart, B. M. (2013). Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts. Political Analysis, 21(3), 267–297. https://doi.org/f458q9\n\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "sessions/ms-session-09.html",
    "href": "sessions/ms-session-09.html",
    "title": "Text processing in R",
    "section": "",
    "text": "🖥️ Slides\n📋 Showcase"
  },
  {
    "objectID": "sessions/ms-session-09.html#participate",
    "href": "sessions/ms-session-09.html#participate",
    "title": "Text processing in R",
    "section": "",
    "text": "🖥️ Slides\n📋 Showcase"
  },
  {
    "objectID": "sessions/ms-session-09.html#practice",
    "href": "sessions/ms-session-09.html#practice",
    "title": "Text processing in R",
    "section": "Practice",
    "text": "Practice\n✍️ Exercise"
  },
  {
    "objectID": "sessions/ms-session-09.html#suggested-readings",
    "href": "sessions/ms-session-09.html#suggested-readings",
    "title": "Text processing in R",
    "section": "Suggested readings",
    "text": "Suggested readings\n\nTopic Modeling\n\nChang, J., Boyd-Graber, J., Gerrish, S., Wang, C., & Blei, D. (2009). Reading tea leaves: How humans interpret topic models. 32, 288–296.\nChen, Y., Peng, Z., Kim, S.-H., & Choi, C. W. (2023). What We Can Do and Cannot Do with Topic Modeling: A Systematic Review. Communication Methods and Measures, 17(2), 111–130. https://doi.org/10.1080/19312458.2023.2167965\nEgger, R., & Yu, J. (2022). A topic modeling comparison between LDA, NMF, Top2Vec, and BERTopic to demystify twitter posts. Frontiers in Sociology, 7, 886498. https://doi.org/10.3389/fsoc.2022.886498\nFriemel, T. N. (2017). Social Network Analysis (J. Matthes, C. S. Davis, & R. F. Potter, Eds.; 1st ed., pp. 1–14). Wiley. https://onlinelibrary.wiley.com/doi/10.1002/9781118901731.iecrm0235\nHase, V. (2023). Automated Content Analysis (F. Oehmer-Pedrazzi, S. H. Kessler, E. Humprecht, K. Sommer, & L. Castro, Eds.; pp. 23–36). Springer Fachmedien Wiesbaden. https://link.springer.com/10.1007/978-3-658-36179-2_3\nHimelboim, I. (2017). Social Network Analysis (Social Media) (J. Matthes, C. S. Davis, & R. F. Potter, Eds.; 1st ed., pp. 1–15). Wiley. https://onlinelibrary.wiley.com/doi/10.1002/9781118901731.iecrm0236\nMaier, D., Waldherr, A., Miltner, P., Wiedemann, G., Niekler, A., Keinert, A., Pfetsch, B., Heyer, G., Reber, U., Häussler, T., Schmid-Petri, H., & Adam, S. (2018). Applying LDA Topic Modeling in Communication Research: Toward a Valid and Reliable Methodology. Communication Methods and Measures, 12(2-3), 93–118. https://doi.org/10.1080/19312458.2018.1430754\n\n\n\nTopic modeling in R\n\nAtteveldt, W. van, Trilling, D., & Arcíla, C. (2021). Computational analysis of communication: A practical introduction to the analysis of texts, networks, and images with code examples in python and r. John Wiley & Sons.\nSilge, J., & Hvitfeldt, E. (n.d.). Supervised machine learning for text analysis in r. https://smltar.com/\nSilge, J., & Robinson, D. (2017). Text mining with r: A tidy approach (First edition). O’Reilly.\nWelbers, K., Van Atteveldt, W., & Benoit, K. (2017). Text Analysis in R. Communication Methods and Measures, 11(4), 245–265. https://doi.org/10.1080/19312458.2017.1387238"
  },
  {
    "objectID": "sessions/ms-session-09.html#useful-resources",
    "href": "sessions/ms-session-09.html#useful-resources",
    "title": "Text processing in R",
    "section": "Useful resources",
    "text": "Useful resources\n\nTutorials des CCS-Amsterdam zu “quanteda-based” Textanalyse:\n\n📖 Text analysis (including 🎥 video tutorial)\n📖 Lexical sentiment analysis (including 🎥 video tutorial)\n📖 LDA Topic Modeling (including 🎥 video tutorial)\n📖 Structural Topic Modeling (including 🎥 video tutorial)\n\n\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "slides/ms-slides-09.html#seminarplan",
    "href": "slides/ms-slides-09.html#seminarplan",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Seminarplan",
    "text": "Seminarplan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSitzung\nDatum\nThema (synchron)\nÜbung (asynchron)\nDozent:in\n\n\n\n\n1\n18.04.2024\nEinführung & Überblick\n\nAM & CA\n\n\n\n📚\nTeil 1: Systematic Review\n\n\n\n\n2\n25.04.2024\nEinführung in Systematic Reviews I\nR-Einführung\nAM\n\n\n3\n02.05.2024\nEinführung in Systematic Reviews II\nR-Einführung\nAM\n\n\n\n09.05.2024\n🏖️ Feiertag\nR-Einführung\n\n\n\n4\n16.05.2024\nAutomatisierung von SRs & KI-Tools\nR-Einführung\nAM\n\n\n\n23.05.2024\n🍻 WiSo-Projekt-Woche\nR-Einführung\n\n\n\n5\n04.06.2024\n🍕 Gastvortrag: Prof. Dr. Emese Domahidi\nR-Einführung\nED\n\n\n6\n06.06.2024\nAutomatisierung von SRs & KI-Tools\nR-Einführung\nAM\n\n\n\n💻\nTeil 2: Text as Data & Unsupervised Machine Learning\n\n\n\n\n7\n13.06.2024\nIntroduction to Text as Data\nzur Sitzung\nCA\n\n\n8\n20.06.2024\nText processing\nzur Sitzung\nCA\n\n\n9\n27.06.2024\nUnsupervised Machine Learning I\nzur Sitzung\nCA\n\n\n10\n04.07.2024\nUnsupervised Machine Learning II\nzur Sitzung\nCA & AM\n\n\n11\n11.07.2024\nRecap & Ausblick\nzur Sitzung\nCA & AM\n\n\n12\n18.07.2024\n🏁 Semesterabschluss\nzur Sitzung\nCA & AM"
  },
  {
    "objectID": "slides/ms-slides-09.html#besprechung-der-r-übung",
    "href": "slides/ms-slides-09.html#besprechung-der-r-übung",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Besprechung der R-Übung",
    "text": "Besprechung der R-Übung\nSollten wir die Daten weiter eingrenzen?\n\n\n\nBitte scannt den QR-Code oder nutzt den folgenden Link für die Teilnahme an einer kurzen Umfrage:\n\nhttps://www.menti.com/als6ys2e2y69\nTemporary Access Code: 2250 1954\n\n\n\n\n \n\n    \n\n\n\n\n\n−+\n02:00"
  },
  {
    "objectID": "slides/ms-slides-09.html#ergebnis",
    "href": "slides/ms-slides-09.html#ergebnis",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Ergebnis",
    "text": "Ergebnis"
  },
  {
    "objectID": "slides/ms-slides-09.html#quick-reminder-die-tidytext-pipeline",
    "href": "slides/ms-slides-09.html#quick-reminder-die-tidytext-pipeline",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Quick reminder: Die tidytext Pipeline",
    "text": "Quick reminder: Die tidytext Pipeline\nFokus auf einzelne Wörter, deren Beziehungen zueinander und Sentiments\n\n\n\n\n\n\n\n\n\n\n\n\n\nSilge & Robinson (2017)"
  },
  {
    "objectID": "slides/ms-slides-09.html#expansion-der-pipeline",
    "href": "slides/ms-slides-09.html#expansion-der-pipeline",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Expansion der Pipeline",
    "text": "Expansion der Pipeline\nFokus auf die Modelierung der Beziehung zwischen Wörtern & Dokumenten\n\n\n\n\n\n\n\n\n\n\n\n\n\nSilge & Robinson (2017)"
  },
  {
    "objectID": "slides/ms-slides-09.html#possibilities-over-possibilities",
    "href": "slides/ms-slides-09.html#possibilities-over-possibilities",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Possibilities over possibilities",
    "text": "Possibilities over possibilities\nÜberblick über verschiedene Methoden der Textanalyse (Grimmer & Stewart, 2013)"
  },
  {
    "objectID": "slides/ms-slides-09.html#promises-pitfalls",
    "href": "slides/ms-slides-09.html#promises-pitfalls",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Promises & pitfalls",
    "text": "Promises & pitfalls\nVier Grundsätze der quantitativen Textanalyse (Grimmer & Stewart, 2013)\n1️⃣ All quantitative models of language are wrong — but some are useful.\n2️⃣ Quantitative methods for text amplify resources and augment humans.\n3️⃣ There is no globally best method for automated text analysis.\n4️⃣ Validate, Validate, Validate!"
  },
  {
    "objectID": "slides/ms-slides-09.html#verteilung-von-wörtern-auf-themen-auf-dokumente",
    "href": "slides/ms-slides-09.html#verteilung-von-wörtern-auf-themen-auf-dokumente",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Verteilung von Wörtern auf Themen auf Dokumente",
    "text": "Verteilung von Wörtern auf Themen auf Dokumente\nDie Grundidee des (LDA) Topic Modeling\n\n(Blei, 2012)\nEach topic is a distribution of words\nEach document is a mixture of corpus-wide topics\nEach words is drawn from one of those topics"
  },
  {
    "objectID": "slides/ms-slides-09.html#in-a-nutshell",
    "href": "slides/ms-slides-09.html#in-a-nutshell",
    "title": "Unsupervised Machine Learning (I)",
    "section": "In a nutshell",
    "text": "In a nutshell\nGrundlagen des Topic Modeling kurz zusammengefasst\n\nVerfahren des unüberwachten maschinellen Lernens, das sich daher insbesondere zur Exploration und Deskription großer Textmengen eignet\nThemen werden strikt auf Basis von Worthäufigkeiten in den einzelnen Dokumenten vermeintlich objektiv berechnet, ganz ohne subjektive Einschätzungen und damit einhergehenden etwaigen Verzerrungen\nBekanntesten dieser Verfahren sind LDA (Latent Dirichlet Allocation) sowie die darauf aufbauenden CTM (Correlated Topic Models) und STM (Structural Topic Models)"
  },
  {
    "objectID": "slides/ms-slides-09.html#wenn-missing-values-zum-problem-werden",
    "href": "slides/ms-slides-09.html#wenn-missing-values-zum-problem-werden",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Wenn Missing Values zum Problem werden",
    "text": "Wenn Missing Values zum Problem werden\nExkurs zu Überprüfung der Daten auf fehlende Werte\n\nvisdat::vis_miss(review_subsample, warn_large_data = FALSE)"
  },
  {
    "objectID": "slides/ms-slides-09.html#bereinigung-der-subsample",
    "href": "slides/ms-slides-09.html#bereinigung-der-subsample",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Bereinigung der Subsample",
    "text": "Bereinigung der Subsample\nAusschluss von Referenzen mit fehlendem Abstract\n\n\n\n# Create subsample\nreview_subsample &lt;- review_works_correct %&gt;% \n  # Eingrenzung: Sprache und Typ\n  filter(language == \"en\") %&gt;% \n  filter(type == \"article\") %&gt;%\n  # Datentranformation\n  unnest(topics, names_sep = \"_\") %&gt;%\n  filter(topics_name == \"field\") %&gt;% \n  filter(topics_i == \"1\") %&gt;% \n  # Eingrenzung: Forschungsfeldes\n  filter(\n   topics_display_name == \"Social Sciences\"|\n   topics_display_name == \"Psychology\"\n    )\n\n# Overview\nreview_subsample %&gt;% glimpse  \n\n\nRows: 45,221\nColumns: 45\n$ id                          &lt;chr&gt; \"https://openalex.org/W4293003987\", \"https…\n$ title                       &lt;chr&gt; \"The WHO-5 Well-Being Index: A Systematic …\n$ display_name                &lt;chr&gt; \"The WHO-5 Well-Being Index: A Systematic …\n$ author                      &lt;list&gt; [&lt;data.frame[4 x 12]&gt;], [&lt;data.frame[2 x …\n$ ab                          &lt;chr&gt; \"The 5-item World Health Organization Well…\n$ publication_date            &lt;chr&gt; \"2015-01-01\", \"2017-08-28\", \"2014-01-01\", …\n$ relevance_score             &lt;dbl&gt; 938.7603, 752.3500, 591.2553, 576.1210, 56…\n$ so                          &lt;chr&gt; \"Psychotherapy and psychosomatics\", \"Journ…\n$ so_id                       &lt;chr&gt; \"https://openalex.org/S184803288\", \"https:…\n$ host_organization           &lt;chr&gt; \"Karger Publishers\", \"SAGE Publishing\", NA…\n$ issn_l                      &lt;chr&gt; \"0033-3190\", \"0739-456X\", NA, \"2214-7829\",…\n$ url                         &lt;chr&gt; \"https://doi.org/10.1159/000376585\", \"http…\n$ pdf_url                     &lt;chr&gt; \"https://www.karger.com/Article/Pdf/376585…\n$ license                     &lt;chr&gt; \"cc-by-nc\", NA, NA, \"cc-by\", NA, NA, \"cc-b…\n$ version                     &lt;chr&gt; \"publishedVersion\", NA, \"publishedVersion\"…\n$ first_page                  &lt;chr&gt; \"167\", \"93\", NA, \"89\", \"55\", \"2150\", \"e356…\n$ last_page                   &lt;chr&gt; \"176\", \"112\", NA, \"106\", \"64\", \"2159\", \"e3…\n$ volume                      &lt;chr&gt; \"84\", \"39\", NA, \"6\", \"277\", \"32\", \"2\", \"24…\n$ issue                       &lt;chr&gt; \"3\", \"1\", NA, NA, NA, \"19\", \"8\", NA, \"9\", …\n$ is_oa                       &lt;lgl&gt; TRUE, FALSE, TRUE, TRUE, TRUE, FALSE, TRUE…\n$ is_oa_anywhere              &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE,…\n$ oa_status                   &lt;chr&gt; \"hybrid\", \"green\", \"bronze\", \"gold\", \"bron…\n$ oa_url                      &lt;chr&gt; \"https://www.karger.com/Article/Pdf/376585…\n$ any_repository_has_fulltext &lt;lgl&gt; FALSE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE…\n$ language                    &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", …\n$ grants                      &lt;list&gt; NA, NA, NA, &lt;\"https://openalex.org/F43203…\n$ cited_by_count              &lt;int&gt; 2657, 1375, 2568, 803, 3664, 1553, 2895, 9…\n$ counts_by_year              &lt;list&gt; [&lt;data.frame[11 x 2]&gt;], [&lt;data.frame[7 x …\n$ publication_year            &lt;int&gt; 2015, 2017, 2014, 2016, 2020, 2014, 2017, …\n$ cited_by_api_url            &lt;chr&gt; \"https://api.openalex.org/works?filter=cit…\n$ ids                         &lt;list&gt; &lt;\"https://openalex.org/W4293003987\", \"htt…\n$ doi                         &lt;chr&gt; \"https://doi.org/10.1159/000376585\", \"http…\n$ type                        &lt;chr&gt; \"article\", \"article\", \"article\", \"article\"…\n$ referenced_works            &lt;list&gt; &lt;\"https://openalex.org/W1492518593\", \"htt…\n$ related_works               &lt;list&gt; &lt;\"https://openalex.org/W3020194755\", \"htt…\n$ is_paratext                 &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ is_retracted                &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ concepts                    &lt;list&gt; [&lt;data.frame[7 x 5]&gt;], [&lt;data.frame[18 x …\n$ topics_i                    &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ topics_score                &lt;dbl&gt; 0.9926, 0.9050, 0.9995, 0.9987, 0.9999, 1.…\n$ topics_name                 &lt;chr&gt; \"field\", \"field\", \"field\", \"field\", \"field…\n$ topics_id                   &lt;chr&gt; \"https://openalex.org/fields/32\", \"https:/…\n$ topics_display_name         &lt;chr&gt; \"Psychology\", \"Social Sciences\", \"Psycholo…\n$ publication_year_fct        &lt;fct&gt; 2015, 2017, 2014, 2016, 2020, 2014, 2017, …\n$ type_fct                    &lt;fct&gt; article, article, article, article, articl…"
  },
  {
    "objectID": "slides/ms-slides-09.html#bereinigung-der-subsample-1",
    "href": "slides/ms-slides-09.html#bereinigung-der-subsample-1",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Bereinigung der Subsample",
    "text": "Bereinigung der Subsample\nAusschluss von Referenzen mit fehlendem Abstract\n\n\n\n# Create subsample\nreview_subsample &lt;- review_works_correct %&gt;% \n  # Eingrenzung: Sprache und Typ\n  filter(language == \"en\") %&gt;% \n  filter(type == \"article\") %&gt;%\n  # Datentranformation\n  unnest(topics, names_sep = \"_\") %&gt;%\n  filter(topics_name == \"field\") %&gt;% \n  filter(topics_i == \"1\") %&gt;% \n  # Eingrenzung: Forschungsfeldes\n  filter(\n   topics_display_name == \"Social Sciences\"|\n   topics_display_name == \"Psychology\"\n    ) %&gt;% \n  # Eingrenzung: Keine Einträge ohne Abstract\n  filter(!is.na(ab))\n\n# Overview\nreview_subsample %&gt;% glimpse  \n\n\nRows: 36,680\nColumns: 45\n$ id                          &lt;chr&gt; \"https://openalex.org/W4293003987\", \"https…\n$ title                       &lt;chr&gt; \"The WHO-5 Well-Being Index: A Systematic …\n$ display_name                &lt;chr&gt; \"The WHO-5 Well-Being Index: A Systematic …\n$ author                      &lt;list&gt; [&lt;data.frame[4 x 12]&gt;], [&lt;data.frame[2 x …\n$ ab                          &lt;chr&gt; \"The 5-item World Health Organization Well…\n$ publication_date            &lt;chr&gt; \"2015-01-01\", \"2017-08-28\", \"2014-01-01\", …\n$ relevance_score             &lt;dbl&gt; 938.7603, 752.3500, 591.2553, 576.1210, 56…\n$ so                          &lt;chr&gt; \"Psychotherapy and psychosomatics\", \"Journ…\n$ so_id                       &lt;chr&gt; \"https://openalex.org/S184803288\", \"https:…\n$ host_organization           &lt;chr&gt; \"Karger Publishers\", \"SAGE Publishing\", NA…\n$ issn_l                      &lt;chr&gt; \"0033-3190\", \"0739-456X\", NA, \"2214-7829\",…\n$ url                         &lt;chr&gt; \"https://doi.org/10.1159/000376585\", \"http…\n$ pdf_url                     &lt;chr&gt; \"https://www.karger.com/Article/Pdf/376585…\n$ license                     &lt;chr&gt; \"cc-by-nc\", NA, NA, \"cc-by\", NA, NA, \"cc-b…\n$ version                     &lt;chr&gt; \"publishedVersion\", NA, \"publishedVersion\"…\n$ first_page                  &lt;chr&gt; \"167\", \"93\", NA, \"89\", \"55\", \"2150\", \"e356…\n$ last_page                   &lt;chr&gt; \"176\", \"112\", NA, \"106\", \"64\", \"2159\", \"e3…\n$ volume                      &lt;chr&gt; \"84\", \"39\", NA, \"6\", \"277\", \"32\", \"2\", \"24…\n$ issue                       &lt;chr&gt; \"3\", \"1\", NA, NA, NA, \"19\", \"8\", NA, \"9\", …\n$ is_oa                       &lt;lgl&gt; TRUE, FALSE, TRUE, TRUE, TRUE, FALSE, TRUE…\n$ is_oa_anywhere              &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE,…\n$ oa_status                   &lt;chr&gt; \"hybrid\", \"green\", \"bronze\", \"gold\", \"bron…\n$ oa_url                      &lt;chr&gt; \"https://www.karger.com/Article/Pdf/376585…\n$ any_repository_has_fulltext &lt;lgl&gt; FALSE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE…\n$ language                    &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", …\n$ grants                      &lt;list&gt; NA, NA, NA, &lt;\"https://openalex.org/F43203…\n$ cited_by_count              &lt;int&gt; 2657, 1375, 2568, 803, 3664, 1553, 2895, 9…\n$ counts_by_year              &lt;list&gt; [&lt;data.frame[11 x 2]&gt;], [&lt;data.frame[7 x …\n$ publication_year            &lt;int&gt; 2015, 2017, 2014, 2016, 2020, 2014, 2017, …\n$ cited_by_api_url            &lt;chr&gt; \"https://api.openalex.org/works?filter=cit…\n$ ids                         &lt;list&gt; &lt;\"https://openalex.org/W4293003987\", \"htt…\n$ doi                         &lt;chr&gt; \"https://doi.org/10.1159/000376585\", \"http…\n$ type                        &lt;chr&gt; \"article\", \"article\", \"article\", \"article\"…\n$ referenced_works            &lt;list&gt; &lt;\"https://openalex.org/W1492518593\", \"htt…\n$ related_works               &lt;list&gt; &lt;\"https://openalex.org/W3020194755\", \"htt…\n$ is_paratext                 &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ is_retracted                &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ concepts                    &lt;list&gt; [&lt;data.frame[7 x 5]&gt;], [&lt;data.frame[18 x …\n$ topics_i                    &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ topics_score                &lt;dbl&gt; 0.9926, 0.9050, 0.9995, 0.9987, 0.9999, 1.…\n$ topics_name                 &lt;chr&gt; \"field\", \"field\", \"field\", \"field\", \"field…\n$ topics_id                   &lt;chr&gt; \"https://openalex.org/fields/32\", \"https:/…\n$ topics_display_name         &lt;chr&gt; \"Psychology\", \"Social Sciences\", \"Psycholo…\n$ publication_year_fct        &lt;fct&gt; 2015, 2017, 2014, 2016, 2020, 2014, 2017, …\n$ type_fct                    &lt;fct&gt; article, article, article, article, articl…"
  },
  {
    "objectID": "slides/ms-slides-09.html#aus-text-werden-zahlen",
    "href": "slides/ms-slides-09.html#aus-text-werden-zahlen",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Aus Text werden Zahlen",
    "text": "Aus Text werden Zahlen\nDocument-Term-Matrix [DTM] im Fokus\n\n\nSilge & Robinson (2017)"
  },
  {
    "objectID": "slides/ms-slides-09.html#kurzer-rückblick-auf-die-document-term-matrix-dtm",
    "href": "slides/ms-slides-09.html#kurzer-rückblick-auf-die-document-term-matrix-dtm",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Kurzer Rückblick auf die Document-Term Matrix [DTM]",
    "text": "Kurzer Rückblick auf die Document-Term Matrix [DTM]\nHäufig verwendete Datenstruktur für (klassische) Textanalyse\n\n\nEine Matrix, bei der:\n\njede Zeile steht für ein Dokument (z.B. ein Abstract),\njede Spalte einen Begriff darstellt, und\njeder Wert (in der Regel) die Häufigkeit des Begriffs in einem Dokument enthält.\n\n\n\n\n\n\n\n\n(Zheng & Casari, 2018)"
  },
  {
    "objectID": "slides/ms-slides-09.html#schritt-für-schritt-zur-dtm",
    "href": "slides/ms-slides-09.html#schritt-für-schritt-zur-dtm",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Schritt für Schritt zur DTM",
    "text": "Schritt für Schritt zur DTM\nTextverarbeitung entlang der tidytext Pipeline: Tokenize\n\n\n\n# Create tidy data\nsubsample_tidy &lt;- review_subsample %&gt;% \n    tidytext::unnest_tokens(\"text\", ab) %&gt;% \n    filter(!text %in% tidytext::stop_words$word)\n\n# Preview\nsubsample_tidy %&gt;% \n  select(id, text) %&gt;% \n  print(n = 15)\n\n\n# A tibble: 4,872,424 × 2\n   id                               text          \n   &lt;chr&gt;                            &lt;chr&gt;         \n 1 https://openalex.org/W4293003987 5             \n 2 https://openalex.org/W4293003987 item          \n 3 https://openalex.org/W4293003987 world         \n 4 https://openalex.org/W4293003987 health        \n 5 https://openalex.org/W4293003987 organization  \n 6 https://openalex.org/W4293003987 index         \n 7 https://openalex.org/W4293003987 5             \n 8 https://openalex.org/W4293003987 widely        \n 9 https://openalex.org/W4293003987 questionnaires\n10 https://openalex.org/W4293003987 assessing     \n11 https://openalex.org/W4293003987 subjective    \n12 https://openalex.org/W4293003987 psychological \n13 https://openalex.org/W4293003987 publication   \n14 https://openalex.org/W4293003987 1998          \n15 https://openalex.org/W4293003987 5             \n# ℹ 4,872,409 more rows"
  },
  {
    "objectID": "slides/ms-slides-09.html#schritt-für-schritt-zur-dtm-1",
    "href": "slides/ms-slides-09.html#schritt-für-schritt-zur-dtm-1",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Schritt für Schritt zur DTM",
    "text": "Schritt für Schritt zur DTM\nTextverarbeitung entlang der tidytext Pipeline: Tokenize ▶️ Summarize\n\n\n\n# Create tidy data\nsubsample_tidy &lt;- review_subsample %&gt;% \n    tidytext::unnest_tokens(\"text\", ab) %&gt;% \n    filter(!text %in% tidytext::stop_words$word)\n\n# Create summarized data\nsubsample_summarized &lt;- subsample_tidy %&gt;% \n  count(id, text) \n\n# Preview \nsubsample_summarized %&gt;% \n  print(n = 15)\n\n\n# A tibble: 3,280,664 × 3\n   id                               text           n\n   &lt;chr&gt;                            &lt;chr&gt;      &lt;int&gt;\n 1 https://openalex.org/W1000529773 aim            1\n 2 https://openalex.org/W1000529773 anne           1\n 3 https://openalex.org/W1000529773 approaches     1\n 4 https://openalex.org/W1000529773 critical       1\n 5 https://openalex.org/W1000529773 current        1\n 6 https://openalex.org/W1000529773 dick           1\n 7 https://openalex.org/W1000529773 effective      1\n 8 https://openalex.org/W1000529773 employed       1\n 9 https://openalex.org/W1000529773 enhancing      1\n10 https://openalex.org/W1000529773 evaluation     1\n11 https://openalex.org/W1000529773 examined       1\n12 https://openalex.org/W1000529773 explored       1\n13 https://openalex.org/W1000529773 extended       2\n14 https://openalex.org/W1000529773 girls          1\n15 https://openalex.org/W1000529773 government     1\n# ℹ 3,280,649 more rows"
  },
  {
    "objectID": "slides/ms-slides-09.html#schritt-für-schritt-zur-dtm-2",
    "href": "slides/ms-slides-09.html#schritt-für-schritt-zur-dtm-2",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Schritt für Schritt zur DTM",
    "text": "Schritt für Schritt zur DTM\nTextverarbeitung entlang der tidytext Pipeline: Tokenize ▶️ Summarize ▶️ DTM\n\n\n\n# Create tidy data\nsubsample_tidy &lt;- review_subsample %&gt;% \n    tidytext::unnest_tokens(\"text\", ab) %&gt;% \n    filter(!text %in% tidytext::stop_words$word)\n\n# Create summarized data\nsubsample_summarized &lt;- subsample_tidy %&gt;% \n  count(id, text) \n\n# Create DTM\nsubsample_dtm &lt;- subsample_summarized %&gt;% \n  cast_dtm(id, text, n)\n\n# Preview\nsubsample_dtm\n\n\n&lt;&lt;DocumentTermMatrix (documents: 36654, terms: 122147)&gt;&gt;\nNon-/sparse entries: 3280664/4473895474\nSparsity           : 100%\nMaximal term length: 188\nWeighting          : term frequency (tf)"
  },
  {
    "objectID": "slides/ms-slides-09.html#einfach-mit-tidytext-präzise-mit-quanteda",
    "href": "slides/ms-slides-09.html#einfach-mit-tidytext-präzise-mit-quanteda",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Einfach mit tidytext, präzise mit quanteda",
    "text": "Einfach mit tidytext, präzise mit quanteda\nVergleich von Texttransformation mit verschiedenen Paketen\n\n\n\n\n# Create tidy data\nsubsample_tidy &lt;- review_subsample %&gt;% \n    tidytext::unnest_tokens(\"text\", ab) %&gt;% \n    filter(!text %in% tidytext::stop_words$word)\n\n# Create summarized data\nsubsample_summarized &lt;- subsample_tidy %&gt;% \n  count(id, text) \n\n# Create DTM\nsubsample_dtm &lt;- subsample_summarized %&gt;% \n  cast_dtm(id, text, n)\n\n# Preview\nsubsample_dtm\n\n\n\n# Create corpus\nquanteda_corpus &lt;- review_subsample %&gt;% \n  quanteda::corpus(\n    docid_field = \"id\", \n    text_field = \"ab\"\n  )\n\n# Tokenize\nquanteda_token &lt;- quanteda_corpus %&gt;% \n  quanteda::tokens(\n    remove_punct = TRUE,\n    remove_symbols = TRUE, \n    remove_numbers = TRUE, \n    remove_url = TRUE, \n    split_tags = FALSE # keep hashtags and mentions\n  ) %&gt;% \n  quanteda::tokens_tolower() %&gt;% \n  quanteda::tokens_remove(\n    pattern = stopwords(\"en\")\n    )\n\n# Convert to Document-Feature-Matrix (DFM)\nquanteda_dfm &lt;- quanteda_token %&gt;% \n  quanteda::dfm()"
  },
  {
    "objectID": "slides/ms-slides-09.html#netzwerk-der-top-begriffe",
    "href": "slides/ms-slides-09.html#netzwerk-der-top-begriffe",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Netzwerk der Top-Begriffe",
    "text": "Netzwerk der Top-Begriffe\nVergleich zwischen tidytext & quanteda\n\n\n\n# Extract most common hashtags\ntop_features_tidy &lt;- subsample_tidy %&gt;% \n  count(text, sort = TRUE) %&gt;%\n  slice_head(n = 20) %&gt;% \n  pull(text)\n\n# Visualize\nsubsample_tidy %&gt;% \n  count(id, text, sort = TRUE) %&gt;% \n  filter(!is.na(text)) %&gt;% \n  cast_dfm(id, text, n) %&gt;% \n  quanteda::fcm() %&gt;% \n  quanteda::fcm_select(\n    pattern = top_features_tidy,\n    case_insensitive = FALSE\n  ) %&gt;%  \n  quanteda.textplots::textplot_network(\n    edge_color = \"#04316A\"\n  )"
  },
  {
    "objectID": "slides/ms-slides-09.html#netzwerk-der-top-begriffe-1",
    "href": "slides/ms-slides-09.html#netzwerk-der-top-begriffe-1",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Netzwerk der Top-Begriffe",
    "text": "Netzwerk der Top-Begriffe\nVergleich zwischen tidytext & quanteda\n\n\n\n\n# Extract most common features \ntop_features_quanteda &lt;- quanteda_dfm %&gt;% \n  topfeatures(20) %&gt;% \n  names()\n\n# Construct feature-occurrence matrix of features\nquanteda_dfm %&gt;% \n  fcm() %&gt;% \n  fcm_select(pattern = top_features_quanteda) %&gt;% \n  textplot_network(\n    edge_color = \"#C50F3C\"\n  )"
  },
  {
    "objectID": "slides/ms-slides-09.html#netzwerk-der-top-begriffe-2",
    "href": "slides/ms-slides-09.html#netzwerk-der-top-begriffe-2",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Netzwerk der Top-Begriffe",
    "text": "Netzwerk der Top-Begriffe\nVergleich zwischen tidytext & quanteda\n\n\n\n\nExpand for full code\nsubsample_tidy %&gt;% \n  count(id, text, sort = TRUE) %&gt;% \n  filter(!is.na(text)) %&gt;% \n  cast_dfm(id, text, n) %&gt;% \n  quanteda::fcm() %&gt;% \n  quanteda::fcm_select(\n    pattern = top_features_tidy,\n    case_insensitive = FALSE\n  ) %&gt;%  \n  quanteda.textplots::textplot_network(\n    edge_color = \"#04316A\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nExpand for full code\nquanteda_dfm %&gt;% \n  fcm() %&gt;% \n  fcm_select(pattern = top_features_quanteda) %&gt;% \n  textplot_network(\n    edge_color = \"#C50F3C\"\n  )"
  },
  {
    "objectID": "slides/ms-slides-09.html#neuer-input-in-die-pipeline",
    "href": "slides/ms-slides-09.html#neuer-input-in-die-pipeline",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Neuer Input in die Pipeline",
    "text": "Neuer Input in die Pipeline\nUnsupervised learning example: Topic modeling\n\n\nSilge & Robinson (2017)"
  },
  {
    "objectID": "slides/ms-slides-09.html#building-a-shared-vocabulary-again",
    "href": "slides/ms-slides-09.html#building-a-shared-vocabulary-again",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Building a shared vocabulary … again",
    "text": "Building a shared vocabulary … again\nGrundbegriffe und Definitionen im Kontext des Topic Modelings\n\nK: Anzahl der Themen, die für ein bestimmtes Themenmodell berechnen werden.\nWord-Topic-Matrix: Matrix, die die bedingte Wahrscheinlichkeit (beta) beschreibt, mit der ein Wort in einem bestimmten Thema vorkommt.\nDocument-Topic-Matrix: Matrix, die die bedingte Wahrscheinlichkeit (gamma) beschreibt, mit der ein Thema in einem bestimmten Dokument vorkommt."
  },
  {
    "objectID": "slides/ms-slides-09.html#beyond-lda",
    "href": "slides/ms-slides-09.html#beyond-lda",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Beyond LDA",
    "text": "Beyond LDA\nVerschiedene Ansätze der Themenmodellierung\n\n\nLatent Dirichlet Allocation [LDA] (Blei et al., 2003) ist ein probabilistisches generatives Modell, das davon ausgeht, dass jedes Dokumentin einem KorpuseineMischung von Themen ist und jedes Wort im Dokument einem der Themen des Dokuments zuzuordnenist.\nStructural Topic Modeling [STM] (Roberts et al., 2016; Roberts et al., 2019) erweitert LDA durch die Einbeziehung von Kovariaten auf Dokumentenebene und ermöglicht die Modellierung des Einflusses externer Faktoren auf die Themenprävalenz.\nWord embeddings (Word2Vec (Mikolov et al., 2013) , Glove (Pennington et al., 2014)) stellen Wörter als kontinuierliche Vektoren in einem hochdimensionalen Raum dar und erfassen semantische Beziehungen zwischen Wörtern basierend auf ihrem Kontext in den Daten.\nTopic Modeling mit Neural Networks (BERTopic(Devlin et al., 2019), Doc2Vec(Le & Mikolov, 2014)) nutzt Deep Learning-Architekturen, um automatisch latente Themen aus Textdaten zu lernen"
  },
  {
    "objectID": "slides/ms-slides-09.html#preparation-is-everything",
    "href": "slides/ms-slides-09.html#preparation-is-everything",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Preparation is everything",
    "text": "Preparation is everything\nEmpfohlene Vorverarbeitungsschritte für das Topic Modeling nach Maier et al. (2018)\n\n\n\n⚠️ Deduplication;\n✅ tokenization;\n✅ transforming all characters to lowercase;\n✅ removing punctuation and special characters;\n✅ Removing stop-words;\n⚠️ term unification (lemmatizing or stemming);\n🏗️ relative pruning (attributed to Zipf’s law);\n\n\n\n# Pruning\nquanteda_dfm_trim &lt;- quanteda_dfm %&gt;% \n  dfm_trim( \n    min_docfreq = 10/nrow(review_subsample),\n    max_docfreq = 0.99, \n    docfreq_type = \"prop\")\n\n# Convert for stm topic modeling\nquanteda_stm &lt;- quanteda_dfm_trim %&gt;% \n   convert(to = \"stm\")\n\n\n\n\nZipf’s law states that the frequency that a word appears is inversely proportional to its rank."
  },
  {
    "objectID": "slides/ms-slides-09.html#ein-erstes-modell",
    "href": "slides/ms-slides-09.html#ein-erstes-modell",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Ein erstes Modell",
    "text": "Ein erstes Modell\nSchätzung und Sichtung eines Structural Topic Models mit 20 Themen\n\n\n\nstm_mdl &lt;- stm::stm(\n  documents = quanteda_stm$documents,\n  vocab = quanteda_stm$vocab, \n  K = 20, \n  seed = 42,\n  max.em.its = 10,\n  init.type = \"Spectral\",\n  verbose = TRUE)\n\n\n\nstm_mdl\n\nA topic model with 20 topics, 36650 documents and a 14322 word dictionary."
  },
  {
    "objectID": "slides/ms-slides-09.html#ein-erster-überblick",
    "href": "slides/ms-slides-09.html#ein-erster-überblick",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Ein erster Überblick",
    "text": "Ein erster Überblick\nVerteilung und Beschreibung der Themen\n\nstm_mdl %&gt;% plot(type = \"summary\")"
  },
  {
    "objectID": "slides/ms-slides-09.html#selber-überblick-anderes-format",
    "href": "slides/ms-slides-09.html#selber-überblick-anderes-format",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Selber Überblick, anderes Format",
    "text": "Selber Überblick, anderes Format\nVerteilung und Beschreibung der Themen\n\n\nExpand for full code\ntop_gamma &lt;- stm_mdl %&gt;%\n  tidy(matrix = \"gamma\") %&gt;% \n  dplyr::group_by(topic) %&gt;%\n  dplyr::summarise(gamma = mean(gamma), .groups = \"drop\") %&gt;%\n  dplyr::arrange(desc(gamma))\n\ntop_beta &lt;- stm_mdl %&gt;%\n  tidytext::tidy(.) %&gt;% \n  dplyr::group_by(topic) %&gt;%\n  dplyr::arrange(-beta) %&gt;%\n  dplyr::top_n(10, wt = beta) %&gt;% \n  dplyr::select(topic, term) %&gt;%\n  dplyr::summarise(terms_beta = toString(term), .groups = \"drop\")\n\ntop_topics_terms &lt;- top_beta %&gt;% \n  dplyr::left_join(top_gamma, by = \"topic\") %&gt;%\n  dplyr::mutate(\n          topic = reorder(topic, gamma)\n      )\n\n# Preview\ntop_topics_terms %&gt;%\n  mutate(across(gamma, ~round(.,3))) %&gt;% \n  dplyr::arrange(-gamma) %&gt;% \n  gt() %&gt;% \n  gt::tab_options(\n    table.font.size = \"14px\") %&gt;% \n  cols_label(\n    topic = \"Topic\", \n    terms_beta = \"Top Terms (based on beta)\",\n    gamma = \"Gamma\"\n  ) %&gt;% \n  gtExtras::gt_theme_538()\n\n\n\n\n\n\n\n\nTopic\nTop Terms (based on beta)\nGamma\n\n\n\n\n16\nresearch, literature, review, paper, systematic, future, study, analysis, findings, knowledge\n0.142\n\n\n19\nstudies, interventions, review, systematic, evidence, outcomes, included, quality, intervention, health\n0.096\n\n\n13\nhealth, mental, care, review, support, family, children, social, factors, studies\n0.079\n\n\n9\nlearning, students, education, school, review, skills, educational, teachers, teaching, study\n0.072\n\n\n11\nstudy, literature, research, can, work, development, review, also, human, economic\n0.068\n\n\n15\nstudies, ci, effect, meta-analysis, depression, p, trials, anxiety, effects, interventions\n0.061\n\n\n14\nsocial, media, information, use, digital, data, technology, communication, review, research\n0.060\n\n\n17\nstudies, children, relationship, review, language, variables, research, factors, results, effects\n0.052\n\n\n6\nprevalence, studies, covid-19, suicide, among, risk, pandemic, countries, ci, vaccine\n0.050\n\n\n1\nsleep, studies, eating, cognitive, review, associated, weight, body, may, association\n0.044\n\n\n2\nhealth, studies, women, gender, review, care, social, services, cultural, access\n0.043\n\n\n18\nstudies, health, used, measures, review, tools, instruments, assessment, training, n\n0.039\n\n\n3\ntreatment, disorder, disorders, patients, ptsd, symptoms, clinical, studies, therapy, anxiety\n0.036\n\n\n7\narticles, review, adolescents, studies, literature, search, use, results, systematic, databases\n0.036\n\n\n4\npatients, articles, review, music, therapy, cancer, can, study, pain, life\n0.032\n\n\n12\nviolence, studies, use, sexual, risk, h3, ipv, substance, alcohol, review\n0.031\n\n\n5\net, al, university, review, gt, literature, lt, author, p, search\n0.030\n\n\n8\nphysical, training, studies, disability, exercise, disabilities, employment, ed, review, strength\n0.015\n\n\n20\nattachment, studies, scholar, google, science, review, social, styles, welfare, research\n0.009\n\n\n10\nde, la, y, en, los, e, 的, el, se, que\n0.004"
  },
  {
    "objectID": "slides/ms-slides-09.html#verbindung-der-themen-untereinander",
    "href": "slides/ms-slides-09.html#verbindung-der-themen-untereinander",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Verbindung der Themen untereinander",
    "text": "Verbindung der Themen untereinander\nKorrelation der Themen\n\nstm_corr &lt;- stm::topicCorr(stm_mdl)\nplot(stm_corr)"
  },
  {
    "objectID": "slides/ms-slides-09.html#prominente-wörter-einzelner-themen",
    "href": "slides/ms-slides-09.html#prominente-wörter-einzelner-themen",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Prominente Wörter einzelner Themen",
    "text": "Prominente Wörter einzelner Themen\nÜberblick über Top-Begriffe verschiedener Themen\n\n# Fokus auf Themas 16 (höchtes Gamma)\nstm::labelTopics(stm_mdl, topic=16)\n\nTopic 16 Top Words:\n     Highest Prob: research, literature, review, paper, systematic, future, study \n     FREX: tourism, sustainable, sustainability, originality, innovation, conceptual, agenda \n     Lift: paradigmatic, hrd, edlm, positivist, internationalisation, tccm, wom \n     Score: tourism, research, literature, paper, leadership, themes, sustainable \n\n# Fokus auf Thema 10 (isoliertes Thema)\nstm::labelTopics(stm_mdl, topic=10)\n\nTopic 10 Top Words:\n     Highest Prob: de, la, y, en, los, e, 的 \n     FREX: resultados, foram, que, sobre, intervenciones, riesgo, uma \n     Lift: criterios, efecto, así, comparación, cumplieron, debido, depresión \n     Score: de, la, 的, en, los, y, que"
  },
  {
    "objectID": "slides/ms-slides-09.html#and-now-you-textanalyse-mit-r",
    "href": "slides/ms-slides-09.html#and-now-you-textanalyse-mit-r",
    "title": "Unsupervised Machine Learning (I)",
    "section": "🧪 And now … you: Textanalyse mit R",
    "text": "🧪 And now … you: Textanalyse mit R\nNext Steps: Wiederholung der Inhalte\n\nLaden Sie die auf StudOn bereitgestellten Dateien für die Sitzungen herunter\nLaden Sie die .zip-Datei in Ihren RStudio Workspace\nNavigieren Sie zu dem Ordner, in dem die Datei ps_24_binder.Rproj liegt. Öffnen Sie diese Datei mit einem Doppelklick. Nur dadurch ist gewährleistet, dass alle Dependencies korrekt funktionieren.\nÖffnen Sie die Datei exercise-09.qmd im Ordner exercises und lesen Sie sich gründlich die Anweisungen durch.\nTipp: Sie finden alle in den Folien verwendeten Code-Bausteine in der Datei showcase.qmd (für den “rohen” Code) oder showcase.html (mit gerenderten Ausgaben)."
  },
  {
    "objectID": "slides/ms-slides-09.html#references",
    "href": "slides/ms-slides-09.html#references",
    "title": "Unsupervised Machine Learning (I)",
    "section": "References",
    "text": "References\n\n\nBlei, D. M. (2012). Probabilistic topic models. Communications of the ACM, 55(4), 77–84. https://doi.org/10.1145/2133806.2133826\n\n\nBlei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichlet allocation. The Journal of Machine Learning Research, 3, 9931022.\n\n\nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of deep bidirectional transformers for language understanding (J. Burstein, C. Doran, & T. Solorio, Eds.; p. 41714186). Association for Computational Linguistics. https://doi.org/10.18653/v1/N19-1423\n\n\nGrimmer, J., & Stewart, B. M. (2013). Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts. Political Analysis, 21(3), 267–297. https://doi.org/f458q9\n\n\nLe, Q., & Mikolov, T. (2014). Distributed representations of sentences and documents (E. P. Xing & T. Jebara, Eds.; Vol. 32, p. 11881196). PMLR. https://proceedings.mlr.press/v32/le14.html\n\n\nMaier, D., Waldherr, A., Miltner, P., Wiedemann, G., Niekler, A., Keinert, A., Pfetsch, B., Heyer, G., Reber, U., Häussler, T., Schmid-Petri, H., & Adam, S. (2018). Applying LDA Topic Modeling in Communication Research: Toward a Valid and Reliable Methodology. Communication Methods and Measures, 12(2-3), 93–118. https://doi.org/10.1080/19312458.2018.1430754\n\n\nMikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed representations of words and phrases and their compositionality (C. J. Burges, L. Bottou, M. Welling, Z. Ghahramani, & K. Q. Weinberger, Eds.; Vol. 26). Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf\n\n\nPennington, J., Socher, R., & Manning, C. D. (2014). Glove: Global vectors for word representation. 15321543. https://doi.org/10.3115/v1/D14-1162\n\n\nRoberts, M. E., Stewart, B. M., & Airoldi, E. M. (2016). A model of text for experimentation in the social sciences. Journal of the American Statistical Association, 111(515), 988–1003. https://doi.org/f88tzh\n\n\nRoberts, M. E., Stewart, B. M., & Tingley, D. (2019). stm: An R Package for Structural Topic Models. Journal of Statistical Software, 91(1), 1–40. https://doi.org/10.18637/jss.v091.i02\n\n\nSilge, J., & Robinson, D. (2017). Text mining with r: A tidy approach (First edition). O’Reilly.\n\n\nZheng, A., & Casari, A. (2018). Feature engineering for machine learning: Principles and techniques for data scientists (First edition). O’Reilly.\n\n\n\n\n\n\nHome"
  },
  {
    "objectID": "course-team.html",
    "href": "course-team.html",
    "title": "Teaching team",
    "section": "",
    "text": "is Tenure-Track Assistant Professor for Communication Science at Friedrich-Alexander-Universität (FAU) Erlangen-Nürnberg.\nHis work investigates the positive and negative consequences of digital communication (e.g., via smartphones and social media) for users’ well-being, health, and self-regulation. His current research focusses on users’ digital well-being at the interface of work and leisure, for instance, digital stress, availability pressures, procrastinatory media use, and digital communication when working from home.",
    "crumbs": [
      "Hintergrundinformationen",
      "zum Teaching Team"
    ]
  },
  {
    "objectID": "course-team.html#prof.-dr.-adrian-meier",
    "href": "course-team.html#prof.-dr.-adrian-meier",
    "title": "Teaching team",
    "section": "",
    "text": "is Tenure-Track Assistant Professor for Communication Science at Friedrich-Alexander-Universität (FAU) Erlangen-Nürnberg.\nHis work investigates the positive and negative consequences of digital communication (e.g., via smartphones and social media) for users’ well-being, health, and self-regulation. His current research focusses on users’ digital well-being at the interface of work and leisure, for instance, digital stress, availability pressures, procrastinatory media use, and digital communication when working from home.",
    "crumbs": [
      "Hintergrundinformationen",
      "zum Teaching Team"
    ]
  },
  {
    "objectID": "course-team.html#christoph-adrian-hehim",
    "href": "course-team.html#christoph-adrian-hehim",
    "title": "Teaching team",
    "section": "Christoph Adrian (he/him)",
    "text": "Christoph Adrian (he/him)\n\n   \nis a Research Assistant at the Chair of Communication Science at Friedrich-Alexander-Universität (FAU) Erlangen-Nürnberg.\nHis work focuses on computational methods, especially Text as Data Approaches and workin with Digital behavioral data, with an emphasis on computing, reproducible research, student-centered learning, and open-source education.",
    "crumbs": [
      "Hintergrundinformationen",
      "zum Teaching Team"
    ]
  },
  {
    "objectID": "ms-schedule.html",
    "href": "ms-schedule.html",
    "title": "Semesterplan",
    "section": "",
    "text": "Note\n\n\n\nDiese Seite erhält eine Übersicht über die Sitzung bzw. Themen des Methodenseminars im Sommersemester 2024. Bitte beachten Sie, dass die Inhalte des Kurses ( Folien,  Exercises,  Showcases und  Hintergrundinformationen) im Laufe des Semesters stetig aktualisiert werden, wobei alle Änderungen hier dokumentiert werden.\n\n\n\n\n\n\n\n\n\n\n\nSitzung\nDatum\nThema\nUnterlagen\n\n\n\n\n\n\nIntroduction\n\n\n\n1\n18.04.2024\nEinführung & Überblick\n\n\n\n\n📚\nBlock I: Systematic Review\n\n\n\n2\n25.04.2024\nEinführung in Systematic Reviews I\n\n\n\n3\n02.05.2024\nEinführung in Systematic Reviews II\n\n\n\n\n09.05.2024\n🏖️ Feiertag\n\n\n\n4\n16.05.2024\nAutomatisierung von SRs & KI-Tools I\n\n\n\n\n23.05.2024\n🍻 WiSo-Projekt-Woche\n\n\n\n\n30.05.2024\n🏖️ Feiertag\n\n\n\n5\n04.06.2024\n🍕 Gastvortrag: Prof. Dr. Emese Domahidi\n\n\n\n6\n06.06.2024\nAutomatisierung von SRs & KI-Tools\n\n\n\n\n💻\nBlock II: Text as Data\n\n\n\n7\n13.06.2024\nIntroduction to Text as Data\n |  |  | \n\n\n8\n20.06.2024\nText processing in R\n |  |  | \n\n\n9\n27.06.2024\nUnsupervised Machine Learning I\n |  |  | \n\n\n10\n04.07.2024\nUnsupervised Machine Learning II\n\n\n\n11\n11.07.2024\nRecap & Ausblick\n\n\n\n12\n18.07.2024\n🏁 Semesterabschluss",
    "crumbs": [
      "Kursunterlagen",
      "Methodenseminar"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "KI nutzen, um KI-Folgen zu verstehen",
    "section": "",
    "text": "Generative und kommunikative KI (bspw. ChatGPT, Bing, Gemini) ist derzeit in aller Munde. Den KI-Innovationen der letzten Jahre wird als „general purpose technology“ eine enorme transformative Kraft zugeschrieben, mit potenziell weitreichenden Folgen für Individuen, Wirtschaft und Gesellschaft. Die sozialwissenschaftliche Forschung auf diesem Gebiet entwickelt sich – ebenso wie ihr soziotechnologischer Gegenstand – rasant. Binnen kürzester Zeit entstehen weltweit hunderte Studien, die die individuellen und gesellschaftlichen Implikationen von generativer KI aus psychologischer, kommunikationswissenschaftlicher oder soziologischer Perspektive untersuchen. Gleichzeitig ist der Bedarf an evidenzbasierten Prognosen über mögliche Folgen von generativer KI groß, etwa im Kontext von Regulierungsvorhaben wie dem AI Act der EU. Wie jedoch lassen sich die rasant anwachsende Forschung kosten- und ressourceneffizient überblicken und Handlungsempfehlungen aus ihr ableiten?\nEine mögliche Antwort liegt in der KI selbst. In diesem Projektseminar wollen wir daher den Versuch wagen, die aktuelle Forschung zu gesellschaftlichen Folgen von generativer und kommunikativer KI mit Hilfe von KI-gestützten Systematic Review Methoden schnell und zuverlässig zu synthetisieren (siehe begleitendes Seminar Angewandte Methoden)."
  },
  {
    "objectID": "slides/ms-slides-07.html#seminarplan",
    "href": "slides/ms-slides-07.html#seminarplan",
    "title": "Introduction to Text as Data",
    "section": "Seminarplan",
    "text": "Seminarplan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSitzung\nDatum\nThema (synchron)\nÜbung (asynchron)\nDozent:in\n\n\n\n\n1\n18.04.2024\nEinführung & Überblick\n\nAM & CA\n\n\n\n📚\nTeil 1: Systematic Review\n\n\n\n\n2\n25.04.2024\nEinführung in Systematic Reviews I\nR-Einführung\nAM\n\n\n3\n02.05.2024\nEinführung in Systematic Reviews II\nR-Einführung\nAM\n\n\n\n09.05.2024\n🏖️ Feiertag\nR-Einführung\n\n\n\n4\n16.05.2024\nAutomatisierung von SRs & KI-Tools\nR-Einführung\nAM\n\n\n\n23.05.2024\n🍻 WiSo-Projekt-Woche\nR-Einführung\n\n\n\n5\n04.06.2024\n🍕 Gastvortrag: Prof. Dr. Emese Domahidi\nR-Einführung\nED\n\n\n6\n06.06.2024\nAutomatisierung von SRs & KI-Tools\nR-Einführung\nAM\n\n\n\n💻\nTeil 2: Text as Data & Unsupervised Machine Learning\n\n\n\n\n7\n13.06.2024\nIntroduction to Text as Data\nzur Sitzung\nCA\n\n\n8\n20.06.2024\nText processing\nzur Sitzung\nCA\n\n\n9\n27.06.2024\nUnsupervised Machine Learning I\nzur Sitzung\nCA\n\n\n10\n04.07.2024\nUnsupervised Machine Learning II\nzur Sitzung\nCA & AM\n\n\n11\n11.07.2024\nRecap & Ausblick\nzur Sitzung\nCA & AM\n\n\n12\n18.07.2024\n🏁 Semesterabschluss\nzur Sitzung\nCA & AM"
  },
  {
    "objectID": "slides/ms-slides-07.html#erst-input-dann-vertiefung",
    "href": "slides/ms-slides-07.html#erst-input-dann-vertiefung",
    "title": "Introduction to Text as Data",
    "section": "Erst Input, dann Vertiefung",
    "text": "Erst Input, dann Vertiefung\nTypischer Aufbau der nächsten vier Sitzungen\nTeil 1️⃣: Input (ca. 30-45 Minuten)\n\nVorstellung der “theoretischen Grundlagen” inklusive zentraler Begriffe und Konzepte\nVorstellung der Methode(n) sowie des Kontext der praktischen Anwendung\n\nTeil 2️⃣: Praktische Anwendung (ca. 45-60 Minuten)\n\nVertiefung der Inhalte durch Bearbeitung kleiner Aufgaben, entweder in Einzel- oder Gruppenarbeit\nAufgaben zur Arbeit mit R, die im Kurs angefangen aber (vermutlich) außerhalb der Sitzung abgeschlossen werden"
  },
  {
    "objectID": "slides/ms-slides-07.html#bitte-rstudio-server-benutzen",
    "href": "slides/ms-slides-07.html#bitte-rstudio-server-benutzen",
    "title": "Introduction to Text as Data",
    "section": "Bitte RStudio Server benutzen!",
    "text": "Bitte RStudio Server benutzen!\nInformation zur Nutzung des RStudio Servers während der Sitzung\n\n\n⏰ Zur Erinnerung:\n\nFunktion der RStudio-Projekte für die praktische Anwendung in Serverumgebung getestet\nNutzung des RStudio Servers vermeidet aufwendiges & zeitraubendes Trouble-Shooting\n\n\n\n\nℹ️ Infos zum RStudio Server:\n\nNutzung nur über W-LAN der FAU (ggf. mit aktivem VPN) möglich\nVerfügbar unter: http://10.204.20.178:8787\nZugangsdaten auf Teams\n\n\n\n\n\n\n\n\n\nBei Problemen: Fragen in den Teams-Kanal!"
  },
  {
    "objectID": "slides/ms-slides-07.html#was-versteht-ihr-unter-text-as-data",
    "href": "slides/ms-slides-07.html#was-versteht-ihr-unter-text-as-data",
    "title": "Introduction to Text as Data",
    "section": "Was versteht ihr unter Text as Data?",
    "text": "Was versteht ihr unter Text as Data?\nBitte nehmt an einer kurzen Umfrage teil\n\n\n\nBitte scannt den QR-Code oder nutzt den folgenden Link für die Teilnahme an einer kurzen Umfrage:\n\nhttps://www.menti.com/aly1gft3568e\nTemporary Access Code: 8113 5474\n\n\n\n\n \n\n    \n\n\n\n\n\n−+\n01:00"
  },
  {
    "objectID": "slides/ms-slides-07.html#ergebnis",
    "href": "slides/ms-slides-07.html#ergebnis",
    "title": "Introduction to Text as Data",
    "section": "Ergebnis",
    "text": "Ergebnis"
  },
  {
    "objectID": "slides/ms-slides-07.html#altes-phänomen-neue-dimension",
    "href": "slides/ms-slides-07.html#altes-phänomen-neue-dimension",
    "title": "Introduction to Text as Data",
    "section": "Altes Phänomen, neue Dimension",
    "text": "Altes Phänomen, neue Dimension\nHintergrund zu dem Phänomen Text as Data\n\n\nLange Tradition der Text- und Inhaltsanalyse\nNeue Chancen & Herausforderungen durch explosionsartige Vergrößerung des (Text-)Datenaufkommen und deren Verfügbarkeit in den letzten Jahren (Websites, Plattformen & Digitalisierung) Verfügbarkeit von (neuen) Datenquellen als Resultat der Digitalisierung"
  },
  {
    "objectID": "slides/ms-slides-07.html#from-text-to-data-to-data-analysis",
    "href": "slides/ms-slides-07.html#from-text-to-data-to-data-analysis",
    "title": "Introduction to Text as Data",
    "section": "From text to data to data analysis",
    "text": "From text to data to data analysis\nTransformation als essenzieller Bestandteil von Text as Data (Curini & Franzese, 2020)"
  },
  {
    "objectID": "slides/ms-slides-07.html#review-über-literaturreviews",
    "href": "slides/ms-slides-07.html#review-über-literaturreviews",
    "title": "Introduction to Text as Data",
    "section": "Review über Literaturreviews",
    "text": "Review über Literaturreviews\nGrundidee, Ziel und Schwerpunkte und der kommenden Sitzungen zu Text as data\n\nIdee: Überblick über Literatur zu Literaturüberblicken verschaffen\nZiel: Durchführung einer Kombination aus (elaboriertem) Scoping Search & Scoping Review\nFokus: Computergestütze Umsetzung möglichst vieler Bestandteile des Review-Workflows, wie z.B.:\n\nEigene Datenerhebung via (OpenAlex-)API\nOberflächliche bibliometrische Analyse (zur Datenexploration und -bereinigung)\nAnalye der Abstracts mit Hilfe von unüberwachtem Machine Learning (Topic Modeling)"
  },
  {
    "objectID": "slides/ms-slides-07.html#was-war-das-nochmal",
    "href": "slides/ms-slides-07.html#was-war-das-nochmal",
    "title": "Introduction to Text as Data",
    "section": "Was war das nochmal?",
    "text": "Was war das nochmal?\nZur Erinnerung: Definition von Scoping Search und Scoping Review\n🔎 Scoping searches …\n\n\nschnelle explorative Suche, die sich auf bestimmtes Konzept konzentriert\nsind oft Teil der Suchstringentwicklung\nsind nicht ausreichend für ein systematic review\n\n\n\n… Scoping Review 📋\n\n\n\neine spezifische Form eines literature reviews\nmap a vast body of research literature in a field of interest in terms of the volume, nature, and characteristics of the primary research (Pham et al., 2014)\ndo not aim to produce a critically appraised and synthesised result/answer to a particular question, [they] rather aim to provide an overview or map of the evidence (Munn et al., 2018)"
  },
  {
    "objectID": "slides/ms-slides-07.html#what-we-do-not-do",
    "href": "slides/ms-slides-07.html#what-we-do-not-do",
    "title": "Introduction to Text as Data",
    "section": "What we (do not) do",
    "text": "What we (do not) do\nDisclaimer zum Inhalt und der Zielsetzung des Sitzungen zu Text as Data\n\n\n❌ Kein vollständig dokumentiertes Scoping Review, dass alle notwendigen (SALSA-)Schritte in vollem Umfang und nach wissenschaftlichen Standarts durchläuft\n\n❌ Keine umfassendes Einführung in die Textanalyse mit R\n\n\n\n✅ Exemplarische Darstellung einzelner Schritte des Workflows, mit Fokus auf die computergestützte Umsetzung\n \n✅ Überblick über verschiedene Verfahren, mit Schwerpunkt auf Methoden, die im Kontext von Literaturreviews notwendig und nützlich sind"
  },
  {
    "objectID": "slides/ms-slides-07.html#wer-oder-was-ist-openalex",
    "href": "slides/ms-slides-07.html#wer-oder-was-ist-openalex",
    "title": "Introduction to Text as Data",
    "section": "Wer oder was ist OpenAlex?",
    "text": "Wer oder was ist OpenAlex?\nKurze Vorstellung und Hintergrundinformationen zur Datenquelle (OpenAlex)\n\n\nopen(-source) catalog of the world’s scholarly research system\ndata is free and reusable, available via bulk download or API\ngoverned by a sustainable and transparent nonprofit"
  },
  {
    "objectID": "slides/ms-slides-07.html#first-scoping-search",
    "href": "slides/ms-slides-07.html#first-scoping-search",
    "title": "Introduction to Text as Data",
    "section": "First scoping search",
    "text": "First scoping search\nSichtung der Daten- und Identfikation der Analysegrundlage\n\n\n\n\nEine simple Suchquery resultiert zwar in sehr vielen Treffern, bringt aber auch (praktische) Probleme mit sich:\n\nDeutliche Überschreibung des tägliches API-Limit beträgt 100.000 Referenzen\n“Lokale” Datenbearbeitung und -analye benötigt bei der Menge an Daten ehrhebliche Rechenkapazität\nLösung: Optimierung der Suchquery durch Spezifizierung des Untersuchungsgegenstandes"
  },
  {
    "objectID": "slides/ms-slides-07.html#fine-tuning-der-search-query",
    "href": "slides/ms-slides-07.html#fine-tuning-der-search-query",
    "title": "Introduction to Text as Data",
    "section": "Fine-Tuning der Search Query",
    "text": "Fine-Tuning der Search Query\nÜberblick über verwendete Search Query und ausgewählte deskriptive Statistiken\n\n\n\n\n\n\n\n\n\n\n    \nLink zur Suche"
  },
  {
    "objectID": "slides/ms-slides-07.html#openalex-openalexr",
    "href": "slides/ms-slides-07.html#openalex-openalexr",
    "title": "Introduction to Text as Data",
    "section": "OpenAlex 🤝 openalexR",
    "text": "OpenAlex 🤝 openalexR\nZusammenspiel aus Datenbank und R-Package openalexR (Aria et al., 2024)\n\n\n\n\n\n\n\n\n\n\nopenalexR helps you interface with the OpenAlex API to retrieve bibliographic information about publications, authors, institutions, sources, funders, publishers, topics and concepts.\n\n\n\n\n\nManueller Export von Ergebnissen mit Hilfe des Web-Interface von OpenAlex möglich, im Bulk aber umständlich\nSelbstständige Interaktion mit API ist aufwendig: Design der Query, Programmierung der Abfrage, Verarbeitung der Daten (nicht im Tabellenformat verfügbar)\nR-Package bietet niedrigschwelligere Alternative für API-Abfragen"
  },
  {
    "objectID": "slides/ms-slides-07.html#reproduktion-der-webabfrage-mit-r",
    "href": "slides/ms-slides-07.html#reproduktion-der-webabfrage-mit-r",
    "title": "Introduction to Text as Data",
    "section": "Reproduktion der Webabfrage mit R",
    "text": "Reproduktion der Webabfrage mit R\nAbfrage, Download und Transformation der Daten mit einer Funktion\n\n# Download data via API\nreview_works &lt;- openalexR::oa_fetch(\n  entity = \"works\",\n  title.search = \"(literature OR systematic) AND review\",\n  primary_topic.domain.id = \"domains/2\", # Social Science\n  publication_year = \"2013 - 2023\",\n  verbose = TRUE\n)\n\n# Overview\nreview_works\n\n\n\n# A tibble: 93,655 × 39\n  id      title display_name author ab    publication_date relevance_score so   \n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;        &lt;list&gt; &lt;chr&gt; &lt;chr&gt;                      &lt;dbl&gt; &lt;chr&gt;\n1 https:… The … The PRISMA … &lt;df&gt;   The … 2021-03-29                 1625. BMJ  \n2 https:… Pref… Preferred r… &lt;df&gt;   Syst… 2015-01-01                 1340. Syst…\n3 https:… Rayy… Rayyan—a we… &lt;df&gt;   Synt… 2016-12-01                 1314. Syst…\n4 https:… Syst… Systematic … &lt;df&gt;   Scop… 2018-11-19                  990. BMC …\n5 https:… Upda… Updated gui… &lt;df&gt;   On a… 2019-10-03                  963. Coch…\n# ℹ 93,650 more rows\n# ℹ 31 more variables: so_id &lt;chr&gt;, host_organization &lt;chr&gt;, issn_l &lt;chr&gt;,\n#   url &lt;chr&gt;, pdf_url &lt;chr&gt;, license &lt;chr&gt;, version &lt;chr&gt;, first_page &lt;chr&gt;,\n#   last_page &lt;chr&gt;, volume &lt;chr&gt;, issue &lt;chr&gt;, is_oa &lt;lgl&gt;,\n#   is_oa_anywhere &lt;lgl&gt;, oa_status &lt;chr&gt;, oa_url &lt;chr&gt;,\n#   any_repository_has_fulltext &lt;lgl&gt;, language &lt;chr&gt;, grants &lt;list&gt;,\n#   cited_by_count &lt;int&gt;, counts_by_year &lt;list&gt;, publication_year &lt;int&gt;, …"
  },
  {
    "objectID": "slides/ms-slides-07.html#das-ergebnis-der-abfrage",
    "href": "slides/ms-slides-07.html#das-ergebnis-der-abfrage",
    "title": "Introduction to Text as Data",
    "section": "Das Ergebnis der Abfrage",
    "text": "Das Ergebnis der Abfrage\nFlüchtiger Blick auf den R-Datensatz inklusive erster Qualtiätsprüfung\n\n\n\nTypische Überprüfungen\n\n\n\n\nWie viele Fälle sind enthalten? Wie viele Variablen? Sind die Variablennamen aussagekräftig?\nWelchen Typ hat jede Variable, z. B. numerisch, kategorisch, logisch?\n\n\n\n\n\n\nreview_works %&gt;% glimpse()\n\nRows: 93,655\nColumns: 39\n$ id                          &lt;chr&gt; \"https://openalex.org/W3118615836\", \"https…\n$ title                       &lt;chr&gt; \"The PRISMA 2020 statement: an updated gui…\n$ display_name                &lt;chr&gt; \"The PRISMA 2020 statement: an updated gui…\n$ author                      &lt;list&gt; [&lt;data.frame[26 x 12]&gt;], [&lt;data.frame[8 x…\n$ ab                          &lt;chr&gt; \"The Preferred Reporting Items for Systema…\n$ publication_date            &lt;chr&gt; \"2021-03-29\", \"2015-01-01\", \"2016-12-01\", …\n$ relevance_score             &lt;dbl&gt; 1625.1708, 1340.1902, 1314.3904, 990.4521,…\n$ so                          &lt;chr&gt; \"BMJ\", \"Systematic reviews\", \"Systematic r…\n$ so_id                       &lt;chr&gt; \"https://openalex.org/S4393917726\", \"https…\n$ host_organization           &lt;chr&gt; NA, \"BioMed Central\", \"BioMed Central\", \"B…\n$ issn_l                      &lt;chr&gt; \"1756-1833\", \"2046-4053\", \"2046-4053\", \"14…\n$ url                         &lt;chr&gt; \"https://doi.org/10.1136/bmj.n71\", \"https:…\n$ pdf_url                     &lt;chr&gt; \"https://www.bmj.com/content/bmj/372/bmj.n…\n$ license                     &lt;chr&gt; \"cc-by\", \"cc-by\", \"cc-by\", \"cc-by\", NA, \"c…\n$ version                     &lt;chr&gt; \"publishedVersion\", \"publishedVersion\", \"p…\n$ first_page                  &lt;chr&gt; \"n71\", NA, NA, NA, NA, \"167\", \"g7647\", \"93…\n$ last_page                   &lt;chr&gt; \"n71\", NA, NA, NA, NA, \"176\", \"g7647\", \"11…\n$ volume                      &lt;chr&gt; NA, \"4\", \"5\", \"18\", NA, \"84\", \"349\", \"39\",…\n$ issue                       &lt;chr&gt; NA, \"1\", \"1\", \"1\", NA, \"3\", \"jan02 1\", \"1\"…\n$ is_oa                       &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, FALSE, TRUE, TRUE,…\n$ is_oa_anywhere              &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, …\n$ oa_status                   &lt;chr&gt; \"hybrid\", \"gold\", \"gold\", \"gold\", \"green\",…\n$ oa_url                      &lt;chr&gt; \"https://www.bmj.com/content/bmj/372/bmj.n…\n$ any_repository_has_fulltext &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE,…\n$ language                    &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", …\n$ grants                      &lt;list&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, &lt;\"htt…\n$ cited_by_count              &lt;int&gt; 30303, 17347, 10540, 5298, 5664, 2657, 909…\n$ counts_by_year              &lt;list&gt; [&lt;data.frame[5 x 2]&gt;], [&lt;data.frame[11 x …\n$ publication_year            &lt;int&gt; 2021, 2015, 2016, 2018, 2019, 2015, 2015, …\n$ cited_by_api_url            &lt;chr&gt; \"https://api.openalex.org/works?filter=cit…\n$ ids                         &lt;list&gt; &lt;\"https://openalex.org/W3118615836\", \"htt…\n$ doi                         &lt;chr&gt; \"https://doi.org/10.1136/bmj.n71\", \"https:…\n$ type                        &lt;chr&gt; \"article\", \"article\", \"article\", \"article\"…\n$ referenced_works            &lt;list&gt; &lt;\"https://openalex.org/W1528251861\", \"htt…\n$ related_works               &lt;list&gt; &lt;\"https://openalex.org/W4234875088\", \"htt…\n$ is_paratext                 &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ is_retracted                &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ concepts                    &lt;list&gt; [&lt;data.frame[20 x 5]&gt;], [&lt;data.frame[18 x…\n$ topics                      &lt;list&gt; [&lt;tbl_df[12 x 5]&gt;], [&lt;tbl_df[12 x 5]&gt;], […"
  },
  {
    "objectID": "slides/ms-slides-07.html#wichtigkeit-von-gewissenhaftigkeit",
    "href": "slides/ms-slides-07.html#wichtigkeit-von-gewissenhaftigkeit",
    "title": "Introduction to Text as Data",
    "section": "Wichtigkeit von Gewissenhaftigkeit",
    "text": "Wichtigkeit von Gewissenhaftigkeit\nGute Gewohnheiten helfen bei Qualitätsprüfung und Datenverarbeitung\n\n\n\nPraktische Empfehlungen\n\n\n\n\nEinheitlicher Code-Style, Bearbeitungsschritte kommentieren\nVeränderungen in neuen Datensatz speichern\n\n\n\n\n\nStreben nach:\n\n# Corrections based on first glimpse \nreview_works_correct &lt;- review_works %&gt;% \n    mutate(\n        # Create additional factor variables\n        publication_year_fct = as.factor(publication_year), \n        type_fct = as.factor(type)\n        )\n\nBitte vermeiden:\n\nReWoCo&lt;-review_works %&gt;% mutate(pub_year_fct = as.factor(publication_year), type_fct = as.factor(type))"
  },
  {
    "objectID": "slides/ms-slides-07.html#ein-datensatz-im-datensatz",
    "href": "slides/ms-slides-07.html#ein-datensatz-im-datensatz",
    "title": "Introduction to Text as Data",
    "section": "Ein Datensatz im Datensatz",
    "text": "Ein Datensatz im Datensatz\nExkurs zu verschachtelten (nested) Daten und Möglichkeiten zur Verabeitung in R\n\nBesonderheit: Informationen zu (Themen-)Katalogisierung als Liste im Datensatz\n\n\nreview_works_correct$topics %&gt;% head()\n\n[[1]]\n# A tibble: 12 × 5\n       i score name     id                                  display_name        \n   &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;                               &lt;chr&gt;               \n 1     1 0.999 topic    https://openalex.org/T10206         Methods for Evidenc…\n 2     1 0.999 subfield https://openalex.org/subfields/1804 Statistics, Probabi…\n 3     1 0.999 field    https://openalex.org/fields/18      Decision Sciences   \n 4     1 0.999 domain   https://openalex.org/domains/2      Social Sciences     \n 5     2 0.983 topic    https://openalex.org/T10416         Epidemiology and Im…\n 6     2 0.983 subfield https://openalex.org/subfields/2713 Epidemiology        \n 7     2 0.983 field    https://openalex.org/fields/27      Medicine            \n 8     2 0.983 domain   https://openalex.org/domains/4      Health Sciences     \n 9     3 0.946 topic    https://openalex.org/T12443         The Delphi Method i…\n10     3 0.946 subfield https://openalex.org/subfields/3312 Sociology and Polit…\n11     3 0.946 field    https://openalex.org/fields/33      Social Sciences     \n12     3 0.946 domain   https://openalex.org/domains/2      Social Sciences     \n\n[[2]]\n# A tibble: 12 × 5\n       i score name     id                                  display_name        \n   &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;                               &lt;chr&gt;               \n 1     1 0.999 topic    https://openalex.org/T10206         Methods for Evidenc…\n 2     1 0.999 subfield https://openalex.org/subfields/1804 Statistics, Probabi…\n 3     1 0.999 field    https://openalex.org/fields/18      Decision Sciences   \n 4     1 0.999 domain   https://openalex.org/domains/2      Social Sciences     \n 5     2 0.982 topic    https://openalex.org/T12443         The Delphi Method i…\n 6     2 0.982 subfield https://openalex.org/subfields/3312 Sociology and Polit…\n 7     2 0.982 field    https://openalex.org/fields/33      Social Sciences     \n 8     2 0.982 domain   https://openalex.org/domains/2      Social Sciences     \n 9     3 0.957 topic    https://openalex.org/T10416         Epidemiology and Im…\n10     3 0.957 subfield https://openalex.org/subfields/2713 Epidemiology        \n11     3 0.957 field    https://openalex.org/fields/27      Medicine            \n12     3 0.957 domain   https://openalex.org/domains/4      Health Sciences     \n\n[[3]]\n# A tibble: 4 × 5\n      i score name     id                                  display_name         \n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;                               &lt;chr&gt;                \n1     1 0.937 topic    https://openalex.org/T10206         Methods for Evidence…\n2     1 0.937 subfield https://openalex.org/subfields/1804 Statistics, Probabil…\n3     1 0.937 field    https://openalex.org/fields/18      Decision Sciences    \n4     1 0.937 domain   https://openalex.org/domains/2      Social Sciences      \n\n[[4]]\n# A tibble: 12 × 5\n       i score name     id                                  display_name        \n   &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;                               &lt;chr&gt;               \n 1     1 0.997 topic    https://openalex.org/T10206         Methods for Evidenc…\n 2     1 0.997 subfield https://openalex.org/subfields/1804 Statistics, Probabi…\n 3     1 0.997 field    https://openalex.org/fields/18      Decision Sciences   \n 4     1 0.997 domain   https://openalex.org/domains/2      Social Sciences     \n 5     2 0.981 topic    https://openalex.org/T12443         The Delphi Method i…\n 6     2 0.981 subfield https://openalex.org/subfields/3312 Sociology and Polit…\n 7     2 0.981 field    https://openalex.org/fields/33      Social Sciences     \n 8     2 0.981 domain   https://openalex.org/domains/2      Social Sciences     \n 9     3 0.973 topic    https://openalex.org/T10416         Epidemiology and Im…\n10     3 0.973 subfield https://openalex.org/subfields/2713 Epidemiology        \n11     3 0.973 field    https://openalex.org/fields/27      Medicine            \n12     3 0.973 domain   https://openalex.org/domains/4      Health Sciences     \n\n[[5]]\n# A tibble: 12 × 5\n       i score name     id                                  display_name        \n   &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;                               &lt;chr&gt;               \n 1     1 0.993 topic    https://openalex.org/T10206         Methods for Evidenc…\n 2     1 0.993 subfield https://openalex.org/subfields/1804 Statistics, Probabi…\n 3     1 0.993 field    https://openalex.org/fields/18      Decision Sciences   \n 4     1 0.993 domain   https://openalex.org/domains/2      Social Sciences     \n 5     2 0.964 topic    https://openalex.org/T11744         Implementation of E…\n 6     2 0.964 subfield https://openalex.org/subfields/3600 General Health Prof…\n 7     2 0.964 field    https://openalex.org/fields/36      Health Professions  \n 8     2 0.964 domain   https://openalex.org/domains/4      Health Sciences     \n 9     3 0.954 topic    https://openalex.org/T12664         Development and Eva…\n10     3 0.954 subfield https://openalex.org/subfields/2739 Public Health, Envi…\n11     3 0.954 field    https://openalex.org/fields/27      Medicine            \n12     3 0.954 domain   https://openalex.org/domains/4      Health Sciences     \n\n[[6]]\n# A tibble: 12 × 5\n       i score name     id                                  display_name        \n   &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;                               &lt;chr&gt;               \n 1     1 0.993 topic    https://openalex.org/T10475         Role of Positive Em…\n 2     1 0.993 subfield https://openalex.org/subfields/3207 Social Psychology   \n 3     1 0.993 field    https://openalex.org/fields/32      Psychology          \n 4     1 0.993 domain   https://openalex.org/domains/2      Social Sciences     \n 5     2 0.992 topic    https://openalex.org/T10804         Health Economics an…\n 6     2 0.992 subfield https://openalex.org/subfields/2002 Economics and Econo…\n 7     2 0.992 field    https://openalex.org/fields/20      Economics, Economet…\n 8     2 0.992 domain   https://openalex.org/domains/2      Social Sciences     \n 9     3 0.984 topic    https://openalex.org/T12947         Salutogenesis and S…\n10     3 0.984 subfield https://openalex.org/subfields/3600 General Health Prof…\n11     3 0.984 field    https://openalex.org/fields/36      Health Professions  \n12     3 0.984 domain   https://openalex.org/domains/4      Health Sciences"
  },
  {
    "objectID": "slides/ms-slides-07.html#entpacken-der-schachteln-steigert-die-fallzahl",
    "href": "slides/ms-slides-07.html#entpacken-der-schachteln-steigert-die-fallzahl",
    "title": "Introduction to Text as Data",
    "section": "Entpacken der Schachteln steigert die Fallzahl",
    "text": "Entpacken der Schachteln steigert die Fallzahl\nExkurs zu verschachtelten (nested) Daten und Möglichkeiten zur Verabeitung in R\n\nreview_works_correct %&gt;% \n    unnest(topics, names_sep = \"_\") %&gt;%\n    glimpse()\n\nRows: 942,560\nColumns: 45\n$ id                          &lt;chr&gt; \"https://openalex.org/W3118615836\", \"https…\n$ title                       &lt;chr&gt; \"The PRISMA 2020 statement: an updated gui…\n$ display_name                &lt;chr&gt; \"The PRISMA 2020 statement: an updated gui…\n$ author                      &lt;list&gt; [&lt;data.frame[26 x 12]&gt;], [&lt;data.frame[26 …\n$ ab                          &lt;chr&gt; \"The Preferred Reporting Items for Systema…\n$ publication_date            &lt;chr&gt; \"2021-03-29\", \"2021-03-29\", \"2021-03-29\", …\n$ relevance_score             &lt;dbl&gt; 1625.171, 1625.171, 1625.171, 1625.171, 16…\n$ so                          &lt;chr&gt; \"BMJ\", \"BMJ\", \"BMJ\", \"BMJ\", \"BMJ\", \"BMJ\", …\n$ so_id                       &lt;chr&gt; \"https://openalex.org/S4393917726\", \"https…\n$ host_organization           &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ issn_l                      &lt;chr&gt; \"1756-1833\", \"1756-1833\", \"1756-1833\", \"17…\n$ url                         &lt;chr&gt; \"https://doi.org/10.1136/bmj.n71\", \"https:…\n$ pdf_url                     &lt;chr&gt; \"https://www.bmj.com/content/bmj/372/bmj.n…\n$ license                     &lt;chr&gt; \"cc-by\", \"cc-by\", \"cc-by\", \"cc-by\", \"cc-by…\n$ version                     &lt;chr&gt; \"publishedVersion\", \"publishedVersion\", \"p…\n$ first_page                  &lt;chr&gt; \"n71\", \"n71\", \"n71\", \"n71\", \"n71\", \"n71\", …\n$ last_page                   &lt;chr&gt; \"n71\", \"n71\", \"n71\", \"n71\", \"n71\", \"n71\", …\n$ volume                      &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ issue                       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ is_oa                       &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, …\n$ is_oa_anywhere              &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, …\n$ oa_status                   &lt;chr&gt; \"hybrid\", \"hybrid\", \"hybrid\", \"hybrid\", \"h…\n$ oa_url                      &lt;chr&gt; \"https://www.bmj.com/content/bmj/372/bmj.n…\n$ any_repository_has_fulltext &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, …\n$ language                    &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", …\n$ grants                      &lt;list&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ cited_by_count              &lt;int&gt; 30303, 30303, 30303, 30303, 30303, 30303, …\n$ counts_by_year              &lt;list&gt; [&lt;data.frame[5 x 2]&gt;], [&lt;data.frame[5 x 2…\n$ publication_year            &lt;int&gt; 2021, 2021, 2021, 2021, 2021, 2021, 2021, …\n$ cited_by_api_url            &lt;chr&gt; \"https://api.openalex.org/works?filter=cit…\n$ ids                         &lt;list&gt; &lt;\"https://openalex.org/W3118615836\", \"htt…\n$ doi                         &lt;chr&gt; \"https://doi.org/10.1136/bmj.n71\", \"https:…\n$ type                        &lt;chr&gt; \"article\", \"article\", \"article\", \"article\"…\n$ referenced_works            &lt;list&gt; &lt;\"https://openalex.org/W1528251861\", \"htt…\n$ related_works               &lt;list&gt; &lt;\"https://openalex.org/W4234875088\", \"htt…\n$ is_paratext                 &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ is_retracted                &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ concepts                    &lt;list&gt; [&lt;data.frame[20 x 5]&gt;], [&lt;data.frame[20 x…\n$ topics_i                    &lt;int&gt; 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 1, 1, …\n$ topics_score                &lt;dbl&gt; 0.9993, 0.9993, 0.9993, 0.9993, 0.9832, 0.…\n$ topics_name                 &lt;chr&gt; \"topic\", \"subfield\", \"field\", \"domain\", \"t…\n$ topics_id                   &lt;chr&gt; \"https://openalex.org/T10206\", \"https://op…\n$ topics_display_name         &lt;chr&gt; \"Methods for Evidence Synthesis in Researc…\n$ publication_year_fct        &lt;fct&gt; 2021, 2021, 2021, 2021, 2021, 2021, 2021, …\n$ type_fct                    &lt;fct&gt; article, article, article, article, articl…"
  },
  {
    "objectID": "slides/ms-slides-07.html#deskriptive-statistiken-zur-datenabfrage",
    "href": "slides/ms-slides-07.html#deskriptive-statistiken-zur-datenabfrage",
    "title": "Introduction to Text as Data",
    "section": "Deskriptive Statistiken zur Datenabfrage",
    "text": "Deskriptive Statistiken zur Datenabfrage\nRekonstruktion und Erweiterung des OpenAlex Web-Dashboards mit R\n\n\n\nIm Fokus:\n\n🔍 Publikationen im Zeitverlauf\nFoschungsfelder\nRelevante Publikationen\nLageparameter\n\n\n\n\n\nreview_works_correct %&gt;% \n    ggplot(aes(publication_year_fct)) +\n    geom_bar() +\n    theme_pubr()"
  },
  {
    "objectID": "slides/ms-slides-07.html#deskriptive-statistiken-zur-datenabfrage-1",
    "href": "slides/ms-slides-07.html#deskriptive-statistiken-zur-datenabfrage-1",
    "title": "Introduction to Text as Data",
    "section": "Deskriptive Statistiken zur Datenabfrage",
    "text": "Deskriptive Statistiken zur Datenabfrage\nRekonstruktion und Erweiterung des OpenAlex Web-Dashboards mit R\n\n\n\nIm Fokus:\n\nPublikationen im Zeitverlauf\n🔍 Foschungsfelder\nRelevante Publikationen\nLageparameter\n\n\n\n\n\nreview_works_correct %&gt;% \n    unnest(topics, names_sep = \"_\") %&gt;% \n    filter(topics_name == \"field\") %&gt;% \n    filter(topics_i == 1) %&gt;% \n    sjmisc::frq(topics_display_name, sort.frq = \"desc\")\n\ntopics_display_name &lt;character&gt; \n# total N=93655 valid N=93655 mean=4.41 sd=1.62\n\nValue                               |     N | Raw % | Valid % | Cum. %\n----------------------------------------------------------------------\nSocial Sciences                     | 30580 | 32.65 |   32.65 |  32.65\nPsychology                          | 29054 | 31.02 |   31.02 |  63.67\nBusiness, Management and Accounting | 15558 | 16.61 |   16.61 |  80.29\nDecision Sciences                   |  7261 |  7.75 |    7.75 |  88.04\nEconomics, Econometrics and Finance |  6796 |  7.26 |    7.26 |  95.30\nArts and Humanities                 |  4406 |  4.70 |    4.70 | 100.00\n&lt;NA&gt;                                |     0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;"
  },
  {
    "objectID": "slides/ms-slides-07.html#deskriptive-statistiken-zur-datenabfrage-2",
    "href": "slides/ms-slides-07.html#deskriptive-statistiken-zur-datenabfrage-2",
    "title": "Introduction to Text as Data",
    "section": "Deskriptive Statistiken zur Datenabfrage",
    "text": "Deskriptive Statistiken zur Datenabfrage\nRekonstruktion und Erweiterung des OpenAlex Web-Dashboards mit R\n\n\n\nIm Fokus:\n\nPublikationen im Zeitverlauf\nFoschungsfelder\n🔍 Relevante Publikationen\nLageparameter\n\n\n\n\n\nreview_works_correct %&gt;% \n    arrange(desc(relevance_score)) %&gt;%\n    select(publication_year_fct, relevance_score, title) %&gt;% \n    head(5) %&gt;% \n    gt::gt()\n\n\n\n\n\n\n\npublication_year_fct\nrelevance_score\ntitle\n\n\n\n\n2021\n1625.1708\nThe PRISMA 2020 statement: an updated guideline for reporting systematic reviews\n\n\n2015\n1340.1902\nPreferred reporting items for systematic review and meta-analysis protocols (PRISMA-P) 2015 statement\n\n\n2016\n1314.3904\nRayyan—a web and mobile app for systematic reviews\n\n\n2018\n990.4521\nSystematic review or scoping review? Guidance for authors when choosing between a systematic or scoping review approach\n\n\n2019\n962.6738\nUpdated guidance for trusted systematic reviews: a new edition of the Cochrane Handbook for Systematic Reviews of Interventions"
  },
  {
    "objectID": "slides/ms-slides-07.html#deskriptive-statistiken-zur-datenabfrage-3",
    "href": "slides/ms-slides-07.html#deskriptive-statistiken-zur-datenabfrage-3",
    "title": "Introduction to Text as Data",
    "section": "Deskriptive Statistiken zur Datenabfrage",
    "text": "Deskriptive Statistiken zur Datenabfrage\nRekonstruktion und Erweiterung des OpenAlex Web-Dashboards mit R\n\n\n\nIm Fokus:\n\nPublikationen im Zeitverlauf\nFoschungsfelder\nRelevante Publikationen\n🔍 Lageparameter\n\n\n\n\n\nreview_works_correct %&gt;% \n  select(where(is.numeric)) %&gt;% \n  datawizard::describe_distribution() %&gt;% \n  print_html()\n\n\n\n\n\n\n\nVariable\nMean\nSD\nIQR\nMin\nMax\nSkewness\nKurtosis\nn\nn_Missing\n\n\n\n\nrelevance_score\n31.73\n42.51\n36.48\n1.17\n1625.17\n4.75\n67.87\n93655\n0\n\n\ncited_by_count\n18.33\n146.36\n10.00\n0.00\n30303.00\n123.44\n22236.55\n93655\n0\n\n\npublication_year\n2019.40\n2.97\n5.00\n2013.00\n2023.00\n-0.58\n-0.77\n93655\n0"
  },
  {
    "objectID": "slides/ms-slides-07.html#and-now-you-übung-eingrenzung",
    "href": "slides/ms-slides-07.html#and-now-you-übung-eingrenzung",
    "title": "Introduction to Text as Data",
    "section": "🧪 And now … you: Übung & Eingrenzung",
    "text": "🧪 And now … you: Übung & Eingrenzung\nNext Steps: Wiederholung der R-Grundlagen an OpenAlex-Daten\n\nLaden Sie die auf StudOn bereitgestellten Dateien für die Sitzungen herunter\nLaden Sie die .zip-Datei in Ihren RStudio Workspace\nNavigieren Sie zu dem Ordner, in dem die Datei ps_24_binder.Rproj liegt. Öffnen Sie diese Datei mit einem Doppelklick. Nur dadurch ist gewährleistet, dass alle Dependencies korrekt funktionieren.\nÖffnen Sie die Datei exercise-07.qmd im Ordner exercises und lesen Sie sich gründlich die Anweisungen durch.\nTipp: Sie finden alle in den Folien verwendeten Code-Bausteine in der Datei showcase.qmd (für den “rohen” Code) oder showcase.html (mit gerenderten Ausgaben)."
  },
  {
    "objectID": "slides/ms-slides-07.html#literatur",
    "href": "slides/ms-slides-07.html#literatur",
    "title": "Introduction to Text as Data",
    "section": "Literatur",
    "text": "Literatur\n\n\nAria, M., Le, T., Cuccurullo, C., Belfiore, A., & Choe, J. (2024). openalexR: An R-Tool for Collecting Bibliometric Data from OpenAlex. The R Journal, 15(4), 167–180. https://doi.org/10.32614/rj-2023-089\n\n\nCurini, L., & Franzese, R. (2020). Text as Data: An Overview. SAGE Publications Ltd. https://methods.sagepub.com/book/research-methods-in-political-science-and-international-relations\n\n\nGrimmer, J., & Stewart, B. M. (2013). Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts. Political Analysis, 21(3), 267–297. https://doi.org/f458q9\n\n\nMunn, Z., Peters, M. D. J., Stern, C., Tufanaru, C., McArthur, A., & Aromataris, E. (2018). Systematic review or scoping review? Guidance for authors when choosing between a systematic or scoping review approach. BMC Medical Research Methodology, 18(1). https://doi.org/10.1186/s12874-018-0611-x\n\n\nPham, M. T., Rajić, A., Greig, J. D., Sargeant, J. M., Papadopoulos, A., & McEwen, S. A. (2014). A scoping review of scoping reviews: advancing the approach and enhancing the consistency. Research Synthesis Methods, 5(4), 371–385. https://doi.org/10.1002/jrsm.1123\n\n\n\n\n\n\nHome"
  },
  {
    "objectID": "slides/ms-slides-08.html#seminarplan",
    "href": "slides/ms-slides-08.html#seminarplan",
    "title": "Text processing",
    "section": "Seminarplan",
    "text": "Seminarplan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSitzung\nDatum\nThema (synchron)\nÜbung (asynchron)\nDozent:in\n\n\n\n\n1\n18.04.2024\nEinführung & Überblick\n\nAM & CA\n\n\n\n📚\nTeil 1: Systematic Review\n\n\n\n\n2\n25.04.2024\nEinführung in Systematic Reviews I\nR-Einführung\nAM\n\n\n3\n02.05.2024\nEinführung in Systematic Reviews II\nR-Einführung\nAM\n\n\n\n09.05.2024\n🏖️ Feiertag\nR-Einführung\n\n\n\n4\n16.05.2024\nAutomatisierung von SRs & KI-Tools\nR-Einführung\nAM\n\n\n\n23.05.2024\n🍻 WiSo-Projekt-Woche\nR-Einführung\n\n\n\n5\n04.06.2024\n🍕 Gastvortrag: Prof. Dr. Emese Domahidi\nR-Einführung\nED\n\n\n6\n06.06.2024\nAutomatisierung von SRs & KI-Tools\nR-Einführung\nAM\n\n\n\n💻\nTeil 2: Text as Data & Unsupervised Machine Learning\n\n\n\n\n7\n13.06.2024\nIntroduction to Text as Data\nzur Sitzung\nCA\n\n\n8\n20.06.2024\nText processing\nzur Sitzung\nCA\n\n\n9\n27.06.2024\nUnsupervised Machine Learning I\nzur Sitzung\nCA\n\n\n10\n04.07.2024\nUnsupervised Machine Learning II\nzur Sitzung\nCA & AM\n\n\n11\n11.07.2024\nRecap & Ausblick\nzur Sitzung\nCA & AM\n\n\n12\n18.07.2024\n🏁 Semesterabschluss\nzur Sitzung\nCA & AM"
  },
  {
    "objectID": "slides/ms-slides-08.html#building-a-shared-vocabulary",
    "href": "slides/ms-slides-08.html#building-a-shared-vocabulary",
    "title": "Text processing",
    "section": "Building a shared vocabulary",
    "text": "Building a shared vocabulary\nWichtige Begriffe und Konzepte\n\n by Analytics Vidhya\nToken: A token is a string with a known meaning, and a token may be a word, number or just characters like punctuation. “Hello”, “123”, and “-” are some examples of tokens.\nSentence: A sentence is a group of tokens that is complete in meaning. “The weather looks good” is an example of a sentence, and the tokens of the sentence are [“The”, “weather”, “looks”, “good].\nParagraph: A paragraph is a collection of sentences or phrases, and a sentence can alternatively be viewed as a token of a paragraph.\nDocuments: A document might be a sentence, a paragraph, or a set of paragraphs. A text message sent to an individual is an example of a document.\nCorpus: A corpus is typically an extensive collection of documents as a Bag-of-words. A corpus comprises each word’s id and frequency count in each record. An example of a corpus is a collection of emails or text messages sent to a particular person."
  },
  {
    "objectID": "slides/ms-slides-08.html#a-bag-of-words",
    "href": "slides/ms-slides-08.html#a-bag-of-words",
    "title": "Text processing",
    "section": "A “bag of words”",
    "text": "A “bag of words”\nEinfache Technik im Natural Language Processing (NLP)\n\n by Shubham Gandhi\na collection of words, disregarding grammar, word order, and context."
  },
  {
    "objectID": "slides/ms-slides-08.html#digitales-wörterbuch-der-deutschen-sprache",
    "href": "slides/ms-slides-08.html#digitales-wörterbuch-der-deutschen-sprache",
    "title": "Text processing",
    "section": "Digitales Wörterbuch der deutschen Sprache",
    "text": "Digitales Wörterbuch der deutschen Sprache\nGroßes, frei verfügbares & deutschsprachiges Textkorpora"
  },
  {
    "objectID": "slides/ms-slides-08.html#vom-korpus-zum-token",
    "href": "slides/ms-slides-08.html#vom-korpus-zum-token",
    "title": "Text processing",
    "section": "Vom Korpus zum Token",
    "text": "Vom Korpus zum Token\nEinfaches Beispiel zur Darstellung der verschiedenen Konzepte\n\n by Mina Ghashami"
  },
  {
    "objectID": "slides/ms-slides-08.html#vom-korpus-zum-token-zum-model",
    "href": "slides/ms-slides-08.html#vom-korpus-zum-token-zum-model",
    "title": "Text processing",
    "section": "Vom Korpus zum Token zum Model",
    "text": "Vom Korpus zum Token zum Model\nKomplexer Prozess der Textverarbeitung\n\n by Jiawei Hu"
  },
  {
    "objectID": "slides/ms-slides-08.html#sätze-token-lemma-pos",
    "href": "slides/ms-slides-08.html#sätze-token-lemma-pos",
    "title": "Text processing",
    "section": "Sätze ➜ Token ➜ Lemma ➜ POS",
    "text": "Sätze ➜ Token ➜ Lemma ➜ POS\nBeispielhafte Darstellung des Text Preprocessing\n\n\n\n\n\n\n\n1. Satzerkennung\n\n\nWas gibt’s in New York zu sehen?\n\n\n\n\n\n\n\n\n\n2. Tokenisierung\n\n\nwas; gibt; `s; in; new; york; zu; sehen; ?\n\n\n\n\n\n\n\n\n\n3. Lemmatisierung\n\n\nwas; geben; `s; in; new; york; zu; sehen; ?\n\n\n\n\n\n\n\n\n\n4. Part-Of-Speech (POS) Tagging\n\n\n&gt;Was/PWS &gt;gibt/VVFIN &gt;’s/PPER &gt;in/APPR &gt;New/NE &gt;York/NE &gt;zu/PTKZU &gt;sehen/VVINF\n\n\n\n\nSatzerkennung: Auflösung der Satzstruktur; Aber: Probleme mit Datumsangaben, Uhrzeit, Abkürzungen, URLS\nTokenisierung: Zerteilung in kleinste Einheiten, Abtrennung von Satzzeichen; Fragen: Umgang mit Zeichen, Symbolen, Zahlen, N-Gramme …\nDefinition Lemmatisierung: Grundform eines Worters, als diejenige Form, unter dem an einen Begriff in einem Nachschlagewerk findet / Rückführung auf die „Vollfrom”\nDefinition POS: Zuordnung von Wörtern und Satzzeichen eines Textes zu Wortarten"
  },
  {
    "objectID": "slides/ms-slides-08.html#von-bow-zu-dfm",
    "href": "slides/ms-slides-08.html#von-bow-zu-dfm",
    "title": "Text processing",
    "section": "Von BoW zu DFM",
    "text": "Von BoW zu DFM\nTransformation des Bag-of-Words (BOW) zur Document-Feature-Matrix (DFM)\n\n by OpenClassrooms\nBag-of-Words-Modell: es zählt lediglich die Worthäufigkeit je Dokument, die syntaktischen und grammatikalischen Zusammenhänge zwischen einzelnen Wörtern werden ignoriert."
  },
  {
    "objectID": "slides/ms-slides-08.html#what-we-did-so-far",
    "href": "slides/ms-slides-08.html#what-we-did-so-far",
    "title": "Text processing",
    "section": "What we did so far",
    "text": "What we did so far\nInformationen zur Datengrundlage und -quelle\n\nSuche nach Literatur zur (Sytematischen) Literaturüberblicken auf OpenAlex\nDownload von knapp 100.000 Literaturverweisen via API mit openalexR (Aria et al., 2024)\nDeskriptive Auswertung der Daten (“Rekonstruktion” des OpenAlex Web-Dashboards) mit R\n\nHeutige Ziele:\n\nEingrenzung der Datenbasis für weiterführende Analysen\nAnwendung einfacher Textanalyseverfahren zur Untersuchung der Abstracts"
  },
  {
    "objectID": "slides/ms-slides-08.html#euer-input-ist-gefragt",
    "href": "slides/ms-slides-08.html#euer-input-ist-gefragt",
    "title": "Text processing",
    "section": "Euer Input ist gefragt!",
    "text": "Euer Input ist gefragt!\nWie sollen die Daten weiter eingegrenzt werden?\n\n\n\nBitte scannt den QR-Code oder nutzt den folgenden Link für die Teilnahme an einer kurzen Umfrage:\n\nhttps://www.menti.com/albpi1xur7et\nTemporary Access Code: 3332 2971\n\n\n\n\n \n\n    \n\n\n\n\n\n−+\n01:00"
  },
  {
    "objectID": "slides/ms-slides-08.html#ergebnis",
    "href": "slides/ms-slides-08.html#ergebnis",
    "title": "Text processing",
    "section": "Ergebnis",
    "text": "Ergebnis"
  },
  {
    "objectID": "slides/ms-slides-08.html#build-the-subsample",
    "href": "slides/ms-slides-08.html#build-the-subsample",
    "title": "Text processing",
    "section": "Build the subsample",
    "text": "Build the subsample\nFokus auf englische Artikel aus den Sozialwissenschaften und der Psychologie\n\n\n\nreview_subsample &lt;- review_works_correct %&gt;% \n  # Eingrenzung: Sprache und Typ\n  filter(language == \"en\") %&gt;% \n  filter(type == \"article\") %&gt;%\n  # Datentranformation\n  unnest(topics, names_sep = \"_\") %&gt;%\n  filter(topics_name == \"field\") %&gt;% \n  filter(topics_i == \"1\") %&gt;% \n  # Eingrenzung: Forschungsfeldes\n  filter(\n   topics_display_name == \"Social Sciences\"|\n   topics_display_name == \"Psychology\"\n    )\n\n\n\n\nExpand for full code\nreview_works_correct %&gt;% \n  mutate(\n    included = ifelse(id %in% review_subsample$id, \"Ja\", \"Nein\"),\n    included = factor(included, levels = c(\"Nein\", \"Ja\"))\n    ) %&gt;%\n  ggplot(aes(x = publication_year_fct, fill = included)) +\n    geom_bar() +\n    labs(\n      x = \"\",\n      y = \"Anzahl der Einträge\", \n      fill = \"In Subsample enthalten?\"\n     ) +\n    scale_fill_manual(values = c(\"#A0ACBD50\", \"#FF707F\")) +\n    theme_pubr()"
  },
  {
    "objectID": "slides/ms-slides-08.html#explore-abstracts",
    "href": "slides/ms-slides-08.html#explore-abstracts",
    "title": "Text processing",
    "section": "Explore abstracts",
    "text": "Explore abstracts\nTidy data principles als Grundlage für Analyse-Workflow\n\n(Silge & Robinson, 2017)\nTidy data struture (each variable is column, each observation a row, each value is a cell, each type of observaional unit is a table) results in a table with one-token-per-row (Silge & Robinson, 2017)."
  },
  {
    "objectID": "slides/ms-slides-08.html#tokenization-der-abstracts",
    "href": "slides/ms-slides-08.html#tokenization-der-abstracts",
    "title": "Text processing",
    "section": "Tokenization der Abstracts",
    "text": "Tokenization der Abstracts\nTransform data to tidy text\n\n# Create tidy data\nreview_tidy &lt;- review_subsample %&gt;% \n    # Tokenization\n    tidytext::unnest_tokens(\"text\", ab) %&gt;% \n    # Remove stopwords\n    filter(!text %in% tidytext::stop_words$word)\n\n# Preview\nreview_tidy %&gt;% \n  select(id, text) %&gt;% \n  print(n = 10)\n\n# A tibble: 4,880,965 × 2\n   id                               text          \n   &lt;chr&gt;                            &lt;chr&gt;         \n 1 https://openalex.org/W4293003987 5             \n 2 https://openalex.org/W4293003987 item          \n 3 https://openalex.org/W4293003987 world         \n 4 https://openalex.org/W4293003987 health        \n 5 https://openalex.org/W4293003987 organization  \n 6 https://openalex.org/W4293003987 index         \n 7 https://openalex.org/W4293003987 5             \n 8 https://openalex.org/W4293003987 widely        \n 9 https://openalex.org/W4293003987 questionnaires\n10 https://openalex.org/W4293003987 assessing     \n# ℹ 4,880,955 more rows"
  },
  {
    "objectID": "slides/ms-slides-08.html#before-and-after-the-transformation",
    "href": "slides/ms-slides-08.html#before-and-after-the-transformation",
    "title": "Text processing",
    "section": "Before and after the transformation",
    "text": "Before and after the transformation\nVergleich eines Abstraktes in Rohform und nach Tokenisierung\n\n\n\nreview_subsample$ab[[1]]\n\n[1] \"The 5-item World Health Organization Well-Being Index (WHO-5) is among the most widely used questionnaires assessing subjective psychological well-being. Since its first publication in 1998, the WHO-5 has been translated into more than 30 languages and has been used in research studies all over the world. We now provide a systematic review of the literature on the WHO-5.We conducted a systematic search for literature on the WHO-5 in PubMed and PsycINFO in accordance with the PRISMA guidelines. In our review of the identified articles, we focused particularly on the following aspects: (1) the clinimetric validity of the WHO-5; (2) the responsiveness/sensitivity of the WHO-5 in controlled clinical trials; (3) the potential of the WHO-5 as a screening tool for depression, and (4) the applicability of the WHO-5 across study fields.A total of 213 articles met the predefined criteria for inclusion in the review. The review demonstrated that the WHO-5 has high clinimetric validity, can be used as an outcome measure balancing the wanted and unwanted effects of treatments, is a sensitive and specific screening tool for depression and its applicability across study fields is very high.The WHO-5 is a short questionnaire consisting of 5 simple and non-invasive questions, which tap into the subjective well-being of the respondents. The scale has adequate validity both as a screening tool for depression and as an outcome measure in clinical trials and has been applied successfully across a wide range of study fields.\"\n\n\n\n\nreview_tidy %&gt;% \n  filter(id == \"https://openalex.org/W4293003987\") %&gt;% \n  pull(text) %&gt;% \n  paste(collapse = \" \")\n\n[1] \"5 item world health organization index 5 widely questionnaires assessing subjective psychological publication 1998 5 translated 30 languages research studies world provide systematic review literature 5 conducted systematic search literature 5 pubmed psycinfo accordance prisma guidelines review identified articles focused aspects 1 clinimetric validity 5 2 responsiveness sensitivity 5 controlled clinical trials 3 potential 5 screening tool depression 4 applicability 5 study fields.a total 213 articles met predefined criteria inclusion review review demonstrated 5 clinimetric validity outcome measure balancing unwanted effects treatments sensitive specific screening tool depression applicability study fields high.the 5 short questionnaire consisting 5 simple invasive questions tap subjective respondents scale adequate validity screening tool depression outcome measure clinical trials applied successfully wide range study fields\""
  },
  {
    "objectID": "slides/ms-slides-08.html#count-token-frequency",
    "href": "slides/ms-slides-08.html#count-token-frequency",
    "title": "Text processing",
    "section": "Count token frequency",
    "text": "Count token frequency\nSummarize all tokens over all tweets\n\n\n# Create summarized data\nreview_summarized &lt;- review_tidy %&gt;% \n  count(text, sort = TRUE) \n\n# Preview Top 15 token\nreview_summarized %&gt;% \n    print(n = 15)\n\n\n# A tibble: 122,148 × 2\n   text              n\n   &lt;chr&gt;         &lt;int&gt;\n 1 studies       73398\n 2 review        57878\n 3 research      42689\n 4 health        35108\n 5 systematic    32431\n 6 literature    31374\n 7 study         29012\n 8 interventions 22731\n 9 included      21987\n10 social        21528\n11 articles      20631\n12 results       20166\n13 analysis      19624\n14 based         18929\n15 evidence      18545\n# ℹ 122,133 more rows"
  },
  {
    "objectID": "slides/ms-slides-08.html#the-unavoidable-word-cloud",
    "href": "slides/ms-slides-08.html#the-unavoidable-word-cloud",
    "title": "Text processing",
    "section": "The (Unavoidable) Word Cloud",
    "text": "The (Unavoidable) Word Cloud\nVisualization of Top 50 token\n\nreview_summarized %&gt;% \n    top_n(50) %&gt;% \n    ggplot(aes(label = text, size = n)) +\n    ggwordcloud::geom_text_wordcloud() +\n    scale_size_area(max_size = 20) +\n    theme_minimal()"
  },
  {
    "objectID": "slides/ms-slides-08.html#mehr-als-nur-ein-wort",
    "href": "slides/ms-slides-08.html#mehr-als-nur-ein-wort",
    "title": "Text processing",
    "section": "Mehr als nur ein Wort",
    "text": "Mehr als nur ein Wort\nModellierung von Wortzusammenhängen: n-grams and correlations\n\n\nViele der wirklich interessanten Ergebnisse von Textanalysen basieren auf den Beziehungen zwischen Wörtern, z.B.\n\nwelche Wörter dazu “neigen”, unmittelbar auf einander zu folgen (n-grams),\noder innerhalb desselben Dokuments gemeinsam aufzutreten (Korrelation)\n\n\n\n\n\n(Silge & Robinson, 2017)"
  },
  {
    "objectID": "slides/ms-slides-08.html#häufige-wortpaare",
    "href": "slides/ms-slides-08.html#häufige-wortpaare",
    "title": "Text processing",
    "section": "Häufige Wortpaare",
    "text": "Häufige Wortpaare\nWortkombinationen (n-grams) im FokusH\n\n\n# Create word paris\nreview_word_pairs &lt;- review_tidy %&gt;% \n    widyr::pairwise_count(\n        text,\n        id,\n        sort = TRUE)\n\n# Preview\nreview_word_pairs %&gt;% \n    print(n = 14)\n\n\n# A tibble: 114,446,724 × 3\n   item1      item2          n\n   &lt;chr&gt;      &lt;chr&gt;      &lt;dbl&gt;\n 1 review     studies    20494\n 2 studies    review     20494\n 3 review     systematic 20266\n 4 systematic review     20266\n 5 review     research   16902\n 6 research   review     16902\n 7 literature review     16754\n 8 review     literature 16754\n 9 systematic studies    16097\n10 studies    systematic 16097\n11 study      review     13391\n12 review     study      13391\n13 studies    research   13173\n14 research   studies    13173\n# ℹ 114,446,710 more rows"
  },
  {
    "objectID": "slides/ms-slides-08.html#häufig-zusammen-selten-allein",
    "href": "slides/ms-slides-08.html#häufig-zusammen-selten-allein",
    "title": "Text processing",
    "section": "Häufig zusammen, selten allein",
    "text": "Häufig zusammen, selten allein\nWortkorrelationen im Fokus\n\n\n# Create word correlation\nreview_pairs_corr &lt;- review_tidy %&gt;% \n    group_by(text) %&gt;% \n    filter(n() &gt;= 300) %&gt;% \n    pairwise_cor(\n        text, \n        id, \n        sort = TRUE)\n\n# Preview\nreview_pairs_corr %&gt;% \n    print(n = 15)\n\n\n# A tibble: 5,529,552 × 3\n   item1      item2      correlation\n   &lt;chr&gt;      &lt;chr&gt;            &lt;dbl&gt;\n 1 ottawa     newcastle        0.977\n 2 newcastle  ottawa           0.977\n 3 briggs     joanna           0.967\n 4 joanna     briggs           0.967\n 5 scholar    google           0.938\n 6 google     scholar          0.938\n 7 obsessive  compulsive       0.929\n 8 compulsive obsessive        0.929\n 9 nervosa    anorexia         0.893\n10 anorexia   nervosa          0.893\n11 ci         95               0.887\n12 95         ci               0.887\n13 las        los              0.886\n14 los        las              0.886\n15 gay        bisexual         0.861\n# ℹ 5,529,537 more rows"
  },
  {
    "objectID": "slides/ms-slides-08.html#spezifische-partner-in-spezifischen-umgebungen",
    "href": "slides/ms-slides-08.html#spezifische-partner-in-spezifischen-umgebungen",
    "title": "Text processing",
    "section": "Spezifische “Partner” in spezifischen Umgebungen",
    "text": "Spezifische “Partner” in spezifischen Umgebungen\nHäufig auftretenden Wörter in der Umgebung von review, literature, systematic\n\n\nreview_pairs_corr %&gt;% \n  filter(\n    item1 %in% c(\n      \"review\",\n      \"literature\",\n      \"systematic\")\n    ) %&gt;% \n  group_by(item1) %&gt;% \n  slice_max(correlation, n = 5) %&gt;% \n  ungroup() %&gt;% \n  mutate(\n    item2 = reorder(item2, correlation)\n    ) %&gt;% \n  ggplot(\n    aes(item2, correlation, fill = item1)\n    ) +\n  geom_bar(stat = \"identity\") +\n  facet_wrap(~ item1, scales = \"free_y\") +\n  coord_flip() +\n  scale_fill_manual(\n    values = c(\n      \"#04316A\",\n      \"#C50F3C\",\n      \"#00B2D1\")) +\n  theme_pubr()"
  },
  {
    "objectID": "slides/ms-slides-08.html#lets-talk-about-sentiments",
    "href": "slides/ms-slides-08.html#lets-talk-about-sentiments",
    "title": "Text processing",
    "section": "Let’s talk about sentiments",
    "text": "Let’s talk about sentiments\nDictionary based approach of text analysis\n\n(Silge & Robinson, 2017)\n\n\n\n\n\nAtteveldt et al. (2021) argue that sentiment, in fact, are quite a complex concepts that are often hard to capture with dictionaries."
  },
  {
    "objectID": "slides/ms-slides-08.html#über-die-bedeutung-von-positivnegativ",
    "href": "slides/ms-slides-08.html#über-die-bedeutung-von-positivnegativ",
    "title": "Text processing",
    "section": "Über die Bedeutung von “positiv;negativ”",
    "text": "Über die Bedeutung von “positiv;negativ”\nDie häufigsten “positiven” und “negativen” Wörter in den Abstracts\n\n\nreview_sentiment_count &lt;- review_tidy %&gt;% \n  inner_join(\n     get_sentiments(\"bing\"),\n     by = c(\"text\" = \"word\"),\n     relationship = \"many-to-many\") %&gt;% \n  count(text, sentiment)\n  \n# Preview\nreview_sentiment_count %&gt;% \n  group_by(sentiment) %&gt;%\n  slice_max(n, n = 10) %&gt;% \n  ungroup() %&gt;% \n  mutate(text = reorder(text, n)) %&gt;%\n  ggplot(aes(n, text, fill = sentiment)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(\n    ~sentiment, scales = \"free_y\") +\n  labs(x = \"Contribution to sentiment\",\n       y = NULL) +\n  scale_fill_manual(\n    values = c(\"#C50F3C\", \"#007900\")) +\n  theme_pubr()"
  },
  {
    "objectID": "slides/ms-slides-08.html#anreicherung-der-daten",
    "href": "slides/ms-slides-08.html#anreicherung-der-daten",
    "title": "Text processing",
    "section": "Anreicherung der Daten",
    "text": "Anreicherung der Daten\nVerknüpfung des Sentiemnt (Scores) mit den Abstracts\n\nreview_sentiment &lt;- review_tidy %&gt;% \n  inner_join(\n     get_sentiments(\"bing\"),\n     by = c(\"text\" = \"word\"),\n     relationship = \"many-to-many\") %&gt;% \n  count(id, sentiment) %&gt;% \n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %&gt;% \n  mutate(sentiment = positive - negative)\n  \n# Check\nreview_sentiment \n\n# A tibble: 35,710 × 4\n   id                               negative positive sentiment\n   &lt;chr&gt;                               &lt;int&gt;    &lt;int&gt;     &lt;int&gt;\n 1 https://openalex.org/W1000529773        2        2         0\n 2 https://openalex.org/W1006561082        0        1         1\n 3 https://openalex.org/W100685805         4       15        11\n 4 https://openalex.org/W1007410967        0        7         7\n 5 https://openalex.org/W1008209175        8        1        -7\n 6 https://openalex.org/W1009104829        2        4         2\n 7 https://openalex.org/W1009607471       15        8        -7\n 8 https://openalex.org/W1031503832       13        6        -7\n 9 https://openalex.org/W1035654938       10        5        -5\n10 https://openalex.org/W1044055445        5        0        -5\n# ℹ 35,700 more rows"
  },
  {
    "objectID": "slides/ms-slides-08.html#neutral-mit-einem-leicht-negativen-unterton",
    "href": "slides/ms-slides-08.html#neutral-mit-einem-leicht-negativen-unterton",
    "title": "Text processing",
    "section": "Neutral, mit einem leicht “negativen” Unterton",
    "text": "Neutral, mit einem leicht “negativen” Unterton\nVerteilung des Sentiment (Scores) in den Abstracts\n\n\n[1] 0.4858737\n\n\n\nreview_sentiment %&gt;% \n  ggplot(aes(sentiment)) +\n  geom_histogram(binwidth = 0.5, fill = \"#FF707F\") +\n  labs(\n    x = \"Sentiment (Score) des Abstracts\", \n    y = \"Anzahl der Einträge\"\n  ) +\n  theme_pubr() +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))"
  },
  {
    "objectID": "slides/ms-slides-08.html#keep-it-neutral",
    "href": "slides/ms-slides-08.html#keep-it-neutral",
    "title": "Text processing",
    "section": "Keep it neutral",
    "text": "Keep it neutral\nEntwicklung des Sentiment (Scores) der Abstracts im Zeitverlauf\n\n\nExpand for full code\n# Create first graph\ng1 &lt;- review_works_correct %&gt;% \n  filter(id %in% review_sentiment$id) %&gt;% \n  left_join(review_sentiment, by = join_by(id)) %&gt;% \n  sjmisc::rec(\n    sentiment,\n    rec = \"min:-2=negative; -1:1=neutral; 2:max=positive\") %&gt;% \n  ggplot(aes(x = publication_year_fct, fill = as.factor(sentiment_r))) +\n    geom_bar() +\n    labs(\n      x = \"\",\n      y = \"Anzahl der Einträge\", \n      fill = \"Sentiment (Score)\") +\n    scale_fill_manual(values = c(\"#C50F3C\", \"#90A0AF\", \"#007900\")) +\n    theme_pubr() \n    #theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n# Create second graph\ng2 &lt;- review_works_correct %&gt;% \n  filter(id %in% review_sentiment$id) %&gt;% \n  left_join(review_sentiment, by = join_by(id)) %&gt;% \n  sjmisc::rec(\n    sentiment,\n    rec = \"min:-2=negative; -1:1=neutral; 2:max=positive\") %&gt;% \n  ggplot(aes(x = publication_year_fct, fill = as.factor(sentiment_r))) +\n    geom_bar(position = \"fill\") +\n    labs(\n      x = \"\",\n      y = \"Anteil der Einträge\", \n      fill = \"Sentiment (Score)\") +\n    scale_fill_manual(values = c(\"#C50F3C\", \"#90A0AF\", \"#007D29\")) +\n    theme_pubr() \n\n# COMBINE GRPAHS\nggarrange(g1, g2,\n          nrow = 1, ncol = 2, \n          align = \"hv\",\n          common.legend = TRUE)"
  },
  {
    "objectID": "slides/ms-slides-08.html#and-now-you-wiederholung",
    "href": "slides/ms-slides-08.html#and-now-you-wiederholung",
    "title": "Text processing",
    "section": "🧪 And now … you: Wiederholung",
    "text": "🧪 And now … you: Wiederholung\nNext Steps: Wiederholung der R-Grundlagen an OpenAlex-Daten\n\nLaden Sie die auf StudOn bereitgestellten Dateien für die Sitzungen herunter\nLaden Sie die .zip-Datei in Ihren RStudio Workspace\nNavigieren Sie zu dem Ordner, in dem die Datei ps_24_binder.Rproj liegt. Öffnen Sie diese Datei mit einem Doppelklick. Nur dadurch ist gewährleistet, dass alle Dependencies korrekt funktionieren.\nÖffnen Sie die Datei exercise-08.qmd im Ordner exercises und lesen Sie sich gründlich die Anweisungen durch.\nTipp: Sie finden alle in den Folien verwendeten Code-Bausteine in der Datei showcase.qmd (für den “rohen” Code) oder showcase.html (mit gerenderten Ausgaben)."
  },
  {
    "objectID": "slides/ms-slides-08.html#literatur",
    "href": "slides/ms-slides-08.html#literatur",
    "title": "Text processing",
    "section": "Literatur",
    "text": "Literatur\n\n\nAria, M., Le, T., Cuccurullo, C., Belfiore, A., & Choe, J. (2024). openalexR: An R-Tool for Collecting Bibliometric Data from OpenAlex. The R Journal, 15(4), 167–180. https://doi.org/10.32614/rj-2023-089\n\n\nAtteveldt, W. van, Trilling, D., & Arcíla, C. (2021). Computational analysis of communication: A practical introduction to the analysis of texts, networks, and images with code examples in python and r. John Wiley & Sons.\n\n\nSilge, J., & Robinson, D. (2017). Text mining with r: A tidy approach (First edition). O’Reilly.\n\n\n\n\n\n\nHome"
  },
  {
    "objectID": "sessions/ms-session-08.html",
    "href": "sessions/ms-session-08.html",
    "title": "Text processing in R",
    "section": "",
    "text": "🖥️ Slides\n📋 Showcase"
  },
  {
    "objectID": "sessions/ms-session-08.html#participate",
    "href": "sessions/ms-session-08.html#participate",
    "title": "Text processing in R",
    "section": "",
    "text": "🖥️ Slides\n📋 Showcase"
  },
  {
    "objectID": "sessions/ms-session-08.html#practice",
    "href": "sessions/ms-session-08.html#practice",
    "title": "Text processing in R",
    "section": "Practice",
    "text": "Practice\n✍️ Exercise"
  },
  {
    "objectID": "sessions/ms-session-08.html#suggested-readings",
    "href": "sessions/ms-session-08.html#suggested-readings",
    "title": "Text processing in R",
    "section": "Suggested readings",
    "text": "Suggested readings\n\nAtteveldt, W. van, Trilling, D., & Arcíla, C. (2021). Computational analysis of communication: A practical introduction to the analysis of texts, networks, and images with code examples in python and r. John Wiley & Sons.\nSilge, J., & Hvitfeldt, E. (n.d.). Supervised machine learning for text analysis in r. https://smltar.com/\nSilge, J., & Robinson, D. (2017). Text mining with r: A tidy approach (First edition). O’Reilly.\nWelbers, K., Van Atteveldt, W., & Benoit, K. (2017). Text Analysis in R. Communication Methods and Measures, 11(4), 245–265. https://doi.org/10.1080/19312458.2017.1387238"
  },
  {
    "objectID": "sessions/ms-session-08.html#useful-resources",
    "href": "sessions/ms-session-08.html#useful-resources",
    "title": "Text processing in R",
    "section": "Useful resources",
    "text": "Useful resources\n\nTutorials des CCS-Amsterdam zu “tidytext-based” Textanalyse:\n\n📖 TidyText basics\n📖 Dictionary analysis with TidyText\n\n\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "exercises/ms-showcase-08.html",
    "href": "exercises/ms-showcase-08.html",
    "title": "Text processing in R",
    "section": "",
    "text": "Link to slides"
  },
  {
    "objectID": "exercises/ms-showcase-08.html#preparation",
    "href": "exercises/ms-showcase-08.html#preparation",
    "title": "Text processing in R",
    "section": "Preparation",
    "text": "Preparation\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n    here, qs, # file management\n    magrittr, janitor, # data wrangling\n    easystats, sjmisc, # data analysis\n    gt, gtExtras, # table visualization\n    ggpubr, ggwordcloud, # visualization\n    tidytext, widyr, # text analysis    \n    openalexR, \n    tidyverse # load last to avoid masking issues\n  )"
  },
  {
    "objectID": "exercises/ms-showcase-08.html#codechunks-aus-der-sitzung",
    "href": "exercises/ms-showcase-08.html#codechunks-aus-der-sitzung",
    "title": "Text processing in R",
    "section": "Codechunks aus der Sitzung",
    "text": "Codechunks aus der Sitzung\n\nErstelle Subsample\n\nreview_subsample &lt;- review_works_correct %&gt;% \n  # Eingrenzung: Sprache und Typ\n  filter(language == \"en\") %&gt;% \n  filter(type == \"article\") %&gt;%\n  # Datentranformation\n  unnest(topics, names_sep = \"_\") %&gt;%\n  filter(topics_name == \"field\") %&gt;% \n  filter(topics_i == \"1\") %&gt;% \n  # Eingrenzung: Forschungsfeldes\n  filter(\n   topics_display_name == \"Social Sciences\"|\n   topics_display_name == \"Psychology\"\n    )\n\n\n\nSubsample im Zeitverlauf\n\nreview_works_correct %&gt;% \n  mutate(\n    included = ifelse(id %in% review_subsample$id, \"Ja\", \"Nein\"),\n    included = factor(included, levels = c(\"Nein\", \"Ja\"))\n    ) %&gt;%\n  ggplot(aes(x = publication_year_fct, fill = included)) +\n    geom_bar() +\n    labs(\n      x = \"\",\n      y = \"Anzahl der Einträge\", \n      fill = \"In Subsample enthalten?\"\n     ) +\n    scale_fill_manual(values = c(\"#A0ACBD50\", \"#FF707F\")) +\n    theme_pubr() \n\n\n\n\n\n\n\n\n\n\nTokenization der Abstracts\n\n# Create tidy data\nreview_tidy &lt;- review_subsample %&gt;% \n    # Tokenization\n    tidytext::unnest_tokens(\"text\", ab) %&gt;% \n    # Remove stopwords\n    filter(!text %in% tidytext::stop_words$word)\n\n# Preview\nreview_tidy %&gt;% \n  select(id, text) %&gt;% \n  print(n = 10)\n\n# A tibble: 4,880,965 × 2\n   id                               text          \n   &lt;chr&gt;                            &lt;chr&gt;         \n 1 https://openalex.org/W4293003987 5             \n 2 https://openalex.org/W4293003987 item          \n 3 https://openalex.org/W4293003987 world         \n 4 https://openalex.org/W4293003987 health        \n 5 https://openalex.org/W4293003987 organization  \n 6 https://openalex.org/W4293003987 index         \n 7 https://openalex.org/W4293003987 5             \n 8 https://openalex.org/W4293003987 widely        \n 9 https://openalex.org/W4293003987 questionnaires\n10 https://openalex.org/W4293003987 assessing     \n# ℹ 4,880,955 more rows\n\n\n\nVergleich eines Abstraktes in Rohform und nach Tokenisierung\n\nreview_subsample$ab[[1]]\n\n[1] \"The 5-item World Health Organization Well-Being Index (WHO-5) is among the most widely used questionnaires assessing subjective psychological well-being. Since its first publication in 1998, the WHO-5 has been translated into more than 30 languages and has been used in research studies all over the world. We now provide a systematic review of the literature on the WHO-5.We conducted a systematic search for literature on the WHO-5 in PubMed and PsycINFO in accordance with the PRISMA guidelines. In our review of the identified articles, we focused particularly on the following aspects: (1) the clinimetric validity of the WHO-5; (2) the responsiveness/sensitivity of the WHO-5 in controlled clinical trials; (3) the potential of the WHO-5 as a screening tool for depression, and (4) the applicability of the WHO-5 across study fields.A total of 213 articles met the predefined criteria for inclusion in the review. The review demonstrated that the WHO-5 has high clinimetric validity, can be used as an outcome measure balancing the wanted and unwanted effects of treatments, is a sensitive and specific screening tool for depression and its applicability across study fields is very high.The WHO-5 is a short questionnaire consisting of 5 simple and non-invasive questions, which tap into the subjective well-being of the respondents. The scale has adequate validity both as a screening tool for depression and as an outcome measure in clinical trials and has been applied successfully across a wide range of study fields.\"\n\n\n\nreview_tidy %&gt;% \n  filter(id == \"https://openalex.org/W4293003987\") %&gt;% \n  pull(text) %&gt;% \n  paste(collapse = \" \")\n\n[1] \"5 item world health organization index 5 widely questionnaires assessing subjective psychological publication 1998 5 translated 30 languages research studies world provide systematic review literature 5 conducted systematic search literature 5 pubmed psycinfo accordance prisma guidelines review identified articles focused aspects 1 clinimetric validity 5 2 responsiveness sensitivity 5 controlled clinical trials 3 potential 5 screening tool depression 4 applicability 5 study fields.a total 213 articles met predefined criteria inclusion review review demonstrated 5 clinimetric validity outcome measure balancing unwanted effects treatments sensitive specific screening tool depression applicability study fields high.the 5 short questionnaire consisting 5 simple invasive questions tap subjective respondents scale adequate validity screening tool depression outcome measure clinical trials applied successfully wide range study fields\"\n\n\n\n\n\nCount token frequency\n\n# Create summarized data\nreview_summarized &lt;- review_tidy %&gt;% \n  count(text, sort = TRUE) \n\n# Preview Top 15 token\nreview_summarized %&gt;% \n    print(n = 15)\n\n# A tibble: 122,148 × 2\n   text              n\n   &lt;chr&gt;         &lt;int&gt;\n 1 studies       73398\n 2 review        57878\n 3 research      42689\n 4 health        35108\n 5 systematic    32431\n 6 literature    31374\n 7 study         29012\n 8 interventions 22731\n 9 included      21987\n10 social        21528\n11 articles      20631\n12 results       20166\n13 analysis      19624\n14 based         18929\n15 evidence      18545\n# ℹ 122,133 more rows"
  },
  {
    "objectID": "exercises/ms-showcase-08.html#the-unavoidable-word-cloud",
    "href": "exercises/ms-showcase-08.html#the-unavoidable-word-cloud",
    "title": "Text processing in R",
    "section": "The (Unavoidable) Word Cloud",
    "text": "The (Unavoidable) Word Cloud\n\nreview_summarized %&gt;% \n    top_n(50) %&gt;% \n    ggplot(aes(label = text, size = n)) +\n    ggwordcloud::geom_text_wordcloud() +\n    scale_size_area(max_size = 20) +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\nWortkombinationen (n-grams)\n\n# Create word paris\nreview_word_pairs &lt;- review_tidy %&gt;% \n    widyr::pairwise_count(\n        text,\n        id,\n        sort = TRUE)\n\n# Preview\nreview_word_pairs %&gt;% \n    print(n = 14)\n\n# A tibble: 114,446,724 × 3\n   item1      item2          n\n   &lt;chr&gt;      &lt;chr&gt;      &lt;dbl&gt;\n 1 review     studies    20494\n 2 studies    review     20494\n 3 review     systematic 20266\n 4 systematic review     20266\n 5 review     research   16902\n 6 research   review     16902\n 7 literature review     16754\n 8 review     literature 16754\n 9 systematic studies    16097\n10 studies    systematic 16097\n11 study      review     13391\n12 review     study      13391\n13 studies    research   13173\n14 research   studies    13173\n# ℹ 114,446,710 more rows\n\n\n\n\nWortkorrelationen\n\n# Create word correlation\nreview_pairs_corr &lt;- review_tidy %&gt;% \n    group_by(text) %&gt;% \n    filter(n() &gt;= 300) %&gt;% \n    pairwise_cor(\n        text, \n        id, \n        sort = TRUE)\n\n# Preview\nreview_pairs_corr %&gt;% \n    print(n = 15)\n\n# A tibble: 5,529,552 × 3\n   item1      item2      correlation\n   &lt;chr&gt;      &lt;chr&gt;            &lt;dbl&gt;\n 1 ottawa     newcastle        0.977\n 2 newcastle  ottawa           0.977\n 3 briggs     joanna           0.967\n 4 joanna     briggs           0.967\n 5 scholar    google           0.938\n 6 google     scholar          0.938\n 7 obsessive  compulsive       0.929\n 8 compulsive obsessive        0.929\n 9 nervosa    anorexia         0.893\n10 anorexia   nervosa          0.893\n11 ci         95               0.887\n12 95         ci               0.887\n13 las        los              0.886\n14 los        las              0.886\n15 gay        bisexual         0.861\n# ℹ 5,529,537 more rows\n\n\n\n\nSpezifische “Partner” in spezifischen Umgebungen\n\nreview_pairs_corr %&gt;% #| \n  filter(\n    item1 %in% c(\n      \"review\",\n      \"literature\",\n      \"systematic\")\n    ) %&gt;% \n  group_by(item1) %&gt;% \n  slice_max(correlation, n = 5) %&gt;% \n  ungroup() %&gt;% \n  mutate(\n    item2 = reorder(item2, correlation)\n    ) %&gt;% \n  ggplot(\n    aes(item2, correlation, fill = item1)\n    ) +\n  geom_bar(stat = \"identity\") +\n  facet_wrap(~ item1, scales = \"free_y\") +\n  coord_flip() +\n  scale_fill_manual(\n    values = c(\n      \"#04316A\",\n      \"#C50F3C\",\n      \"#00B2D1\")) +\n  theme_pubr()\n\n\n\n\n\n\n\n\n\n\nDie häufigsten “positiven” und “negativen” Wörter in den Abstracts\n\nreview_sentiment_count &lt;- review_tidy %&gt;% \n  inner_join(\n     get_sentiments(\"bing\"),\n     by = c(\"text\" = \"word\"),\n     relationship = \"many-to-many\") %&gt;% \n  count(text, sentiment)\n  \n# Preview\nreview_sentiment_count %&gt;% \n  group_by(sentiment) %&gt;%\n  slice_max(n, n = 10) %&gt;% \n  ungroup() %&gt;% \n  mutate(text = reorder(text, n)) %&gt;%\n  ggplot(aes(n, text, fill = sentiment)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(\n    ~sentiment, scales = \"free_y\") +\n  labs(x = \"Contribution to sentiment\",\n       y = NULL) +\n  scale_fill_manual(\n    values = c(\"#C50F3C\", \"#007900\")) +\n  theme_pubr()\n\n\n\n\n\n\n\n\n\n\nVerknüpfung des Sentiemnt (“Scores”) mit den Abstracts\n\nreview_sentiment &lt;- review_tidy %&gt;% \n  inner_join(\n     get_sentiments(\"bing\"),\n     by = c(\"text\" = \"word\"),\n     relationship = \"many-to-many\") %&gt;% \n  count(id, sentiment) %&gt;% \n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %&gt;% \n  mutate(sentiment = positive - negative)\n  \n# Check\nreview_sentiment \n\n# A tibble: 35,710 × 4\n   id                               negative positive sentiment\n   &lt;chr&gt;                               &lt;int&gt;    &lt;int&gt;     &lt;int&gt;\n 1 https://openalex.org/W1000529773        2        2         0\n 2 https://openalex.org/W1006561082        0        1         1\n 3 https://openalex.org/W100685805         4       15        11\n 4 https://openalex.org/W1007410967        0        7         7\n 5 https://openalex.org/W1008209175        8        1        -7\n 6 https://openalex.org/W1009104829        2        4         2\n 7 https://openalex.org/W1009607471       15        8        -7\n 8 https://openalex.org/W1031503832       13        6        -7\n 9 https://openalex.org/W1035654938       10        5        -5\n10 https://openalex.org/W1044055445        5        0        -5\n# ℹ 35,700 more rows\n\n\n\n\nVerteilung des Sentiment (Scores) in den Abstracts\n\n\n[1] 0.4858737\n\n\n\nreview_sentiment %&gt;% \n  ggplot(aes(sentiment)) +\n  geom_histogram(binwidth = 0.5, fill = \"#FF707F\") +\n  labs(\n    x = \"Sentiment (Score) des Abstracts\", \n    y = \"Anzahl der Einträge\"\n  ) +\n  theme_pubr() +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\n\n\n\n\n\n\n\n\nEntwicklung des Sentiment (Scores) der Abstracts im Zeitverlauf\n\n# Create first graph\ng1 &lt;- review_works_correct %&gt;% \n  filter(id %in% review_sentiment$id) %&gt;% \n  left_join(review_sentiment, by = join_by(id)) %&gt;% \n  sjmisc::rec(\n    sentiment,\n    rec = \"min:-2=negative; -1:1=neutral; 2:max=positive\") %&gt;% \n  ggplot(aes(x = publication_year_fct, fill = as.factor(sentiment_r))) +\n    geom_bar() +\n    labs(\n      x = \"\",\n      y = \"Anzahl der Einträge\", \n      fill = \"Sentiment (Score)\") +\n    scale_fill_manual(values = c(\"#C50F3C\", \"#90A0AF\", \"#007900\")) +\n    theme_pubr() \n    #theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n# Create second graph\ng2 &lt;- review_works_correct %&gt;% \n  filter(id %in% review_sentiment$id) %&gt;% \n  left_join(review_sentiment, by = join_by(id)) %&gt;% \n  sjmisc::rec(\n    sentiment,\n    rec = \"min:-2=negative; -1:1=neutral; 2:max=positive\") %&gt;% \n  ggplot(aes(x = publication_year_fct, fill = as.factor(sentiment_r))) +\n    geom_bar(position = \"fill\") +\n    labs(\n      x = \"\",\n      y = \"Anteil der Einträge\", \n      fill = \"Sentiment (Score)\") +\n    scale_fill_manual(values = c(\"#C50F3C\", \"#90A0AF\", \"#007D29\")) +\n    theme_pubr() \n\n# COMBINE GRPAHS\nggarrange(g1, g2,\n          nrow = 1, ncol = 2, \n          align = \"hv\",\n          common.legend = TRUE)"
  },
  {
    "objectID": "exercises/ms-exercise-08_solution.html",
    "href": "exercises/ms-exercise-08_solution.html",
    "title": "Text processing with R",
    "section": "",
    "text": "Link to source file"
  },
  {
    "objectID": "exercises/ms-exercise-08_solution.html#background",
    "href": "exercises/ms-exercise-08_solution.html#background",
    "title": "Text processing with R",
    "section": "Background",
    "text": "Background\n\n\n\n\n\n\nTodays’s data basis: OpenAlex\n\n\n\n\nVia API bzw. openalexR (Aria et al. 2024) gesammelte “works” der Datenbank OpenAlex mit Bezug zu Literaturriews in den Sozialwissenschaften zwischen 2013 und 2023\nDetaillierte Informationen und Ergebnisse zur Suchquery finden Sie hier."
  },
  {
    "objectID": "exercises/ms-exercise-08_solution.html#preparation",
    "href": "exercises/ms-exercise-08_solution.html#preparation",
    "title": "Text processing with R",
    "section": "Preparation",
    "text": "Preparation\n\n\n\n\n\n\nWichtige Information\n\n\n\n\nBitte stellen Sie sicher, dass Sie das jeweilige R-Studio Projekt zur Übung geöffnet haben. Nur so funktionieren alle Dependencies korrekt.\nUm den einwandfreien Ablauf der Übung zu gewährleisten, wird für die Aufgaben auf eine eigenständige Datenerhebung verzichtet und ein Übungsdatensatz zu verfügung gestelt.\n\n\n\n\nPackages\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n    here, qs, # file management\n    magrittr, janitor, # data wrangling\n    easystats, sjmisc, # data analysis\n    gt, gtExtras, # table visualization\n    ggpubr, ggwordcloud, # visualization\n    tidytext, widyr, # text analysis    \n    openalexR, \n    tidyverse # load last to avoid masking issues\n  )\n\n\n\nImport und Vorverarbeitung der Daten\n\n# Import from local\nreview_works &lt;- qs::qread(here(\"data/session-07/openalex-review_works-2013_2023.qs\"))\n\n# Create correct data\nreview_works_correct &lt;- review_works %&gt;% \n    mutate(\n        # Create additional factor variables\n        publication_year_fct = as.factor(publication_year), \n        type_fct = as.factor(type)\n        )"
  },
  {
    "objectID": "exercises/ms-exercise-08_solution.html#praktische-anwendung",
    "href": "exercises/ms-exercise-08_solution.html#praktische-anwendung",
    "title": "Text processing with R",
    "section": "🛠️ Praktische Anwendung",
    "text": "🛠️ Praktische Anwendung\n\n\n\n\n\n\nAchtung, bitte lesen!\n\n\n\n\nBevor Sie mit der Arbeit an den folgenden 📋 Exercises beginnen, stellen Sie bitte sicher, dass Sie alle Chunks des Abschnitts Preparation gerendert haben. Das können Sie tun, indem Sie den “Run all chunks above”-Knopf des nächsten Chunks benutzen.\nBei Fragen zum Code lohnt sich ein Blick in den Showcase (.qmd oder .html). Beim Showcase handelt es sich um eine kompakte Darstellung des in der Präsentation verwenden R-Codes. Sie können das Showcase also nutzen, um sich die Code-Bausteine anzusehen, die für die R-Outputs auf den Slides benutzt wurden.\n\n\n\n\n📋 Exercise 1: Neues Subsample\n\n\n\n\n\n\nZiel der Aufgabe\n\n\n\n\nErstellung eines neuen Datensatzes review_subsample_new, der sich auf englischsprachig Bücher bzw. Buchrartikel beschränkt.\n\n\n\n\nErstellen Sie einen neuen Datensatz review_subsample_new\n\nBasierend auf dem Datensatzes review_works_correct:\n\nNutzen Sie die filter()-Funktion, um\n\nnur englischsprachige (language),\nBücher und Buchkapitel (type) herauszufiltern.\n\nSpeichern Sie diese Umwandlung in einem neuen Datensatz mit dem Namen review_subsample_new\n\n\nÜberprüfen Sie die Transformation mit Hilfe der glimpse()-Funktion.\n✍️ Notieren Sie, wie viele Artikel im neuen Subsample enthalten sind.\n\n\n\nLösung anzeigen\n# Erstellung Subsample\nreview_subsample_new &lt;- review_works_correct %&gt;% \n  # Eingrenzung: Sprache und Typ\n  filter(language == \"en\") %&gt;% \n  filter(type == \"preprint\")\n\n# Überprüfung\nreview_subsample_new %&gt;% glimpse\n\n\nRows: 3,547\nColumns: 41\n$ id                          &lt;chr&gt; \"https://openalex.org/W4236476849\", \"https…\n$ title                       &lt;chr&gt; \"The PRISMA 2020 statement: an updated gui…\n$ display_name                &lt;chr&gt; \"The PRISMA 2020 statement: an updated gui…\n$ author                      &lt;list&gt; [&lt;data.frame[26 x 12]&gt;], [&lt;data.frame[3 x…\n$ ab                          &lt;chr&gt; \"Background: The Preferred Reporting Items…\n$ publication_date            &lt;chr&gt; \"2020-09-14\", \"2019-07-01\", \"2017-04-01\", …\n$ relevance_score             &lt;dbl&gt; 584.98030, 253.79811, 214.51546, 199.14885…\n$ so                          &lt;chr&gt; NA, \"Technological forecasting & social ch…\n$ so_id                       &lt;chr&gt; NA, \"https://openalex.org/S39307421\", \"htt…\n$ host_organization           &lt;chr&gt; NA, \"Elsevier BV\", \"Elsevier BV\", \"Faculty…\n$ issn_l                      &lt;chr&gt; NA, \"0040-1625\", \"0959-6526\", \"2046-1402\",…\n$ url                         &lt;chr&gt; \"https://doi.org/10.31222/osf.io/v7gm2\", \"…\n$ pdf_url                     &lt;chr&gt; \"https://osf.io/v7gm2/download\", NA, NA, \"…\n$ license                     &lt;chr&gt; NA, NA, NA, \"cc-by\", NA, NA, \"cc-by\", NA, …\n$ version                     &lt;chr&gt; \"submittedVersion\", NA, NA, \"publishedVers…\n$ first_page                  &lt;chr&gt; NA, \"251\", \"1278\", \"588\", \"281\", \"113113\",…\n$ last_page                   &lt;chr&gt; NA, \"269\", \"1302\", \"588\", \"312\", \"113113\",…\n$ volume                      &lt;chr&gt; NA, \"144\", \"149\", \"6\", \"32\", \"125\", \"6\", N…\n$ issue                       &lt;chr&gt; NA, NA, NA, NA, \"3-4\", NA, NA, NA, NA, NA,…\n$ is_oa                       &lt;lgl&gt; TRUE, FALSE, FALSE, TRUE, FALSE, FALSE, TR…\n$ is_oa_anywhere              &lt;lgl&gt; TRUE, FALSE, FALSE, TRUE, FALSE, FALSE, TR…\n$ oa_status                   &lt;chr&gt; \"green\", \"closed\", \"closed\", \"gold\", \"clos…\n$ oa_url                      &lt;chr&gt; \"https://osf.io/v7gm2/download\", NA, NA, \"…\n$ any_repository_has_fulltext &lt;lgl&gt; TRUE, FALSE, FALSE, TRUE, FALSE, FALSE, TR…\n$ language                    &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", …\n$ grants                      &lt;list&gt; NA, NA, &lt;\"https://openalex.org/F432032109…\n$ cited_by_count              &lt;int&gt; 3320, 222, 153, 190, 105, 140, 132, 57, 95…\n$ counts_by_year              &lt;list&gt; [&lt;data.frame[5 x 2]&gt;], [&lt;data.frame[7 x 2…\n$ publication_year            &lt;int&gt; 2020, 2019, 2017, 2017, 2019, 2019, 2017, …\n$ cited_by_api_url            &lt;chr&gt; \"https://api.openalex.org/works?filter=cit…\n$ ids                         &lt;list&gt; &lt;\"https://openalex.org/W4236476849\", \"htt…\n$ doi                         &lt;chr&gt; \"https://doi.org/10.31222/osf.io/v7gm2\", \"…\n$ type                        &lt;chr&gt; \"preprint\", \"preprint\", \"preprint\", \"prepr…\n$ referenced_works            &lt;list&gt; &lt;\"https://openalex.org/W2022190222\", \"htt…\n$ related_works               &lt;list&gt; &lt;\"https://openalex.org/W2921208823\", \"htt…\n$ is_paratext                 &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ is_retracted                &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ concepts                    &lt;list&gt; [&lt;data.frame[20 x 5]&gt;], [&lt;data.frame[20 x…\n$ topics                      &lt;list&gt; [&lt;tbl_df[4 x 5]&gt;], [&lt;tbl_df[12 x 5]&gt;], [&lt;…\n$ publication_year_fct        &lt;fct&gt; 2020, 2019, 2017, 2017, 2019, 2019, 2017, …\n$ type_fct                    &lt;fct&gt; preprint, preprint, preprint, preprint, pr…\n\n\nLösung anzeigen\n# Notiz:\n# Subsample enthält 3547 Einträge\n\n\n\n\n📋 Exercise 2: Umwandlung zu ‘tidy text’\n\nErstellen Sie einen neuen Datensatz subsample_new_tidy,\n\nBasierend auf dem Datensatz review_subsample_new, mit folgenden Schritten:\n\nTokenisierung der Abstracts (ab) mit der Funktion unnest_tokens.\nAusschluss von Stoppwörter mit filter und stopwords$words heraus.\nSpeichern Sie diese Umwandlung, indem Sie einen neuen Datensatz mit dem Namen subsample_new_tidy erstellen.\n\n\nPrüfen Sie, ob die Umwandlung erfolgreich war (z.B. mit der Funktion glimpse())\n✍️ Notieren Sie, wie viele Token im neuen Datensatz subsample_new_tidy enthalten sind.\n\n\n\nLösung anzeigen\n# Erstellung des neuen Datensatzes `subsample_new_tidy`\nsubsample_new_tidy &lt;- review_subsample_new %&gt;% \n  tidytext::unnest_tokens(\"text\", ab) %&gt;% \n   filter(!text %in% tidytext::stop_words$word)\n\n# Überprüfung\nsubsample_new_tidy %&gt;% print()\n\n\n# A tibble: 498,535 × 41\n   id     title display_name author publication_date relevance_score so    so_id\n   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;        &lt;list&gt; &lt;chr&gt;                      &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n 1 https… The … The PRISMA … &lt;df&gt;   2020-09-14                  585. &lt;NA&gt;  &lt;NA&gt; \n 2 https… The … The PRISMA … &lt;df&gt;   2020-09-14                  585. &lt;NA&gt;  &lt;NA&gt; \n 3 https… The … The PRISMA … &lt;df&gt;   2020-09-14                  585. &lt;NA&gt;  &lt;NA&gt; \n 4 https… The … The PRISMA … &lt;df&gt;   2020-09-14                  585. &lt;NA&gt;  &lt;NA&gt; \n 5 https… The … The PRISMA … &lt;df&gt;   2020-09-14                  585. &lt;NA&gt;  &lt;NA&gt; \n 6 https… The … The PRISMA … &lt;df&gt;   2020-09-14                  585. &lt;NA&gt;  &lt;NA&gt; \n 7 https… The … The PRISMA … &lt;df&gt;   2020-09-14                  585. &lt;NA&gt;  &lt;NA&gt; \n 8 https… The … The PRISMA … &lt;df&gt;   2020-09-14                  585. &lt;NA&gt;  &lt;NA&gt; \n 9 https… The … The PRISMA … &lt;df&gt;   2020-09-14                  585. &lt;NA&gt;  &lt;NA&gt; \n10 https… The … The PRISMA … &lt;df&gt;   2020-09-14                  585. &lt;NA&gt;  &lt;NA&gt; \n# ℹ 498,525 more rows\n# ℹ 33 more variables: host_organization &lt;chr&gt;, issn_l &lt;chr&gt;, url &lt;chr&gt;,\n#   pdf_url &lt;chr&gt;, license &lt;chr&gt;, version &lt;chr&gt;, first_page &lt;chr&gt;,\n#   last_page &lt;chr&gt;, volume &lt;chr&gt;, issue &lt;chr&gt;, is_oa &lt;lgl&gt;,\n#   is_oa_anywhere &lt;lgl&gt;, oa_status &lt;chr&gt;, oa_url &lt;chr&gt;,\n#   any_repository_has_fulltext &lt;lgl&gt;, language &lt;chr&gt;, grants &lt;list&gt;,\n#   cited_by_count &lt;int&gt;, counts_by_year &lt;list&gt;, publication_year &lt;int&gt;, …\n\n\nLösung anzeigen\n# Notiz:\n# Der neue Datensatz enthält 498535 Token. \n\n\n\n\n📋 Exercise 3: Auswertung der Token\n\nErstellen Sie einen neuen Datensatz subsample_new_summarized,\n\nFassen Sie auf der Grundlage des Datensatzes subsample_new_tidy die Häufigkeit der einzelnen Token zusammen, indem Sie die Funktion count() auf die Variable text anwenden. Verwenden Sie das Argument sort = TRUE, um den Datensatz nach absteigender Häufigkeit der Token zu sortieren.\nSpeichern Sie diese Transformation, indem Sie einen neuen Datensatz mit dem Namen subsample_new_summarized erstellen.\n\nPrüfen Sie, ob die Umwandlung erfolgreich war, indem Sie die Funktion print() verwenden.\n\nVerwenden Sie das Argument n = 50, um die 50 wichtigsten Token anzuzeigen (nur möglich, wenn das Argument sort = TRUE bei der Ausführung der Funktion count() verwendet wurde)\n\nVerteilung der Token prüfen\n\nVerwenden Sie die Funktion datawizard::describe_distribution(), um verschiedene Verteilungsparameter des neuen Datensatzes zu überprüfen\n✍️ Notieren Sie, wie viele Token ein Abstract durchschnittlich enthält.\n\n\n\nOptional: Ergebnisse mit einer Wortwolke überprüfen\n\nBasierend auf dem sortierten Datensatz subsample_new_summarized\n\nAuswahl der 50 häufigsten Token mit Hilfe der Funktion top_n()\nErstellen Sie eine ggplot()-Basis mit label = text und size = n als aes() und\nBenutze ggwordcloud::geom_text_wordclout() um die Wortwolke zu erstellen.\nVerwenden Sie scale_size_are(), um die Skalierung der Wortwolke zu übernehmen.\nVerwenden Sie theme_minimal() für eine saubere Visualisierung.\n\n\n\n\n\nLösung anzeigen\n# Erstellung des neuen Datensatzes `subsample_new_summmarized`\nsubsample_new_summmarized &lt;- subsample_new_tidy %&gt;% \n  count(text, sort = TRUE) \n\n# Preview Top 50 token\nsubsample_new_summmarized %&gt;% \n    print(n = 50)\n\n\n# A tibble: 22,001 × 2\n   text              n\n   &lt;chr&gt;         &lt;int&gt;\n 1 studies        7115\n 2 review         5857\n 3 title          5223\n 4 sec            5133\n 5 health         4591\n 6 systematic     4197\n 7 research       3597\n 8 results        2981\n 9 study          2949\n10 literature     2881\n11 data           2764\n12 analysis       2600\n13 interventions  2532\n14 included       2491\n15 methods        2469\n16 evidence       2229\n17 meta           2122\n18 quality        2069\n19 based          2017\n20 mental         1998\n21 reviews        1812\n22 social         1752\n23 articles       1734\n24 risk           1653\n25 search         1540\n26 outcomes       1515\n27 conducted      1498\n28 lt             1481\n29 background     1434\n30 19             1426\n31 identified     1420\n32 factors        1307\n33 effects        1300\n34 covid          1279\n35 findings       1254\n36 care           1248\n37 95             1247\n38 gt             1245\n39 published      1241\n40 related        1234\n41 abstract       1214\n42 databases      1211\n43 reported       1191\n44 2              1138\n45 conclusions    1133\n46 effect         1116\n47 ci             1111\n48 intervention   1110\n49 criteria       1077\n50 support        1065\n# ℹ 21,951 more rows\n\n\nLösung anzeigen\n# Check distribution parameters \nsubsample_new_summmarized %&gt;%\n  datawizard::describe_distribution()\n\n\nVariable |  Mean |     SD | IQR |           Range | Skewness | Kurtosis |     n | n_Missing\n-------------------------------------------------------------------------------------------\nn        | 22.66 | 136.91 |   7 | [1.00, 7115.00] |    24.15 |   858.87 | 22001 |         0\n\n\nLösung anzeigen\n# Notiz:\n# Ein Absatz enthält durchschnittlich 22 Token. \n\n# Optional: Check results with a wordcloud\nsubsample_new_summmarized %&gt;% \n    top_n(50) %&gt;% \n    ggplot(aes(label = text, size = n)) +\n    ggwordcloud::geom_text_wordcloud() +\n    scale_size_area(max_size = 15) +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n📋 Exercise 4: Wortbeziehungen im Fokus\n\n4.1 Couting word pairs\n\nZählen von häufigen Wortpaaren\n\nZählen Sie auf der Grundlage des Datensatzes subsample_new_tidy Wortpaare mit widyr::pairwise_count(), mit den Argumenten item = text, feature = id und sort = TRUE.\nSpeichern Sie diese Transformation, indem Sie einen neuen Datensatz mit dem Namen subsample_new_word_pairs erstellen.\n\nPrüfen Sie, ob die Umwandlung erfolgreich war, indem Sie die Funktion print() verwenden.\n\nVerwenden Sie das Argument n = 50, um die 50 wichtigsten Token anzuzeigen (nur möglich, wenn bei der Ausführung der Funktion count() das Argument sort = TRUE verwendet wurde)\n\n\n\n\nLösung anzeigen\n# Couting word pairs among sections\nsubsample_new_word_pairs &lt;- subsample_new_tidy %&gt;% \n  widyr::pairwise_count(\n    item = text,\n    feature = id,\n    sort = TRUE)\n\n# Check \nsubsample_new_word_pairs %&gt;% print(n = 50)\n\n\n# A tibble: 12,662,794 × 3\n   item1      item2          n\n   &lt;chr&gt;      &lt;chr&gt;      &lt;dbl&gt;\n 1 review     systematic  2068\n 2 systematic review      2068\n 3 studies    review      1909\n 4 review     studies     1909\n 5 studies    systematic  1668\n 6 results    review      1668\n 7 review     results     1668\n 8 systematic studies     1668\n 9 studies    results     1502\n10 results    studies     1502\n11 research   review      1483\n12 review     research    1483\n13 results    systematic  1478\n14 systematic results     1478\n15 literature review      1471\n16 review     literature  1471\n17 methods    review      1453\n18 review     methods     1453\n19 methods    results     1387\n20 results    methods     1387\n21 study      review      1336\n22 review     study       1336\n23 methods    systematic  1320\n24 systematic methods     1320\n25 studies    methods     1306\n26 methods    studies     1306\n27 included   review      1236\n28 review     included    1236\n29 review     background  1227\n30 background review      1227\n31 methods    background  1224\n32 background methods     1224\n33 research   systematic  1216\n34 systematic research    1216\n35 results    background  1204\n36 background results     1204\n37 included   studies     1198\n38 studies    included    1198\n39 study      studies     1197\n40 studies    study       1197\n41 study      systematic  1195\n42 systematic study       1195\n43 research   studies     1178\n44 studies    research    1178\n45 health     review      1153\n46 review     health      1153\n47 analysis   review      1152\n48 review     analysis    1152\n49 studies    background  1148\n50 background studies     1148\n# ℹ 12,662,744 more rows\n\n\n\n\n4.2 Pairwise correlation\n\nErmittlung der paarweisen Korrelation\n\nBasierend auf dem Datensatz subsample_new_tidy,\ngruppieren Sie die Daten mit der Funktion group_by() nach der Variable text und\nverwenden Sie filter(n() &gt;= X), um nur Token zu verwenden, die mindestens in einer bestimmte Anzahl (X) vorkommen; Sie können für X einen Wert Ihrer Wahl wählen, ich würde jedoch dringend empfehlen, ein X &gt; 100 zu wählen, da die folgende Funktion sonst möglicherweise nicht in der Lage ist, die Berechnung durchzuführen.\nErstellen Sie Wortkorrelationen mit widyr::pairwise_cor(), mit den Argumenten item = text,feature = id und sort = TRUE.\nSpeichern Sie diese Transformation, indem Sie einen neuen Datensatz mit dem Namen subsample_new_corr erstellen.\n\nPrüfen Sie die Paare mit der höchsten Korrelation mit der Funktion print()..\n\n\n\nLösung anzeigen\n# Getting pairwise correlation \nsubsample_new_corr &lt;- subsample_new_tidy %&gt;% \n  group_by(text) %&gt;% \n  filter(n() &gt;= 250) %&gt;% \n  pairwise_cor(text, id, sort = TRUE)\n\n# Check pairs with highest correlation\nsubsample_new_corr %&gt;% print(n = 50)\n\n\n# A tibble: 131,406 × 3\n   item1        item2        correlation\n   &lt;chr&gt;        &lt;chr&gt;              &lt;dbl&gt;\n 1 sec          title              0.935\n 2 title        sec                0.935\n 3 ci           95                 0.881\n 4 95           ci                 0.881\n 5 19           covid              0.844\n 6 covid        19                 0.844\n 7 items        preferred          0.835\n 8 preferred    items              0.835\n 9 pandemic     covid              0.812\n10 covid        pandemic           0.812\n11 vaccination  vaccine            0.796\n12 vaccine      vaccination        0.796\n13 trials       controlled         0.763\n14 controlled   trials             0.763\n15 web          science            0.755\n16 science      web                0.755\n17 sec          objective          0.715\n18 objective    sec                0.715\n19 srs          sr                 0.710\n20 sr           srs                0.710\n21 randomized   controlled         0.708\n22 controlled   randomized         0.708\n23 pandemic     19                 0.695\n24 19           pandemic           0.695\n25 objective    title              0.669\n26 title        objective          0.669\n27 gt           lt                 0.662\n28 lt           gt                 0.662\n29 trials       randomized         0.658\n30 randomized   trials             0.658\n31 depression   anxiety            0.650\n32 anxiety      depression         0.650\n33 bold         ns4                0.636\n34 ns4          bold               0.636\n35 methods      background         0.613\n36 background   methods            0.613\n37 inclusion    criteria           0.607\n38 criteria     inclusion          0.607\n39 reporting    preferred          0.599\n40 preferred    reporting          0.599\n41 items        reporting          0.599\n42 reporting    items              0.599\n43 peer         reviewed           0.592\n44 reviewed     peer               0.592\n45 1            2                  0.585\n46 2            1                  0.585\n47 sec          conclusions        0.577\n48 conclusions  sec                0.577\n49 registration prospero           0.574\n50 prospero     registration       0.574\n# ℹ 131,356 more rows\n\n\n\n\n\n📋 Exercise 5: Inhaltlicher Vergleich\n\nVergleichen Sie die Ergebnisse der Übung mit den Auswertungen der Folien:\n\nWie unterscheiden sich die Ergebnisse?\nWürden Sie die Bücher bzw. Buchabschnitte mit in die Untersuchung integrieren?"
  },
  {
    "objectID": "exercises/ms-exercise-08.html",
    "href": "exercises/ms-exercise-08.html",
    "title": "Text processing with R",
    "section": "",
    "text": "Link to source file"
  },
  {
    "objectID": "exercises/ms-exercise-08.html#background",
    "href": "exercises/ms-exercise-08.html#background",
    "title": "Text processing with R",
    "section": "Background",
    "text": "Background\n\n\n\n\n\n\nTodays’s data basis: OpenAlex\n\n\n\n\nVia API bzw. openalexR (Aria et al. 2024) gesammelte “works” der Datenbank OpenAlex mit Bezug zu Literaturriews in den Sozialwissenschaften zwischen 2013 und 2023\nDetaillierte Informationen und Ergebnisse zur Suchquery finden Sie hier."
  },
  {
    "objectID": "exercises/ms-exercise-08.html#preparation",
    "href": "exercises/ms-exercise-08.html#preparation",
    "title": "Text processing with R",
    "section": "Preparation",
    "text": "Preparation\n\n\n\n\n\n\nWichtige Information\n\n\n\n\nBitte stellen Sie sicher, dass Sie das jeweilige R-Studio Projekt zur Übung geöffnet haben. Nur so funktionieren alle Dependencies korrekt.\nUm den einwandfreien Ablauf der Übung zu gewährleisten, wird für die Aufgaben auf eine eigenständige Datenerhebung verzichtet und ein Übungsdatensatz zu verfügung gestelt.\n\n\n\n\nPackages\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n    here, qs, # file management\n    magrittr, janitor, # data wrangling\n    easystats, sjmisc, # data analysis\n    gt, gtExtras, # table visualization\n    ggpubr, ggwordcloud, # visualization\n    tidytext, widyr, # text analysis    \n    openalexR, \n    tidyverse # load last to avoid masking issues\n  )\n\n\n\nImport und Vorverarbeitung der Daten\n\n# Import from local\nreview_works &lt;- qs::qread(here(\"data/session-07/openalex-review_works-2013_2023.qs\"))\n\n# Create correct data\nreview_works_correct &lt;- review_works %&gt;% \n    mutate(\n        # Create additional factor variables\n        publication_year_fct = as.factor(publication_year), \n        type_fct = as.factor(type)\n        )"
  },
  {
    "objectID": "exercises/ms-exercise-08.html#praktische-anwendung",
    "href": "exercises/ms-exercise-08.html#praktische-anwendung",
    "title": "Text processing with R",
    "section": "🛠️ Praktische Anwendung",
    "text": "🛠️ Praktische Anwendung\n\n\n\n\n\n\nAchtung, bitte lesen!\n\n\n\n\nBevor Sie mit der Arbeit an den folgenden 📋 Exercises beginnen, stellen Sie bitte sicher, dass Sie alle Chunks des Abschnitts Preparation gerendert haben. Das können Sie tun, indem Sie den “Run all chunks above”-Knopf des nächsten Chunks benutzen.\nBei Fragen zum Code lohnt sich ein Blick in den Showcase (.qmd oder .html). Beim Showcase handelt es sich um eine kompakte Darstellung des in der Präsentation verwenden R-Codes. Sie können das Showcase also nutzen, um sich die Code-Bausteine anzusehen, die für die R-Outputs auf den Slides benutzt wurden.\n\n\n\n\n📋 Exercise 1: Neues Subsample\n\n\n\n\n\n\nZiel der Aufgabe\n\n\n\n\nErstellung eines neuen Datensatzes review_subsample_new, der sich auf englischsprachig Bücher bzw. Buchrartikel beschränkt.\n\n\n\n\nErstellen Sie einen neuen Datensatz review_subsample_new\n\nBasierend auf dem Datensatzes review_works_correct:\n\nNutzen Sie die filter()-Funktion, um\n\nnur englischsprachige (language),\nBücher und Buchkapitel (type) herauszufiltern.\n\nSpeichern Sie diese Umwandlung in einem neuen Datensatz mit dem Namen review_subsample_new\n\n\nÜberprüfen Sie die Transformation mit Hilfe der glimpse()-Funktion.\n✍️ Notieren Sie, wie viele Artikel im neuen Subsample enthalten sind.\n\n\n# Erstellung Subsample\n\n# Überprüfung\n\n# Notiz:\n# Subsample enthält ... Einträge\n\n\n\n📋 Exercise 2: Umwandlung zu ‘tidy text’\n\nErstellen Sie einen neuen Datensatz subsample_new_tidy,\n\nBasierend auf dem Datensatz review_subsample_new, mit folgenden Schritten:\n\nTokenisierung der Abstracts (ab) mit der Funktion unnest_tokens.\nAusschluss von Stoppwörter mit filter und stopwords$words heraus.\nSpeichern Sie diese Umwandlung, indem Sie einen neuen Datensatz mit dem Namen subsample_new_tidy erstellen.\n\n\nPrüfen Sie, ob die Umwandlung erfolgreich war (z.B. mit der Funktion glimpse())\n✍️ Notieren Sie, wie viele Token im neuen Datensatz subsample_new_tidy enthalten sind.\n\n\n# Erstellung des neuen Datensatzes `subsample_new_tidy`\n\n# Überprüfung\n\n# Notiz:\n# Der neue Datensatz enthält ... Token. \n\n\n\n📋 Exercise 3: Auswertung der Token\n\nErstellen Sie einen neuen Datensatz subsample_new_summarized,\n\nFassen Sie auf der Grundlage des Datensatzes subsample_new_tidy die Häufigkeit der einzelnen Token zusammen, indem Sie die Funktion count() auf die Variable text anwenden. Verwenden Sie das Argument sort = TRUE, um den Datensatz nach absteigender Häufigkeit der Token zu sortieren.\nSpeichern Sie diese Transformation, indem Sie einen neuen Datensatz mit dem Namen subsample_new_summarized erstellen.\n\nPrüfen Sie, ob die Umwandlung erfolgreich war, indem Sie die Funktion print() verwenden.\n\nVerwenden Sie das Argument n = 50, um die 50 wichtigsten Token anzuzeigen (nur möglich, wenn das Argument sort = TRUE bei der Ausführung der Funktion count() verwendet wurde)\n\nVerteilung der Token prüfen\n\nVerwenden Sie die Funktion datawizard::describe_distribution(), um verschiedene Verteilungsparameter des neuen Datensatzes zu überprüfen\n✍️ Notieren Sie, wie viele Token ein Abstract durchschnittlich enthält.\n\n\n\nOptional: Ergebnisse mit einer Wortwolke überprüfen\n\nBasierend auf dem sortierten Datensatz subsample_new_summarized\n\nAuswahl der 50 häufigsten Token mit Hilfe der Funktion top_n()\nErstellen Sie eine ggplot()-Basis mit label = text und size = n als aes() und\nBenutze ggwordcloud::geom_text_wordclout() um die Wortwolke zu erstellen.\nVerwenden Sie scale_size_are(), um die Skalierung der Wortwolke zu übernehmen.\nVerwenden Sie theme_minimal() für eine saubere Visualisierung.\n\n\n\n\n# Erstellung des neuen Datensatzes `subsample_new_summmarized`\n\n# Preview Top 50 token\n\n# Check distribution parameters \n\n# Notiz:\n# Ein Absatz enthält durchschnittlich ... Token. \n\n\n# Optional: Check results with a wordcloud\n\n\n\n📋 Exercise 4: Wortbeziehungen im Fokus\n\n4.1 Couting word pairs\n\nZählen von häufigen Wortpaaren\n\nZählen Sie auf der Grundlage des Datensatzes subsample_new_tidy Wortpaare mit widyr::pairwise_count(), mit den Argumenten item = text, feature = id und sort = TRUE.\nSpeichern Sie diese Transformation, indem Sie einen neuen Datensatz mit dem Namen subsample_new_word_pairs erstellen.\n\nPrüfen Sie, ob die Umwandlung erfolgreich war, indem Sie die Funktion print() verwenden.\n\nVerwenden Sie das Argument n = 50, um die 50 wichtigsten Token anzuzeigen (nur möglich, wenn bei der Ausführung der Funktion count() das Argument sort = TRUE verwendet wurde)\n\n\n\n# Couting word pairs among sections\n\n# Überprüfung \n\n\n\n4.2 Pairwise correlation\n\nErmittlung der paarweisen Korrelation\n\nBasierend auf dem Datensatz subsample_new_tidy,\ngruppieren Sie die Daten mit der Funktion group_by() nach der Variable text und\nverwenden Sie filter(n() &gt;= X), um nur Token zu verwenden, die mindestens in einer bestimmte Anzahl (X) vorkommen; Sie können für X einen Wert Ihrer Wahl wählen, ich würde jedoch dringend empfehlen, ein X &gt; 100 zu wählen, da die folgende Funktion sonst möglicherweise nicht in der Lage ist, die Berechnung durchzuführen.\nErstellen Sie Wortkorrelationen mit widyr::pairwise_cor(), mit den Argumenten item = text,feature = id und sort = TRUE.\nSpeichern Sie diese Transformation, indem Sie einen neuen Datensatz mit dem Namen subsample_new_corr erstellen.\n\nPrüfen Sie die Paare mit der höchsten Korrelation mit der Funktion print()..\n\n\n# Getting pairwise correlation \n\n# Check pairs with highest correlation\n\n\n\n\n📋 Exercise 5: Inhaltlicher Vergleich\n\nVergleichen Sie die Ergebnisse der Übung mit den Auswertungen der Folien:\n\nWie unterscheiden sich die Ergebnisse?\nWürden Sie die Bücher bzw. Buchabschnitte mit in die Untersuchung integrieren?"
  },
  {
    "objectID": "exercises/ms-exercise-07.html",
    "href": "exercises/ms-exercise-07.html",
    "title": "API mining and data wrangling with R",
    "section": "",
    "text": "Link to source file"
  },
  {
    "objectID": "exercises/ms-exercise-07.html#background",
    "href": "exercises/ms-exercise-07.html#background",
    "title": "API mining and data wrangling with R",
    "section": "Background",
    "text": "Background\n\n\n\n\n\n\nTodays’s data basis: OpenAlex\n\n\n\nOpenAlex is a free and open catalog of the global research system. It’s named after the ancient Library of Alexandria and made by the nonprofit OurResearch.\n\n\n\n\n\n\n\n\nAt the heart of OpenAlex is our dataset—a catalog of works. A work is any sort of scholarly output. A research article is one kind of work, but there are others such as datasets, books, and dissertations. We keep track of these works—their titles (and abstracts and full text in many cases), when they were created, etc. But that’s not all we do. We also keep track of the connections between these works, finding associations through things like journals, authors, institutional affiliations, citations, topics, and funders. There are hundreds of millions of works out there, and tens of thousands more being created every day, so it’s important that we have these relationships to help us make sense of research at a large scale."
  },
  {
    "objectID": "exercises/ms-exercise-07.html#preparation",
    "href": "exercises/ms-exercise-07.html#preparation",
    "title": "API mining and data wrangling with R",
    "section": "Preparation",
    "text": "Preparation\n\n\n\n\n\n\nWichtige Information\n\n\n\n\nBitte stellen Sie sicher, dass Sie das jeweilige R-Studio Projekt zur Übung geöffnet haben. Nur so funktionieren alle Dependencies korrekt.\nUm den einwandfreien Ablauf der Übung zu gewährleisten, wird für die Aufgaben auf eine eigenständige Datenerhebung verzichtet und ein Übungsdatensatz zu verfügung gestelt.\n\n\n\n\nPackages\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  here, qs, # file management\n  magrittr, janitor, # data wrangling\n  easystats, sjmisc, # data analysis\n  ggpubr, # visualization\n  openalexR, \n  tidyverse # load last to avoid masking issues\n  )\n\n\n\nImport und Vorverarbeitung der Daten\n\n# Import from local\nreview_works &lt;- qs::qread(here(\"data/session-07/openalex-review_works-2013_2023.qs\"))\n\n# Create correct data\nreview_works_correct &lt;- review_works %&gt;% \n    mutate(\n        # Create additional factor variables\n        publication_year_fct = as.factor(publication_year), \n        type_fct = as.factor(type)\n        )"
  },
  {
    "objectID": "exercises/ms-exercise-07.html#praktische-anwendung",
    "href": "exercises/ms-exercise-07.html#praktische-anwendung",
    "title": "API mining and data wrangling with R",
    "section": "🛠️ Praktische Anwendung",
    "text": "🛠️ Praktische Anwendung\n\n\n\n\n\n\nAchtung, bitte lesen!\n\n\n\n\nBevor Sie mit der Arbeit an den folgenden 📋 Exercises beginnen, stellen Sie bitte sicher, dass Sie alle Chunks des Abschnitts Preparation gerendert haben. Das können Sie tun, indem Sie den “Run all chunks above”-Knopf des nächsten Chunks benutzen.\nBei Fragen zum Code lohnt sich ein Blick in den Showcase (.qmd oder .html). Beim Showcase handelt es sich um eine kompakte Darstellung des in der Präsentation verwenden R-Codes. Sie können das Showcase also nutzen, um sich die Code-Bausteine anzusehen, die für die R-Outputs auf den Slides benutzt wurden.\n\n\n\n\n📋 Exercise 1: Sprache der Publikationen\n\n\n\n\n\n\nZiel der Aufgabe\n\n\n\n\nIdentifizieren Sie die für die Untersuchung relevanten Artikel auf Basis von deren Sprache (language)\nHintergrundinformation zur Variable language finden Sie in der API-Dokumentation von OpenAlex.\n\n\n\n\nNutzen Sie die Funktion sjmisc::frq() und schauen Sie sich im Datensatz review_works_correct die Variablen language an. Verwenden Sie das Argument sort.frq = \"desc\", um die Häufigkeit der Sprachen absteigend zu sortieren.\nNotieren Sie sich den jeweilgen ISO 639-1 language code, um Ihn später bei 📋 Exercise 4: Erstellung Subsample als Filter zu nutzen.\n\n\n# Create frequency table for the variable language\n...\n\n\n\n📋 Exercise 2: Typ der Publikationen\n\n\n\n\n\n\nZiel der Aufgabe\n\n\n\n\nIdentifizieren Sie die für die Untersuchung relevanten Artikel auf Basis deres Typen (type).\nHintergrundinformation zur Variable type finden Sie in der API-Dokumentation von OpenAlex.\n\n\n\n\nNutzen Sie die Funktion sjmisc::frq() und schauen Sie sich im Datensatz review_works_correct die Variablen type an. Verwenden Sie das Argument sort.frq = \"desc\", um die Typen in Abhängigkeit Ihrer Häufigkeit absteigend zu sortieren.\nNotieren Sie sich die Ausprägungen der Variable type, die aus Ihrer Sicht später bei 📋 Exercise 4: Erstellung Subsample als Filter genutzt werden soll.\n\n\n# Create frequency table for the variable type\n...\n\n\n\n📋 Exercise 3: Forschungsfeld der Publikationen\n\n\n\n\n\n\nZiel der Aufgabe\n\n\n\n\nIdentifizieren Sie die für die Untersuchung relevanten Artikel auf Basis des von OpenAlex dem Artikel zugeordnenten Foschungsfeldes (field)\nHintergrundinformation zur Variable field finden Sie in der API-Dokumentation von OpenAlex.\n\n\n\n\nBasierend auf dem Datensatz review_works_correct\n\nnutzen Sie die Funktion unnest() um die Variablen der topics-Liste zu extrahieren. Verwenden Sie dabei das Argument names_sep = \"_\". um doppelte Variablennamen durch Hinzufügen des Prefixes topics_ zu verhindern.\nfiltern Sie anschließen mit Hilfe der Funktion filter und der Variable bzw. dem Argument topics_name == \"field\" nur die Informationen zum Forschungsfeld, sowie mit der Variable bzw. dem Argument topics_i == \"1\" nur die erste Zuordnung.\nNutzen Sie die Funktion sjmisc::frq() und schauen Sie sich die Variablen topics_display_name an. Verwenden Sie das Argument sort.frq = \"desc\", um die Forschungsfelder in Abhängigkeit Ihrer Häufigkeit absteigend zu sortieren.\n\nNotieren Sie sich die Ausprägungen der Variable topics_display_name, die aus Ihrer Sicht später bei 📋 Exercise 4: Erstellung Subsample als Filter genutzt werden soll.\n\n\n# Unnest topis variable and create frequency table for the variable topics_display_name\n...\n\n\n\n📋 Exercise 4: Erstellung Subsample\n\n\n\n\n\n\nZiel der Aufgabe\n\n\n\n\nErstellung sie den Datensatz review_subsample, in dem Sie mit Hilfe der Funktionen select() und/oder filter() das Datenmaterial weiter eingrenzen. Sie können sich sowohl auf die Variablen aus der Übung, als auch auf die aus der Sitzung (bzw. den Slides) beziehen.\nDer Code dieses Chunks wird in der nächsten Sitzung benötigt bzw. besprochen, halten Sie diesen deshalb bitte bereit.\n\n\n\n\nreview_subsample &lt;- review_works_correct %&gt;% \n    filter(...)"
  },
  {
    "objectID": "exercises/ms-exercise-07_solution.html",
    "href": "exercises/ms-exercise-07_solution.html",
    "title": "API mining and data wrangling with R",
    "section": "",
    "text": "Link to source file"
  },
  {
    "objectID": "exercises/ms-exercise-07_solution.html#background",
    "href": "exercises/ms-exercise-07_solution.html#background",
    "title": "API mining and data wrangling with R",
    "section": "Background",
    "text": "Background\n\n\n\n\n\n\nTodays’s data basis: OpenAlex\n\n\n\nOpenAlex is a free and open catalog of the global research system. It’s named after the ancient Library of Alexandria and made by the nonprofit OurResearch.\n\n\n\n\n\n\n\n\nAt the heart of OpenAlex is our dataset—a catalog of works. A work is any sort of scholarly output. A research article is one kind of work, but there are others such as datasets, books, and dissertations. We keep track of these works—their titles (and abstracts and full text in many cases), when they were created, etc. But that’s not all we do. We also keep track of the connections between these works, finding associations through things like journals, authors, institutional affiliations, citations, topics, and funders. There are hundreds of millions of works out there, and tens of thousands more being created every day, so it’s important that we have these relationships to help us make sense of research at a large scale."
  },
  {
    "objectID": "exercises/ms-exercise-07_solution.html#preparation",
    "href": "exercises/ms-exercise-07_solution.html#preparation",
    "title": "API mining and data wrangling with R",
    "section": "Preparation",
    "text": "Preparation\n\n\n\n\n\n\nWichtige Information\n\n\n\n\nBitte stellen Sie sicher, dass Sie das jeweilige R-Studio Projekt zur Übung geöffnet haben. Nur so funktionieren alle Dependencies korrekt.\nUm den einwandfreien Ablauf der Übung zu gewährleisten, wird für die Aufgaben auf eine eigenständige Datenerhebung verzichtet und ein Übungsdatensatz zu verfügung gestelt.\n\n\n\n\nPackages\n\nZum Laden der Pakete wird das Paket pacman::pload() genutzt, dass gegenüber der herkömmlichen Methode mit library() eine Reihe an Vorteile hat:\n\nPrägnante Syntax\nAutomatische Installation (wenn Paket noch nicht vorhanden)\nLaden mehrerer Pakete auf einmal\nAutomatische Suche nach dependencies\n\n\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  here, qs, # file management\n  magrittr, janitor, # data wrangling\n  easystats, sjmisc, # data analysis\n  ggpubr, # visualization\n  openalexR, \n  tidyverse # load last to avoid masking issues\n  )\n\n\n\nImport und Vorverarbeitung der Daten\n\n# Import from local\nreview_works &lt;- qs::qread(here(\"data/session-07/openalex-review_works-2013_2023.qs\"))\n\n# Create correct data\nreview_works_correct &lt;- review_works %&gt;% \n    mutate(\n        # Create additional factor variables\n        publication_year_fct = as.factor(publication_year), \n        type_fct = as.factor(type)\n        )"
  },
  {
    "objectID": "exercises/ms-exercise-07_solution.html#praktische-anwendung",
    "href": "exercises/ms-exercise-07_solution.html#praktische-anwendung",
    "title": "API mining and data wrangling with R",
    "section": "🛠️ Praktische Anwendung",
    "text": "🛠️ Praktische Anwendung\n\n\n\n\n\n\nAchtung, bitte lesen!\n\n\n\n\nBevor Sie mit der Arbeit an den folgenden 📋 Exercises beginnen, stellen Sie bitte sicher, dass Sie alle Chunks des Abschnitts Preparation gerendert haben. Das können Sie tun, indem Sie den “Run all chunks above”-Knopf  des nächsten Chunks benutzen.\nBei Fragen zum Code lohnt sich ein Blick in den Showcase (.qmd oder .html). Beim Showcase handelt es sich um eine kompakte Darstellung des in der Präsentation verwenden R-Codes. Sie können das Showcase also nutzen, um sich die Code-Bausteine anzusehen, die für die R-Outputs auf den Slides benutzt wurden.\n\n\n\n\n📋 Exercise 1: Sprache der Publikationen\n\n\n\n\n\n\nZiel der Aufgabe\n\n\n\n\nIdentifizieren Sie die für die Untersuchung relevanten Artikel auf Basis von deren Sprache (language)\nHintergrundinformation zur Variable language finden Sie in der API-Dokumentation von OpenAlex.\n\n\n\n\nNutzen Sie die Funktion sjmisc::frq() und schauen Sie sich im Datensatz review_works_correct die Variablen language an. Verwenden Sie das Argument sort.frq = \"desc\", um die Häufigkeit der Sprachen absteigend zu sortieren.\nNotieren Sie sich den jeweilgen ISO 639-1 language code, um Ihn später bei 📋 Exercise 4: Erstellung Subsample als Filter zu nutzen.\n\n\n\nLösung anzeigen\nreview_works_correct %&gt;% \n    sjmisc::frq(\"language\", sort.frq = \"desc\")\n\n\nlanguage &lt;character&gt; \n# total N=93655 valid N=93476 mean=10.33 sd=2.27\n\nValue |     N | Raw % | Valid % | Cum. %\n----------------------------------------\nen    | 90653 | 96.79 |   96.98 |  96.98\nid    |  1118 |  1.19 |    1.20 |  98.18\npt    |   521 |  0.56 |    0.56 |  98.73\nes    |   309 |  0.33 |    0.33 |  99.06\ntr    |   182 |  0.19 |    0.19 |  99.26\nko    |   130 |  0.14 |    0.14 |  99.40\nfr    |   127 |  0.14 |    0.14 |  99.53\nru    |    72 |  0.08 |    0.08 |  99.61\nde    |    59 |  0.06 |    0.06 |  99.67\nit    |    43 |  0.05 |    0.05 |  99.72\npl    |    36 |  0.04 |    0.04 |  99.76\nfa    |    20 |  0.02 |    0.02 |  99.78\nro    |    20 |  0.02 |    0.02 |  99.80\nja    |    19 |  0.02 |    0.02 |  99.82\nca    |    16 |  0.02 |    0.02 |  99.84\nuk    |    16 |  0.02 |    0.02 |  99.86\nar    |    15 |  0.02 |    0.02 |  99.87\nth    |    14 |  0.01 |    0.01 |  99.89\nnl    |    13 |  0.01 |    0.01 |  99.90\nsv    |    13 |  0.01 |    0.01 |  99.91\nhr    |    11 |  0.01 |    0.01 |  99.93\nhu    |    11 |  0.01 |    0.01 |  99.94\nsl    |    10 |  0.01 |    0.01 |  99.95\naf    |     8 |  0.01 |    0.01 |  99.96\ncs    |     6 |  0.01 |    0.01 |  99.96\nda    |     5 |  0.01 |    0.01 |  99.97\nel    |     5 |  0.01 |    0.01 |  99.97\nbg    |     4 |  0.00 |    0.00 |  99.98\nzh-cn |     4 |  0.00 |    0.00 |  99.98\net    |     3 |  0.00 |    0.00 |  99.99\nlt    |     3 |  0.00 |    0.00 |  99.99\nfi    |     2 |  0.00 |    0.00 |  99.99\nmk    |     2 |  0.00 |    0.00 |  99.99\nno    |     2 |  0.00 |    0.00 | 100.00\nta    |     2 |  0.00 |    0.00 | 100.00\ncy    |     1 |  0.00 |    0.00 | 100.00\nhi    |     1 |  0.00 |    0.00 | 100.00\n&lt;NA&gt;  |   179 |  0.19 |    &lt;NA&gt; |   &lt;NA&gt;\n\n\nLösung anzeigen\n# Notiz(en)\n# - Überwiegende Anzahl der heruntergeladenen Datenbankbeiträge ist in englischer Sprache (en) verfasst.\n# -Die Sprache en wird als Filterkriterium für die Erstellung des Subsamples genutzt, auch um spätere Probleme bei der Analyse durch multi-linguale Texte zu vermeiden.\n\n\n\n\n📋 Exercise 2: Typ der Publikationen\n\n\n\n\n\n\nZiel der Aufgabe\n\n\n\n\nIdentifizieren Sie die für die Untersuchung relevanten Artikel auf Basis deres Typen (type).\nHintergrundinformation zur Variable type finden Sie in der API-Dokumentation von OpenAlex.\n\n\n\n\nNutzen Sie die Funktion sjmisc::frq() und schauen Sie sich im Datensatz review_works_correct die Variablen type an. Verwenden Sie das Argument sort.frq = \"desc\", um die Typen in Abhängigkeit Ihrer Häufigkeit absteigend zu sortieren.\nNotieren Sie sich die Ausprägungen der Variable type, die aus Ihrer Sicht später bei 📋 Exercise 4: Erstellung Subsample als Filter genutzt werden soll.\n\n\n\nLösung anzeigen\nreview_works_correct %&gt;% \n    sjmisc::frq(\"type\", sort.frq = \"desc\")\n\n\ntype &lt;character&gt; \n# total N=93655 valid N=93655 mean=3.09 sd=4.64\n\nValue                   |     N | Raw % | Valid % | Cum. %\n----------------------------------------------------------\narticle                 | 73716 | 78.71 |   78.71 |  78.71\nreview                  |  6437 |  6.87 |    6.87 |  85.58\npreprint                |  3570 |  3.81 |    3.81 |  89.40\nbook-chapter            |  2860 |  3.05 |    3.05 |  92.45\nlibguides               |  2414 |  2.58 |    2.58 |  95.03\ndataset                 |  1471 |  1.57 |    1.57 |  96.60\npeer-review             |  1400 |  1.49 |    1.49 |  98.09\ndissertation            |   975 |  1.04 |    1.04 |  99.13\nreport                  |   422 |  0.45 |    0.45 |  99.58\nbook                    |   167 |  0.18 |    0.18 |  99.76\nother                   |    64 |  0.07 |    0.07 |  99.83\nparatext                |    57 |  0.06 |    0.06 |  99.89\nerratum                 |    47 |  0.05 |    0.05 |  99.94\nletter                  |    25 |  0.03 |    0.03 |  99.97\neditorial               |    17 |  0.02 |    0.02 |  99.99\nsupplementary-materials |     7 |  0.01 |    0.01 |  99.99\nreference-entry         |     6 |  0.01 |    0.01 | 100.00\n&lt;NA&gt;                    |     0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\n\nLösung anzeigen\n# Notiz(en)\n# - Überwiegende Anzahl der heruntergeladenen Datenbankbeiträge sind `article.`\n# - Die folgenden Auswertungen beziehen sich auf die Publikationen des Typs article, da uns besonders die praktische Anwendung/Umsetzung der Methode in verschiedenen Kontexten interessiert. Läge der Fokus auf (die Entwicklung) der Methode selbst, wäre vermutlich eher die Typen book oder book-chapter relevant.\n\n\n\n\n📋 Exercise 3: Forschungsfeld der Publikationen\n\n\n\n\n\n\nZiel der Aufgabe\n\n\n\n\nIdentifizieren Sie die für die Untersuchung relevanten Artikel auf Basis des von OpenAlex dem Artikel zugeordnenten Foschungsfeldes (field))\nHintergrundinformation zur Variable field finden Sie in der API-Dokumentation von OpenAlex.\n\n\n\n\nBasierend auf dem Datensatz review_works_correct\n\nnutzen Sie die Funktion unnest() um die Variablen der topics-Liste zu extrahieren. Verwenden Sie dabei das Argument names_sep = \"_\". um doppelte Variablennamen durch Hinzufügen des Prefixes topics_ zu verhindern.\nfiltern Sie anschließen mit Hilfe der Funktion filter und der Variable bzw. dem Argument topics_name == \"field\" nur die Informationen zum Forschungsfeld, sowie mit der Variable bzw. dem Argument topics_i == \"1\" nur die erste Zuordnung.\nNutzen Sie die Funktion sjmisc::frq() und schauen Sie sich die Variablen topics_display_name an. Verwenden Sie das Argument sort.frq = \"desc\", um die Forschungsfelder in Abhängigkeit Ihrer Häufigkeit absteigend zu sortieren.\n\nNotieren Sie sich die Ausprägungen der Variable topics_display_name, die aus Ihrer Sicht später bei 📋 Exercise 4: Erstellung Subsample als Filter genutzt werden soll.\n\n\n\nLösung anzeigen\nreview_works_correct %&gt;% \n    unnest(topics, names_sep = \"_\") %&gt;%\n    filter(topics_name == \"field\") %&gt;% \n    filter(topics_i == \"1\") %&gt;% \n    sjmisc::frq(\"topics_display_name\", sort.frq = \"desc\")\n\n\ntopics_display_name &lt;character&gt; \n# total N=93655 valid N=93655 mean=4.41 sd=1.62\n\nValue                               |     N | Raw % | Valid % | Cum. %\n----------------------------------------------------------------------\nSocial Sciences                     | 30580 | 32.65 |   32.65 |  32.65\nPsychology                          | 29054 | 31.02 |   31.02 |  63.67\nBusiness, Management and Accounting | 15558 | 16.61 |   16.61 |  80.29\nDecision Sciences                   |  7261 |  7.75 |    7.75 |  88.04\nEconomics, Econometrics and Finance |  6796 |  7.26 |    7.26 |  95.30\nArts and Humanities                 |  4406 |  4.70 |    4.70 | 100.00\n&lt;NA&gt;                                |     0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\n\nLösung anzeigen\n# Notiz(en)\n# - Focus auf \"primäre\" Forschungsfelder `Social Sciences` & `Psychology`\n\n\n\n\n📋 Exercise 4: Erstellung Subsample\n\n\n\n\n\n\nZiel der Aufgabe\n\n\n\n\nErstellung sie den Datensatz review_subsample, in dem Sie mit Hilfe der Funktionen select() und/oder filter() das Datenmaterial weiter eingrenzen. Sie können sich sowohl auf die Variablen aus der Übung, als auch auf die aus der Sitzung (bzw. den Slides) beziehen.\nDer Code dieses Chunks wird in der nächsten Sitzung benötigt bzw. besprochen, halten Sie diesen deshalb bitte bereit.\n\n\n\n\n\nLösung anzeigen\nreview_subsample &lt;- review_works_correct %&gt;% \n  filter(language == \"en\") %&gt;% # nur englischsprachige Einträge\n  filter(type == \"article\") %&gt;% # nur Artikel\n  # Datentranformation\n  unnest(topics, names_sep = \"_\") %&gt;%\n  filter(topics_name == \"field\") %&gt;% \n  filter(topics_i == \"1\") %&gt;% \n  filter(\n    topics_display_name == \"Social Sciences\"|\n    topics_display_name == \"Psychology\"\n    ) %&gt;%\n  glimpse()\n\n\nRows: 45,221\nColumns: 45\n$ id                          &lt;chr&gt; \"https://openalex.org/W4293003987\", \"https…\n$ title                       &lt;chr&gt; \"The WHO-5 Well-Being Index: A Systematic …\n$ display_name                &lt;chr&gt; \"The WHO-5 Well-Being Index: A Systematic …\n$ author                      &lt;list&gt; [&lt;data.frame[4 x 12]&gt;], [&lt;data.frame[2 x …\n$ ab                          &lt;chr&gt; \"The 5-item World Health Organization Well…\n$ publication_date            &lt;chr&gt; \"2015-01-01\", \"2017-08-28\", \"2014-01-01\", …\n$ relevance_score             &lt;dbl&gt; 938.7603, 752.3500, 591.2553, 576.1210, 56…\n$ so                          &lt;chr&gt; \"Psychotherapy and psychosomatics\", \"Journ…\n$ so_id                       &lt;chr&gt; \"https://openalex.org/S184803288\", \"https:…\n$ host_organization           &lt;chr&gt; \"Karger Publishers\", \"SAGE Publishing\", NA…\n$ issn_l                      &lt;chr&gt; \"0033-3190\", \"0739-456X\", NA, \"2214-7829\",…\n$ url                         &lt;chr&gt; \"https://doi.org/10.1159/000376585\", \"http…\n$ pdf_url                     &lt;chr&gt; \"https://www.karger.com/Article/Pdf/376585…\n$ license                     &lt;chr&gt; \"cc-by-nc\", NA, NA, \"cc-by\", NA, NA, \"cc-b…\n$ version                     &lt;chr&gt; \"publishedVersion\", NA, \"publishedVersion\"…\n$ first_page                  &lt;chr&gt; \"167\", \"93\", NA, \"89\", \"55\", \"2150\", \"e356…\n$ last_page                   &lt;chr&gt; \"176\", \"112\", NA, \"106\", \"64\", \"2159\", \"e3…\n$ volume                      &lt;chr&gt; \"84\", \"39\", NA, \"6\", \"277\", \"32\", \"2\", \"24…\n$ issue                       &lt;chr&gt; \"3\", \"1\", NA, NA, NA, \"19\", \"8\", NA, \"9\", …\n$ is_oa                       &lt;lgl&gt; TRUE, FALSE, TRUE, TRUE, TRUE, FALSE, TRUE…\n$ is_oa_anywhere              &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE,…\n$ oa_status                   &lt;chr&gt; \"hybrid\", \"green\", \"bronze\", \"gold\", \"bron…\n$ oa_url                      &lt;chr&gt; \"https://www.karger.com/Article/Pdf/376585…\n$ any_repository_has_fulltext &lt;lgl&gt; FALSE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE…\n$ language                    &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", …\n$ grants                      &lt;list&gt; NA, NA, NA, &lt;\"https://openalex.org/F43203…\n$ cited_by_count              &lt;int&gt; 2657, 1375, 2568, 803, 3664, 1553, 2895, 9…\n$ counts_by_year              &lt;list&gt; [&lt;data.frame[11 x 2]&gt;], [&lt;data.frame[7 x …\n$ publication_year            &lt;int&gt; 2015, 2017, 2014, 2016, 2020, 2014, 2017, …\n$ cited_by_api_url            &lt;chr&gt; \"https://api.openalex.org/works?filter=cit…\n$ ids                         &lt;list&gt; &lt;\"https://openalex.org/W4293003987\", \"htt…\n$ doi                         &lt;chr&gt; \"https://doi.org/10.1159/000376585\", \"http…\n$ type                        &lt;chr&gt; \"article\", \"article\", \"article\", \"article\"…\n$ referenced_works            &lt;list&gt; &lt;\"https://openalex.org/W1492518593\", \"htt…\n$ related_works               &lt;list&gt; &lt;\"https://openalex.org/W3020194755\", \"htt…\n$ is_paratext                 &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ is_retracted                &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ concepts                    &lt;list&gt; [&lt;data.frame[7 x 5]&gt;], [&lt;data.frame[18 x …\n$ topics_i                    &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ topics_score                &lt;dbl&gt; 0.9926, 0.9050, 0.9995, 0.9987, 0.9999, 1.…\n$ topics_name                 &lt;chr&gt; \"field\", \"field\", \"field\", \"field\", \"field…\n$ topics_id                   &lt;chr&gt; \"https://openalex.org/fields/32\", \"https:/…\n$ topics_display_name         &lt;chr&gt; \"Psychology\", \"Social Sciences\", \"Psycholo…\n$ publication_year_fct        &lt;fct&gt; 2015, 2017, 2014, 2016, 2020, 2014, 2017, …\n$ type_fct                    &lt;fct&gt; article, article, article, article, articl…"
  },
  {
    "objectID": "computing/computing-cheatsheets.html",
    "href": "computing/computing-cheatsheets.html",
    "title": "R cheatsheets",
    "section": "",
    "text": "The following cheatsheets come from https://posit.co/resources/cheatsheets. We haven’t covered every function and functionality listed on them, but you might still find them useful as references.",
    "crumbs": [
      "Computing",
      "R Cheatsheets"
    ]
  }
]