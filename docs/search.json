[
  {
    "objectID": "computing/computing-textbooks.html",
    "href": "computing/computing-textbooks.html",
    "title": "R textbooks",
    "section": "",
    "text": "While there is no official R textbook for the course, there are a few to look at:\n\n🔗 R for Data Science, 2nd Edition\n🔗 Data Visualization: A Practical Introduction\n🔗 Tidy modeling with R\n🔗 Text Mining with R",
    "crumbs": [
      "Computing",
      "R Textbooks"
    ]
  },
  {
    "objectID": "computing/computing-useful_links.html",
    "href": "computing/computing-useful_links.html",
    "title": "Useful sources",
    "section": "",
    "text": "This is selection of useful R sources:\n\n Quarto tutorials by Andy Field\n Automatisierte Inhaltsanalyse mit R by Kornelius Puschmann",
    "crumbs": [
      "Computing",
      "Useful R sources"
    ]
  },
  {
    "objectID": "exercises/ms-showcase-07.html",
    "href": "exercises/ms-showcase-07.html",
    "title": "API mining and data wrangling with R",
    "section": "",
    "text": "Link to slides"
  },
  {
    "objectID": "exercises/ms-showcase-07.html#packages",
    "href": "exercises/ms-showcase-07.html#packages",
    "title": "API mining and data wrangling with R",
    "section": "Packages",
    "text": "Packages\n\nZum Laden der Pakete wird das Paket pacman::pload() genutzt, dass gegenüber der herkömmlichen Methode mit library() eine Reihe an Vorteile hat:\n\nPrägnante Syntax\nAutomatische Installation (wenn Paket noch nicht vorhanden)\nLaden mehrerer Pakete auf einmal\nAutomatische Suche nach dependencies\n\n\n\npacman::p_load(\n  here, qs, \n  magrittr, janitor,\n  easystats, sjmisc,\n  ggpubr, \n  openalexR, \n  tidyverse\n)"
  },
  {
    "objectID": "exercises/ms-showcase-07.html#codechunks-aus-der-sitzung",
    "href": "exercises/ms-showcase-07.html#codechunks-aus-der-sitzung",
    "title": "API mining and data wrangling with R",
    "section": "Codechunks aus der Sitzung",
    "text": "Codechunks aus der Sitzung\n\nDatenerhebung via API\n\n# Set openalexR.mailto option so that your requests go to the polite pool for faster response times\noptions(openalexR.mailto = \"christoph.adrian@fau.de\")\n\n\n# Download data via API\nreview_works &lt;- openalexR::oa_fetch(\n  entity = \"works\",\n  title.search = \"(literature OR systematic) AND review\",\n  primary_topic.domain.id = \"domains/2\", # Social Science\n  publication_year = \"2013 - 2023\",\n  verbose = TRUE\n)\n\n\n# Overview\nreview_works \n\n# A tibble: 93,655 × 39\n   id     title display_name author ab    publication_date relevance_score so   \n   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;        &lt;list&gt; &lt;chr&gt; &lt;chr&gt;                      &lt;dbl&gt; &lt;chr&gt;\n 1 https… The … The PRISMA … &lt;df&gt;   The … 2021-03-29                 1625. BMJ  \n 2 https… Pref… Preferred r… &lt;df&gt;   Syst… 2015-01-01                 1340. Syst…\n 3 https… Rayy… Rayyan—a we… &lt;df&gt;   Synt… 2016-12-01                 1314. Syst…\n 4 https… Syst… Systematic … &lt;df&gt;   Scop… 2018-11-19                  990. BMC …\n 5 https… Upda… Updated gui… &lt;df&gt;   On a… 2019-10-03                  963. Coch…\n 6 https… The … The WHO-5 W… &lt;df&gt;   The … 2015-01-01                  939. Psyc…\n 7 https… Pref… Preferred r… &lt;df&gt;   Prot… 2015-01-02                  914. BMJ  \n 8 https… Guid… Guidance on… &lt;df&gt;   Lite… 2017-08-28                  752. Jour…\n 9 https… Lite… Literature … &lt;df&gt;   Know… 2019-11-01                  705. Jour…\n10 https… The … The PRISMA … &lt;df&gt;   The … 2021-04-01                  653. Inte…\n# ℹ 93,645 more rows\n# ℹ 31 more variables: so_id &lt;chr&gt;, host_organization &lt;chr&gt;, issn_l &lt;chr&gt;,\n#   url &lt;chr&gt;, pdf_url &lt;chr&gt;, license &lt;chr&gt;, version &lt;chr&gt;, first_page &lt;chr&gt;,\n#   last_page &lt;chr&gt;, volume &lt;chr&gt;, issue &lt;chr&gt;, is_oa &lt;lgl&gt;,\n#   is_oa_anywhere &lt;lgl&gt;, oa_status &lt;chr&gt;, oa_url &lt;chr&gt;,\n#   any_repository_has_fulltext &lt;lgl&gt;, language &lt;chr&gt;, grants &lt;list&gt;,\n#   cited_by_count &lt;int&gt;, counts_by_year &lt;list&gt;, publication_year &lt;int&gt;, …\n\n\n\n\nInitiale Sichtung und Überprüfung der Datem\n\n\n\n\n\n\nTypische Bestandteile\n\n\n\n\nWie viele Fälle sind enthalten? Wie viele Variablen? Sind die Variablennamen aussagekräftig?\nWelchen Typ hat jede Variable, z. B. numerisch, kategorisch, logisch?\n\n\n\n\nreview_works %&gt;% glimpse()\n\nRows: 93,655\nColumns: 39\n$ id                          &lt;chr&gt; \"https://openalex.org/W3118615836\", \"https…\n$ title                       &lt;chr&gt; \"The PRISMA 2020 statement: an updated gui…\n$ display_name                &lt;chr&gt; \"The PRISMA 2020 statement: an updated gui…\n$ author                      &lt;list&gt; [&lt;data.frame[26 x 12]&gt;], [&lt;data.frame[8 x…\n$ ab                          &lt;chr&gt; \"The Preferred Reporting Items for Systema…\n$ publication_date            &lt;chr&gt; \"2021-03-29\", \"2015-01-01\", \"2016-12-01\", …\n$ relevance_score             &lt;dbl&gt; 1625.1708, 1340.1902, 1314.3904, 990.4521,…\n$ so                          &lt;chr&gt; \"BMJ\", \"Systematic reviews\", \"Systematic r…\n$ so_id                       &lt;chr&gt; \"https://openalex.org/S4393917726\", \"https…\n$ host_organization           &lt;chr&gt; NA, \"BioMed Central\", \"BioMed Central\", \"B…\n$ issn_l                      &lt;chr&gt; \"1756-1833\", \"2046-4053\", \"2046-4053\", \"14…\n$ url                         &lt;chr&gt; \"https://doi.org/10.1136/bmj.n71\", \"https:…\n$ pdf_url                     &lt;chr&gt; \"https://www.bmj.com/content/bmj/372/bmj.n…\n$ license                     &lt;chr&gt; \"cc-by\", \"cc-by\", \"cc-by\", \"cc-by\", NA, \"c…\n$ version                     &lt;chr&gt; \"publishedVersion\", \"publishedVersion\", \"p…\n$ first_page                  &lt;chr&gt; \"n71\", NA, NA, NA, NA, \"167\", \"g7647\", \"93…\n$ last_page                   &lt;chr&gt; \"n71\", NA, NA, NA, NA, \"176\", \"g7647\", \"11…\n$ volume                      &lt;chr&gt; NA, \"4\", \"5\", \"18\", NA, \"84\", \"349\", \"39\",…\n$ issue                       &lt;chr&gt; NA, \"1\", \"1\", \"1\", NA, \"3\", \"jan02 1\", \"1\"…\n$ is_oa                       &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, FALSE, TRUE, TRUE,…\n$ is_oa_anywhere              &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, …\n$ oa_status                   &lt;chr&gt; \"hybrid\", \"gold\", \"gold\", \"gold\", \"green\",…\n$ oa_url                      &lt;chr&gt; \"https://www.bmj.com/content/bmj/372/bmj.n…\n$ any_repository_has_fulltext &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE,…\n$ language                    &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", …\n$ grants                      &lt;list&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, &lt;\"htt…\n$ cited_by_count              &lt;int&gt; 30303, 17347, 10540, 5298, 5664, 2657, 909…\n$ counts_by_year              &lt;list&gt; [&lt;data.frame[5 x 2]&gt;], [&lt;data.frame[11 x …\n$ publication_year            &lt;int&gt; 2021, 2015, 2016, 2018, 2019, 2015, 2015, …\n$ cited_by_api_url            &lt;chr&gt; \"https://api.openalex.org/works?filter=cit…\n$ ids                         &lt;list&gt; &lt;\"https://openalex.org/W3118615836\", \"htt…\n$ doi                         &lt;chr&gt; \"https://doi.org/10.1136/bmj.n71\", \"https:…\n$ type                        &lt;chr&gt; \"article\", \"article\", \"article\", \"article\"…\n$ referenced_works            &lt;list&gt; &lt;\"https://openalex.org/W1528251861\", \"htt…\n$ related_works               &lt;list&gt; &lt;\"https://openalex.org/W4234875088\", \"htt…\n$ is_paratext                 &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ is_retracted                &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ concepts                    &lt;list&gt; [&lt;data.frame[20 x 5]&gt;], [&lt;data.frame[18 x…\n$ topics                      &lt;list&gt; [&lt;tbl_df[12 x 5]&gt;], [&lt;tbl_df[12 x 5]&gt;], […\n\n\n\n\nDatentransformationen\n\nKorrektur der Rohdaten\n\nreview_works_correct &lt;- review_works %&gt;% \n    mutate(\n        # Create additional factor variables\n        publication_year_fct = as.factor(publication_year), \n        type_fct = as.factor(type)\n        )\n\n\n\nUnnest topics\n\nreview_works_correct %&gt;% \n    unnest(topics, names_sep = \"_\") %&gt;%\n    glimpse()\n\nRows: 942,560\nColumns: 45\n$ id                          &lt;chr&gt; \"https://openalex.org/W3118615836\", \"https…\n$ title                       &lt;chr&gt; \"The PRISMA 2020 statement: an updated gui…\n$ display_name                &lt;chr&gt; \"The PRISMA 2020 statement: an updated gui…\n$ author                      &lt;list&gt; [&lt;data.frame[26 x 12]&gt;], [&lt;data.frame[26 …\n$ ab                          &lt;chr&gt; \"The Preferred Reporting Items for Systema…\n$ publication_date            &lt;chr&gt; \"2021-03-29\", \"2021-03-29\", \"2021-03-29\", …\n$ relevance_score             &lt;dbl&gt; 1625.171, 1625.171, 1625.171, 1625.171, 16…\n$ so                          &lt;chr&gt; \"BMJ\", \"BMJ\", \"BMJ\", \"BMJ\", \"BMJ\", \"BMJ\", …\n$ so_id                       &lt;chr&gt; \"https://openalex.org/S4393917726\", \"https…\n$ host_organization           &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ issn_l                      &lt;chr&gt; \"1756-1833\", \"1756-1833\", \"1756-1833\", \"17…\n$ url                         &lt;chr&gt; \"https://doi.org/10.1136/bmj.n71\", \"https:…\n$ pdf_url                     &lt;chr&gt; \"https://www.bmj.com/content/bmj/372/bmj.n…\n$ license                     &lt;chr&gt; \"cc-by\", \"cc-by\", \"cc-by\", \"cc-by\", \"cc-by…\n$ version                     &lt;chr&gt; \"publishedVersion\", \"publishedVersion\", \"p…\n$ first_page                  &lt;chr&gt; \"n71\", \"n71\", \"n71\", \"n71\", \"n71\", \"n71\", …\n$ last_page                   &lt;chr&gt; \"n71\", \"n71\", \"n71\", \"n71\", \"n71\", \"n71\", …\n$ volume                      &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ issue                       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ is_oa                       &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, …\n$ is_oa_anywhere              &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, …\n$ oa_status                   &lt;chr&gt; \"hybrid\", \"hybrid\", \"hybrid\", \"hybrid\", \"h…\n$ oa_url                      &lt;chr&gt; \"https://www.bmj.com/content/bmj/372/bmj.n…\n$ any_repository_has_fulltext &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, …\n$ language                    &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", …\n$ grants                      &lt;list&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ cited_by_count              &lt;int&gt; 30303, 30303, 30303, 30303, 30303, 30303, …\n$ counts_by_year              &lt;list&gt; [&lt;data.frame[5 x 2]&gt;], [&lt;data.frame[5 x 2…\n$ publication_year            &lt;int&gt; 2021, 2021, 2021, 2021, 2021, 2021, 2021, …\n$ cited_by_api_url            &lt;chr&gt; \"https://api.openalex.org/works?filter=cit…\n$ ids                         &lt;list&gt; &lt;\"https://openalex.org/W3118615836\", \"htt…\n$ doi                         &lt;chr&gt; \"https://doi.org/10.1136/bmj.n71\", \"https:…\n$ type                        &lt;chr&gt; \"article\", \"article\", \"article\", \"article\"…\n$ referenced_works            &lt;list&gt; &lt;\"https://openalex.org/W1528251861\", \"htt…\n$ related_works               &lt;list&gt; &lt;\"https://openalex.org/W4234875088\", \"htt…\n$ is_paratext                 &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ is_retracted                &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ concepts                    &lt;list&gt; [&lt;data.frame[20 x 5]&gt;], [&lt;data.frame[20 x…\n$ topics_i                    &lt;int&gt; 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 1, 1, …\n$ topics_score                &lt;dbl&gt; 0.9993, 0.9993, 0.9993, 0.9993, 0.9832, 0.…\n$ topics_name                 &lt;chr&gt; \"topic\", \"subfield\", \"field\", \"domain\", \"t…\n$ topics_id                   &lt;chr&gt; \"https://openalex.org/T10206\", \"https://op…\n$ topics_display_name         &lt;chr&gt; \"Methods for Evidence Synthesis in Researc…\n$ publication_year_fct        &lt;fct&gt; 2021, 2021, 2021, 2021, 2021, 2021, 2021, …\n$ type_fct                    &lt;fct&gt; article, article, article, article, articl…\n\n\n\n\n\nRekonstruktion OpexAlex Dashboard\n\nPublikationen im Zeitverlauf\n\nreview_works_correct %&gt;% \n    ggplot(aes(publication_year)) +\n    geom_bar() +\n    theme_pubr()\n\n\n\n\n\n\n\n\n\n\nHäufigkeit Forschungsfelder\n\nreview_works_correct %&gt;% \n    unnest(topics, names_sep = \"_\") %&gt;% \n    filter(topics_name == \"field\") %&gt;% \n    filter(topics_i == 1) %&gt;% \n    sjmisc::frq(topics_display_name, sort.frq = \"desc\")\n\ntopics_display_name &lt;character&gt; \n# total N=93655 valid N=93655 mean=4.41 sd=1.62\n\nValue                               |     N | Raw % | Valid % | Cum. %\n----------------------------------------------------------------------\nSocial Sciences                     | 30580 | 32.65 |   32.65 |  32.65\nPsychology                          | 29054 | 31.02 |   31.02 |  63.67\nBusiness, Management and Accounting | 15558 | 16.61 |   16.61 |  80.29\nDecision Sciences                   |  7261 |  7.75 |    7.75 |  88.04\nEconomics, Econometrics and Finance |  6796 |  7.26 |    7.26 |  95.30\nArts and Humanities                 |  4406 |  4.70 |    4.70 | 100.00\n&lt;NA&gt;                                |     0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\n\n\n\nRelevanteste Publikationen\n\nreview_works_correct %&gt;% \n    arrange(desc(relevance_score)) %&gt;%\n    select(publication_year_fct, relevance_score, title) %&gt;% \n    head(5) %&gt;% \n    gt::gt()\n\n\n\n\n\n\n\npublication_year_fct\nrelevance_score\ntitle\n\n\n\n\n2021\n1625.1708\nThe PRISMA 2020 statement: an updated guideline for reporting systematic reviews\n\n\n2015\n1340.1902\nPreferred reporting items for systematic review and meta-analysis protocols (PRISMA-P) 2015 statement\n\n\n2016\n1314.3904\nRayyan—a web and mobile app for systematic reviews\n\n\n2018\n990.4521\nSystematic review or scoping review? Guidance for authors when choosing between a systematic or scoping review approach\n\n\n2019\n962.6738\nUpdated guidance for trusted systematic reviews: a new edition of the Cochrane Handbook for Systematic Reviews of Interventions\n\n\n\n\n\n\n\n\n\nLageparameter\n\nreview_works_correct %&gt;% \n  select(where(is.numeric)) %&gt;% \n  datawizard::describe_distribution() %&gt;% \n  print_html()\n\n\n\n\n\n\n\nVariable\nMean\nSD\nIQR\nMin\nMax\nSkewness\nKurtosis\nn\nn_Missing\n\n\n\n\nrelevance_score\n31.73\n42.51\n36.48\n1.17\n1625.17\n4.75\n67.87\n93655\n0\n\n\ncited_by_count\n18.33\n146.36\n10.00\n0.00\n30303.00\n123.44\n22236.55\n93655\n0\n\n\npublication_year\n2019.40\n2.97\n5.00\n2013.00\n2023.00\n-0.58\n-0.77\n93655\n0"
  },
  {
    "objectID": "exercises/ms-exercise-10.html",
    "href": "exercises/ms-exercise-10.html",
    "title": "Unsupervised Machine Learning II",
    "section": "",
    "text": "Link to source file"
  },
  {
    "objectID": "exercises/ms-exercise-10.html#background",
    "href": "exercises/ms-exercise-10.html#background",
    "title": "Unsupervised Machine Learning II",
    "section": "Background",
    "text": "Background\n\n\n\n\n\n\nTodays’s data basis: OpenAlex\n\n\n\n\nVia API bzw. openalexR (Aria et al. 2024) gesammelte “works” der Datenbank OpenAlex mit Bezug zu Literaturriews in den Sozialwissenschaften zwischen 2013 und 2023\nDetaillierte Informationen und Ergebnisse zur Suchquery finden Sie hier."
  },
  {
    "objectID": "exercises/ms-exercise-10.html#preparation",
    "href": "exercises/ms-exercise-10.html#preparation",
    "title": "Unsupervised Machine Learning II",
    "section": "Preparation",
    "text": "Preparation\n\n\n\n\n\n\nWichtige Information\n\n\n\n\nBitte stellen Sie sicher, dass Sie das jeweilige R-Studio Projekt zur Übung geöffnet haben. Nur so funktionieren alle Dependencies korrekt.\nUm den einwandfreien Ablauf der Übung zu gewährleisten, wird für die Aufgaben auf eine eigenständige Datenerhebung verzichtet und ein Übungsdatensatz zu verfügung gestelt.\n\n\n\n\nPackages\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n    here, qs, # file management\n    magrittr, janitor, # data wrangling\n    easystats, sjmisc, # data analysis\n    gt, gtExtras, # table visualization\n    ggpubr, ggwordcloud, # visualization\n    # text analysis    \n    tidytext, widyr, # based on tidytext\n    quanteda, # based on quanteda\n    quanteda.textmodels, quanteda.textplots, quanteda.textstats, \n    stm, # structural topic modeling\n    openalexR, pushoverr, tictoc, \n    tidyverse # load last to avoid masking issues\n  )\n\n\n\nImport und Vorverarbeitung der Daten\n\nreview_works &lt;- qs::qread(here(\"data/session-07/openalex-review_works-2013_2023.qs\"))\n\n# Create correct data\nreview_subsample &lt;- review_works %&gt;% \n    # Create additional factor variables\n    mutate(\n        publication_year_fct = as.factor(publication_year), \n        type_fct = as.factor(type)\n        ) %&gt;%\n    # Eingrenzung: Sprache und Typ\n    filter(language == \"en\") %&gt;% \n    filter(type == \"article\") %&gt;%\n    # Datentranformation\n    unnest(topics, names_sep = \"_\") %&gt;%\n    filter(topics_name == \"field\") %&gt;% \n    filter(topics_i == \"1\") %&gt;% \n    # Eingrenzung: Forschungsfeldes\n    filter(\n    topics_display_name == \"Social Sciences\"|\n    topics_display_name == \"Psychology\"\n    ) %&gt;% \n    mutate(\n        field = as.factor(topics_display_name)\n    ) %&gt;% \n    # Eingrenzung: Keine Einträge ohne Abstract\n    filter(!is.na(ab))\n\n\n# Create corpus\nquanteda_corpus &lt;- review_subsample %&gt;% \n  quanteda::corpus(\n    docid_field = \"id\", \n    text_field = \"ab\"\n  )\n\n# Tokenize\nquanteda_token &lt;- quanteda_corpus %&gt;% \n  quanteda::tokens(\n    remove_punct = TRUE,\n    remove_symbols = TRUE, \n    remove_numbers = TRUE, \n    remove_url = TRUE, \n    split_tags = FALSE # keep hashtags and mentions\n  ) %&gt;% \n  quanteda::tokens_tolower() %&gt;% \n  quanteda::tokens_remove(\n    pattern = stopwords(\"en\")\n    )\n\n# Convert to Document-Feature-Matrix (DFM)\nquanteda_dfm &lt;- quanteda_token %&gt;% \n  quanteda::dfm()\n\n# Pruning\nquanteda_dfm_trim &lt;- quanteda_dfm %&gt;% \n  dfm_trim( \n    min_docfreq = 10/nrow(review_subsample),\n    max_docfreq = 0.99, \n    docfreq_type = \"prop\")\n\n# Convert for stm topic modeling\nquanteda_stm &lt;- quanteda_dfm_trim %&gt;% \n   convert(to = \"stm\")"
  },
  {
    "objectID": "exercises/ms-exercise-10.html#praktische-anwendung",
    "href": "exercises/ms-exercise-10.html#praktische-anwendung",
    "title": "Unsupervised Machine Learning II",
    "section": "🛠️ Praktische Anwendung",
    "text": "🛠️ Praktische Anwendung\n\n\n\n\n\n\nAchtung, bitte lesen!\n\n\n\n\nBevor Sie mit der Arbeit an den folgenden 📋 Exercises beginnen, stellen Sie bitte sicher, dass Sie alle Chunks des Abschnitts Preparation gerendert haben. Das können Sie tun, indem Sie den “Run all chunks above”-Knopf des nächsten Chunks benutzen.\nBei Fragen zum Code lohnt sich ein Blick in den Showcase (.qmd oder .html). Beim Showcase handelt es sich um eine kompakte Darstellung des in der Präsentation verwenden R-Codes. Sie können das Showcase also nutzen, um sich die Code-Bausteine anzusehen, die für die R-Outputs auf den Slides benutzt wurden.\n\n\n\n\n📋 Exercise 1: Visualisierung der Themenprävalenz\n\n1.1. Auswahl des passenden Models\n\nErstelen Sie einen neuen Datensatz stm_mdl_k40\n\nbasierend auf dem Datensatz stm_serach\n\nVerwenden Sie filter(k == 40), um das Modell mit 40 Themen zu auszuwählen.\nVerwenden Sie pull(mdl) %&gt;% .[[1]] um die Spalte und das Element zu extrahieren, die das Modell enthält.\nSpeichern Sie diese Umwandlung, indem Sie einen neuen Datensatz mit dem Namen stm_mdl_k40 erstellen.\n\n\nÜberprüfen Sie die Transformation indem Sie stm_mdl_k40 in die Konsole eingeben.\n\n\n# Pull tpm with 40 topics\n\n# Check\n\n\n\n1.2. Identifikation der Top-Terms für jedes Thema\n\nErstellen Sie einen neuen Datensatz td_beta\n\nbasierend auf dem Datensatz stm_mdl_k40,\nVerwenden Sie tidy(method = \"frex\"), um die Beta-Matrix zu erstellen.\n\nErstellen Sie einen neuen Datensatz top_terms\n\nbasierend auf dem Datenastz td_beta,\n\nVerwenden Sie arrange(beta), um die Begriffe nach Beta zu sortieren.\nGruppieren Sie die Begriffe nach topic mit group_by(topic).\nExtrahieren Sie die 7 häufigsten Begriffe mit top_n(7, beta).\nSortieren Sie die Begriffe absteigend mit arrange(-beta).\nWählen Sie die Variablen topic und term mit select(topic, term) aus.\nExtrahieren Sie die Top-Begriffe pro Thema mit summarise(terms = list(term)).\nTransformieren Sie die extrahierten Begriffe pro Thema mit map(terms, paste, collapse = \", \") zu einem String.\n“Entpacken” Sie die Begriffe aus der Liste (unnesten) mit unnest(cols = c(terms)).\n\n\nÜberprüfen Sie die Transformation indem Sie top_terms in die Konsole eingeben.\n\n\n# Create tidy beta matrix\n\n# Create top terms\n\n# Output\n\n\n\n1.3 Erstellung der Prävalenz-Tabelle für die Themen\n\nErstellen Sie einen neuen Datensatz td_gamma\n\nbasierend auf dem Datensatz stm_mdl_k40,\nVerwenden Sie tidy(), um die Gamma-Matrix zu erstellen.\nVerwenden Sie document_names = names(quanteda_stm$documents) um die Dokumentennamen zu speichern\n\nErstellen Sie einen neuen Datensatz prevalence\n\nbasierend auf dem Datensatz td_gamma,\n\nGruppieren Sie die Themen nach topic mit group_by(topic).\nBerechnen Sie den Durchschnitt der gamma-Werte pro Thema mit summarise(gamma = mean(gamma)).\nSortieren Sie die Themen (absteigend) nach gamma mit arrange(desc(gamma)).\nVerknüpfen Sie die Top-Begriffe mit den Themen mit left_join(top_terms, by = \"topic\").\nÜberarbeiten Sie die Variable topic mit dem mutate-Befehl:\n\nErstellen Sie eine neue Variable topic mit paste0(\"Topic \",sprintf(\"%02d\", topic)).\nOrdnen Sie die Themen nach gamma mit reorder(topic, gamma).\n\n\n\nErstellung Sie eine Tabelle als Output\n\nbasierend auf dem Datenastz prevalence,\n\nVerwenden Sie gt() um eine Tabelle zu erstellen.\nFormatieren Sie die Spalte gamma mit fmt_number(columns = vars(gamma), decimals = 2) um nur zwei Nachkommastellen anzuzeigen.\nVerwenden Sie gtExtras::gt_theme_538() um das Design der Tabelle anzupassen.\n\n\n✍️ Auf Basis des Outputs von prevalence Notieren Sie, welche Themen Sie als problematisch sehen und warum.\n\n\n# Create tidy gamma matrix\n\n# Create prevalence\n\n# Output\n\n\n\n\n📋 Exercise 2: Einfluss der Metadaten\n\n2.1. Schätzung der Meta-Effekte\n\nErstellen Sie einen neuen Datensatz effects:\n\nVerwenden Sie die Funktion estimateEffect(), um die Effekte zu schätzen.\nVerwenden Sie für das formular-Argument 1:40 ~ publication_year_fct + field, um die Effekte der Veröffentlichungsjahre und Fachbereiche zu schätzen.\nVerwenden Sie stm_mdl_40 als das zu analysierende Modell.\nVerwenden Sie meta = quanteda_stm$meta, um die Metadaten für die Schätzung zu verwenden.\n\n\n\n# Create data\n\n\n\n2.2. Untersuchung der Effekte\n\nErstellen Sie einen neuen Datensatz effects_tidy eine bereinigte Tabelle der Effekte:\n\nBasierend auf dem Datensatz effects\n\n\nVerwenden Sie die tidy() Funktion, um die Effekte in ein aufbereitetes Format zu bringen.\nFiltern Sie die Daten:\n\nEntfernen Sie Zeilen, bei denen term den Wert (Intercept) hat.\nBehalten Sie nur die Zeilen, bei denen term == \"fieldSocial Sciences\" ist.\n\nEntfernen Sie die Spalte term mit select(-term)\n\nErstellen Sie ein Tabelle zur Überprüfung der Effekte\n\nBasierend auf dem Datensatz effects_tidy:\n\n\nVerwenden Sie die Funktion gt(), um eine Tabelle zu erstellen.\nFormatieren Sie alle numerischen Variablen mit fmt_number(columns = -c(topic), decimals = 3), um lediglich drei Dezimalstellen darzustellen.\nVerwenden Sie data_color(columns = estimate, method = \"numeric\", palette = \"viridis\"), um die Schätzwerte farblich zu kennzeichnen.\nWenden Sie das Design gtExtras::gt_theme_538() an.\n\n✍️ Notieren Sie, welches Thema am stärksten im Forschungsfeld “Social Science” vertreten ist.\n\n\n# Filter effect tidy data\n\n# Explore effects (table outpu)\n\n#### Notes:\n# \n\n\n\n\n📋 Exercise 3: Einzelthema im Fokus\n\n3.1. Benennung des Themas k = 20\n\nBenennen Sie das Thema k = 20 aus dem Modell stm_mdl_40:\n\nVerwenden Sie die Funktion labelTopics().\nGeben Sie das Thema 20 als Parameter mit topic = 20 an.\n\n✍️ Notieren Sie die Themennamen. Begründen Sie kurz Ihre Entscheidung.\n\n\n# Create topic label\n\n# Themenname:\n\n\n\n3.2. Zusammenführung mit OpenAlex-Daten\n\nErstellen Sie einen neuen Datensatz gamma_export\n\nbasierend auf dem Datensatz stm_mdl_k40:\n\nVerwenden Sie tidy() um die Gamma-Matrix zu erstellen. Geben Sie matrix = \"gamma\" und document_names = names(quanteda_stm$documents) als Parameter an.\nGruppieren Sie die Dokumente nach document mit group_by(document).\nWählen Sie die Dokumente mit dem höchsten gamma-Wert mit slice_max(gamma).\nLösen Sie die Gruppierung mit dplyr::ungroup().\nVerknüpfen Sie die Daten mit review_subsample mittels left_join(review_subsample, by = c(\"document\" = \"id\")).\n\nBenennen Sie die Spalte document in id um mit dplyr::rename(id = document)\nErstellen Sie eine neue Variable stm_topic mit Hilfe des mutate()-Befehls. Verwenden Sie as.factor(paste(\"Topic\", sprintf(\"%02d\", topic))) um die Themen zu benennen und als Faktor zu speichern.\n\nÜberprüfen Sie Transformation mit Hilfe der glimpse()-Funktion, um sicherzustellen, dass die Daten korrekt erstellt wurden.\n\n\n# Create gamma export\n\n# Check\n\n\n\n3.3 Verteilungsparameter von Thema 20\n\nErstellung eines Outputs zur Überprüfung der Lageparameter\n\nBasierend auf dem Datensatz gamma_export:\n\nFiltern Sie die Daten nach topic == 20.\nWählen Sie mit Hilfe der select()-Funktion die Variablen gamma, relevance_score und cited_by_count aus.\nVerwenden Sie die Funktion datawizard::describe_distribution() um die Verteilungsparameter zu berechnen.\n\n\n✍️ Identifizieren und notieren Sie folgende Informationen:\n\nWie viele Abstracts haben Thema 20 als Hauptthema?\nWie hoch ist der durschnittliche Relevance Score?\nWie viele Zitationen haben die Dokumente im Durchschnitt?\nWie viel Zitate hat das hochzitierteste Dokument?\n\n\n# Create distribution parameters\ngamma_export %&gt;% \n  filter(topic == 20) %&gt;%\n  select(gamma, relevance_score, cited_by_count) %&gt;% \n  datawizard::describe_distribution()\n\n#### Notes\n# Anzahl der Abstrats von Thema 20\n# Durchschnittlicher Relevace Score: \n# Durchschnittliche Zitationen:\n# Anzahl der Zitationen des am meisten zitierten Dokuments:\n\n\n\n\n3.4. Top-Dokumente des Themas\n\nIdentifizierung der Top-Dokumente\n\nBasierend auf dem Datensatz gamma_export:\n\nFiltern Sie den Datensatz nach stm_topic == \"Topic 20\".\nSortieren Sie die Daten absteigend nach gamma mit arrange(-gamma).\nWählen Sie die Variablen title, so, gamma, type, und ab mit select() aus.\nWählen Sie die obersten 5 Zeilen mit slice_head(n = 5).\n\n\nErstellung eines Outputs zur Überprüfung der Top-Dokumente\n\nBasierend auf dem Datensatz top_docs_k20:\n\nVerwenden Sie gt() um eine Tabelle zu erstellen.\nFormatieren Sie die Spalte gamma mit fmt_number(columns = vars(gamma), decimals = 2) um nur zwei Nachkommastellen anzuzeigen.\nVerwenden Sie gtExtras::gt_theme_538() um das Design der Tabelle anzupassen.\n\n\n\n✍️ Basierend auf den den Abstracts und den Titeln der Top-Dokumente:\n\nWelche Themenbereiche decken die Dokumente ab?\nWürden Sie den im Abschnitt 3.1. gewählten Themennamen beibehalten oder abändern?\n\n\n\n# Identify top documents for topic 20\n\n# Creae output"
  },
  {
    "objectID": "exercises/ms-exercise-09_solution.html",
    "href": "exercises/ms-exercise-09_solution.html",
    "title": "Unsupervised Machine Learning I",
    "section": "",
    "text": "Link to source file"
  },
  {
    "objectID": "exercises/ms-exercise-09_solution.html#background",
    "href": "exercises/ms-exercise-09_solution.html#background",
    "title": "Unsupervised Machine Learning I",
    "section": "Background",
    "text": "Background\n\n\n\n\n\n\nTodays’s data basis: OpenAlex\n\n\n\n\nVia API bzw. openalexR (Aria et al. 2024) gesammelte “works” der Datenbank OpenAlex mit Bezug zu Literaturriews in den Sozialwissenschaften zwischen 2013 und 2023\nDetaillierte Informationen und Ergebnisse zur Suchquery finden Sie hier."
  },
  {
    "objectID": "exercises/ms-exercise-09_solution.html#preparation",
    "href": "exercises/ms-exercise-09_solution.html#preparation",
    "title": "Unsupervised Machine Learning I",
    "section": "Preparation",
    "text": "Preparation\n\n\n\n\n\n\nWichtige Information\n\n\n\n\nBitte stellen Sie sicher, dass Sie das jeweilige R-Studio Projekt zur Übung geöffnet haben. Nur so funktionieren alle Dependencies korrekt.\nUm den einwandfreien Ablauf der Übung zu gewährleisten, wird für die Aufgaben auf eine eigenständige Datenerhebung verzichtet und ein Übungsdatensatz zu verfügung gestelt.\n\n\n\n\nPackages\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n    here, qs, # file management\n    magrittr, janitor, # data wrangling\n    easystats, sjmisc, # data analysis\n    gt, gtExtras, # table visualization\n    ggpubr, ggwordcloud, # visualization\n    # text analysis    \n    tidytext, widyr, # based on tidytext\n    quanteda, # based on quanteda\n    quanteda.textmodels, quanteda.textplots, quanteda.textstats, \n    openalexR, \n    tidyverse # load last to avoid masking issues\n  )\n\n\n\nImport und Vorverarbeitung der Daten\n\n# Import from local\nreview_works &lt;- qs::qread(here(\"data/session-07/openalex-review_works-2013_2023.qs\"))\n\n# Create correct data\nreview_works_correct &lt;- review_works %&gt;% \n    mutate(\n        # Create additional factor variables\n        publication_year_fct = as.factor(publication_year), \n        type_fct = as.factor(type)\n        )\n\n# Create subsample\nreview_subsample &lt;- review_works_correct %&gt;%\n    # Eingrenzung: Sprache und Typ\n    filter(language == \"en\") %&gt;% \n    filter(type == \"article\") %&gt;%\n    # Eingrenzung: Keine Einträge ohne Abstract\n    filter(!is.na(ab)) %&gt;% \n    # Datentranformation\n    unnest(topics, names_sep = \"_\") %&gt;%\n    filter(topics_name == \"field\" ) %&gt;% \n    filter(topics_i == \"1\") %&gt;% \n    # Eingrenzung: Forschungsfeldes\n    filter(\n    topics_display_name == \"Social Sciences\"|\n    topics_display_name == \"Psychology\"\n    ) %&gt;% \n    # Eingrenzung: Keine Einträge ohne Abstract\n    filter(!is.na(ab))   \n\n\n\nErstellung Korpus & DFM\n\n\nLösung anzeigen\n# Create corpus\nquanteda_corpus &lt;- review_subsample %&gt;% \n  quanteda::corpus(\n    docid_field = \"id\", \n    text_field = \"ab\"\n  )\n\n# Tokenize\nquanteda_token &lt;- quanteda_corpus %&gt;% \n  quanteda::tokens(\n    remove_punct = TRUE,\n    remove_symbols = TRUE, \n    remove_numbers = TRUE, \n    remove_url = TRUE, \n    split_tags = FALSE # keep hashtags and mentions\n  ) %&gt;% \n  quanteda::tokens_tolower() %&gt;% \n  quanteda::tokens_remove(\n    pattern = stopwords(\"en\")\n    )\n\n# Convert to Document-Feature-Matrix (DFM)\nquanteda_dfm &lt;- quanteda_token %&gt;% \n  quanteda::dfm()"
  },
  {
    "objectID": "exercises/ms-exercise-09_solution.html#praktische-anwendung",
    "href": "exercises/ms-exercise-09_solution.html#praktische-anwendung",
    "title": "Unsupervised Machine Learning I",
    "section": "🛠️ Praktische Anwendung",
    "text": "🛠️ Praktische Anwendung\n\n\n\n\n\n\nAchtung, bitte lesen!\n\n\n\n\nBevor Sie mit der Arbeit an den folgenden 📋 Exercises beginnen, stellen Sie bitte sicher, dass Sie alle Chunks des Abschnitts Preparation gerendert haben. Das können Sie tun, indem Sie den “Run all chunks above”-Knopf des nächsten Chunks benutzen.\nBei Fragen zum Code lohnt sich ein Blick in den Showcase (.qmd oder .html). Beim Showcase handelt es sich um eine kompakte Darstellung des in der Präsentation verwenden R-Codes. Sie können das Showcase also nutzen, um sich die Code-Bausteine anzusehen, die für die R-Outputs auf den Slides benutzt wurden.\n\n\n\n\n📋 Exercise 1: Cleaned DFM\n\n\n\n\n\n\nZiel der Aufgabe\n\n\n\n\n\n\n\nErstelen Sie einen neuen Datensatz quanteda_dfm_cleaned\n\nbasierend auf dem Datensatz quanteda_dfm\n\nVerwenden Sie quanteda::dfm_remove(pattern = c(\"systematic\", \"literature\", \"review\"), um die Suchquery zu entfernen.\nSpeichern Sie diese Umwandlung, indem Sie einen neuen Datensatz mit dem Namen quanteda_dfm_cleaned erstellen.\n\n\nÜberprüfen Sie die Transformation indem Sie quanteda_dfm_cleaned in die Konsole eingeben.\n✍️ Notieren Sie, wie viele Dokumente & Features in quanteda_dfm_cleaned enthalten sind.\n\n\n\nLösung anzeigen\n# `quanteda_dfm_cleaned` erstellen\nquanteda_dfm_cleaned &lt;- quanteda_dfm %&gt;% \n  quanteda::dfm_remove(pattern = c(\"systematic\", \"literature\", \"review\"))\n\n# Überprüfung\nquanteda_dfm_cleaned\n\n\nDocument-feature matrix of: 36,680 documents, 135,074 features (99.93% sparse) and 43 docvars.\n                                  features\ndocs                               5-item world health organization well-being\n  https://openalex.org/W4293003987      1     2      1            1          3\n  https://openalex.org/W2750168540      0     0      0            0          0\n  https://openalex.org/W1998933811      0     0      0            0          0\n  https://openalex.org/W2547134104      0     0      7            0          4\n  https://openalex.org/W3047898105      0     0      4            0          0\n  https://openalex.org/W2149640470      0     0      0            0          0\n                                  features\ndocs                               index who-5 among widely used\n  https://openalex.org/W4293003987     1    10     1      1    3\n  https://openalex.org/W2750168540     0     0     0      0    0\n  https://openalex.org/W1998933811     0     0     0      0    0\n  https://openalex.org/W2547134104     0     0     0      0    1\n  https://openalex.org/W3047898105     0     0     0      0    0\n  https://openalex.org/W2149640470     0     0     0      0    1\n[ reached max_ndoc ... 36,674 more documents, reached max_nfeat ... 135,064 more features ]\n\n\nLösung anzeigen\n# Notiz:\n# `quanteda_dfm_cleaned` enthält 36680 Dokumente und 135074 Features.\n\n\n\n\n📋 Exercise 2: Neues Netzwerk der Top-Begriffe\n\n\n\n\n\n\nZiel der Aufgabe\n\n\n\n\nErstellung eines neuen Datensatzes review_subsample_new, der sich auf englischsprachig Bücher bzw. Buchrartikel beschränkt.\n\n\n\n\nNeues Dataset top_features_quanteda erstellen\n\nBasierend auf dem Dataset quanteda_dfm_cleaned,\nVerwenden Sie quanteda::topfeatures(20), um die 20 häufigsten Begriffe zu extrahieren.\nVerwenden Sie names(), um nur die Namen (nicht die Werte) zu speichern.\nSpeichern Sie diese Transformation, indem Sie einen neuen Datensatz mit dem Namen top_features_quanteda erstellen.\n\nVisualisierung des Netzwerks an Top-Begriffen\n\nBasierend auf dem Dataset quanteda_dfm_cleaned,\nTransformieren Sie die Daten mit quanteda::fcm() in eine Feature-Co-Occurrence-Matrix [FCM].\nAuswahl relevanter Hashtags mit quanteda::fcm_select(pattern = top_hashtags_quanteda, case_insensitive = FALSE).\nVisualisierung mit quanteda.textplots::textplot_network().\n\nErgebnisse interpretieren und vergleichen\n\nAnalysieren Sie die Beziehungen zwischen den Top-Begriffen.\nVergleichen Sie die Ergebnisse mit den Auswertungen der Folien. Welche Unterschiede gibt es?\n\n\n\n\nLösung anzeigen\n# Create top features\ntop_features_quanteda &lt;- quanteda_dfm_cleaned %&gt;% \n  topfeatures(20) %&gt;% \n  names()\n\n# Construct feature-occurrence matrix of features\nquanteda_dfm_cleaned %&gt;% \n  fcm() %&gt;% \n  fcm_select(pattern = top_features_quanteda) %&gt;% \n  textplot_network(\n    edge_color = \"#C50F3C\"\n  )"
  },
  {
    "objectID": "exercises/ms-exercise-09.html",
    "href": "exercises/ms-exercise-09.html",
    "title": "Unsupervised Machine Learning I",
    "section": "",
    "text": "Link to source file"
  },
  {
    "objectID": "exercises/ms-exercise-09.html#background",
    "href": "exercises/ms-exercise-09.html#background",
    "title": "Unsupervised Machine Learning I",
    "section": "Background",
    "text": "Background\n\n\n\n\n\n\nTodays’s data basis: OpenAlex\n\n\n\n\nVia API bzw. openalexR (Aria et al. 2024) gesammelte “works” der Datenbank OpenAlex mit Bezug zu Literaturriews in den Sozialwissenschaften zwischen 2013 und 2023\nDetaillierte Informationen und Ergebnisse zur Suchquery finden Sie hier."
  },
  {
    "objectID": "exercises/ms-exercise-09.html#preparation",
    "href": "exercises/ms-exercise-09.html#preparation",
    "title": "Unsupervised Machine Learning I",
    "section": "Preparation",
    "text": "Preparation\n\n\n\n\n\n\nWichtige Information\n\n\n\n\nBitte stellen Sie sicher, dass Sie das jeweilige R-Studio Projekt zur Übung geöffnet haben. Nur so funktionieren alle Dependencies korrekt.\nUm den einwandfreien Ablauf der Übung zu gewährleisten, wird für die Aufgaben auf eine eigenständige Datenerhebung verzichtet und ein Übungsdatensatz zu verfügung gestelt.\n\n\n\n\nPackages\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n    here, qs, # file management\n    magrittr, janitor, # data wrangling\n    easystats, sjmisc, # data analysis\n    gt, gtExtras, # table visualization\n    ggpubr, ggwordcloud, # visualization\n    # text analysis    \n    tidytext, widyr, # based on tidytext\n    quanteda, # based on quanteda\n    quanteda.textmodels, quanteda.textplots, quanteda.textstats, \n    openalexR, \n    tidyverse # load last to avoid masking issues\n  )\n\n\n\nImport und Vorverarbeitung der Daten\n\n# Import from local\nreview_works &lt;- qs::qread(here(\"data/session-07/openalex-review_works-2013_2023.qs\"))\n\n# Create correct data\nreview_works_correct &lt;- review_works %&gt;% \n    mutate(\n        # Create additional factor variables\n        publication_year_fct = as.factor(publication_year), \n        type_fct = as.factor(type)\n        )\n\n# Create subsample\nreview_subsample &lt;- review_works_correct %&gt;%\n    # Eingrenzung: Sprache und Typ\n    filter(language == \"en\") %&gt;% \n    filter(type == \"article\") %&gt;%\n    # Eingrenzung: Keine Einträge ohne Abstract\n    filter(!is.na(ab)) %&gt;% \n    # Datentranformation\n    unnest(topics, names_sep = \"_\") %&gt;%\n    filter(topics_name == \"field\" ) %&gt;% \n    filter(topics_i == \"1\") %&gt;% \n    # Eingrenzung: Forschungsfeldes\n    filter(\n    topics_display_name == \"Social Sciences\"|\n    topics_display_name == \"Psychology\"\n    ) %&gt;% \n    # Eingrenzung: Keine Einträge ohne Abstract\n    filter(!is.na(ab))   \n\n\n\nErstellung Korpus & DFM\n\n\nLösung anzeigen\n# Create corpus\nquanteda_corpus &lt;- review_subsample %&gt;% \n  quanteda::corpus(\n    docid_field = \"id\", \n    text_field = \"ab\"\n  )\n\n# Tokenize\nquanteda_token &lt;- quanteda_corpus %&gt;% \n  quanteda::tokens(\n    remove_punct = TRUE,\n    remove_symbols = TRUE, \n    remove_numbers = TRUE, \n    remove_url = TRUE, \n    split_tags = FALSE # keep hashtags and mentions\n  ) %&gt;% \n  quanteda::tokens_tolower() %&gt;% \n  quanteda::tokens_remove(\n    pattern = stopwords(\"en\")\n    )\n\n# Convert to Document-Feature-Matrix (DFM)\nquanteda_dfm &lt;- quanteda_token %&gt;% \n  quanteda::dfm()"
  },
  {
    "objectID": "exercises/ms-exercise-09.html#praktische-anwendung",
    "href": "exercises/ms-exercise-09.html#praktische-anwendung",
    "title": "Unsupervised Machine Learning I",
    "section": "🛠️ Praktische Anwendung",
    "text": "🛠️ Praktische Anwendung\n\n\n\n\n\n\nAchtung, bitte lesen!\n\n\n\n\nBevor Sie mit der Arbeit an den folgenden 📋 Exercises beginnen, stellen Sie bitte sicher, dass Sie alle Chunks des Abschnitts Preparation gerendert haben. Das können Sie tun, indem Sie den “Run all chunks above”-Knopf des nächsten Chunks benutzen.\nBei Fragen zum Code lohnt sich ein Blick in den Showcase (.qmd oder .html). Beim Showcase handelt es sich um eine kompakte Darstellung des in der Präsentation verwenden R-Codes. Sie können das Showcase also nutzen, um sich die Code-Bausteine anzusehen, die für die R-Outputs auf den Slides benutzt wurden.\n\n\n\n\n📋 Exercise 1: Cleaned DFM\n\n\n\n\n\n\nZiel der Aufgabe\n\n\n\n\n\n\n\nErstelen Sie einen neuen Datensatz quanteda_dfm_cleaned\n\nbasierend auf dem Datensatz quanteda_dfm\n\nVerwenden Sie quanteda::dfm_remove(pattern = c(\"systematic\", \"literature\", \"review\"), um die Suchquery zu entfernen.\nSpeichern Sie diese Umwandlung, indem Sie einen neuen Datensatz mit dem Namen quanteda_dfm_cleaned erstellen.\n\n\nÜberprüfen Sie die Transformation indem Sie quanteda_dfm_cleaned in die Konsole eingeben.\n✍️ Notieren Sie, wie viele Dokumente & Features in quanteda_dfm_cleaned enthalten sind.\n\n\n\nLösung anzeigen\n# `quanteda_dfm_cleaned` erstellen\n\n# Überprüfung\n\n# Notiz:\n# `quanteda_dfm_cleaned` enthält 36680 Dokumente und 135074 Features.\n\n\n\n\n📋 Exercise 2: Neues Netzwerk der Top-Begriffe\n\n\n\n\n\n\nZiel der Aufgabe\n\n\n\n\nErstellung eines neuen Datensatzes review_subsample_new, der sich auf englischsprachig Bücher bzw. Buchrartikel beschränkt.\n\n\n\n\nNeues Dataset top_features_quanteda erstellen\n\nBasierend auf dem Dataset quanteda_dfm_cleaned,\nVerwenden Sie quanteda::topfeatures(20), um die 20 häufigsten Begriffe zu extrahieren.\nVerwenden Sie names(), um nur die Namen (nicht die Werte) zu speichern.\nSpeichern Sie diese Transformation, indem Sie einen neuen Datensatz mit dem Namen top_features_quanteda erstellen.\n\nVisualisierung des Netzwerks an Top-Begriffen\n\nBasierend auf dem Dataset quanteda_dfm_cleaned,\nTransformieren Sie die Daten mit quanteda::fcm() in eine Feature-Co-Occurrence-Matrix [FCM].\nAuswahl relevanter Hashtags mit quanteda::fcm_select(pattern = top_hashtags_quanteda, case_insensitive = FALSE).\nVisualisierung mit quanteda.textplots::textplot_network().\n\nErgebnisse interpretieren und vergleichen\n\nAnalysieren Sie die Beziehungen zwischen den Top-Begriffen.\nVergleichen Sie die Ergebnisse mit den Auswertungen der Folien. Welche Unterschiede gibt es?\n\n\n\n\nLösung anzeigen\n# Create top features\n\n# Visualisierung des Netzwerks an Top-Begriffen"
  },
  {
    "objectID": "exercises/ms-showcase-09.html",
    "href": "exercises/ms-showcase-09.html",
    "title": "Unsupervised Machine Learning I",
    "section": "",
    "text": "Link to slides"
  },
  {
    "objectID": "exercises/ms-showcase-09.html#preparation",
    "href": "exercises/ms-showcase-09.html#preparation",
    "title": "Unsupervised Machine Learning I",
    "section": "Preparation",
    "text": "Preparation\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n    here, qs, # file management\n    magrittr, janitor, # data wrangling\n    easystats, sjmisc, # data analysis\n    gt, gtExtras, # table visualization\n    ggpubr, ggwordcloud, # visualization\n    # text analysis    \n    tidytext, widyr, # based on tidytext\n    quanteda, # based on quanteda\n    quanteda.textmodels, quanteda.textplots, quanteda.textstats, \n    stm, # structural topic modeling\n    openalexR, \n    tidyverse # load last to avoid masking issues\n  )"
  },
  {
    "objectID": "exercises/ms-showcase-09.html#codechunks-aus-der-sitzung",
    "href": "exercises/ms-showcase-09.html#codechunks-aus-der-sitzung",
    "title": "Unsupervised Machine Learning I",
    "section": "Codechunks aus der Sitzung",
    "text": "Codechunks aus der Sitzung\n\nErstellung und Bearbeitung der Subsample\n\nreview_subsample &lt;- review_works_correct %&gt;% \n  # Eingrenzung: Sprache und Typ\n  filter(language == \"en\") %&gt;% \n  filter(type == \"article\") %&gt;%\n  # Datentranformation\n  unnest(topics, names_sep = \"_\") %&gt;%\n  filter(topics_name == \"field\") %&gt;% \n  filter(topics_i == \"1\") %&gt;% \n  # Eingrenzung: Forschungsfeldes\n  filter(\n   topics_display_name == \"Social Sciences\"|\n   topics_display_name == \"Psychology\"\n    )\n\n# Überblick \nreview_subsample %&gt;% glimpse\n\nRows: 45,221\nColumns: 45\n$ id                          &lt;chr&gt; \"https://openalex.org/W4293003987\", \"https…\n$ title                       &lt;chr&gt; \"The WHO-5 Well-Being Index: A Systematic …\n$ display_name                &lt;chr&gt; \"The WHO-5 Well-Being Index: A Systematic …\n$ author                      &lt;list&gt; [&lt;data.frame[4 x 12]&gt;], [&lt;data.frame[2 x …\n$ ab                          &lt;chr&gt; \"The 5-item World Health Organization Well…\n$ publication_date            &lt;chr&gt; \"2015-01-01\", \"2017-08-28\", \"2014-01-01\", …\n$ relevance_score             &lt;dbl&gt; 938.7603, 752.3500, 591.2553, 576.1210, 56…\n$ so                          &lt;chr&gt; \"Psychotherapy and psychosomatics\", \"Journ…\n$ so_id                       &lt;chr&gt; \"https://openalex.org/S184803288\", \"https:…\n$ host_organization           &lt;chr&gt; \"Karger Publishers\", \"SAGE Publishing\", NA…\n$ issn_l                      &lt;chr&gt; \"0033-3190\", \"0739-456X\", NA, \"2214-7829\",…\n$ url                         &lt;chr&gt; \"https://doi.org/10.1159/000376585\", \"http…\n$ pdf_url                     &lt;chr&gt; \"https://www.karger.com/Article/Pdf/376585…\n$ license                     &lt;chr&gt; \"cc-by-nc\", NA, NA, \"cc-by\", NA, NA, \"cc-b…\n$ version                     &lt;chr&gt; \"publishedVersion\", NA, \"publishedVersion\"…\n$ first_page                  &lt;chr&gt; \"167\", \"93\", NA, \"89\", \"55\", \"2150\", \"e356…\n$ last_page                   &lt;chr&gt; \"176\", \"112\", NA, \"106\", \"64\", \"2159\", \"e3…\n$ volume                      &lt;chr&gt; \"84\", \"39\", NA, \"6\", \"277\", \"32\", \"2\", \"24…\n$ issue                       &lt;chr&gt; \"3\", \"1\", NA, NA, NA, \"19\", \"8\", NA, \"9\", …\n$ is_oa                       &lt;lgl&gt; TRUE, FALSE, TRUE, TRUE, TRUE, FALSE, TRUE…\n$ is_oa_anywhere              &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE,…\n$ oa_status                   &lt;chr&gt; \"hybrid\", \"green\", \"bronze\", \"gold\", \"bron…\n$ oa_url                      &lt;chr&gt; \"https://www.karger.com/Article/Pdf/376585…\n$ any_repository_has_fulltext &lt;lgl&gt; FALSE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE…\n$ language                    &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", …\n$ grants                      &lt;list&gt; NA, NA, NA, &lt;\"https://openalex.org/F43203…\n$ cited_by_count              &lt;int&gt; 2657, 1375, 2568, 803, 3664, 1553, 2895, 9…\n$ counts_by_year              &lt;list&gt; [&lt;data.frame[11 x 2]&gt;], [&lt;data.frame[7 x …\n$ publication_year            &lt;int&gt; 2015, 2017, 2014, 2016, 2020, 2014, 2017, …\n$ cited_by_api_url            &lt;chr&gt; \"https://api.openalex.org/works?filter=cit…\n$ ids                         &lt;list&gt; &lt;\"https://openalex.org/W4293003987\", \"htt…\n$ doi                         &lt;chr&gt; \"https://doi.org/10.1159/000376585\", \"http…\n$ type                        &lt;chr&gt; \"article\", \"article\", \"article\", \"article\"…\n$ referenced_works            &lt;list&gt; &lt;\"https://openalex.org/W1492518593\", \"htt…\n$ related_works               &lt;list&gt; &lt;\"https://openalex.org/W3020194755\", \"htt…\n$ is_paratext                 &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ is_retracted                &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ concepts                    &lt;list&gt; [&lt;data.frame[7 x 5]&gt;], [&lt;data.frame[18 x …\n$ topics_i                    &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ topics_score                &lt;dbl&gt; 0.9926, 0.9050, 0.9995, 0.9987, 0.9999, 1.…\n$ topics_name                 &lt;chr&gt; \"field\", \"field\", \"field\", \"field\", \"field…\n$ topics_id                   &lt;chr&gt; \"https://openalex.org/fields/32\", \"https:/…\n$ topics_display_name         &lt;chr&gt; \"Psychology\", \"Social Sciences\", \"Psycholo…\n$ publication_year_fct        &lt;fct&gt; 2015, 2017, 2014, 2016, 2020, 2014, 2017, …\n$ type_fct                    &lt;fct&gt; article, article, article, article, articl…\n\n\n\n\nExkurs: Identfikation von fehlenden Werten\n\nvisdat::vis_miss(review_subsample, warn_large_data = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnpassung der Subsample\n\nreview_subsample &lt;- review_works_correct %&gt;%\n  # Eingrenzung: Sprache und Typ\n  filter(language == \"en\") %&gt;% \n  filter(type == \"article\") %&gt;%\n  # Datentranformation\n  unnest(topics, names_sep = \"_\") %&gt;%\n  filter(topics_name == \"field\") %&gt;% \n  filter(topics_i == \"1\") %&gt;% \n  # Eingrenzung: Forschungsfeldes\n  filter(\n   topics_display_name == \"Social Sciences\"|\n   topics_display_name == \"Psychology\"\n   ) %&gt;% \n  # Eingrenzung: Keine Einträge ohne Abstract\n  filter(!is.na(ab))\n\n# Überblick \nreview_subsample %&gt;% glimpse\n\nRows: 36,680\nColumns: 45\n$ id                          &lt;chr&gt; \"https://openalex.org/W4293003987\", \"https…\n$ title                       &lt;chr&gt; \"The WHO-5 Well-Being Index: A Systematic …\n$ display_name                &lt;chr&gt; \"The WHO-5 Well-Being Index: A Systematic …\n$ author                      &lt;list&gt; [&lt;data.frame[4 x 12]&gt;], [&lt;data.frame[2 x …\n$ ab                          &lt;chr&gt; \"The 5-item World Health Organization Well…\n$ publication_date            &lt;chr&gt; \"2015-01-01\", \"2017-08-28\", \"2014-01-01\", …\n$ relevance_score             &lt;dbl&gt; 938.7603, 752.3500, 591.2553, 576.1210, 56…\n$ so                          &lt;chr&gt; \"Psychotherapy and psychosomatics\", \"Journ…\n$ so_id                       &lt;chr&gt; \"https://openalex.org/S184803288\", \"https:…\n$ host_organization           &lt;chr&gt; \"Karger Publishers\", \"SAGE Publishing\", NA…\n$ issn_l                      &lt;chr&gt; \"0033-3190\", \"0739-456X\", NA, \"2214-7829\",…\n$ url                         &lt;chr&gt; \"https://doi.org/10.1159/000376585\", \"http…\n$ pdf_url                     &lt;chr&gt; \"https://www.karger.com/Article/Pdf/376585…\n$ license                     &lt;chr&gt; \"cc-by-nc\", NA, NA, \"cc-by\", NA, NA, \"cc-b…\n$ version                     &lt;chr&gt; \"publishedVersion\", NA, \"publishedVersion\"…\n$ first_page                  &lt;chr&gt; \"167\", \"93\", NA, \"89\", \"55\", \"2150\", \"e356…\n$ last_page                   &lt;chr&gt; \"176\", \"112\", NA, \"106\", \"64\", \"2159\", \"e3…\n$ volume                      &lt;chr&gt; \"84\", \"39\", NA, \"6\", \"277\", \"32\", \"2\", \"24…\n$ issue                       &lt;chr&gt; \"3\", \"1\", NA, NA, NA, \"19\", \"8\", NA, \"9\", …\n$ is_oa                       &lt;lgl&gt; TRUE, FALSE, TRUE, TRUE, TRUE, FALSE, TRUE…\n$ is_oa_anywhere              &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE,…\n$ oa_status                   &lt;chr&gt; \"hybrid\", \"green\", \"bronze\", \"gold\", \"bron…\n$ oa_url                      &lt;chr&gt; \"https://www.karger.com/Article/Pdf/376585…\n$ any_repository_has_fulltext &lt;lgl&gt; FALSE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE…\n$ language                    &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", …\n$ grants                      &lt;list&gt; NA, NA, NA, &lt;\"https://openalex.org/F43203…\n$ cited_by_count              &lt;int&gt; 2657, 1375, 2568, 803, 3664, 1553, 2895, 9…\n$ counts_by_year              &lt;list&gt; [&lt;data.frame[11 x 2]&gt;], [&lt;data.frame[7 x …\n$ publication_year            &lt;int&gt; 2015, 2017, 2014, 2016, 2020, 2014, 2017, …\n$ cited_by_api_url            &lt;chr&gt; \"https://api.openalex.org/works?filter=cit…\n$ ids                         &lt;list&gt; &lt;\"https://openalex.org/W4293003987\", \"htt…\n$ doi                         &lt;chr&gt; \"https://doi.org/10.1159/000376585\", \"http…\n$ type                        &lt;chr&gt; \"article\", \"article\", \"article\", \"article\"…\n$ referenced_works            &lt;list&gt; &lt;\"https://openalex.org/W1492518593\", \"htt…\n$ related_works               &lt;list&gt; &lt;\"https://openalex.org/W3020194755\", \"htt…\n$ is_paratext                 &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ is_retracted                &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ concepts                    &lt;list&gt; [&lt;data.frame[7 x 5]&gt;], [&lt;data.frame[18 x …\n$ topics_i                    &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ topics_score                &lt;dbl&gt; 0.9926, 0.9050, 0.9995, 0.9987, 0.9999, 1.…\n$ topics_name                 &lt;chr&gt; \"field\", \"field\", \"field\", \"field\", \"field…\n$ topics_id                   &lt;chr&gt; \"https://openalex.org/fields/32\", \"https:/…\n$ topics_display_name         &lt;chr&gt; \"Psychology\", \"Social Sciences\", \"Psycholo…\n$ publication_year_fct        &lt;fct&gt; 2015, 2017, 2014, 2016, 2020, 2014, 2017, …\n$ type_fct                    &lt;fct&gt; article, article, article, article, articl…\n\n\n\n\nDTM/DFM Erstellung\n\n`tidytext``\n\n# Create tidy data\nsubsample_tidy &lt;- review_subsample %&gt;% \n    tidytext::unnest_tokens(\"text\", ab) %&gt;% \n    filter(!text %in% tidytext::stop_words$word)\n\n# Create summarized data\nsubsample_summarized &lt;- subsample_tidy %&gt;% \n  count(id, text) \n\n# Create DTM\nsubsample_dtm &lt;- subsample_summarized %&gt;% \n  cast_dtm(id, text, n)\n\n# Preview\nsubsample_dtm\n\n&lt;&lt;DocumentTermMatrix (documents: 36654, terms: 122147)&gt;&gt;\nNon-/sparse entries: 3280664/4473895474\nSparsity           : 100%\nMaximal term length: 188\nWeighting          : term frequency (tf)\n\n\n\n\nquanteda\n\n# Create corpus\nquanteda_corpus &lt;- review_subsample %&gt;% \n  quanteda::corpus(\n    docid_field = \"id\", \n    text_field = \"ab\"\n  )\n\n# Tokenize\nquanteda_token &lt;- quanteda_corpus %&gt;% \n  quanteda::tokens(\n    remove_punct = TRUE,\n    remove_symbols = TRUE, \n    remove_numbers = TRUE, \n    remove_url = TRUE, \n    split_tags = FALSE # keep hashtags and mentions\n  ) %&gt;% \n  quanteda::tokens_tolower() %&gt;% \n  quanteda::tokens_remove(\n    pattern = stopwords(\"en\")\n    )\n\n# Convert to Document-Feature-Matrix (DFM)\nquanteda_dfm &lt;- quanteda_token %&gt;% \n  quanteda::dfm()\n\n# Preview\nquanteda_dfm\n\n\n\n\nNetzwerk der Top-Begriffe\n\ntidytext\n\n# Extract most common hashtags\ntop50_features_tidy &lt;- subsample_tidy %&gt;% \n  count(text, sort = TRUE) %&gt;%\n  slice_head(n = 50) %&gt;% \n  pull(text)\n\n# Visualize\nsubsample_tidy %&gt;% \n  count(id, text, sort = TRUE) %&gt;% \n  filter(!is.na(text)) %&gt;% \n  cast_dfm(id, text, n) %&gt;% \n  quanteda::fcm() %&gt;% \n  quanteda::fcm_select(\n    pattern = top50_features_tidy,\n    case_insensitive = FALSE\n  ) %&gt;%  \n  quanteda.textplots::textplot_network(\n    edge_color = \"#04316A\"\n  )\n\n\n\n\n\n\n\n\n\n\nquanteda\n\n# Extract most common features \ntop50_features_quanteda &lt;- quanteda_dfm %&gt;% \n  topfeatures(50) %&gt;% \n  names()\n\n# Construct feature-occurrence matrix of features\nquanteda_dfm %&gt;% \n  fcm() %&gt;% \n  fcm_select(pattern = top50_features_quanteda) %&gt;% \n  textplot_network(\n    edge_color = \"#C50F3C\"\n  ) \n\n\n\n\n\n\n\n\n\n\n\nPreparation for STM\n\n# Pruning\nquanteda_dfm_trim &lt;- quanteda_dfm %&gt;% \n  dfm_trim( \n    min_docfreq = 10/nrow(review_subsample),\n    max_docfreq = 0.99, \n    docfreq_type = \"prop\")\n\n# Convert for stm topic modeling\nquanteda_stm &lt;- quanteda_dfm_trim %&gt;% \n   convert(to = \"stm\")\n\n\n\nStructural Topic Model\n\nSchätzung\n\ntictoc::tic()\nstm_mdl &lt;- stm::stm(\n  documents = quanteda_stm$documents,\n  vocab = quanteda_stm$vocab, \n  K = 20, \n  seed = 42,\n  max.em.its = 10,\n  init.type = \"Spectral\",\n  verbose = TRUE)\ntictoc::toc(log = TRUE)\n\n\n\nModelinformationen\n\n# Überblick über STM\nstm_mdl\n\nA topic model with 20 topics, 36650 documents and a 14322 word dictionary.\n\n\n\n\nÜberblick über die Themen\n\n# Simple\nplot(stm_mdl, type = \"summary\")\n\n\n\n\n\n\n\n\n\n# Komplex\ntop_gamma &lt;- stm_mdl %&gt;%\n  tidy(matrix = \"gamma\") %&gt;% \n  dplyr::group_by(topic) %&gt;%\n  dplyr::summarise(gamma = mean(gamma), .groups = \"drop\") %&gt;%\n  dplyr::arrange(desc(gamma))\n\ntop_beta &lt;- stm_mdl %&gt;%\n  tidytext::tidy(.) %&gt;% \n  dplyr::group_by(topic) %&gt;%\n  dplyr::arrange(-beta) %&gt;%\n  dplyr::top_n(10, wt = beta) %&gt;% \n  dplyr::select(topic, term) %&gt;%\n  dplyr::summarise(terms_beta = toString(term), .groups = \"drop\")\n\ntop_topics_terms &lt;- top_beta %&gt;% \n  dplyr::left_join(top_gamma, by = \"topic\") %&gt;%\n  dplyr::mutate(\n          topic = reorder(topic, gamma)\n      )\n\n# Preview\ntop_topics_terms %&gt;%\n  mutate(across(gamma, ~round(.,3))) %&gt;% \n  dplyr::arrange(-gamma) %&gt;% \n  gt() %&gt;% \n  cols_label(\n    topic = \"Topic\", \n    terms_beta = \"Top Terms (based on beta)\",\n    gamma = \"Gamma\"\n  ) %&gt;% \n  gtExtras::gt_theme_538()\n\n\n\n\n\n\n\nTopic\nTop Terms (based on beta)\nGamma\n\n\n\n\n16\nresearch, literature, review, paper, systematic, future, study, analysis, findings, knowledge\n0.142\n\n\n19\nstudies, interventions, review, systematic, evidence, outcomes, included, quality, intervention, health\n0.096\n\n\n13\nhealth, mental, care, review, support, family, children, social, factors, studies\n0.079\n\n\n9\nlearning, students, education, school, review, skills, educational, teachers, teaching, study\n0.072\n\n\n11\nstudy, literature, research, can, work, development, review, also, human, economic\n0.068\n\n\n15\nstudies, ci, effect, meta-analysis, depression, p, trials, anxiety, effects, interventions\n0.061\n\n\n14\nsocial, media, information, use, digital, data, technology, communication, review, research\n0.060\n\n\n17\nstudies, children, relationship, review, language, variables, research, factors, results, effects\n0.052\n\n\n6\nprevalence, studies, covid-19, suicide, among, risk, pandemic, countries, ci, vaccine\n0.050\n\n\n1\nsleep, studies, eating, cognitive, review, associated, weight, body, may, association\n0.044\n\n\n2\nhealth, studies, women, gender, review, care, social, services, cultural, access\n0.043\n\n\n18\nstudies, health, used, measures, review, tools, instruments, assessment, training, n\n0.039\n\n\n3\ntreatment, disorder, disorders, patients, ptsd, symptoms, clinical, studies, therapy, anxiety\n0.036\n\n\n7\narticles, review, adolescents, studies, literature, search, use, results, systematic, databases\n0.036\n\n\n4\npatients, articles, review, music, therapy, cancer, can, study, pain, life\n0.032\n\n\n12\nviolence, studies, use, sexual, risk, h3, ipv, substance, alcohol, review\n0.031\n\n\n5\net, al, university, review, gt, literature, lt, author, p, search\n0.030\n\n\n8\nphysical, training, studies, disability, exercise, disabilities, employment, ed, review, strength\n0.015\n\n\n20\nattachment, studies, scholar, google, science, review, social, styles, welfare, research\n0.009\n\n\n10\nde, la, y, en, los, e, 的, el, se, que\n0.004\n\n\n\n\n\n\n\n\n\nThemenkorrelation\n\nstm_corr &lt;- stm::topicCorr(stm_mdl)\nplot(stm_corr)\n\n\n\n\n\n\n\n\n\n\nFokus auf einzele Themen\n\nProminente Wörter\n\n# Fokus auf Themas 16\nstm::labelTopics(stm_mdl, topic=16)\n\nTopic 16 Top Words:\n     Highest Prob: research, literature, review, paper, systematic, future, study \n     FREX: tourism, sustainable, sustainability, originality, innovation, conceptual, agenda \n     Lift: paradigmatic, hrd, edlm, positivist, internationalisation, tccm, wom \n     Score: tourism, research, literature, paper, leadership, themes, sustainable \n\n# Fokus auf Thema 10\nstm::labelTopics(stm_mdl, topic=10)\n\nTopic 10 Top Words:\n     Highest Prob: de, la, y, en, los, e, 的 \n     FREX: resultados, foram, que, sobre, intervenciones, riesgo, uma \n     Lift: criterios, efecto, así, comparación, cumplieron, debido, depresión \n     Score: de, la, 的, en, los, y, que"
  },
  {
    "objectID": "exercises/ms-exercise-10_solution.html",
    "href": "exercises/ms-exercise-10_solution.html",
    "title": "Unsupervised Machine Learning II",
    "section": "",
    "text": "Link to source file"
  },
  {
    "objectID": "exercises/ms-exercise-10_solution.html#background",
    "href": "exercises/ms-exercise-10_solution.html#background",
    "title": "Unsupervised Machine Learning II",
    "section": "Background",
    "text": "Background\n\n\n\n\n\n\nTodays’s data basis: OpenAlex\n\n\n\n\nVia API bzw. openalexR (Aria et al. 2024) gesammelte “works” der Datenbank OpenAlex mit Bezug zu Literaturriews in den Sozialwissenschaften zwischen 2013 und 2023\nDetaillierte Informationen und Ergebnisse zur Suchquery finden Sie hier."
  },
  {
    "objectID": "exercises/ms-exercise-10_solution.html#preparation",
    "href": "exercises/ms-exercise-10_solution.html#preparation",
    "title": "Unsupervised Machine Learning II",
    "section": "Preparation",
    "text": "Preparation\n\n\n\n\n\n\nWichtige Information\n\n\n\n\nBitte stellen Sie sicher, dass Sie das jeweilige R-Studio Projekt zur Übung geöffnet haben. Nur so funktionieren alle Dependencies korrekt.\nUm den einwandfreien Ablauf der Übung zu gewährleisten, wird für die Aufgaben auf eine eigenständige Datenerhebung verzichtet und ein Übungsdatensatz zu verfügung gestelt.\n\n\n\n\nPackages\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n    here, qs, # file management\n    magrittr, janitor, # data wrangling\n    easystats, sjmisc, # data analysis\n    gt, gtExtras, # table visualization\n    ggpubr, ggwordcloud, # visualization\n    # text analysis    \n    tidytext, widyr, # based on tidytext\n    quanteda, # based on quanteda\n    quanteda.textmodels, quanteda.textplots, quanteda.textstats, \n    stm, # structural topic modeling\n    openalexR, pushoverr, tictoc, \n    tidyverse # load last to avoid masking issues\n  )\n\n\n\nImport und Vorverarbeitung der Daten\n\n\nLösung anzeigen\nreview_works &lt;- qs::qread(here(\"data/session-07/openalex-review_works-2013_2023.qs\"))\n\n# Create correct data\nreview_subsample &lt;- review_works %&gt;% \n    # Create additional factor variables\n    mutate(\n        publication_year_fct = as.factor(publication_year), \n        type_fct = as.factor(type)\n        ) %&gt;%\n    # Eingrenzung: Sprache und Typ\n    filter(language == \"en\") %&gt;% \n    filter(type == \"article\") %&gt;%\n    # Datentranformation\n    unnest(topics, names_sep = \"_\") %&gt;%\n    filter(topics_name == \"field\") %&gt;% \n    filter(topics_i == \"1\") %&gt;% \n    # Eingrenzung: Forschungsfeldes\n    filter(\n    topics_display_name == \"Social Sciences\"|\n    topics_display_name == \"Psychology\"\n    ) %&gt;% \n    mutate(\n        field = as.factor(topics_display_name)\n    ) %&gt;% \n    # Eingrenzung: Keine Einträge ohne Abstract\n    filter(!is.na(ab))\n\n\n\n\nLösung anzeigen\n# Create corpus\nquanteda_corpus &lt;- review_subsample %&gt;% \n  quanteda::corpus(\n    docid_field = \"id\", \n    text_field = \"ab\"\n  )\n\n# Tokenize\nquanteda_token &lt;- quanteda_corpus %&gt;% \n  quanteda::tokens(\n    remove_punct = TRUE,\n    remove_symbols = TRUE, \n    remove_numbers = TRUE, \n    remove_url = TRUE, \n    split_tags = FALSE # keep hashtags and mentions\n  ) %&gt;% \n  quanteda::tokens_tolower() %&gt;% \n  quanteda::tokens_remove(\n    pattern = stopwords(\"en\")\n    )\n\n# Convert to Document-Feature-Matrix (DFM)\nquanteda_dfm &lt;- quanteda_token %&gt;% \n  quanteda::dfm()\n\n# Pruning\nquanteda_dfm_trim &lt;- quanteda_dfm %&gt;% \n  dfm_trim( \n    min_docfreq = 10/nrow(review_subsample),\n    max_docfreq = 0.99, \n    docfreq_type = \"prop\")\n\n# Convert for stm topic modeling\nquanteda_stm &lt;- quanteda_dfm_trim %&gt;% \n   convert(to = \"stm\")"
  },
  {
    "objectID": "exercises/ms-exercise-10_solution.html#praktische-anwendung",
    "href": "exercises/ms-exercise-10_solution.html#praktische-anwendung",
    "title": "Unsupervised Machine Learning II",
    "section": "🛠️ Praktische Anwendung",
    "text": "🛠️ Praktische Anwendung\n\n\n\n\n\n\nAchtung, bitte lesen!\n\n\n\n\nBevor Sie mit der Arbeit an den folgenden 📋 Exercises beginnen, stellen Sie bitte sicher, dass Sie alle Chunks des Abschnitts Preparation gerendert haben. Das können Sie tun, indem Sie den “Run all chunks above”-Knopf des nächsten Chunks benutzen.\nBei Fragen zum Code lohnt sich ein Blick in den Showcase (.qmd oder .html). Beim Showcase handelt es sich um eine kompakte Darstellung des in der Präsentation verwenden R-Codes. Sie können das Showcase also nutzen, um sich die Code-Bausteine anzusehen, die für die R-Outputs auf den Slides benutzt wurden.\n\n\n\n\n📋 Exercise 1: Visualisierung der Themenprävalenz\n\n1.1. Auswahl des passenden Models\n\nErstelen Sie einen neuen Datensatz stm_mdl_k40\n\nbasierend auf dem Datensatz stm_serach\n\nVerwenden Sie filter(k == 40), um das Modell mit 40 Themen zu auszuwählen.\nVerwenden Sie pull(mdl) %&gt;% .[[1]] um die Spalte und das Element zu extrahieren, die das Modell enthält.\nSpeichern Sie diese Umwandlung, indem Sie einen neuen Datensatz mit dem Namen stm_mdl_k40 erstellen.\n\n\nÜberprüfen Sie die Transformation indem Sie stm_mdl_k40 in die Konsole eingeben.\n\n\n\nLösung anzeigen\n# Pull tpm with 40 topics\nstm_mdl_k40 &lt;- stm_search %&gt;% \n  filter(k == 40) %&gt;% \n  pull(mdl) %&gt;% \n  .[[1]]\n\n# Check\nstm_mdl_k40\n\n\nA topic model with 40 topics, 36650 documents and a 14322 word dictionary.\n\n\n\n\n1.2. Identifikation der Top-Terms für jedes Thema\n\nErstellen Sie einen neuen Datensatz td_beta\n\nbasierend auf dem Datensatz stm_mdl_k40,\nVerwenden Sie tidy(method = \"frex\"), um die Beta-Matrix zu erstellen.\n\nErstellen Sie einen neuen Datensatz top_terms\n\nbasierend auf dem Datenastz td_beta,\n\nVerwenden Sie arrange(beta), um die Begriffe nach Beta zu sortieren.\nGruppieren Sie die Begriffe nach topic mit group_by(topic).\nExtrahieren Sie die 7 häufigsten Begriffe mit top_n(7, beta).\nSortieren Sie die Begriffe absteigend mit arrange(-beta).\nWählen Sie die Variablen topic und term mit select(topic, term) aus.\nExtrahieren Sie die Top-Begriffe pro Thema mit summarise(terms = list(term)).\nTransformieren Sie die extrahierten Begriffe pro Thema mit map(terms, paste, collapse = \", \") zu einem String.\n“Entpacken” Sie die Begriffe aus der Liste (unnesten) mit unnest(cols = c(terms)).\n\n\nÜberprüfen Sie die Transformation indem Sie top_terms in die Konsole eingeben.\n\n\n\nLösung anzeigen\n# Create tidy beta matrix\ntd_beta &lt;- tidy(stm_mdl_k40, method = \"frex\")\n\n# Create top terms\ntop_terms &lt;- td_beta %&gt;%\n  arrange(beta) %&gt;%\n  group_by(topic) %&gt;%\n  top_n(7, beta) %&gt;%\n  arrange(-beta) %&gt;%\n  select(topic, term) %&gt;%\n  summarise(terms = list(term)) %&gt;%\n  mutate(terms = map(terms, paste, collapse = \", \")) %&gt;% \n  unnest(cols = c(terms))\n\n# Output\ntop_terms\n\n\n# A tibble: 40 × 2\n   topic terms                                                                  \n   &lt;int&gt; &lt;chr&gt;                                                                  \n 1     1 care, nursing, healthcare, nurses, professionals, patients, patient    \n 2     2 students, school, academic, education, educational, schools, literacy  \n 3     3 的, 研究, 和, rs, 在, 了, 性                                           \n 4     4 elderly, #x0d, can, review, literature, google, keywords               \n 5     5 article, journal, decision, describes, aids, pressure, section         \n 6     6 prevalence, countries, among, studies, rates, population, higher       \n 7     7 depression, anxiety, psychological, stress, life, symptoms, cancer     \n 8     8 people, services, community, service, barriers, participation, support \n 9     9 factors, relationship, positive, studies, associated, behavior, negati…\n10    10 b, et, al, r, s, c, d                                                  \n# ℹ 30 more rows\n\n\n\n\n1.3 Erstellung der Prävalenz-Tabelle für die Themen\n\nErstellen Sie einen neuen Datensatz td_gamma\n\nbasierend auf dem Datensatz stm_mdl_k40,\nVerwenden Sie tidy(), um die Gamma-Matrix zu erstellen.\nVerwenden Sie document_names = names(quanteda_stm$documents) um die Dokumentennamen zu speichern\n\nErstellen Sie einen neuen Datensatz prevalence\n\nbasierend auf dem Datensatz td_gamma,\n\nGruppieren Sie die Themen nach topic mit group_by(topic).\nBerechnen Sie den Durchschnitt der gamma-Werte pro Thema mit summarise(gamma = mean(gamma)).\nSortieren Sie die Themen (absteigend) nach gamma mit arrange(desc(gamma)).\nVerknüpfen Sie die Top-Begriffe mit den Themen mit left_join(top_terms, by = \"topic\").\nÜberarbeiten Sie die Variable topic mit dem mutate-Befehl:\n\nErstellen Sie eine neue Variable topic mit paste0(\"Topic \",sprintf(\"%02d\", topic)).\nOrdnen Sie die Themen nach gamma mit reorder(topic, gamma).\n\n\n\nErstellung Sie eine Tabelle als Output\n\nbasierend auf dem Datenastz prevalence,\n\nVerwenden Sie gt() um eine Tabelle zu erstellen.\nFormatieren Sie die Spalte gamma mit fmt_number(columns = vars(gamma), decimals = 2) um nur zwei Nachkommastellen anzuzeigen.\nVerwenden Sie gtExtras::gt_theme_538() um das Design der Tabelle anzupassen.\n\n\n✍️ Auf Basis des Outputs von prevalence Notieren Sie, welche Themen Sie als problematisch sehen und warum.\n\n\n\nLösung anzeigen\n# Create tidy gamma matrix\ntd_gamma &lt;- tidy(\n  stm_mdl_k40, \n  matrix = \"gamma\", \n  document_names = names(quanteda_stm$documents)\n  )\n\n# Create prevalence\nprevalence &lt;- td_gamma %&gt;%\n  group_by(topic) %&gt;%\n  summarise(gamma = mean(gamma)) %&gt;%\n  arrange(desc(gamma)) %&gt;%\n  left_join(top_terms, by = \"topic\") %&gt;%\n  mutate(topic = paste0(\"Topic \",sprintf(\"%02d\", topic)),\n         topic = reorder(topic, gamma))\n\n# Output\nprevalence %&gt;% \n  gt() %&gt;% \n  fmt_number(\n    columns = vars(gamma), \n    decimals = 2) %&gt;% \n  gtExtras::gt_theme_538()\n\n\n\n\n\n\n\n\ntopic\ngamma\nterms\n\n\n\n\nTopic 16\n0.09\nresearch, review, literature, future, findings, systematic, studies\n\n\nTopic 19\n0.07\nstudies, included, review, quality, evidence, data, systematic\n\n\nTopic 38\n0.05\narticles, search, science, databases, review, systematic, criteria\n\n\nTopic 39\n0.04\nstudy, research, literature, analysis, used, results, method\n\n\nTopic 09\n0.04\nfactors, relationship, positive, studies, associated, behavior, negative\n\n\nTopic 25\n0.04\nlearning, education, students, teaching, teachers, skills, higher\n\n\nTopic 29\n0.04\ncultural, change, policy, political, human, identity, different\n\n\nTopic 35\n0.04\ninterventions, intervention, effectiveness, n, outcomes, effective, studies\n\n\nTopic 20\n0.03\nmanagement, tourism, development, public, paper, economic, marketing\n\n\nTopic 14\n0.03\ndigital, use, information, technology, online, communication, technologies\n\n\nTopic 34\n0.03\neffect, effects, meta-analysis, p, ptsd, ci, significant\n\n\nTopic 30\n0.03\ndisorders, disorder, suicide, eating, risk, suicidal, psychiatric\n\n\nTopic 33\n0.03\nphysical, cognitive, activity, studies, body, exercise, weight\n\n\nTopic 28\n0.03\nsleep, ci, meta-analysis, risk, studies, pooled, p\n\n\nTopic 21\n0.03\ntreatment, therapy, trials, patients, music, pain, controlled\n\n\nTopic 18\n0.03\nmeasures, assessment, used, tools, measurement, instruments, measure\n\n\nTopic 32\n0.03\nhealth, mental, stigma, problems, wellbeing, outcomes, review\n\n\nTopic 06\n0.02\nprevalence, countries, among, studies, rates, population, higher\n\n\nTopic 08\n0.02\npeople, services, community, service, barriers, participation, support\n\n\nTopic 15\n0.02\nreviews, outcomes, reporting, systematic, outcome, items, preferred\n\n\nTopic 13\n0.02\nfamily, support, resilience, experiences, caregivers, parents, parental\n\n\nTopic 07\n0.02\ndepression, anxiety, psychological, stress, life, symptoms, cancer\n\n\nTopic 17\n0.02\nchildren, adolescents, language, development, early, child, skills\n\n\nTopic 01\n0.02\ncare, nursing, healthcare, nurses, professionals, patients, patient\n\n\nTopic 23\n0.02\nsocial, media, older, adults, use, loneliness, people\n\n\nTopic 37\n0.02\ntraining, programs, work, program, professional, skills, workplace\n\n\nTopic 36\n0.02\nliterature, history, american, black, book, literary, historical\n\n\nTopic 24\n0.02\nviolence, women, abuse, sexual, ipv, child, trauma\n\n\nTopic 26\n0.02\ncovid-19, pandemic, vaccine, vaccination, health, acceptance, disease\n\n\nTopic 27\n0.02\nuse, gender, sexual, substance, alcohol, sex, men\n\n\nTopic 02\n0.02\nstudents, school, academic, education, educational, schools, literacy\n\n\nTopic 31\n0.02\nenvironment, environmental, urban, travel, physical, transport, safety\n\n\nTopic 40\n0.01\nauthors, interest, information, group, studies, case, term\n\n\nTopic 12\n0.01\ncrime, review, police, et, al, studies, may\n\n\nTopic 11\n0.01\nuniversity, author, papers, college, search, review, share\n\n\nTopic 04\n0.01\nelderly, #x0d, can, review, literature, google, keywords\n\n\nTopic 10\n0.01\nb, et, al, r, s, c, d\n\n\nTopic 05\n0.00\narticle, journal, decision, describes, aids, pressure, section\n\n\nTopic 22\n0.00\nde, la, y, en, los, el, se\n\n\nTopic 03\n0.00\n的, 研究, 和, rs, 在, 了, 性\n\n\n\n\n\n\n\n\n\n\n📋 Exercise 2: Einfluss der Metadaten\n\n2.1. Schätzung der Meta-Effekte\n\nErstellen Sie einen neuen Datensatz effects:\n\nVerwenden Sie die Funktion estimateEffect(), um die Effekte zu schätzen.\nVerwenden Sie für das formular-Argument 1:40 ~ publication_year_fct + field, um die Effekte der Veröffentlichungsjahre und Fachbereiche zu schätzen.\nVerwenden Sie stm_mdl_40 als das zu analysierende Modell.\nVerwenden Sie meta = quanteda_stm$meta, um die Metadaten für die Schätzung zu verwenden.\n\n\n\n\nLösung anzeigen\n# Create data\neffects &lt;- estimateEffect(\n    1:40 ~ publication_year_fct + field,\n    stm_mdl_k40,\n    meta = quanteda_stm$meta)\n\n\n\n\n2.2. Untersuchung der Effekte\n\nErstellen Sie einen neuen Datensatz effects_tidy eine bereinigte Tabelle der Effekte:\n\nBasierend auf dem Datensatz effects\n\n\nVerwenden Sie die tidy() Funktion, um die Effekte in ein aufbereitetes Format zu bringen.\nFiltern Sie die Daten:\n\nEntfernen Sie Zeilen, bei denen term den Wert (Intercept) hat.\nBehalten Sie nur die Zeilen, bei denen term == \"fieldSocial Sciences\" ist.\n\nEntfernen Sie die Spalte term mit select(-term)\n\nErstellen Sie ein Tabelle zur Überprüfung der Effekte\n\nBasierend auf dem Datensatz effects_tidy:\n\n\nVerwenden Sie die Funktion gt(), um eine Tabelle zu erstellen.\nFormatieren Sie alle numerischen Variablen mit fmt_number(columns = -c(topic), decimals = 3), um lediglich drei Dezimalstellen darzustellen.\nVerwenden Sie data_color(columns = estimate, method = \"numeric\", palette = \"viridis\"), um die Schätzwerte farblich zu kennzeichnen.\nWenden Sie das Design gtExtras::gt_theme_538() an.\n\n✍️ Notieren Sie, welches Thema am stärksten im Forschungsfeld “Social Science” vertreten ist.\n\n\n\nLösung anzeigen\n# Filter effect data\neffects_tidy &lt;- effects %&gt;% \n  tidy() %&gt;% \n  filter(\n    term != \"(Intercept)\",\n    term == \"fieldSocial Sciences\") %&gt;% \n    select(-term)\n\n\n# Explore effects (table outpu)\neffects_tidy %&gt;% \n    gt() %&gt;% \n    fmt_number(\n      columns = -c(topic),\n      decimals = 3\n    ) %&gt;% \n    data_color(\n       columns = estimate,\n    method = \"numeric\",\n    palette = \"viridis\"\n  ) %&gt;% \n  gtExtras::gt_theme_538()\n\n\n\n\n\n\n\n\ntopic\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n1\n−0.008\n0.001\n−9.325\n0.000\n\n\n2\n0.011\n0.001\n16.122\n0.000\n\n\n3\n0.000\n0.000\n−1.728\n0.084\n\n\n4\n0.005\n0.001\n5.930\n0.000\n\n\n5\n0.003\n0.000\n11.751\n0.000\n\n\n6\n0.005\n0.001\n6.044\n0.000\n\n\n7\n−0.020\n0.001\n−25.432\n0.000\n\n\n8\n0.012\n0.001\n13.968\n0.000\n\n\n9\n−0.015\n0.001\n−16.445\n0.000\n\n\n10\n−0.001\n0.000\n−1.470\n0.142\n\n\n11\n0.006\n0.001\n9.085\n0.000\n\n\n12\n0.008\n0.001\n11.209\n0.000\n\n\n13\n−0.009\n0.001\n−10.366\n0.000\n\n\n14\n0.021\n0.001\n18.645\n0.000\n\n\n15\n0.001\n0.001\n1.796\n0.073\n\n\n16\n0.037\n0.001\n32.200\n0.000\n\n\n17\n−0.011\n0.001\n−13.087\n0.000\n\n\n18\n−0.011\n0.001\n−14.460\n0.000\n\n\n19\n−0.026\n0.001\n−28.087\n0.000\n\n\n20\n0.053\n0.001\n43.944\n0.000\n\n\n21\n−0.041\n0.001\n−35.138\n0.000\n\n\n22\n−0.001\n0.000\n−2.013\n0.044\n\n\n23\n0.013\n0.001\n18.233\n0.000\n\n\n24\n0.002\n0.001\n1.427\n0.154\n\n\n25\n0.036\n0.001\n26.794\n0.000\n\n\n26\n0.008\n0.001\n6.374\n0.000\n\n\n27\n−0.008\n0.001\n−8.626\n0.000\n\n\n28\n−0.028\n0.001\n−21.997\n0.000\n\n\n29\n0.038\n0.001\n33.595\n0.000\n\n\n30\n−0.048\n0.001\n−43.256\n0.000\n\n\n31\n0.007\n0.001\n6.914\n0.000\n\n\n32\n−0.013\n0.001\n−17.742\n0.000\n\n\n33\n−0.034\n0.001\n−33.255\n0.000\n\n\n34\n−0.039\n0.001\n−31.501\n0.000\n\n\n35\n−0.025\n0.001\n−29.576\n0.000\n\n\n36\n0.022\n0.001\n21.341\n0.000\n\n\n37\n0.012\n0.001\n15.686\n0.000\n\n\n38\n−0.004\n0.001\n−5.524\n0.000\n\n\n39\n0.042\n0.001\n41.260\n0.000\n\n\n40\n0.001\n0.000\n2.172\n0.030\n\n\n\n\n\n\n\nLösung anzeigen\n#### Notes:\n# \n\n\n\n\n\n📋 Exercise 3: Einzelthema im Fokus\n\n3.1. Benennung des Themas k = 20\n\nBenennen Sie das Thema k = 20 aus dem Modell stm_mdl_40:\n\nVerwenden Sie die Funktion labelTopics().\nGeben Sie das Thema 20 als Parameter mit topic = 20 an.\n\n✍️ Notieren Sie die Themennamen. Begründen Sie kurz Ihre Entscheidung.\n\n\n\nLösung anzeigen\n# Create topic label\nstm_mdl_k40 %&gt;% labelTopics(topic = 20)\n\n\nTopic 20 Top Words:\n     Highest Prob: management, tourism, development, public, paper, economic, marketing \n     FREX: tourism, marketing, consumer, halal, sustainable, disaster, economy \n     Lift: hotel, mega-events, tourism, post-disaster, smes, tourist, b2b \n     Score: tourism, marketing, halal, business, governance, sustainable, disaster \n\n\nLösung anzeigen\n# Themenname:\n\n\n\n\n3.2. Zusammenführung mit OpenAlex-Daten\n\nErstellen Sie einen neuen Datensatz gamma_export\n\nbasierend auf dem Datensatz stm_mdl_k40:\n\nVerwenden Sie tidy() um die Gamma-Matrix zu erstellen. Geben Sie matrix = \"gamma\" und document_names = names(quanteda_stm$documents) als Parameter an.\nGruppieren Sie die Dokumente nach document mit group_by(document).\nWählen Sie die Dokumente mit dem höchsten gamma-Wert mit slice_max(gamma).\nLösen Sie die Gruppierung mit dplyr::ungroup().\nVerknüpfen Sie die Daten mit review_subsample mittels left_join(review_subsample, by = c(\"document\" = \"id\")).\n\nBenennen Sie die Spalte document in id um mit dplyr::rename(id = document)\nErstellen Sie eine neue Variable stm_topic mit Hilfe des mutate()-Befehls. Verwenden Sie as.factor(paste(\"Topic\", sprintf(\"%02d\", topic))) um die Themen zu benennen und als Faktor zu speichern.\n\nÜberprüfen Sie Transformation mit Hilfe der glimpse()-Funktion, um sicherzustellen, dass die Daten korrekt erstellt wurden.\n\n\n\nLösung anzeigen\n# Create gamma export\ngamma_export &lt;- stm_mdl_k40 %&gt;% \n  tidytext::tidy(\n    matrix = \"gamma\", \n    document_names = names(quanteda_stm$documents)) %&gt;%\n  dplyr::group_by(document) %&gt;% \n  dplyr::slice_max(gamma) %&gt;% \n  dplyr::ungroup() %&gt;% \n  dplyr::left_join(review_subsample, by = c(\"document\" = \"id\")) %&gt;% \n  dplyr::rename(id = document) %&gt;% \n  dplyr::mutate(\n    stm_topic = as.factor(paste(\"Topic\", sprintf(\"%02d\", topic)))\n  )\n\n# Check\nglimpse(gamma_export)\n\n\nRows: 36,650\nColumns: 49\n$ id                          &lt;chr&gt; \"https://openalex.org/W1000529773\", \"https…\n$ topic                       &lt;int&gt; 25, 14, 14, 14, 7, 16, 34, 10, 30, 19, 9, …\n$ gamma                       &lt;dbl&gt; 0.5042240, 0.2308276, 0.4425161, 0.3935272…\n$ title                       &lt;chr&gt; \"A critical evaluation of the teaching of …\n$ display_name                &lt;chr&gt; \"A critical evaluation of the teaching of …\n$ author                      &lt;list&gt; [&lt;data.frame[1 x 12]&gt;], [&lt;data.frame[2 x …\n$ ab                          &lt;chr&gt; \"A Critical Evaluation of the Teaching of …\n$ publication_date            &lt;chr&gt; \"2014-01-16\", \"2015-05-26\", \"2014-05-22\", …\n$ relevance_score             &lt;dbl&gt; 4.012791, 27.896568, 32.074608, 23.670475,…\n$ so                          &lt;chr&gt; NA, \"Proceedings of the annual conference …\n$ so_id                       &lt;chr&gt; NA, \"https://openalex.org/S4306523984\", \"h…\n$ host_organization           &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, \"Taylor & …\n$ issn_l                      &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, \"1381-1118…\n$ url                         &lt;chr&gt; \"https://uwispace.sta.uwi.edu/dspace/bitst…\n$ pdf_url                     &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"h…\n$ license                     &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"c…\n$ version                     &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"p…\n$ first_page                  &lt;chr&gt; NA, \"76\", NA, NA, NA, NA, NA, NA, \"1\", \"11…\n$ last_page                   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, \"21\", \"122…\n$ volume                      &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, \"20\", \"78\"…\n$ issue                       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, \"1\", NA, \"…\n$ is_oa                       &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ is_oa_anywhere              &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ oa_status                   &lt;chr&gt; \"closed\", \"closed\", \"closed\", \"closed\", \"c…\n$ oa_url                      &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, \"https://e…\n$ any_repository_has_fulltext &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ language                    &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", …\n$ grants                      &lt;list&gt; NA, NA, NA, NA, NA, NA, NA, NA, &lt;\"https:/…\n$ cited_by_count              &lt;int&gt; 0, 1, 1, 1, 1, 0, 2, 0, 226, 159, 122, 31,…\n$ counts_by_year              &lt;list&gt; NA, [&lt;data.frame[1 x 2]&gt;], [&lt;data.frame[1…\n$ publication_year            &lt;int&gt; 2014, 2015, 2014, 2013, 2013, 2015, 2015, …\n$ cited_by_api_url            &lt;chr&gt; \"https://api.openalex.org/works?filter=cit…\n$ ids                         &lt;list&gt; &lt;\"https://openalex.org/W1000529773\", \"100…\n$ doi                         &lt;chr&gt; NA, \"https://doi.org/10.5555/2814058.28141…\n$ type                        &lt;chr&gt; \"article\", \"article\", \"article\", \"article\"…\n$ referenced_works            &lt;list&gt; NA, &lt;\"https://openalex.org/W1526029332\", …\n$ related_works               &lt;list&gt; &lt;\"https://openalex.org/W958254955\", \"http…\n$ is_paratext                 &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ is_retracted                &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ concepts                    &lt;list&gt; [&lt;data.frame[7 x 5]&gt;], [&lt;data.frame[16 x …\n$ topics_i                    &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ topics_score                &lt;dbl&gt; 0.9566, 0.9812, 0.9894, 0.9999, 0.9752, 0.…\n$ topics_name                 &lt;chr&gt; \"field\", \"field\", \"field\", \"field\", \"field…\n$ topics_id                   &lt;chr&gt; \"https://openalex.org/fields/32\", \"https:/…\n$ topics_display_name         &lt;chr&gt; \"Psychology\", \"Social Sciences\", \"Psycholo…\n$ publication_year_fct        &lt;fct&gt; 2014, 2015, 2014, 2013, 2013, 2015, 2015, …\n$ type_fct                    &lt;fct&gt; article, article, article, article, articl…\n$ field                       &lt;fct&gt; Psychology, Social Sciences, Psychology, S…\n$ stm_topic                   &lt;fct&gt; Topic 25, Topic 14, Topic 14, Topic 14, To…\n\n\n\n\n3.3 Verteilungsparameter von Thema 20\n\nErstellung eines Outputs zur Überprüfung der Lageparameter\n\nBasierend auf dem Datensatz gamma_export:\n\nFiltern Sie die Daten nach topic == 20.\nWählen Sie mit Hilfe der select()-Funktion die Variablen gamma, relevance_score und cited_by_count aus.\nVerwenden Sie die Funktion datawizard::describe_distribution() um die Verteilungsparameter zu berechnen.\n\n\n✍️ Identifizieren und notieren Sie folgende Informationen:\n\nWie viele Abstracts haben Thema 20 als Hauptthema?\nWie hoch ist der durschnittliche Relevance Score?\nWie viele Zitationen haben die Dokumente im Durchschnitt?\nWie viel Zitate hat das hochzitierteste Dokument?\n\n\n\nLösung anzeigen\n# Create distribution parameters\ngamma_export %&gt;% \n  filter(topic == 20) %&gt;%\n  select(gamma, relevance_score, cited_by_count) %&gt;% \n  datawizard::describe_distribution()\n\n\nVariable        |  Mean |    SD |   IQR |          Range | Skewness | Kurtosis |    n | n_Missing\n-------------------------------------------------------------------------------------------------\ngamma           |  0.37 |  0.13 |  0.17 |   [0.12, 0.87] |     0.74 |     0.43 | 1477 |         0\nrelevance_score | 32.55 | 40.35 | 36.74 | [2.01, 402.59] |     3.07 |    14.76 | 1477 |         0\ncited_by_count  | 13.54 | 50.51 |  7.00 | [0.00, 948.00] |    10.41 |   143.55 | 1477 |         0\n\n\nLösung anzeigen\n#### Notes\n# Anzahl der Abstrats von Thema 20\n# Durchschnittlicher Relevace Score: \n# Durchschnittliche Zitationen:\n# Anzahl der Zitationen des am meisten zitierten Dokuments:\n\n\n\n\n\n3.4. Top-Dokumente des Themas\n\nIdentifizierung der Top-Dokumente\n\nBasierend auf dem Datensatz gamma_export:\n\nFiltern Sie den Datensatz nach stm_topic == \"Topic 20\".\nSortieren Sie die Daten absteigend nach gamma mit arrange(-gamma).\nWählen Sie die Variablen title, so, gamma, type, und ab mit select() aus.\nWählen Sie die obersten 5 Zeilen mit slice_head(n = 5).\n\n\nErstellung eines Outputs zur Überprüfung der Top-Dokumente\n\nBasierend auf dem Datensatz top_docs_k20:\n\nVerwenden Sie gt() um eine Tabelle zu erstellen.\nFormatieren Sie die Spalte gamma mit fmt_number(columns = vars(gamma), decimals = 2) um nur zwei Nachkommastellen anzuzeigen.\nVerwenden Sie gtExtras::gt_theme_538() um das Design der Tabelle anzupassen.\n\n\n\n✍️ Basierend auf den den Abstracts und den Titeln der Top-Dokumente:\n\nWelche Themenbereiche decken die Dokumente ab?\nWürden Sie den im Abschnitt 3.1. gewählten Themennamen beibehalten oder abändern?\n\n\n\n\nLösung anzeigen\n# Identify top documents for topic 20\ntop_docs_k20 &lt;- gamma_export %&gt;% \n  filter(stm_topic == \"Topic 20\") %&gt;%\n  arrange(-gamma) %&gt;%\n  select(title, so, gamma, type, ab) %&gt;%\n  slice_head(n = 5) \n\n# Creae output\ntop_docs_k20 %&gt;% \n  gt() %&gt;% \n  fmt_number(\n    columns = c(gamma), \n    decimals = 2) %&gt;% \n  gtExtras::gt_theme_538()\n\n\n\n\n\n\n\n\ntitle\nso\ngamma\ntype\nab\n\n\n\n\nLiterature Review of Overseas Tourism Destination Brand Research\nJournal of Chongqing Technology and Business University\n0.87\narticle\nApplying brand theory to the study on tourism destination has been always hot issues for overseas scholars since 1990s.Tourism destination branding management is a significant marketing tool which can bring about effective identification internally,achieving differentiation with external competitors.Systematically reviewing and analyzing recent overseas tourism destination brand literatures,this paper makes conclusion and evaluation of the tourism destination brand construction,branding,brand stakeholders,brand operation,branding performance evaluation to provide reference for domestic tourism destination brand research and management.\n\n\nNatural Disasters in Colombia and Their Impact on the Food Security of the Affected Population. a Quick Review of the Literature.\nSocial Science Research Network\n0.85\narticle\nNatural disasters in Colombia significantly impact multiple domains of the affected population, including food security. Those events can cause food production and distribution interruptions, leading to scarcity and increased prices. Additionally, they can damage infrastructure and limit the communities' ability to access food. Food assistance during disasters is crucial to ensuring food security. The former is crucial for human survival and development, and entities responsible for risk management and food assistance play a fundamental role in protecting populations affected by natural disasters. Entities responsible for risk management in Colombia, such as the National Unit for Disaster Risk Management (UNGRD) and the Departmental and Municipal Risk Management Councils, coordinate efforts to provide food assistance and other basic needs. The Colombia Food Bank also plays an essential role in responding to food emergencies as a first responder. In this article, we investigate some of the leading natural disasters that Colombia has suffered in recent years and how these events have affected different communities. Likewise, we explore how the response has been made from the risk management framework, highlighting food assistance.\n\n\nGovernment Responsibility as the Main Stakeholder in Tourism Development With Collaboration Approach: Literature Review on Heritage Tourism\nNA\n0.84\narticle\ncurrently, the economic growth of some countries is a contribution by the vast development of the tourism sector, and one of the potential destinations is heritage motivation.More than fifty-three of previous study founds five elements that associated with Heritage Tourism Development, which are four elements influencing directly and one element is impact after development as an outcome.The study is focusing on stakeholders responsibility which led by government to do the development of heritage tourism.Barriers of policies and low attention to strategic plants and policies are an influencer to the obstacles because of less attention from the leader.Collaboration approach helped government to control the system in heritage tourism process.\n\n\nBusiness Strategy in Management Perspective: A Literature Review\nIndonesian Journal of Economic & Management Sciences\n0.83\narticle\nBusiness development in the world has entered the era of free markets and broad competition, not only in small areas but also in large areas. Efforts made by a company to win the market are by providing competitive advantages, analyzing competitors, and implementing effective and efficient marketing strategies\n\n\nA Literature Review on Structural Reform of Agricultural Supply Side\nNA\n0.82\narticle\nThe structural reform of the agricultural supply side is the major deployment of the \"No. 1 document\" on agriculture, not only for the direction of agricultural industry development, but also for the agricultural industry structure optimization adjustment to play a needle \"tonic\", also engaged in agricultural economic research experts and scholars Correct future and long-term research direction to play a \"heading\" role.This paper summarizes the policy of \"structural reform of agricultural supply side\", which is related to the optimization and upgrading of agricultural industry structure, the cultivation of agricultural enterprises, the integration of agriculture, the development strategy of agricultural brand, the innovation of agricultural technology, \"Rural land management system reform\", \"agricultural development policy\" and other research results."
  },
  {
    "objectID": "sessions/ms-session-10.html",
    "href": "sessions/ms-session-10.html",
    "title": "Unsupervised Machine Learning II",
    "section": "",
    "text": "🖥️ Slides\n📋 Showcase"
  },
  {
    "objectID": "sessions/ms-session-10.html#participate",
    "href": "sessions/ms-session-10.html#participate",
    "title": "Unsupervised Machine Learning II",
    "section": "",
    "text": "🖥️ Slides\n📋 Showcase"
  },
  {
    "objectID": "sessions/ms-session-10.html#practice",
    "href": "sessions/ms-session-10.html#practice",
    "title": "Unsupervised Machine Learning II",
    "section": "Practice",
    "text": "Practice\n✍️ Exercise"
  },
  {
    "objectID": "sessions/ms-session-10.html#suggested-readings",
    "href": "sessions/ms-session-10.html#suggested-readings",
    "title": "Unsupervised Machine Learning II",
    "section": "Suggested readings",
    "text": "Suggested readings\n\nTopic Modeling\n\nChang, J., Boyd-Graber, J., Gerrish, S., Wang, C., & Blei, D. (2009). Reading tea leaves: How humans interpret topic models. 32, 288–296.\nChen, Y., Peng, Z., Kim, S.-H., & Choi, C. W. (2023). What We Can Do and Cannot Do with Topic Modeling: A Systematic Review. Communication Methods and Measures, 17(2), 111–130. https://doi.org/10.1080/19312458.2023.2167965\nEgger, R., & Yu, J. (2022). A topic modeling comparison between LDA, NMF, Top2Vec, and BERTopic to demystify twitter posts. Frontiers in Sociology, 7, 886498. https://doi.org/10.3389/fsoc.2022.886498\nFriemel, T. N. (2017). Social Network Analysis (J. Matthes, C. S. Davis, & R. F. Potter, Eds.; 1st ed., pp. 1–14). Wiley. https://onlinelibrary.wiley.com/doi/10.1002/9781118901731.iecrm0235\nHase, V. (2023). Automated Content Analysis (F. Oehmer-Pedrazzi, S. H. Kessler, E. Humprecht, K. Sommer, & L. Castro, Eds.; pp. 23–36). Springer Fachmedien Wiesbaden. https://link.springer.com/10.1007/978-3-658-36179-2_3\nHimelboim, I. (2017). Social Network Analysis (Social Media) (J. Matthes, C. S. Davis, & R. F. Potter, Eds.; 1st ed., pp. 1–15). Wiley. https://onlinelibrary.wiley.com/doi/10.1002/9781118901731.iecrm0236\nMaier, D., Waldherr, A., Miltner, P., Wiedemann, G., Niekler, A., Keinert, A., Pfetsch, B., Heyer, G., Reber, U., Häussler, T., Schmid-Petri, H., & Adam, S. (2018). Applying LDA Topic Modeling in Communication Research: Toward a Valid and Reliable Methodology. Communication Methods and Measures, 12(2-3), 93–118. https://doi.org/10.1080/19312458.2018.1430754\n\n\n\nTopic modeling in R\n\nAtteveldt, W. van, Trilling, D., & Arcíla, C. (2021). Computational analysis of communication: A practical introduction to the analysis of texts, networks, and images with code examples in python and r. John Wiley & Sons.\nSilge, J., & Hvitfeldt, E. (n.d.). Supervised machine learning for text analysis in r. https://smltar.com/\nSilge, J., & Robinson, D. (2017). Text mining with r: A tidy approach (First edition). O’Reilly.\nWelbers, K., Van Atteveldt, W., & Benoit, K. (2017). Text Analysis in R. Communication Methods and Measures, 11(4), 245–265. https://doi.org/10.1080/19312458.2017.1387238"
  },
  {
    "objectID": "sessions/ms-session-10.html#useful-resources",
    "href": "sessions/ms-session-10.html#useful-resources",
    "title": "Unsupervised Machine Learning II",
    "section": "Useful resources",
    "text": "Useful resources\n\nTutorials des CCS-Amsterdam zu “quanteda-based” Textanalyse:\n\n📖 Text analysis (including 🎥 video tutorial)\n📖 Lexical sentiment analysis (including 🎥 video tutorial)\n📖 LDA Topic Modeling (including 🎥 video tutorial)\n📖 Structural Topic Modeling (including 🎥 video tutorial)\n\n\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "sessions/ms-session-09.html",
    "href": "sessions/ms-session-09.html",
    "title": "Text processing in R",
    "section": "",
    "text": "🖥️ Slides\n📋 Showcase"
  },
  {
    "objectID": "sessions/ms-session-09.html#participate",
    "href": "sessions/ms-session-09.html#participate",
    "title": "Text processing in R",
    "section": "",
    "text": "🖥️ Slides\n📋 Showcase"
  },
  {
    "objectID": "sessions/ms-session-09.html#practice",
    "href": "sessions/ms-session-09.html#practice",
    "title": "Text processing in R",
    "section": "Practice",
    "text": "Practice\n✍️ Exercise"
  },
  {
    "objectID": "sessions/ms-session-09.html#suggested-readings",
    "href": "sessions/ms-session-09.html#suggested-readings",
    "title": "Text processing in R",
    "section": "Suggested readings",
    "text": "Suggested readings\n\nTopic Modeling\n\nChang, J., Boyd-Graber, J., Gerrish, S., Wang, C., & Blei, D. (2009). Reading tea leaves: How humans interpret topic models. 32, 288–296.\nChen, Y., Peng, Z., Kim, S.-H., & Choi, C. W. (2023). What We Can Do and Cannot Do with Topic Modeling: A Systematic Review. Communication Methods and Measures, 17(2), 111–130. https://doi.org/10.1080/19312458.2023.2167965\nEgger, R., & Yu, J. (2022). A topic modeling comparison between LDA, NMF, Top2Vec, and BERTopic to demystify twitter posts. Frontiers in Sociology, 7, 886498. https://doi.org/10.3389/fsoc.2022.886498\nFriemel, T. N. (2017). Social Network Analysis (J. Matthes, C. S. Davis, & R. F. Potter, Eds.; 1st ed., pp. 1–14). Wiley. https://onlinelibrary.wiley.com/doi/10.1002/9781118901731.iecrm0235\nHase, V. (2023). Automated Content Analysis (F. Oehmer-Pedrazzi, S. H. Kessler, E. Humprecht, K. Sommer, & L. Castro, Eds.; pp. 23–36). Springer Fachmedien Wiesbaden. https://link.springer.com/10.1007/978-3-658-36179-2_3\nHimelboim, I. (2017). Social Network Analysis (Social Media) (J. Matthes, C. S. Davis, & R. F. Potter, Eds.; 1st ed., pp. 1–15). Wiley. https://onlinelibrary.wiley.com/doi/10.1002/9781118901731.iecrm0236\nMaier, D., Waldherr, A., Miltner, P., Wiedemann, G., Niekler, A., Keinert, A., Pfetsch, B., Heyer, G., Reber, U., Häussler, T., Schmid-Petri, H., & Adam, S. (2018). Applying LDA Topic Modeling in Communication Research: Toward a Valid and Reliable Methodology. Communication Methods and Measures, 12(2-3), 93–118. https://doi.org/10.1080/19312458.2018.1430754\n\n\n\nTopic modeling in R\n\nAtteveldt, W. van, Trilling, D., & Arcíla, C. (2021). Computational analysis of communication: A practical introduction to the analysis of texts, networks, and images with code examples in python and r. John Wiley & Sons.\nSilge, J., & Hvitfeldt, E. (n.d.). Supervised machine learning for text analysis in r. https://smltar.com/\nSilge, J., & Robinson, D. (2017). Text mining with r: A tidy approach (First edition). O’Reilly.\nWelbers, K., Van Atteveldt, W., & Benoit, K. (2017). Text Analysis in R. Communication Methods and Measures, 11(4), 245–265. https://doi.org/10.1080/19312458.2017.1387238"
  },
  {
    "objectID": "sessions/ms-session-09.html#useful-resources",
    "href": "sessions/ms-session-09.html#useful-resources",
    "title": "Text processing in R",
    "section": "Useful resources",
    "text": "Useful resources\n\nTutorials des CCS-Amsterdam zu “quanteda-based” Textanalyse:\n\n📖 Text analysis (including 🎥 video tutorial)\n📖 Lexical sentiment analysis (including 🎥 video tutorial)\n📖 LDA Topic Modeling (including 🎥 video tutorial)\n📖 Structural Topic Modeling (including 🎥 video tutorial)\n\n\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "slides/ms-slides-09.html#seminarplan",
    "href": "slides/ms-slides-09.html#seminarplan",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Seminarplan",
    "text": "Seminarplan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSitzung\nDatum\nThema (synchron)\nÜbung (asynchron)\nDozent:in\n\n\n\n\n1\n18.04.2024\nEinführung & Überblick\n\nAM & CA\n\n\n\n📚\nTeil 1: Systematic Review\n\n\n\n\n2\n25.04.2024\nEinführung in Systematic Reviews I\nR-Einführung\nAM\n\n\n3\n02.05.2024\nEinführung in Systematic Reviews II\nR-Einführung\nAM\n\n\n\n09.05.2024\n🏖️ Feiertag\nR-Einführung\n\n\n\n4\n16.05.2024\nAutomatisierung von SRs & KI-Tools\nR-Einführung\nAM\n\n\n\n23.05.2024\n🍻 WiSo-Projekt-Woche\nR-Einführung\n\n\n\n5\n04.06.2024\n🍕 Gastvortrag: Prof. Dr. Emese Domahidi\nR-Einführung\nED\n\n\n6\n06.06.2024\nAutomatisierung von SRs & KI-Tools\nR-Einführung\nAM\n\n\n\n💻\nTeil 2: Text as Data & Unsupervised Machine Learning\n\n\n\n\n7\n13.06.2024\nIntroduction to Text as Data\nzur Sitzung\nCA\n\n\n8\n20.06.2024\nText processing\nzur Sitzung\nCA\n\n\n9\n27.06.2024\nUnsupervised Machine Learning I\nzur Sitzung\nCA\n\n\n10\n04.07.2024\nUnsupervised Machine Learning II\nzur Sitzung\nCA & AM\n\n\n11\n11.07.2024\nRecap & Ausblick\nzur Sitzung\nCA & AM\n\n\n12\n18.07.2024\n🏁 Semesterabschluss\nzur Sitzung\nCA & AM"
  },
  {
    "objectID": "slides/ms-slides-09.html#besprechung-der-r-übung",
    "href": "slides/ms-slides-09.html#besprechung-der-r-übung",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Besprechung der R-Übung",
    "text": "Besprechung der R-Übung\nSollten wir die Daten weiter eingrenzen?\n\n\n\nBitte scannt den QR-Code oder nutzt den folgenden Link für die Teilnahme an einer kurzen Umfrage:\n\nhttps://www.menti.com/als6ys2e2y69\nTemporary Access Code: 2250 1954\n\n\n\n\n \n\n    \n\n\n\n\n−+\n02:00"
  },
  {
    "objectID": "slides/ms-slides-09.html#ergebnis",
    "href": "slides/ms-slides-09.html#ergebnis",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Ergebnis",
    "text": "Ergebnis"
  },
  {
    "objectID": "slides/ms-slides-09.html#quick-reminder-die-tidytext-pipeline",
    "href": "slides/ms-slides-09.html#quick-reminder-die-tidytext-pipeline",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Quick reminder: Die tidytext Pipeline",
    "text": "Quick reminder: Die tidytext Pipeline\nFokus auf einzelne Wörter, deren Beziehungen zueinander und Sentiments\n\n\n\n\n\n\n\n\n\n\n\n\n\nSilge & Robinson (2017)"
  },
  {
    "objectID": "slides/ms-slides-09.html#expansion-der-pipeline",
    "href": "slides/ms-slides-09.html#expansion-der-pipeline",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Expansion der Pipeline",
    "text": "Expansion der Pipeline\nFokus auf die Modelierung der Beziehung zwischen Wörtern & Dokumenten\n\n\n\n\n\n\n\n\n\n\n\n\n\nSilge & Robinson (2017)"
  },
  {
    "objectID": "slides/ms-slides-09.html#possibilities-over-possibilities",
    "href": "slides/ms-slides-09.html#possibilities-over-possibilities",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Possibilities over possibilities",
    "text": "Possibilities over possibilities\nÜberblick über verschiedene Methoden der Textanalyse (Grimmer & Stewart, 2013)"
  },
  {
    "objectID": "slides/ms-slides-09.html#promises-pitfalls",
    "href": "slides/ms-slides-09.html#promises-pitfalls",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Promises & pitfalls",
    "text": "Promises & pitfalls\nVier Grundsätze der quantitativen Textanalyse (Grimmer & Stewart, 2013)\n1️⃣ All quantitative models of language are wrong — but some are useful.\n2️⃣ Quantitative methods for text amplify resources and augment humans.\n3️⃣ There is no globally best method for automated text analysis.\n4️⃣ Validate, Validate, Validate!"
  },
  {
    "objectID": "slides/ms-slides-09.html#verteilung-von-wörtern-auf-themen-auf-dokumente",
    "href": "slides/ms-slides-09.html#verteilung-von-wörtern-auf-themen-auf-dokumente",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Verteilung von Wörtern auf Themen auf Dokumente",
    "text": "Verteilung von Wörtern auf Themen auf Dokumente\nDie Grundidee des (LDA) Topic Modeling\n\n(Blei, 2012)\nEach topic is a distribution of words\nEach document is a mixture of corpus-wide topics\nEach words is drawn from one of those topics"
  },
  {
    "objectID": "slides/ms-slides-09.html#in-a-nutshell",
    "href": "slides/ms-slides-09.html#in-a-nutshell",
    "title": "Unsupervised Machine Learning (I)",
    "section": "In a nutshell",
    "text": "In a nutshell\nGrundlagen des Topic Modeling kurz zusammengefasst\n\nVerfahren des unüberwachten maschinellen Lernens, das sich daher insbesondere zur Exploration und Deskription großer Textmengen eignet\nThemen werden strikt auf Basis von Worthäufigkeiten in den einzelnen Dokumenten vermeintlich objektiv berechnet, ganz ohne subjektive Einschätzungen und damit einhergehenden etwaigen Verzerrungen\nBekanntesten dieser Verfahren sind LDA (Latent Dirichlet Allocation) sowie die darauf aufbauenden CTM (Correlated Topic Models) und STM (Structural Topic Models)"
  },
  {
    "objectID": "slides/ms-slides-09.html#wenn-missing-values-zum-problem-werden",
    "href": "slides/ms-slides-09.html#wenn-missing-values-zum-problem-werden",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Wenn Missing Values zum Problem werden",
    "text": "Wenn Missing Values zum Problem werden\nExkurs zu Überprüfung der Daten auf fehlende Werte\n\nvisdat::vis_miss(review_subsample, warn_large_data = FALSE)"
  },
  {
    "objectID": "slides/ms-slides-09.html#bereinigung-der-subsample",
    "href": "slides/ms-slides-09.html#bereinigung-der-subsample",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Bereinigung der Subsample",
    "text": "Bereinigung der Subsample\nAusschluss von Referenzen mit fehlendem Abstract\n\n\n\n# Create subsample\nreview_subsample &lt;- review_works_correct %&gt;% \n  # Eingrenzung: Sprache und Typ\n  filter(language == \"en\") %&gt;% \n  filter(type == \"article\") %&gt;%\n  # Datentranformation\n  unnest(topics, names_sep = \"_\") %&gt;%\n  filter(topics_name == \"field\") %&gt;% \n  filter(topics_i == \"1\") %&gt;% \n  # Eingrenzung: Forschungsfeldes\n  filter(\n   topics_display_name == \"Social Sciences\"|\n   topics_display_name == \"Psychology\"\n    )\n\n# Overview\nreview_subsample %&gt;% glimpse  \n\n\nRows: 45,221\nColumns: 45\n$ id                          &lt;chr&gt; \"https://openalex.org/W4293003987\", \"https…\n$ title                       &lt;chr&gt; \"The WHO-5 Well-Being Index: A Systematic …\n$ display_name                &lt;chr&gt; \"The WHO-5 Well-Being Index: A Systematic …\n$ author                      &lt;list&gt; [&lt;data.frame[4 x 12]&gt;], [&lt;data.frame[2 x …\n$ ab                          &lt;chr&gt; \"The 5-item World Health Organization Well…\n$ publication_date            &lt;chr&gt; \"2015-01-01\", \"2017-08-28\", \"2014-01-01\", …\n$ relevance_score             &lt;dbl&gt; 938.7603, 752.3500, 591.2553, 576.1210, 56…\n$ so                          &lt;chr&gt; \"Psychotherapy and psychosomatics\", \"Journ…\n$ so_id                       &lt;chr&gt; \"https://openalex.org/S184803288\", \"https:…\n$ host_organization           &lt;chr&gt; \"Karger Publishers\", \"SAGE Publishing\", NA…\n$ issn_l                      &lt;chr&gt; \"0033-3190\", \"0739-456X\", NA, \"2214-7829\",…\n$ url                         &lt;chr&gt; \"https://doi.org/10.1159/000376585\", \"http…\n$ pdf_url                     &lt;chr&gt; \"https://www.karger.com/Article/Pdf/376585…\n$ license                     &lt;chr&gt; \"cc-by-nc\", NA, NA, \"cc-by\", NA, NA, \"cc-b…\n$ version                     &lt;chr&gt; \"publishedVersion\", NA, \"publishedVersion\"…\n$ first_page                  &lt;chr&gt; \"167\", \"93\", NA, \"89\", \"55\", \"2150\", \"e356…\n$ last_page                   &lt;chr&gt; \"176\", \"112\", NA, \"106\", \"64\", \"2159\", \"e3…\n$ volume                      &lt;chr&gt; \"84\", \"39\", NA, \"6\", \"277\", \"32\", \"2\", \"24…\n$ issue                       &lt;chr&gt; \"3\", \"1\", NA, NA, NA, \"19\", \"8\", NA, \"9\", …\n$ is_oa                       &lt;lgl&gt; TRUE, FALSE, TRUE, TRUE, TRUE, FALSE, TRUE…\n$ is_oa_anywhere              &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE,…\n$ oa_status                   &lt;chr&gt; \"hybrid\", \"green\", \"bronze\", \"gold\", \"bron…\n$ oa_url                      &lt;chr&gt; \"https://www.karger.com/Article/Pdf/376585…\n$ any_repository_has_fulltext &lt;lgl&gt; FALSE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE…\n$ language                    &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", …\n$ grants                      &lt;list&gt; NA, NA, NA, &lt;\"https://openalex.org/F43203…\n$ cited_by_count              &lt;int&gt; 2657, 1375, 2568, 803, 3664, 1553, 2895, 9…\n$ counts_by_year              &lt;list&gt; [&lt;data.frame[11 x 2]&gt;], [&lt;data.frame[7 x …\n$ publication_year            &lt;int&gt; 2015, 2017, 2014, 2016, 2020, 2014, 2017, …\n$ cited_by_api_url            &lt;chr&gt; \"https://api.openalex.org/works?filter=cit…\n$ ids                         &lt;list&gt; &lt;\"https://openalex.org/W4293003987\", \"htt…\n$ doi                         &lt;chr&gt; \"https://doi.org/10.1159/000376585\", \"http…\n$ type                        &lt;chr&gt; \"article\", \"article\", \"article\", \"article\"…\n$ referenced_works            &lt;list&gt; &lt;\"https://openalex.org/W1492518593\", \"htt…\n$ related_works               &lt;list&gt; &lt;\"https://openalex.org/W3020194755\", \"htt…\n$ is_paratext                 &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ is_retracted                &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ concepts                    &lt;list&gt; [&lt;data.frame[7 x 5]&gt;], [&lt;data.frame[18 x …\n$ topics_i                    &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ topics_score                &lt;dbl&gt; 0.9926, 0.9050, 0.9995, 0.9987, 0.9999, 1.…\n$ topics_name                 &lt;chr&gt; \"field\", \"field\", \"field\", \"field\", \"field…\n$ topics_id                   &lt;chr&gt; \"https://openalex.org/fields/32\", \"https:/…\n$ topics_display_name         &lt;chr&gt; \"Psychology\", \"Social Sciences\", \"Psycholo…\n$ publication_year_fct        &lt;fct&gt; 2015, 2017, 2014, 2016, 2020, 2014, 2017, …\n$ type_fct                    &lt;fct&gt; article, article, article, article, articl…"
  },
  {
    "objectID": "slides/ms-slides-09.html#bereinigung-der-subsample-1",
    "href": "slides/ms-slides-09.html#bereinigung-der-subsample-1",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Bereinigung der Subsample",
    "text": "Bereinigung der Subsample\nAusschluss von Referenzen mit fehlendem Abstract\n\n\n\n# Create subsample\nreview_subsample &lt;- review_works_correct %&gt;% \n  # Eingrenzung: Sprache und Typ\n  filter(language == \"en\") %&gt;% \n  filter(type == \"article\") %&gt;%\n  # Datentranformation\n  unnest(topics, names_sep = \"_\") %&gt;%\n  filter(topics_name == \"field\") %&gt;% \n  filter(topics_i == \"1\") %&gt;% \n  # Eingrenzung: Forschungsfeldes\n  filter(\n   topics_display_name == \"Social Sciences\"|\n   topics_display_name == \"Psychology\"\n    ) %&gt;% \n  # Eingrenzung: Keine Einträge ohne Abstract\n  filter(!is.na(ab))\n\n# Overview\nreview_subsample %&gt;% glimpse  \n\n\nRows: 36,680\nColumns: 45\n$ id                          &lt;chr&gt; \"https://openalex.org/W4293003987\", \"https…\n$ title                       &lt;chr&gt; \"The WHO-5 Well-Being Index: A Systematic …\n$ display_name                &lt;chr&gt; \"The WHO-5 Well-Being Index: A Systematic …\n$ author                      &lt;list&gt; [&lt;data.frame[4 x 12]&gt;], [&lt;data.frame[2 x …\n$ ab                          &lt;chr&gt; \"The 5-item World Health Organization Well…\n$ publication_date            &lt;chr&gt; \"2015-01-01\", \"2017-08-28\", \"2014-01-01\", …\n$ relevance_score             &lt;dbl&gt; 938.7603, 752.3500, 591.2553, 576.1210, 56…\n$ so                          &lt;chr&gt; \"Psychotherapy and psychosomatics\", \"Journ…\n$ so_id                       &lt;chr&gt; \"https://openalex.org/S184803288\", \"https:…\n$ host_organization           &lt;chr&gt; \"Karger Publishers\", \"SAGE Publishing\", NA…\n$ issn_l                      &lt;chr&gt; \"0033-3190\", \"0739-456X\", NA, \"2214-7829\",…\n$ url                         &lt;chr&gt; \"https://doi.org/10.1159/000376585\", \"http…\n$ pdf_url                     &lt;chr&gt; \"https://www.karger.com/Article/Pdf/376585…\n$ license                     &lt;chr&gt; \"cc-by-nc\", NA, NA, \"cc-by\", NA, NA, \"cc-b…\n$ version                     &lt;chr&gt; \"publishedVersion\", NA, \"publishedVersion\"…\n$ first_page                  &lt;chr&gt; \"167\", \"93\", NA, \"89\", \"55\", \"2150\", \"e356…\n$ last_page                   &lt;chr&gt; \"176\", \"112\", NA, \"106\", \"64\", \"2159\", \"e3…\n$ volume                      &lt;chr&gt; \"84\", \"39\", NA, \"6\", \"277\", \"32\", \"2\", \"24…\n$ issue                       &lt;chr&gt; \"3\", \"1\", NA, NA, NA, \"19\", \"8\", NA, \"9\", …\n$ is_oa                       &lt;lgl&gt; TRUE, FALSE, TRUE, TRUE, TRUE, FALSE, TRUE…\n$ is_oa_anywhere              &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE,…\n$ oa_status                   &lt;chr&gt; \"hybrid\", \"green\", \"bronze\", \"gold\", \"bron…\n$ oa_url                      &lt;chr&gt; \"https://www.karger.com/Article/Pdf/376585…\n$ any_repository_has_fulltext &lt;lgl&gt; FALSE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE…\n$ language                    &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", …\n$ grants                      &lt;list&gt; NA, NA, NA, &lt;\"https://openalex.org/F43203…\n$ cited_by_count              &lt;int&gt; 2657, 1375, 2568, 803, 3664, 1553, 2895, 9…\n$ counts_by_year              &lt;list&gt; [&lt;data.frame[11 x 2]&gt;], [&lt;data.frame[7 x …\n$ publication_year            &lt;int&gt; 2015, 2017, 2014, 2016, 2020, 2014, 2017, …\n$ cited_by_api_url            &lt;chr&gt; \"https://api.openalex.org/works?filter=cit…\n$ ids                         &lt;list&gt; &lt;\"https://openalex.org/W4293003987\", \"htt…\n$ doi                         &lt;chr&gt; \"https://doi.org/10.1159/000376585\", \"http…\n$ type                        &lt;chr&gt; \"article\", \"article\", \"article\", \"article\"…\n$ referenced_works            &lt;list&gt; &lt;\"https://openalex.org/W1492518593\", \"htt…\n$ related_works               &lt;list&gt; &lt;\"https://openalex.org/W3020194755\", \"htt…\n$ is_paratext                 &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ is_retracted                &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ concepts                    &lt;list&gt; [&lt;data.frame[7 x 5]&gt;], [&lt;data.frame[18 x …\n$ topics_i                    &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ topics_score                &lt;dbl&gt; 0.9926, 0.9050, 0.9995, 0.9987, 0.9999, 1.…\n$ topics_name                 &lt;chr&gt; \"field\", \"field\", \"field\", \"field\", \"field…\n$ topics_id                   &lt;chr&gt; \"https://openalex.org/fields/32\", \"https:/…\n$ topics_display_name         &lt;chr&gt; \"Psychology\", \"Social Sciences\", \"Psycholo…\n$ publication_year_fct        &lt;fct&gt; 2015, 2017, 2014, 2016, 2020, 2014, 2017, …\n$ type_fct                    &lt;fct&gt; article, article, article, article, articl…"
  },
  {
    "objectID": "slides/ms-slides-09.html#aus-text-werden-zahlen",
    "href": "slides/ms-slides-09.html#aus-text-werden-zahlen",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Aus Text werden Zahlen",
    "text": "Aus Text werden Zahlen\nDocument-Term-Matrix [DTM] im Fokus\n\n\nSilge & Robinson (2017)"
  },
  {
    "objectID": "slides/ms-slides-09.html#kurzer-rückblick-auf-die-document-term-matrix-dtm",
    "href": "slides/ms-slides-09.html#kurzer-rückblick-auf-die-document-term-matrix-dtm",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Kurzer Rückblick auf die Document-Term Matrix [DTM]",
    "text": "Kurzer Rückblick auf die Document-Term Matrix [DTM]\nHäufig verwendete Datenstruktur für (klassische) Textanalyse\n\n\nEine Matrix, bei der:\n\njede Zeile steht für ein Dokument (z.B. ein Abstract),\njede Spalte einen Begriff darstellt, und\njeder Wert (in der Regel) die Häufigkeit des Begriffs in einem Dokument enthält.\n\n\n\n\n\n\n\n\n(Zheng & Casari, 2018)"
  },
  {
    "objectID": "slides/ms-slides-09.html#schritt-für-schritt-zur-dtm",
    "href": "slides/ms-slides-09.html#schritt-für-schritt-zur-dtm",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Schritt für Schritt zur DTM",
    "text": "Schritt für Schritt zur DTM\nTextverarbeitung entlang der tidytext Pipeline: Tokenize\n\n\n\n# Create tidy data\nsubsample_tidy &lt;- review_subsample %&gt;% \n    tidytext::unnest_tokens(\"text\", ab) %&gt;% \n    filter(!text %in% tidytext::stop_words$word)\n\n# Preview\nsubsample_tidy %&gt;% \n  select(id, text) %&gt;% \n  print(n = 15)\n\n\n# A tibble: 4,872,424 × 2\n   id                               text          \n   &lt;chr&gt;                            &lt;chr&gt;         \n 1 https://openalex.org/W4293003987 5             \n 2 https://openalex.org/W4293003987 item          \n 3 https://openalex.org/W4293003987 world         \n 4 https://openalex.org/W4293003987 health        \n 5 https://openalex.org/W4293003987 organization  \n 6 https://openalex.org/W4293003987 index         \n 7 https://openalex.org/W4293003987 5             \n 8 https://openalex.org/W4293003987 widely        \n 9 https://openalex.org/W4293003987 questionnaires\n10 https://openalex.org/W4293003987 assessing     \n11 https://openalex.org/W4293003987 subjective    \n12 https://openalex.org/W4293003987 psychological \n13 https://openalex.org/W4293003987 publication   \n14 https://openalex.org/W4293003987 1998          \n15 https://openalex.org/W4293003987 5             \n# ℹ 4,872,409 more rows"
  },
  {
    "objectID": "slides/ms-slides-09.html#schritt-für-schritt-zur-dtm-1",
    "href": "slides/ms-slides-09.html#schritt-für-schritt-zur-dtm-1",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Schritt für Schritt zur DTM",
    "text": "Schritt für Schritt zur DTM\nTextverarbeitung entlang der tidytext Pipeline: Tokenize ▶️ Summarize\n\n\n\n# Create tidy data\nsubsample_tidy &lt;- review_subsample %&gt;% \n    tidytext::unnest_tokens(\"text\", ab) %&gt;% \n    filter(!text %in% tidytext::stop_words$word)\n\n# Create summarized data\nsubsample_summarized &lt;- subsample_tidy %&gt;% \n  count(id, text) \n\n# Preview \nsubsample_summarized %&gt;% \n  print(n = 15)\n\n\n# A tibble: 3,280,664 × 3\n   id                               text           n\n   &lt;chr&gt;                            &lt;chr&gt;      &lt;int&gt;\n 1 https://openalex.org/W1000529773 aim            1\n 2 https://openalex.org/W1000529773 anne           1\n 3 https://openalex.org/W1000529773 approaches     1\n 4 https://openalex.org/W1000529773 critical       1\n 5 https://openalex.org/W1000529773 current        1\n 6 https://openalex.org/W1000529773 dick           1\n 7 https://openalex.org/W1000529773 effective      1\n 8 https://openalex.org/W1000529773 employed       1\n 9 https://openalex.org/W1000529773 enhancing      1\n10 https://openalex.org/W1000529773 evaluation     1\n11 https://openalex.org/W1000529773 examined       1\n12 https://openalex.org/W1000529773 explored       1\n13 https://openalex.org/W1000529773 extended       2\n14 https://openalex.org/W1000529773 girls          1\n15 https://openalex.org/W1000529773 government     1\n# ℹ 3,280,649 more rows"
  },
  {
    "objectID": "slides/ms-slides-09.html#schritt-für-schritt-zur-dtm-2",
    "href": "slides/ms-slides-09.html#schritt-für-schritt-zur-dtm-2",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Schritt für Schritt zur DTM",
    "text": "Schritt für Schritt zur DTM\nTextverarbeitung entlang der tidytext Pipeline: Tokenize ▶️ Summarize ▶️ DTM\n\n\n\n# Create tidy data\nsubsample_tidy &lt;- review_subsample %&gt;% \n    tidytext::unnest_tokens(\"text\", ab) %&gt;% \n    filter(!text %in% tidytext::stop_words$word)\n\n# Create summarized data\nsubsample_summarized &lt;- subsample_tidy %&gt;% \n  count(id, text) \n\n# Create DTM\nsubsample_dtm &lt;- subsample_summarized %&gt;% \n  cast_dtm(id, text, n)\n\n# Preview\nsubsample_dtm\n\n\n&lt;&lt;DocumentTermMatrix (documents: 36654, terms: 122147)&gt;&gt;\nNon-/sparse entries: 3280664/4473895474\nSparsity           : 100%\nMaximal term length: 188\nWeighting          : term frequency (tf)"
  },
  {
    "objectID": "slides/ms-slides-09.html#einfach-mit-tidytext-präzise-mit-quanteda",
    "href": "slides/ms-slides-09.html#einfach-mit-tidytext-präzise-mit-quanteda",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Einfach mit tidytext, präzise mit quanteda",
    "text": "Einfach mit tidytext, präzise mit quanteda\nVergleich von Texttransformation mit verschiedenen Paketen\n\n\n\n\n# Create tidy data\nsubsample_tidy &lt;- review_subsample %&gt;% \n    tidytext::unnest_tokens(\"text\", ab) %&gt;% \n    filter(!text %in% tidytext::stop_words$word)\n\n# Create summarized data\nsubsample_summarized &lt;- subsample_tidy %&gt;% \n  count(id, text) \n\n# Create DTM\nsubsample_dtm &lt;- subsample_summarized %&gt;% \n  cast_dtm(id, text, n)\n\n# Preview\nsubsample_dtm\n\n\n\n# Create corpus\nquanteda_corpus &lt;- review_subsample %&gt;% \n  quanteda::corpus(\n    docid_field = \"id\", \n    text_field = \"ab\"\n  )\n\n# Tokenize\nquanteda_token &lt;- quanteda_corpus %&gt;% \n  quanteda::tokens(\n    remove_punct = TRUE,\n    remove_symbols = TRUE, \n    remove_numbers = TRUE, \n    remove_url = TRUE, \n    split_tags = FALSE # keep hashtags and mentions\n  ) %&gt;% \n  quanteda::tokens_tolower() %&gt;% \n  quanteda::tokens_remove(\n    pattern = stopwords(\"en\")\n    )\n\n# Convert to Document-Feature-Matrix (DFM)\nquanteda_dfm &lt;- quanteda_token %&gt;% \n  quanteda::dfm()"
  },
  {
    "objectID": "slides/ms-slides-09.html#netzwerk-der-top-begriffe",
    "href": "slides/ms-slides-09.html#netzwerk-der-top-begriffe",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Netzwerk der Top-Begriffe",
    "text": "Netzwerk der Top-Begriffe\nVergleich zwischen tidytext & quanteda\n\n\n\n# Extract most common hashtags\ntop_features_tidy &lt;- subsample_tidy %&gt;% \n  count(text, sort = TRUE) %&gt;%\n  slice_head(n = 20) %&gt;% \n  pull(text)\n\n# Visualize\nsubsample_tidy %&gt;% \n  count(id, text, sort = TRUE) %&gt;% \n  filter(!is.na(text)) %&gt;% \n  cast_dfm(id, text, n) %&gt;% \n  quanteda::fcm() %&gt;% \n  quanteda::fcm_select(\n    pattern = top_features_tidy,\n    case_insensitive = FALSE\n  ) %&gt;%  \n  quanteda.textplots::textplot_network(\n    edge_color = \"#04316A\"\n  )"
  },
  {
    "objectID": "slides/ms-slides-09.html#netzwerk-der-top-begriffe-1",
    "href": "slides/ms-slides-09.html#netzwerk-der-top-begriffe-1",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Netzwerk der Top-Begriffe",
    "text": "Netzwerk der Top-Begriffe\nVergleich zwischen tidytext & quanteda\n\n\n\n\n# Extract most common features \ntop_features_quanteda &lt;- quanteda_dfm %&gt;% \n  topfeatures(20) %&gt;% \n  names()\n\n# Construct feature-occurrence matrix of features\nquanteda_dfm %&gt;% \n  fcm() %&gt;% \n  fcm_select(pattern = top_features_quanteda) %&gt;% \n  textplot_network(\n    edge_color = \"#C50F3C\"\n  )"
  },
  {
    "objectID": "slides/ms-slides-09.html#netzwerk-der-top-begriffe-2",
    "href": "slides/ms-slides-09.html#netzwerk-der-top-begriffe-2",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Netzwerk der Top-Begriffe",
    "text": "Netzwerk der Top-Begriffe\nVergleich zwischen tidytext & quanteda\n\n\n\n\nExpand for full code\nsubsample_tidy %&gt;% \n  count(id, text, sort = TRUE) %&gt;% \n  filter(!is.na(text)) %&gt;% \n  cast_dfm(id, text, n) %&gt;% \n  quanteda::fcm() %&gt;% \n  quanteda::fcm_select(\n    pattern = top_features_tidy,\n    case_insensitive = FALSE\n  ) %&gt;%  \n  quanteda.textplots::textplot_network(\n    edge_color = \"#04316A\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nExpand for full code\nquanteda_dfm %&gt;% \n  fcm() %&gt;% \n  fcm_select(pattern = top_features_quanteda) %&gt;% \n  textplot_network(\n    edge_color = \"#C50F3C\"\n  )"
  },
  {
    "objectID": "slides/ms-slides-09.html#neuer-input-in-die-pipeline",
    "href": "slides/ms-slides-09.html#neuer-input-in-die-pipeline",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Neuer Input in die Pipeline",
    "text": "Neuer Input in die Pipeline\nUnsupervised learning example: Topic modeling\n\n\nSilge & Robinson (2017)"
  },
  {
    "objectID": "slides/ms-slides-09.html#building-a-shared-vocabulary-again",
    "href": "slides/ms-slides-09.html#building-a-shared-vocabulary-again",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Building a shared vocabulary … again",
    "text": "Building a shared vocabulary … again\nGrundbegriffe und Definitionen im Kontext des Topic Modelings\n\nK: Anzahl der Themen, die für ein bestimmtes Themenmodell berechnen werden.\nWord-Topic-Matrix: Matrix, die die bedingte Wahrscheinlichkeit (beta) beschreibt, mit der ein Wort in einem bestimmten Thema vorkommt.\nDocument-Topic-Matrix: Matrix, die die bedingte Wahrscheinlichkeit (gamma) beschreibt, mit der ein Thema in einem bestimmten Dokument vorkommt."
  },
  {
    "objectID": "slides/ms-slides-09.html#beyond-lda",
    "href": "slides/ms-slides-09.html#beyond-lda",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Beyond LDA",
    "text": "Beyond LDA\nVerschiedene Ansätze der Themenmodellierung\n\n\nLatent Dirichlet Allocation [LDA] (Blei et al., 2003) ist ein probabilistisches generatives Modell, das davon ausgeht, dass jedes Dokumentin einem KorpuseineMischung von Themen ist und jedes Wort im Dokument einem der Themen des Dokuments zuzuordnenist.\nStructural Topic Modeling [STM] (Roberts et al., 2016; Roberts et al., 2019) erweitert LDA durch die Einbeziehung von Kovariaten auf Dokumentenebene und ermöglicht die Modellierung des Einflusses externer Faktoren auf die Themenprävalenz.\nWord embeddings (Word2Vec (Mikolov et al., 2013) , Glove (Pennington et al., 2014)) stellen Wörter als kontinuierliche Vektoren in einem hochdimensionalen Raum dar und erfassen semantische Beziehungen zwischen Wörtern basierend auf ihrem Kontext in den Daten.\nTopic Modeling mit Neural Networks (BERTopic(Devlin et al., 2019), Doc2Vec(Le & Mikolov, 2014)) nutzt Deep Learning-Architekturen, um automatisch latente Themen aus Textdaten zu lernen"
  },
  {
    "objectID": "slides/ms-slides-09.html#preparation-is-everything",
    "href": "slides/ms-slides-09.html#preparation-is-everything",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Preparation is everything",
    "text": "Preparation is everything\nEmpfohlene Vorverarbeitungsschritte für das Topic Modeling nach Maier et al. (2018)\n\n\n\n⚠️ Deduplication;\n✅ tokenization;\n✅ transforming all characters to lowercase;\n✅ removing punctuation and special characters;\n✅ Removing stop-words;\n⚠️ term unification (lemmatizing or stemming);\n🏗️ relative pruning (attributed to Zipf’s law);\n\n\n\n# Pruning\nquanteda_dfm_trim &lt;- quanteda_dfm %&gt;% \n  dfm_trim( \n    min_docfreq = 10/nrow(review_subsample),\n    max_docfreq = 0.99, \n    docfreq_type = \"prop\")\n\n# Convert for stm topic modeling\nquanteda_stm &lt;- quanteda_dfm_trim %&gt;% \n   convert(to = \"stm\")\n\n\n\nZipf’s law states that the frequency that a word appears is inversely proportional to its rank."
  },
  {
    "objectID": "slides/ms-slides-09.html#ein-erstes-modell",
    "href": "slides/ms-slides-09.html#ein-erstes-modell",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Ein erstes Modell",
    "text": "Ein erstes Modell\nSchätzung und Sichtung eines Structural Topic Models mit 20 Themen\n\n\n\nstm_mdl &lt;- stm::stm(\n  documents = quanteda_stm$documents,\n  vocab = quanteda_stm$vocab, \n  K = 20, \n  seed = 42,\n  max.em.its = 10,\n  init.type = \"Spectral\",\n  verbose = TRUE)\n\n\n\nstm_mdl\n\nA topic model with 20 topics, 36650 documents and a 14322 word dictionary."
  },
  {
    "objectID": "slides/ms-slides-09.html#ein-erster-überblick",
    "href": "slides/ms-slides-09.html#ein-erster-überblick",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Ein erster Überblick",
    "text": "Ein erster Überblick\nVerteilung und Beschreibung der Themen\n\nstm_mdl %&gt;% plot(type = \"summary\")"
  },
  {
    "objectID": "slides/ms-slides-09.html#selber-überblick-anderes-format",
    "href": "slides/ms-slides-09.html#selber-überblick-anderes-format",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Selber Überblick, anderes Format",
    "text": "Selber Überblick, anderes Format\nVerteilung und Beschreibung der Themen\n\n\nExpand for full code\ntop_gamma &lt;- stm_mdl %&gt;%\n  tidy(matrix = \"gamma\") %&gt;% \n  dplyr::group_by(topic) %&gt;%\n  dplyr::summarise(gamma = mean(gamma), .groups = \"drop\") %&gt;%\n  dplyr::arrange(desc(gamma))\n\ntop_beta &lt;- stm_mdl %&gt;%\n  tidytext::tidy(.) %&gt;% \n  dplyr::group_by(topic) %&gt;%\n  dplyr::arrange(-beta) %&gt;%\n  dplyr::top_n(10, wt = beta) %&gt;% \n  dplyr::select(topic, term) %&gt;%\n  dplyr::summarise(terms_beta = toString(term), .groups = \"drop\")\n\ntop_topics_terms &lt;- top_beta %&gt;% \n  dplyr::left_join(top_gamma, by = \"topic\") %&gt;%\n  dplyr::mutate(\n          topic = reorder(topic, gamma)\n      )\n\n# Preview\ntop_topics_terms %&gt;%\n  mutate(across(gamma, ~round(.,3))) %&gt;% \n  dplyr::arrange(-gamma) %&gt;% \n  gt() %&gt;% \n  gt::tab_options(\n    table.font.size = \"14px\") %&gt;% \n  cols_label(\n    topic = \"Topic\", \n    terms_beta = \"Top Terms (based on beta)\",\n    gamma = \"Gamma\"\n  ) %&gt;% \n  gtExtras::gt_theme_538()\n\n\n\n\n\n\n\n\nTopic\nTop Terms (based on beta)\nGamma\n\n\n\n\n16\nresearch, literature, review, paper, systematic, future, study, analysis, findings, knowledge\n0.142\n\n\n19\nstudies, interventions, review, systematic, evidence, outcomes, included, quality, intervention, health\n0.096\n\n\n13\nhealth, mental, care, review, support, family, children, social, factors, studies\n0.079\n\n\n9\nlearning, students, education, school, review, skills, educational, teachers, teaching, study\n0.072\n\n\n11\nstudy, literature, research, can, work, development, review, also, human, economic\n0.068\n\n\n15\nstudies, ci, effect, meta-analysis, depression, p, trials, anxiety, effects, interventions\n0.061\n\n\n14\nsocial, media, information, use, digital, data, technology, communication, review, research\n0.060\n\n\n17\nstudies, children, relationship, review, language, variables, research, factors, results, effects\n0.052\n\n\n6\nprevalence, studies, covid-19, suicide, among, risk, pandemic, countries, ci, vaccine\n0.050\n\n\n1\nsleep, studies, eating, cognitive, review, associated, weight, body, may, association\n0.044\n\n\n2\nhealth, studies, women, gender, review, care, social, services, cultural, access\n0.043\n\n\n18\nstudies, health, used, measures, review, tools, instruments, assessment, training, n\n0.039\n\n\n3\ntreatment, disorder, disorders, patients, ptsd, symptoms, clinical, studies, therapy, anxiety\n0.036\n\n\n7\narticles, review, adolescents, studies, literature, search, use, results, systematic, databases\n0.036\n\n\n4\npatients, articles, review, music, therapy, cancer, can, study, pain, life\n0.032\n\n\n12\nviolence, studies, use, sexual, risk, h3, ipv, substance, alcohol, review\n0.031\n\n\n5\net, al, university, review, gt, literature, lt, author, p, search\n0.030\n\n\n8\nphysical, training, studies, disability, exercise, disabilities, employment, ed, review, strength\n0.015\n\n\n20\nattachment, studies, scholar, google, science, review, social, styles, welfare, research\n0.009\n\n\n10\nde, la, y, en, los, e, 的, el, se, que\n0.004"
  },
  {
    "objectID": "slides/ms-slides-09.html#verbindung-der-themen-untereinander",
    "href": "slides/ms-slides-09.html#verbindung-der-themen-untereinander",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Verbindung der Themen untereinander",
    "text": "Verbindung der Themen untereinander\nKorrelation der Themen\n\nstm_corr &lt;- stm::topicCorr(stm_mdl)\nplot(stm_corr)"
  },
  {
    "objectID": "slides/ms-slides-09.html#prominente-wörter-einzelner-themen",
    "href": "slides/ms-slides-09.html#prominente-wörter-einzelner-themen",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Prominente Wörter einzelner Themen",
    "text": "Prominente Wörter einzelner Themen\nÜberblick über Top-Begriffe verschiedener Themen\n\n# Fokus auf Themas 16 (höchtes Gamma)\nstm::labelTopics(stm_mdl, topic=16)\n\nTopic 16 Top Words:\n     Highest Prob: research, literature, review, paper, systematic, future, study \n     FREX: tourism, sustainable, sustainability, originality, innovation, conceptual, agenda \n     Lift: paradigmatic, hrd, edlm, positivist, internationalisation, tccm, wom \n     Score: tourism, research, literature, paper, leadership, themes, sustainable \n\n# Fokus auf Thema 10 (isoliertes Thema)\nstm::labelTopics(stm_mdl, topic=10)\n\nTopic 10 Top Words:\n     Highest Prob: de, la, y, en, los, e, 的 \n     FREX: resultados, foram, que, sobre, intervenciones, riesgo, uma \n     Lift: criterios, efecto, así, comparación, cumplieron, debido, depresión \n     Score: de, la, 的, en, los, y, que"
  },
  {
    "objectID": "slides/ms-slides-09.html#and-now-you-textanalyse-mit-r",
    "href": "slides/ms-slides-09.html#and-now-you-textanalyse-mit-r",
    "title": "Unsupervised Machine Learning (I)",
    "section": "🧪 And now … you: Textanalyse mit R",
    "text": "🧪 And now … you: Textanalyse mit R\nNext Steps: Wiederholung der Inhalte\n\nLaden Sie die auf StudOn bereitgestellten Dateien für die Sitzungen herunter\nLaden Sie die .zip-Datei in Ihren RStudio Workspace\nNavigieren Sie zu dem Ordner, in dem die Datei ps_24_binder.Rproj liegt. Öffnen Sie diese Datei mit einem Doppelklick. Nur dadurch ist gewährleistet, dass alle Dependencies korrekt funktionieren.\nÖffnen Sie die Datei exercise-09.qmd im Ordner exercises und lesen Sie sich gründlich die Anweisungen durch.\nTipp: Sie finden alle in den Folien verwendeten Code-Bausteine in der Datei showcase.qmd (für den “rohen” Code) oder showcase.html (mit gerenderten Ausgaben)."
  },
  {
    "objectID": "slides/ms-slides-09.html#references",
    "href": "slides/ms-slides-09.html#references",
    "title": "Unsupervised Machine Learning (I)",
    "section": "References",
    "text": "References\n\n\nBlei, D. M. (2012). Probabilistic topic models. Communications of the ACM, 55(4), 77–84. https://doi.org/10.1145/2133806.2133826\n\n\nBlei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichlet allocation. The Journal of Machine Learning Research, 3, 9931022.\n\n\nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of deep bidirectional transformers for language understanding (J. Burstein, C. Doran, & T. Solorio, Eds.; p. 41714186). Association for Computational Linguistics. https://doi.org/10.18653/v1/N19-1423\n\n\nGrimmer, J., & Stewart, B. M. (2013). Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts. Political Analysis, 21(3), 267–297. https://doi.org/f458q9\n\n\nLe, Q., & Mikolov, T. (2014). Distributed representations of sentences and documents (E. P. Xing & T. Jebara, Eds.; Vol. 32, p. 11881196). PMLR. https://proceedings.mlr.press/v32/le14.html\n\n\nMaier, D., Waldherr, A., Miltner, P., Wiedemann, G., Niekler, A., Keinert, A., Pfetsch, B., Heyer, G., Reber, U., Häussler, T., Schmid-Petri, H., & Adam, S. (2018). Applying LDA Topic Modeling in Communication Research: Toward a Valid and Reliable Methodology. Communication Methods and Measures, 12(2-3), 93–118. https://doi.org/10.1080/19312458.2018.1430754\n\n\nMikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed representations of words and phrases and their compositionality (C. J. Burges, L. Bottou, M. Welling, Z. Ghahramani, & K. Q. Weinberger, Eds.; Vol. 26). Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf\n\n\nPennington, J., Socher, R., & Manning, C. D. (2014). Glove: Global vectors for word representation. 15321543. https://doi.org/10.3115/v1/D14-1162\n\n\nRoberts, M. E., Stewart, B. M., & Airoldi, E. M. (2016). A model of text for experimentation in the social sciences. Journal of the American Statistical Association, 111(515), 988–1003. https://doi.org/f88tzh\n\n\nRoberts, M. E., Stewart, B. M., & Tingley, D. (2019). stm: An R Package for Structural Topic Models. Journal of Statistical Software, 91(1), 1–40. https://doi.org/10.18637/jss.v091.i02\n\n\nSilge, J., & Robinson, D. (2017). Text mining with r: A tidy approach (First edition). O’Reilly.\n\n\nZheng, A., & Casari, A. (2018). Feature engineering for machine learning: Principles and techniques for data scientists (First edition). O’Reilly.\n\n\n\n\n\n\nHome"
  },
  {
    "objectID": "slides/ms-slides-07.html#seminarplan",
    "href": "slides/ms-slides-07.html#seminarplan",
    "title": "Introduction to Text as Data",
    "section": "Seminarplan",
    "text": "Seminarplan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSitzung\nDatum\nThema (synchron)\nÜbung (asynchron)\nDozent:in\n\n\n\n\n1\n18.04.2024\nEinführung & Überblick\n\nAM & CA\n\n\n\n📚\nTeil 1: Systematic Review\n\n\n\n\n2\n25.04.2024\nEinführung in Systematic Reviews I\nR-Einführung\nAM\n\n\n3\n02.05.2024\nEinführung in Systematic Reviews II\nR-Einführung\nAM\n\n\n\n09.05.2024\n🏖️ Feiertag\nR-Einführung\n\n\n\n4\n16.05.2024\nAutomatisierung von SRs & KI-Tools\nR-Einführung\nAM\n\n\n\n23.05.2024\n🍻 WiSo-Projekt-Woche\nR-Einführung\n\n\n\n5\n04.06.2024\n🍕 Gastvortrag: Prof. Dr. Emese Domahidi\nR-Einführung\nED\n\n\n6\n06.06.2024\nAutomatisierung von SRs & KI-Tools\nR-Einführung\nAM\n\n\n\n💻\nTeil 2: Text as Data & Unsupervised Machine Learning\n\n\n\n\n7\n13.06.2024\nIntroduction to Text as Data\nzur Sitzung\nCA\n\n\n8\n20.06.2024\nText processing\nzur Sitzung\nCA\n\n\n9\n27.06.2024\nUnsupervised Machine Learning I\nzur Sitzung\nCA\n\n\n10\n04.07.2024\nUnsupervised Machine Learning II\nzur Sitzung\nCA & AM\n\n\n11\n11.07.2024\nRecap & Ausblick\nzur Sitzung\nCA & AM\n\n\n12\n18.07.2024\n🏁 Semesterabschluss\nzur Sitzung\nCA & AM"
  },
  {
    "objectID": "slides/ms-slides-07.html#erst-input-dann-vertiefung",
    "href": "slides/ms-slides-07.html#erst-input-dann-vertiefung",
    "title": "Introduction to Text as Data",
    "section": "Erst Input, dann Vertiefung",
    "text": "Erst Input, dann Vertiefung\nTypischer Aufbau der nächsten vier Sitzungen\nTeil 1️⃣: Input (ca. 30-45 Minuten)\n\nVorstellung der “theoretischen Grundlagen” inklusive zentraler Begriffe und Konzepte\nVorstellung der Methode(n) sowie des Kontext der praktischen Anwendung\n\nTeil 2️⃣: Praktische Anwendung (ca. 45-60 Minuten)\n\nVertiefung der Inhalte durch Bearbeitung kleiner Aufgaben, entweder in Einzel- oder Gruppenarbeit\nAufgaben zur Arbeit mit R, die im Kurs angefangen aber (vermutlich) außerhalb der Sitzung abgeschlossen werden"
  },
  {
    "objectID": "slides/ms-slides-07.html#bitte-rstudio-server-benutzen",
    "href": "slides/ms-slides-07.html#bitte-rstudio-server-benutzen",
    "title": "Introduction to Text as Data",
    "section": "Bitte RStudio Server benutzen!",
    "text": "Bitte RStudio Server benutzen!\nInformation zur Nutzung des RStudio Servers während der Sitzung\n\n\n⏰ Zur Erinnerung:\n\nFunktion der RStudio-Projekte für die praktische Anwendung in Serverumgebung getestet\nNutzung des RStudio Servers vermeidet aufwendiges & zeitraubendes Trouble-Shooting\n\n\n\n\nℹ️ Infos zum RStudio Server:\n\nNutzung nur über W-LAN der FAU (ggf. mit aktivem VPN) möglich\nVerfügbar unter: http://10.204.20.178:8787\nZugangsdaten auf Teams\n\n\n\n\n\n\n\n\nBei Problemen: Fragen in den Teams-Kanal!"
  },
  {
    "objectID": "slides/ms-slides-07.html#was-versteht-ihr-unter-text-as-data",
    "href": "slides/ms-slides-07.html#was-versteht-ihr-unter-text-as-data",
    "title": "Introduction to Text as Data",
    "section": "Was versteht ihr unter Text as Data?",
    "text": "Was versteht ihr unter Text as Data?\nBitte nehmt an einer kurzen Umfrage teil\n\n\n\nBitte scannt den QR-Code oder nutzt den folgenden Link für die Teilnahme an einer kurzen Umfrage:\n\nhttps://www.menti.com/aly1gft3568e\nTemporary Access Code: 8113 5474\n\n\n\n\n \n\n    \n\n\n\n\n−+\n01:00"
  },
  {
    "objectID": "slides/ms-slides-07.html#ergebnis",
    "href": "slides/ms-slides-07.html#ergebnis",
    "title": "Introduction to Text as Data",
    "section": "Ergebnis",
    "text": "Ergebnis"
  },
  {
    "objectID": "slides/ms-slides-07.html#altes-phänomen-neue-dimension",
    "href": "slides/ms-slides-07.html#altes-phänomen-neue-dimension",
    "title": "Introduction to Text as Data",
    "section": "Altes Phänomen, neue Dimension",
    "text": "Altes Phänomen, neue Dimension\nHintergrund zu dem Phänomen Text as Data\n\n\nLange Tradition der Text- und Inhaltsanalyse\nNeue Chancen & Herausforderungen durch explosionsartige Vergrößerung des (Text-)Datenaufkommen und deren Verfügbarkeit in den letzten Jahren (Websites, Plattformen & Digitalisierung) Verfügbarkeit von (neuen) Datenquellen als Resultat der Digitalisierung"
  },
  {
    "objectID": "slides/ms-slides-07.html#from-text-to-data-to-data-analysis",
    "href": "slides/ms-slides-07.html#from-text-to-data-to-data-analysis",
    "title": "Introduction to Text as Data",
    "section": "From text to data to data analysis",
    "text": "From text to data to data analysis\nTransformation als essenzieller Bestandteil von Text as Data (Curini & Franzese, 2020)"
  },
  {
    "objectID": "slides/ms-slides-07.html#review-über-literaturreviews",
    "href": "slides/ms-slides-07.html#review-über-literaturreviews",
    "title": "Introduction to Text as Data",
    "section": "Review über Literaturreviews",
    "text": "Review über Literaturreviews\nGrundidee, Ziel und Schwerpunkte und der kommenden Sitzungen zu Text as data\n\nIdee: Überblick über Literatur zu Literaturüberblicken verschaffen\nZiel: Durchführung einer Kombination aus (elaboriertem) Scoping Search & Scoping Review\nFokus: Computergestütze Umsetzung möglichst vieler Bestandteile des Review-Workflows, wie z.B.:\n\nEigene Datenerhebung via (OpenAlex-)API\nOberflächliche bibliometrische Analyse (zur Datenexploration und -bereinigung)\nAnalye der Abstracts mit Hilfe von unüberwachtem Machine Learning (Topic Modeling)"
  },
  {
    "objectID": "slides/ms-slides-07.html#was-war-das-nochmal",
    "href": "slides/ms-slides-07.html#was-war-das-nochmal",
    "title": "Introduction to Text as Data",
    "section": "Was war das nochmal?",
    "text": "Was war das nochmal?\nZur Erinnerung: Definition von Scoping Search und Scoping Review\n🔎 Scoping searches …\n\n\nschnelle explorative Suche, die sich auf bestimmtes Konzept konzentriert\nsind oft Teil der Suchstringentwicklung\nsind nicht ausreichend für ein systematic review\n\n\n\n… Scoping Review 📋\n\n\n\neine spezifische Form eines literature reviews\nmap a vast body of research literature in a field of interest in terms of the volume, nature, and characteristics of the primary research (Pham et al., 2014)\ndo not aim to produce a critically appraised and synthesised result/answer to a particular question, [they] rather aim to provide an overview or map of the evidence (Munn et al., 2018)"
  },
  {
    "objectID": "slides/ms-slides-07.html#what-we-do-not-do",
    "href": "slides/ms-slides-07.html#what-we-do-not-do",
    "title": "Introduction to Text as Data",
    "section": "What we (do not) do",
    "text": "What we (do not) do\nDisclaimer zum Inhalt und der Zielsetzung des Sitzungen zu Text as Data\n\n\n❌ Kein vollständig dokumentiertes Scoping Review, dass alle notwendigen (SALSA-)Schritte in vollem Umfang und nach wissenschaftlichen Standarts durchläuft\n\n❌ Keine umfassendes Einführung in die Textanalyse mit R\n\n\n\n✅ Exemplarische Darstellung einzelner Schritte des Workflows, mit Fokus auf die computergestützte Umsetzung\n \n✅ Überblick über verschiedene Verfahren, mit Schwerpunkt auf Methoden, die im Kontext von Literaturreviews notwendig und nützlich sind"
  },
  {
    "objectID": "slides/ms-slides-07.html#wer-oder-was-ist-openalex",
    "href": "slides/ms-slides-07.html#wer-oder-was-ist-openalex",
    "title": "Introduction to Text as Data",
    "section": "Wer oder was ist OpenAlex?",
    "text": "Wer oder was ist OpenAlex?\nKurze Vorstellung und Hintergrundinformationen zur Datenquelle (OpenAlex)\n\n\nopen(-source) catalog of the world’s scholarly research system\ndata is free and reusable, available via bulk download or API\ngoverned by a sustainable and transparent nonprofit"
  },
  {
    "objectID": "slides/ms-slides-07.html#first-scoping-search",
    "href": "slides/ms-slides-07.html#first-scoping-search",
    "title": "Introduction to Text as Data",
    "section": "First scoping search",
    "text": "First scoping search\nSichtung der Daten- und Identfikation der Analysegrundlage\n\n\n\n\nEine simple Suchquery resultiert zwar in sehr vielen Treffern, bringt aber auch (praktische) Probleme mit sich:\n\nDeutliche Überschreibung des tägliches API-Limit beträgt 100.000 Referenzen\n“Lokale” Datenbearbeitung und -analye benötigt bei der Menge an Daten ehrhebliche Rechenkapazität\nLösung: Optimierung der Suchquery durch Spezifizierung des Untersuchungsgegenstandes"
  },
  {
    "objectID": "slides/ms-slides-07.html#fine-tuning-der-search-query",
    "href": "slides/ms-slides-07.html#fine-tuning-der-search-query",
    "title": "Introduction to Text as Data",
    "section": "Fine-Tuning der Search Query",
    "text": "Fine-Tuning der Search Query\nÜberblick über verwendete Search Query und ausgewählte deskriptive Statistiken\n\n\n\n\n\n\n\n\n\n\n    \nLink zur Suche"
  },
  {
    "objectID": "slides/ms-slides-07.html#openalex-openalexr",
    "href": "slides/ms-slides-07.html#openalex-openalexr",
    "title": "Introduction to Text as Data",
    "section": "OpenAlex 🤝 openalexR",
    "text": "OpenAlex 🤝 openalexR\nZusammenspiel aus Datenbank und R-Package openalexR (Aria et al., 2024)\n\n\n\n\n\n\n\n\n\n\nopenalexR helps you interface with the OpenAlex API to retrieve bibliographic information about publications, authors, institutions, sources, funders, publishers, topics and concepts.\n\n\n\n\nManueller Export von Ergebnissen mit Hilfe des Web-Interface von OpenAlex möglich, im Bulk aber umständlich\nSelbstständige Interaktion mit API ist aufwendig: Design der Query, Programmierung der Abfrage, Verarbeitung der Daten (nicht im Tabellenformat verfügbar)\nR-Package bietet niedrigschwelligere Alternative für API-Abfragen"
  },
  {
    "objectID": "slides/ms-slides-07.html#reproduktion-der-webabfrage-mit-r",
    "href": "slides/ms-slides-07.html#reproduktion-der-webabfrage-mit-r",
    "title": "Introduction to Text as Data",
    "section": "Reproduktion der Webabfrage mit R",
    "text": "Reproduktion der Webabfrage mit R\nAbfrage, Download und Transformation der Daten mit einer Funktion\n\n# Download data via API\nreview_works &lt;- openalexR::oa_fetch(\n  entity = \"works\",\n  title.search = \"(literature OR systematic) AND review\",\n  primary_topic.domain.id = \"domains/2\", # Social Science\n  publication_year = \"2013 - 2023\",\n  verbose = TRUE\n)\n\n# Overview\nreview_works\n\n\n\n# A tibble: 93,655 × 39\n  id      title display_name author ab    publication_date relevance_score so   \n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;        &lt;list&gt; &lt;chr&gt; &lt;chr&gt;                      &lt;dbl&gt; &lt;chr&gt;\n1 https:… The … The PRISMA … &lt;df&gt;   The … 2021-03-29                 1625. BMJ  \n2 https:… Pref… Preferred r… &lt;df&gt;   Syst… 2015-01-01                 1340. Syst…\n3 https:… Rayy… Rayyan—a we… &lt;df&gt;   Synt… 2016-12-01                 1314. Syst…\n4 https:… Syst… Systematic … &lt;df&gt;   Scop… 2018-11-19                  990. BMC …\n5 https:… Upda… Updated gui… &lt;df&gt;   On a… 2019-10-03                  963. Coch…\n# ℹ 93,650 more rows\n# ℹ 31 more variables: so_id &lt;chr&gt;, host_organization &lt;chr&gt;, issn_l &lt;chr&gt;,\n#   url &lt;chr&gt;, pdf_url &lt;chr&gt;, license &lt;chr&gt;, version &lt;chr&gt;, first_page &lt;chr&gt;,\n#   last_page &lt;chr&gt;, volume &lt;chr&gt;, issue &lt;chr&gt;, is_oa &lt;lgl&gt;,\n#   is_oa_anywhere &lt;lgl&gt;, oa_status &lt;chr&gt;, oa_url &lt;chr&gt;,\n#   any_repository_has_fulltext &lt;lgl&gt;, language &lt;chr&gt;, grants &lt;list&gt;,\n#   cited_by_count &lt;int&gt;, counts_by_year &lt;list&gt;, publication_year &lt;int&gt;, …"
  },
  {
    "objectID": "slides/ms-slides-07.html#das-ergebnis-der-abfrage",
    "href": "slides/ms-slides-07.html#das-ergebnis-der-abfrage",
    "title": "Introduction to Text as Data",
    "section": "Das Ergebnis der Abfrage",
    "text": "Das Ergebnis der Abfrage\nFlüchtiger Blick auf den R-Datensatz inklusive erster Qualtiätsprüfung\n\n\n\nTypische Überprüfungen\n\n\n\n\nWie viele Fälle sind enthalten? Wie viele Variablen? Sind die Variablennamen aussagekräftig?\nWelchen Typ hat jede Variable, z. B. numerisch, kategorisch, logisch?\n\n\n\n\n\n\nreview_works %&gt;% glimpse()\n\nRows: 93,655\nColumns: 39\n$ id                          &lt;chr&gt; \"https://openalex.org/W3118615836\", \"https…\n$ title                       &lt;chr&gt; \"The PRISMA 2020 statement: an updated gui…\n$ display_name                &lt;chr&gt; \"The PRISMA 2020 statement: an updated gui…\n$ author                      &lt;list&gt; [&lt;data.frame[26 x 12]&gt;], [&lt;data.frame[8 x…\n$ ab                          &lt;chr&gt; \"The Preferred Reporting Items for Systema…\n$ publication_date            &lt;chr&gt; \"2021-03-29\", \"2015-01-01\", \"2016-12-01\", …\n$ relevance_score             &lt;dbl&gt; 1625.1708, 1340.1902, 1314.3904, 990.4521,…\n$ so                          &lt;chr&gt; \"BMJ\", \"Systematic reviews\", \"Systematic r…\n$ so_id                       &lt;chr&gt; \"https://openalex.org/S4393917726\", \"https…\n$ host_organization           &lt;chr&gt; NA, \"BioMed Central\", \"BioMed Central\", \"B…\n$ issn_l                      &lt;chr&gt; \"1756-1833\", \"2046-4053\", \"2046-4053\", \"14…\n$ url                         &lt;chr&gt; \"https://doi.org/10.1136/bmj.n71\", \"https:…\n$ pdf_url                     &lt;chr&gt; \"https://www.bmj.com/content/bmj/372/bmj.n…\n$ license                     &lt;chr&gt; \"cc-by\", \"cc-by\", \"cc-by\", \"cc-by\", NA, \"c…\n$ version                     &lt;chr&gt; \"publishedVersion\", \"publishedVersion\", \"p…\n$ first_page                  &lt;chr&gt; \"n71\", NA, NA, NA, NA, \"167\", \"g7647\", \"93…\n$ last_page                   &lt;chr&gt; \"n71\", NA, NA, NA, NA, \"176\", \"g7647\", \"11…\n$ volume                      &lt;chr&gt; NA, \"4\", \"5\", \"18\", NA, \"84\", \"349\", \"39\",…\n$ issue                       &lt;chr&gt; NA, \"1\", \"1\", \"1\", NA, \"3\", \"jan02 1\", \"1\"…\n$ is_oa                       &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, FALSE, TRUE, TRUE,…\n$ is_oa_anywhere              &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, …\n$ oa_status                   &lt;chr&gt; \"hybrid\", \"gold\", \"gold\", \"gold\", \"green\",…\n$ oa_url                      &lt;chr&gt; \"https://www.bmj.com/content/bmj/372/bmj.n…\n$ any_repository_has_fulltext &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE,…\n$ language                    &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", …\n$ grants                      &lt;list&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, &lt;\"htt…\n$ cited_by_count              &lt;int&gt; 30303, 17347, 10540, 5298, 5664, 2657, 909…\n$ counts_by_year              &lt;list&gt; [&lt;data.frame[5 x 2]&gt;], [&lt;data.frame[11 x …\n$ publication_year            &lt;int&gt; 2021, 2015, 2016, 2018, 2019, 2015, 2015, …\n$ cited_by_api_url            &lt;chr&gt; \"https://api.openalex.org/works?filter=cit…\n$ ids                         &lt;list&gt; &lt;\"https://openalex.org/W3118615836\", \"htt…\n$ doi                         &lt;chr&gt; \"https://doi.org/10.1136/bmj.n71\", \"https:…\n$ type                        &lt;chr&gt; \"article\", \"article\", \"article\", \"article\"…\n$ referenced_works            &lt;list&gt; &lt;\"https://openalex.org/W1528251861\", \"htt…\n$ related_works               &lt;list&gt; &lt;\"https://openalex.org/W4234875088\", \"htt…\n$ is_paratext                 &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ is_retracted                &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ concepts                    &lt;list&gt; [&lt;data.frame[20 x 5]&gt;], [&lt;data.frame[18 x…\n$ topics                      &lt;list&gt; [&lt;tbl_df[12 x 5]&gt;], [&lt;tbl_df[12 x 5]&gt;], […"
  },
  {
    "objectID": "slides/ms-slides-07.html#wichtigkeit-von-gewissenhaftigkeit",
    "href": "slides/ms-slides-07.html#wichtigkeit-von-gewissenhaftigkeit",
    "title": "Introduction to Text as Data",
    "section": "Wichtigkeit von Gewissenhaftigkeit",
    "text": "Wichtigkeit von Gewissenhaftigkeit\nGute Gewohnheiten helfen bei Qualitätsprüfung und Datenverarbeitung\n\n\n\nPraktische Empfehlungen\n\n\n\n\nEinheitlicher Code-Style, Bearbeitungsschritte kommentieren\nVeränderungen in neuen Datensatz speichern\n\n\n\n\n\nStreben nach:\n\n# Corrections based on first glimpse \nreview_works_correct &lt;- review_works %&gt;% \n    mutate(\n        # Create additional factor variables\n        publication_year_fct = as.factor(publication_year), \n        type_fct = as.factor(type)\n        )\n\nBitte vermeiden:\n\nReWoCo&lt;-review_works %&gt;% mutate(pub_year_fct = as.factor(publication_year), type_fct = as.factor(type))"
  },
  {
    "objectID": "slides/ms-slides-07.html#ein-datensatz-im-datensatz",
    "href": "slides/ms-slides-07.html#ein-datensatz-im-datensatz",
    "title": "Introduction to Text as Data",
    "section": "Ein Datensatz im Datensatz",
    "text": "Ein Datensatz im Datensatz\nExkurs zu verschachtelten (nested) Daten und Möglichkeiten zur Verabeitung in R\n\nBesonderheit: Informationen zu (Themen-)Katalogisierung als Liste im Datensatz\n\n\nreview_works_correct$topics %&gt;% head()\n\n[[1]]\n# A tibble: 12 × 5\n       i score name     id                                  display_name        \n   &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;                               &lt;chr&gt;               \n 1     1 0.999 topic    https://openalex.org/T10206         Methods for Evidenc…\n 2     1 0.999 subfield https://openalex.org/subfields/1804 Statistics, Probabi…\n 3     1 0.999 field    https://openalex.org/fields/18      Decision Sciences   \n 4     1 0.999 domain   https://openalex.org/domains/2      Social Sciences     \n 5     2 0.983 topic    https://openalex.org/T10416         Epidemiology and Im…\n 6     2 0.983 subfield https://openalex.org/subfields/2713 Epidemiology        \n 7     2 0.983 field    https://openalex.org/fields/27      Medicine            \n 8     2 0.983 domain   https://openalex.org/domains/4      Health Sciences     \n 9     3 0.946 topic    https://openalex.org/T12443         The Delphi Method i…\n10     3 0.946 subfield https://openalex.org/subfields/3312 Sociology and Polit…\n11     3 0.946 field    https://openalex.org/fields/33      Social Sciences     \n12     3 0.946 domain   https://openalex.org/domains/2      Social Sciences     \n\n[[2]]\n# A tibble: 12 × 5\n       i score name     id                                  display_name        \n   &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;                               &lt;chr&gt;               \n 1     1 0.999 topic    https://openalex.org/T10206         Methods for Evidenc…\n 2     1 0.999 subfield https://openalex.org/subfields/1804 Statistics, Probabi…\n 3     1 0.999 field    https://openalex.org/fields/18      Decision Sciences   \n 4     1 0.999 domain   https://openalex.org/domains/2      Social Sciences     \n 5     2 0.982 topic    https://openalex.org/T12443         The Delphi Method i…\n 6     2 0.982 subfield https://openalex.org/subfields/3312 Sociology and Polit…\n 7     2 0.982 field    https://openalex.org/fields/33      Social Sciences     \n 8     2 0.982 domain   https://openalex.org/domains/2      Social Sciences     \n 9     3 0.957 topic    https://openalex.org/T10416         Epidemiology and Im…\n10     3 0.957 subfield https://openalex.org/subfields/2713 Epidemiology        \n11     3 0.957 field    https://openalex.org/fields/27      Medicine            \n12     3 0.957 domain   https://openalex.org/domains/4      Health Sciences     \n\n[[3]]\n# A tibble: 4 × 5\n      i score name     id                                  display_name         \n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;                               &lt;chr&gt;                \n1     1 0.937 topic    https://openalex.org/T10206         Methods for Evidence…\n2     1 0.937 subfield https://openalex.org/subfields/1804 Statistics, Probabil…\n3     1 0.937 field    https://openalex.org/fields/18      Decision Sciences    \n4     1 0.937 domain   https://openalex.org/domains/2      Social Sciences      \n\n[[4]]\n# A tibble: 12 × 5\n       i score name     id                                  display_name        \n   &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;                               &lt;chr&gt;               \n 1     1 0.997 topic    https://openalex.org/T10206         Methods for Evidenc…\n 2     1 0.997 subfield https://openalex.org/subfields/1804 Statistics, Probabi…\n 3     1 0.997 field    https://openalex.org/fields/18      Decision Sciences   \n 4     1 0.997 domain   https://openalex.org/domains/2      Social Sciences     \n 5     2 0.981 topic    https://openalex.org/T12443         The Delphi Method i…\n 6     2 0.981 subfield https://openalex.org/subfields/3312 Sociology and Polit…\n 7     2 0.981 field    https://openalex.org/fields/33      Social Sciences     \n 8     2 0.981 domain   https://openalex.org/domains/2      Social Sciences     \n 9     3 0.973 topic    https://openalex.org/T10416         Epidemiology and Im…\n10     3 0.973 subfield https://openalex.org/subfields/2713 Epidemiology        \n11     3 0.973 field    https://openalex.org/fields/27      Medicine            \n12     3 0.973 domain   https://openalex.org/domains/4      Health Sciences     \n\n[[5]]\n# A tibble: 12 × 5\n       i score name     id                                  display_name        \n   &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;                               &lt;chr&gt;               \n 1     1 0.993 topic    https://openalex.org/T10206         Methods for Evidenc…\n 2     1 0.993 subfield https://openalex.org/subfields/1804 Statistics, Probabi…\n 3     1 0.993 field    https://openalex.org/fields/18      Decision Sciences   \n 4     1 0.993 domain   https://openalex.org/domains/2      Social Sciences     \n 5     2 0.964 topic    https://openalex.org/T11744         Implementation of E…\n 6     2 0.964 subfield https://openalex.org/subfields/3600 General Health Prof…\n 7     2 0.964 field    https://openalex.org/fields/36      Health Professions  \n 8     2 0.964 domain   https://openalex.org/domains/4      Health Sciences     \n 9     3 0.954 topic    https://openalex.org/T12664         Development and Eva…\n10     3 0.954 subfield https://openalex.org/subfields/2739 Public Health, Envi…\n11     3 0.954 field    https://openalex.org/fields/27      Medicine            \n12     3 0.954 domain   https://openalex.org/domains/4      Health Sciences     \n\n[[6]]\n# A tibble: 12 × 5\n       i score name     id                                  display_name        \n   &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;                               &lt;chr&gt;               \n 1     1 0.993 topic    https://openalex.org/T10475         Role of Positive Em…\n 2     1 0.993 subfield https://openalex.org/subfields/3207 Social Psychology   \n 3     1 0.993 field    https://openalex.org/fields/32      Psychology          \n 4     1 0.993 domain   https://openalex.org/domains/2      Social Sciences     \n 5     2 0.992 topic    https://openalex.org/T10804         Health Economics an…\n 6     2 0.992 subfield https://openalex.org/subfields/2002 Economics and Econo…\n 7     2 0.992 field    https://openalex.org/fields/20      Economics, Economet…\n 8     2 0.992 domain   https://openalex.org/domains/2      Social Sciences     \n 9     3 0.984 topic    https://openalex.org/T12947         Salutogenesis and S…\n10     3 0.984 subfield https://openalex.org/subfields/3600 General Health Prof…\n11     3 0.984 field    https://openalex.org/fields/36      Health Professions  \n12     3 0.984 domain   https://openalex.org/domains/4      Health Sciences"
  },
  {
    "objectID": "slides/ms-slides-07.html#entpacken-der-schachteln-steigert-die-fallzahl",
    "href": "slides/ms-slides-07.html#entpacken-der-schachteln-steigert-die-fallzahl",
    "title": "Introduction to Text as Data",
    "section": "Entpacken der Schachteln steigert die Fallzahl",
    "text": "Entpacken der Schachteln steigert die Fallzahl\nExkurs zu verschachtelten (nested) Daten und Möglichkeiten zur Verabeitung in R\n\nreview_works_correct %&gt;% \n    unnest(topics, names_sep = \"_\") %&gt;%\n    glimpse()\n\nRows: 942,560\nColumns: 45\n$ id                          &lt;chr&gt; \"https://openalex.org/W3118615836\", \"https…\n$ title                       &lt;chr&gt; \"The PRISMA 2020 statement: an updated gui…\n$ display_name                &lt;chr&gt; \"The PRISMA 2020 statement: an updated gui…\n$ author                      &lt;list&gt; [&lt;data.frame[26 x 12]&gt;], [&lt;data.frame[26 …\n$ ab                          &lt;chr&gt; \"The Preferred Reporting Items for Systema…\n$ publication_date            &lt;chr&gt; \"2021-03-29\", \"2021-03-29\", \"2021-03-29\", …\n$ relevance_score             &lt;dbl&gt; 1625.171, 1625.171, 1625.171, 1625.171, 16…\n$ so                          &lt;chr&gt; \"BMJ\", \"BMJ\", \"BMJ\", \"BMJ\", \"BMJ\", \"BMJ\", …\n$ so_id                       &lt;chr&gt; \"https://openalex.org/S4393917726\", \"https…\n$ host_organization           &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ issn_l                      &lt;chr&gt; \"1756-1833\", \"1756-1833\", \"1756-1833\", \"17…\n$ url                         &lt;chr&gt; \"https://doi.org/10.1136/bmj.n71\", \"https:…\n$ pdf_url                     &lt;chr&gt; \"https://www.bmj.com/content/bmj/372/bmj.n…\n$ license                     &lt;chr&gt; \"cc-by\", \"cc-by\", \"cc-by\", \"cc-by\", \"cc-by…\n$ version                     &lt;chr&gt; \"publishedVersion\", \"publishedVersion\", \"p…\n$ first_page                  &lt;chr&gt; \"n71\", \"n71\", \"n71\", \"n71\", \"n71\", \"n71\", …\n$ last_page                   &lt;chr&gt; \"n71\", \"n71\", \"n71\", \"n71\", \"n71\", \"n71\", …\n$ volume                      &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ issue                       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ is_oa                       &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, …\n$ is_oa_anywhere              &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, …\n$ oa_status                   &lt;chr&gt; \"hybrid\", \"hybrid\", \"hybrid\", \"hybrid\", \"h…\n$ oa_url                      &lt;chr&gt; \"https://www.bmj.com/content/bmj/372/bmj.n…\n$ any_repository_has_fulltext &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, …\n$ language                    &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", …\n$ grants                      &lt;list&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ cited_by_count              &lt;int&gt; 30303, 30303, 30303, 30303, 30303, 30303, …\n$ counts_by_year              &lt;list&gt; [&lt;data.frame[5 x 2]&gt;], [&lt;data.frame[5 x 2…\n$ publication_year            &lt;int&gt; 2021, 2021, 2021, 2021, 2021, 2021, 2021, …\n$ cited_by_api_url            &lt;chr&gt; \"https://api.openalex.org/works?filter=cit…\n$ ids                         &lt;list&gt; &lt;\"https://openalex.org/W3118615836\", \"htt…\n$ doi                         &lt;chr&gt; \"https://doi.org/10.1136/bmj.n71\", \"https:…\n$ type                        &lt;chr&gt; \"article\", \"article\", \"article\", \"article\"…\n$ referenced_works            &lt;list&gt; &lt;\"https://openalex.org/W1528251861\", \"htt…\n$ related_works               &lt;list&gt; &lt;\"https://openalex.org/W4234875088\", \"htt…\n$ is_paratext                 &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ is_retracted                &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ concepts                    &lt;list&gt; [&lt;data.frame[20 x 5]&gt;], [&lt;data.frame[20 x…\n$ topics_i                    &lt;int&gt; 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 1, 1, …\n$ topics_score                &lt;dbl&gt; 0.9993, 0.9993, 0.9993, 0.9993, 0.9832, 0.…\n$ topics_name                 &lt;chr&gt; \"topic\", \"subfield\", \"field\", \"domain\", \"t…\n$ topics_id                   &lt;chr&gt; \"https://openalex.org/T10206\", \"https://op…\n$ topics_display_name         &lt;chr&gt; \"Methods for Evidence Synthesis in Researc…\n$ publication_year_fct        &lt;fct&gt; 2021, 2021, 2021, 2021, 2021, 2021, 2021, …\n$ type_fct                    &lt;fct&gt; article, article, article, article, articl…"
  },
  {
    "objectID": "slides/ms-slides-07.html#deskriptive-statistiken-zur-datenabfrage",
    "href": "slides/ms-slides-07.html#deskriptive-statistiken-zur-datenabfrage",
    "title": "Introduction to Text as Data",
    "section": "Deskriptive Statistiken zur Datenabfrage",
    "text": "Deskriptive Statistiken zur Datenabfrage\nRekonstruktion und Erweiterung des OpenAlex Web-Dashboards mit R\n\n\n\nIm Fokus:\n\n🔍 Publikationen im Zeitverlauf\nFoschungsfelder\nRelevante Publikationen\nLageparameter\n\n\n\n\n\nreview_works_correct %&gt;% \n    ggplot(aes(publication_year_fct)) +\n    geom_bar() +\n    theme_pubr()"
  },
  {
    "objectID": "slides/ms-slides-07.html#deskriptive-statistiken-zur-datenabfrage-1",
    "href": "slides/ms-slides-07.html#deskriptive-statistiken-zur-datenabfrage-1",
    "title": "Introduction to Text as Data",
    "section": "Deskriptive Statistiken zur Datenabfrage",
    "text": "Deskriptive Statistiken zur Datenabfrage\nRekonstruktion und Erweiterung des OpenAlex Web-Dashboards mit R\n\n\n\nIm Fokus:\n\nPublikationen im Zeitverlauf\n🔍 Foschungsfelder\nRelevante Publikationen\nLageparameter\n\n\n\n\n\nreview_works_correct %&gt;% \n    unnest(topics, names_sep = \"_\") %&gt;% \n    filter(topics_name == \"field\") %&gt;% \n    filter(topics_i == 1) %&gt;% \n    sjmisc::frq(topics_display_name, sort.frq = \"desc\")\n\ntopics_display_name &lt;character&gt; \n# total N=93655 valid N=93655 mean=4.41 sd=1.62\n\nValue                               |     N | Raw % | Valid % | Cum. %\n----------------------------------------------------------------------\nSocial Sciences                     | 30580 | 32.65 |   32.65 |  32.65\nPsychology                          | 29054 | 31.02 |   31.02 |  63.67\nBusiness, Management and Accounting | 15558 | 16.61 |   16.61 |  80.29\nDecision Sciences                   |  7261 |  7.75 |    7.75 |  88.04\nEconomics, Econometrics and Finance |  6796 |  7.26 |    7.26 |  95.30\nArts and Humanities                 |  4406 |  4.70 |    4.70 | 100.00\n&lt;NA&gt;                                |     0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;"
  },
  {
    "objectID": "slides/ms-slides-07.html#deskriptive-statistiken-zur-datenabfrage-2",
    "href": "slides/ms-slides-07.html#deskriptive-statistiken-zur-datenabfrage-2",
    "title": "Introduction to Text as Data",
    "section": "Deskriptive Statistiken zur Datenabfrage",
    "text": "Deskriptive Statistiken zur Datenabfrage\nRekonstruktion und Erweiterung des OpenAlex Web-Dashboards mit R\n\n\n\nIm Fokus:\n\nPublikationen im Zeitverlauf\nFoschungsfelder\n🔍 Relevante Publikationen\nLageparameter\n\n\n\n\n\nreview_works_correct %&gt;% \n    arrange(desc(relevance_score)) %&gt;%\n    select(publication_year_fct, relevance_score, title) %&gt;% \n    head(5) %&gt;% \n    gt::gt()\n\n\n\n\n\n\n\npublication_year_fct\nrelevance_score\ntitle\n\n\n\n\n2021\n1625.1708\nThe PRISMA 2020 statement: an updated guideline for reporting systematic reviews\n\n\n2015\n1340.1902\nPreferred reporting items for systematic review and meta-analysis protocols (PRISMA-P) 2015 statement\n\n\n2016\n1314.3904\nRayyan—a web and mobile app for systematic reviews\n\n\n2018\n990.4521\nSystematic review or scoping review? Guidance for authors when choosing between a systematic or scoping review approach\n\n\n2019\n962.6738\nUpdated guidance for trusted systematic reviews: a new edition of the Cochrane Handbook for Systematic Reviews of Interventions"
  },
  {
    "objectID": "slides/ms-slides-07.html#deskriptive-statistiken-zur-datenabfrage-3",
    "href": "slides/ms-slides-07.html#deskriptive-statistiken-zur-datenabfrage-3",
    "title": "Introduction to Text as Data",
    "section": "Deskriptive Statistiken zur Datenabfrage",
    "text": "Deskriptive Statistiken zur Datenabfrage\nRekonstruktion und Erweiterung des OpenAlex Web-Dashboards mit R\n\n\n\nIm Fokus:\n\nPublikationen im Zeitverlauf\nFoschungsfelder\nRelevante Publikationen\n🔍 Lageparameter\n\n\n\n\n\nreview_works_correct %&gt;% \n  select(where(is.numeric)) %&gt;% \n  datawizard::describe_distribution() %&gt;% \n  print_html()\n\n\n\n\n\n\n\nVariable\nMean\nSD\nIQR\nMin\nMax\nSkewness\nKurtosis\nn\nn_Missing\n\n\n\n\nrelevance_score\n31.73\n42.51\n36.48\n1.17\n1625.17\n4.75\n67.87\n93655\n0\n\n\ncited_by_count\n18.33\n146.36\n10.00\n0.00\n30303.00\n123.44\n22236.55\n93655\n0\n\n\npublication_year\n2019.40\n2.97\n5.00\n2013.00\n2023.00\n-0.58\n-0.77\n93655\n0"
  },
  {
    "objectID": "slides/ms-slides-07.html#and-now-you-übung-eingrenzung",
    "href": "slides/ms-slides-07.html#and-now-you-übung-eingrenzung",
    "title": "Introduction to Text as Data",
    "section": "🧪 And now … you: Übung & Eingrenzung",
    "text": "🧪 And now … you: Übung & Eingrenzung\nNext Steps: Wiederholung der R-Grundlagen an OpenAlex-Daten\n\nLaden Sie die auf StudOn bereitgestellten Dateien für die Sitzungen herunter\nLaden Sie die .zip-Datei in Ihren RStudio Workspace\nNavigieren Sie zu dem Ordner, in dem die Datei ps_24_binder.Rproj liegt. Öffnen Sie diese Datei mit einem Doppelklick. Nur dadurch ist gewährleistet, dass alle Dependencies korrekt funktionieren.\nÖffnen Sie die Datei exercise-07.qmd im Ordner exercises und lesen Sie sich gründlich die Anweisungen durch.\nTipp: Sie finden alle in den Folien verwendeten Code-Bausteine in der Datei showcase.qmd (für den “rohen” Code) oder showcase.html (mit gerenderten Ausgaben)."
  },
  {
    "objectID": "slides/ms-slides-07.html#literatur",
    "href": "slides/ms-slides-07.html#literatur",
    "title": "Introduction to Text as Data",
    "section": "Literatur",
    "text": "Literatur\n\n\nAria, M., Le, T., Cuccurullo, C., Belfiore, A., & Choe, J. (2024). openalexR: An R-Tool for Collecting Bibliometric Data from OpenAlex. The R Journal, 15(4), 167–180. https://doi.org/10.32614/rj-2023-089\n\n\nCurini, L., & Franzese, R. (2020). Text as Data: An Overview. SAGE Publications Ltd. https://methods.sagepub.com/book/research-methods-in-political-science-and-international-relations\n\n\nGrimmer, J., & Stewart, B. M. (2013). Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts. Political Analysis, 21(3), 267–297. https://doi.org/f458q9\n\n\nMunn, Z., Peters, M. D. J., Stern, C., Tufanaru, C., McArthur, A., & Aromataris, E. (2018). Systematic review or scoping review? Guidance for authors when choosing between a systematic or scoping review approach. BMC Medical Research Methodology, 18(1). https://doi.org/10.1186/s12874-018-0611-x\n\n\nPham, M. T., Rajić, A., Greig, J. D., Sargeant, J. M., Papadopoulos, A., & McEwen, S. A. (2014). A scoping review of scoping reviews: advancing the approach and enhancing the consistency. Research Synthesis Methods, 5(4), 371–385. https://doi.org/10.1002/jrsm.1123\n\n\n\n\n\n\nHome"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "KI nutzen, um KI-Folgen zu verstehen",
    "section": "",
    "text": "Generative und kommunikative KI (bspw. ChatGPT, Bing, Gemini) ist derzeit in aller Munde. Den KI-Innovationen der letzten Jahre wird als „general purpose technology“ eine enorme transformative Kraft zugeschrieben, mit potenziell weitreichenden Folgen für Individuen, Wirtschaft und Gesellschaft. Die sozialwissenschaftliche Forschung auf diesem Gebiet entwickelt sich – ebenso wie ihr soziotechnologischer Gegenstand – rasant. Binnen kürzester Zeit entstehen weltweit hunderte Studien, die die individuellen und gesellschaftlichen Implikationen von generativer KI aus psychologischer, kommunikationswissenschaftlicher oder soziologischer Perspektive untersuchen. Gleichzeitig ist der Bedarf an evidenzbasierten Prognosen über mögliche Folgen von generativer KI groß, etwa im Kontext von Regulierungsvorhaben wie dem AI Act der EU. Wie jedoch lassen sich die rasant anwachsende Forschung kosten- und ressourceneffizient überblicken und Handlungsempfehlungen aus ihr ableiten?\nEine mögliche Antwort liegt in der KI selbst. In diesem Projektseminar wollen wir daher den Versuch wagen, die aktuelle Forschung zu gesellschaftlichen Folgen von generativer und kommunikativer KI mit Hilfe von KI-gestützten Systematic Review Methoden schnell und zuverlässig zu synthetisieren (siehe begleitendes Seminar Angewandte Methoden)."
  },
  {
    "objectID": "ms-schedule.html",
    "href": "ms-schedule.html",
    "title": "Semesterplan",
    "section": "",
    "text": "Note\n\n\n\nDiese Seite erhält eine Übersicht über die Sitzung bzw. Themen des Methodenseminars im Sommersemester 2024. Bitte beachten Sie, dass die Inhalte des Kurses ( Folien,  Exercises,  Showcases und  Hintergrundinformationen) im Laufe des Semesters stetig aktualisiert werden, wobei alle Änderungen hier dokumentiert werden.\n\n\n\n\n\n\n\n\n\n\n\nSitzung\nDatum\nThema\nUnterlagen\n\n\n\n\n\n\nIntroduction\n\n\n\n1\n18.04.2024\nEinführung & Überblick\n\n\n\n\n📚\nBlock I: Systematic Review\n\n\n\n2\n25.04.2024\nEinführung in Systematic Reviews I\n\n\n\n3\n02.05.2024\nEinführung in Systematic Reviews II\n\n\n\n\n09.05.2024\n🏖️ Feiertag\n\n\n\n4\n16.05.2024\nAutomatisierung von SRs & KI-Tools I\n\n\n\n\n23.05.2024\n🍻 WiSo-Projekt-Woche\n\n\n\n\n30.05.2024\n🏖️ Feiertag\n\n\n\n5\n04.06.2024\n🍕 Gastvortrag: Prof. Dr. Emese Domahidi\n\n\n\n6\n06.06.2024\nAutomatisierung von SRs & KI-Tools\n\n\n\n\n💻\nBlock II: Text as Data\n\n\n\n7\n13.06.2024\nIntroduction to Text as Data\n |  |  | \n\n\n8\n20.06.2024\nText processing in R\n |  |  | \n\n\n9\n27.06.2024\nUnsupervised Machine Learning I\n |  |  | \n\n\n10\n04.07.2024\nUnsupervised Machine Learning II\n |  |  | \n\n\n11\n11.07.2024\nRecap & Ausblick\n\n\n\n12\n18.07.2024\n🏁 Semesterabschluss",
    "crumbs": [
      "Kursunterlagen",
      "Methodenseminar"
    ]
  },
  {
    "objectID": "course-team.html",
    "href": "course-team.html",
    "title": "Teaching team",
    "section": "",
    "text": "is Tenure-Track Assistant Professor for Communication Science at Friedrich-Alexander-Universität (FAU) Erlangen-Nürnberg.\nHis work investigates the positive and negative consequences of digital communication (e.g., via smartphones and social media) for users’ well-being, health, and self-regulation. His current research focusses on users’ digital well-being at the interface of work and leisure, for instance, digital stress, availability pressures, procrastinatory media use, and digital communication when working from home.",
    "crumbs": [
      "Hintergrundinformationen",
      "zum Teaching Team"
    ]
  },
  {
    "objectID": "course-team.html#prof.-dr.-adrian-meier",
    "href": "course-team.html#prof.-dr.-adrian-meier",
    "title": "Teaching team",
    "section": "",
    "text": "is Tenure-Track Assistant Professor for Communication Science at Friedrich-Alexander-Universität (FAU) Erlangen-Nürnberg.\nHis work investigates the positive and negative consequences of digital communication (e.g., via smartphones and social media) for users’ well-being, health, and self-regulation. His current research focusses on users’ digital well-being at the interface of work and leisure, for instance, digital stress, availability pressures, procrastinatory media use, and digital communication when working from home.",
    "crumbs": [
      "Hintergrundinformationen",
      "zum Teaching Team"
    ]
  },
  {
    "objectID": "course-team.html#christoph-adrian-hehim",
    "href": "course-team.html#christoph-adrian-hehim",
    "title": "Teaching team",
    "section": "Christoph Adrian (he/him)",
    "text": "Christoph Adrian (he/him)\n\n   \nis a Research Assistant at the Chair of Communication Science at Friedrich-Alexander-Universität (FAU) Erlangen-Nürnberg.\nHis work focuses on computational methods, especially Text as Data Approaches and workin with Digital behavioral data, with an emphasis on computing, reproducible research, student-centered learning, and open-source education.",
    "crumbs": [
      "Hintergrundinformationen",
      "zum Teaching Team"
    ]
  },
  {
    "objectID": "slides/ms-slides-10.html#seminarplan",
    "href": "slides/ms-slides-10.html#seminarplan",
    "title": "Unsupervised Machine Learning (II)",
    "section": "Seminarplan",
    "text": "Seminarplan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSitzung\nDatum\nThema (synchron)\nÜbung (asynchron)\nDozent:in\n\n\n\n\n1\n18.04.2024\nEinführung & Überblick\n\nAM & CA\n\n\n\n📚\nTeil 1: Systematic Review\n\n\n\n\n2\n25.04.2024\nEinführung in Systematic Reviews I\nR-Einführung\nAM\n\n\n3\n02.05.2024\nEinführung in Systematic Reviews II\nR-Einführung\nAM\n\n\n\n09.05.2024\n🏖️ Feiertag\nR-Einführung\n\n\n\n4\n16.05.2024\nAutomatisierung von SRs & KI-Tools\nR-Einführung\nAM\n\n\n\n23.05.2024\n🍻 WiSo-Projekt-Woche\nR-Einführung\n\n\n\n5\n04.06.2024\n🍕 Gastvortrag: Prof. Dr. Emese Domahidi\nR-Einführung\nED\n\n\n6\n06.06.2024\nAutomatisierung von SRs & KI-Tools\nR-Einführung\nAM\n\n\n\n💻\nTeil 2: Text as Data & Unsupervised Machine Learning\n\n\n\n\n7\n13.06.2024\nIntroduction to Text as Data\nzur Sitzung\nCA\n\n\n8\n20.06.2024\nText processing\nzur Sitzung\nCA\n\n\n9\n27.06.2024\nUnsupervised Machine Learning I\nzur Sitzung\nCA\n\n\n10\n04.07.2024\nUnsupervised Machine Learning II\nzur Sitzung\nCA & AM\n\n\n11\n11.07.2024\nRecap & Ausblick\nzur Sitzung\nCA & AM\n\n\n12\n18.07.2024\n🏁 Semesterabschluss\nzur Sitzung\nCA & AM"
  },
  {
    "objectID": "slides/ms-slides-10.html#was-bisher-geschaht",
    "href": "slides/ms-slides-10.html#was-bisher-geschaht",
    "title": "Unsupervised Machine Learning (II)",
    "section": "Was bisher geschaht …",
    "text": "Was bisher geschaht …\nKurze Wiederholung der wichtigsten Inhalte\nSie sollten in der Lage sein, die folgenden Fragen zu beantworten:\n\nWas verstehen wir unter Topic Modeling?\nWofür wird Topic Modeling eingesetzt?\nWelche Schritte sind notwendig, um Topic Modeling in R umzusetzen?\n\nHeutige Fokus liegt auf Detailfragen:\n\nWie kann ich mein Themenmodell validieren?\nWie finde ich die optimale Anzahl von Themen?"
  },
  {
    "objectID": "slides/ms-slides-10.html#die-bisherige-transformation-der-daten",
    "href": "slides/ms-slides-10.html#die-bisherige-transformation-der-daten",
    "title": "Unsupervised Machine Learning (II)",
    "section": "Die (bisherige) Transformation der Daten",
    "text": "Die (bisherige) Transformation der Daten\nVon der Subsample bis zum (neuen) Modell: Daten\n\n\n\n\n# Create subsample\nreview_subsample &lt;- review_works %&gt;% \n    # Create additional factor variables\n    mutate(\n        publication_year_fct = as.factor(publication_year), \n        type_fct = as.factor(type)\n        ) %&gt;%\n    # Eingrenzung: Sprache und Typ\n    filter(language == \"en\") %&gt;% \n    filter(type == \"article\") %&gt;%\n    # Datentranformation\n    unnest(topics, names_sep = \"_\") %&gt;%\n    filter(topics_name == \"field\") %&gt;% \n    filter(topics_i == \"1\") %&gt;% \n    # Eingrenzung: Forschungsfeldes\n    filter(\n    topics_display_name == \"Social Sciences\"|\n    topics_display_name == \"Psychology\"\n    ) %&gt;% \n    mutate(\n        field = as.factor(topics_display_name)\n    ) %&gt;% \n    # Eingrenzung: Keine Einträge ohne Abstract\n    filter(!is.na(ab))\n\n\n\n# Overview\nreview_subsample %&gt;% glimpse  \n\nRows: 36,680\nColumns: 46\n$ id                          &lt;chr&gt; \"https://openalex.org/W4293003987\", \"https…\n$ title                       &lt;chr&gt; \"The WHO-5 Well-Being Index: A Systematic …\n$ display_name                &lt;chr&gt; \"The WHO-5 Well-Being Index: A Systematic …\n$ author                      &lt;list&gt; [&lt;data.frame[4 x 12]&gt;], [&lt;data.frame[2 x …\n$ ab                          &lt;chr&gt; \"The 5-item World Health Organization Well…\n$ publication_date            &lt;chr&gt; \"2015-01-01\", \"2017-08-28\", \"2014-01-01\", …\n$ relevance_score             &lt;dbl&gt; 938.7603, 752.3500, 591.2553, 576.1210, 56…\n$ so                          &lt;chr&gt; \"Psychotherapy and psychosomatics\", \"Journ…\n$ so_id                       &lt;chr&gt; \"https://openalex.org/S184803288\", \"https:…\n$ host_organization           &lt;chr&gt; \"Karger Publishers\", \"SAGE Publishing\", NA…\n$ issn_l                      &lt;chr&gt; \"0033-3190\", \"0739-456X\", NA, \"2214-7829\",…\n$ url                         &lt;chr&gt; \"https://doi.org/10.1159/000376585\", \"http…\n$ pdf_url                     &lt;chr&gt; \"https://www.karger.com/Article/Pdf/376585…\n$ license                     &lt;chr&gt; \"cc-by-nc\", NA, NA, \"cc-by\", NA, NA, \"cc-b…\n$ version                     &lt;chr&gt; \"publishedVersion\", NA, \"publishedVersion\"…\n$ first_page                  &lt;chr&gt; \"167\", \"93\", NA, \"89\", \"55\", \"2150\", \"e356…\n$ last_page                   &lt;chr&gt; \"176\", \"112\", NA, \"106\", \"64\", \"2159\", \"e3…\n$ volume                      &lt;chr&gt; \"84\", \"39\", NA, \"6\", \"277\", \"32\", \"2\", \"24…\n$ issue                       &lt;chr&gt; \"3\", \"1\", NA, NA, NA, \"19\", \"8\", NA, \"9\", …\n$ is_oa                       &lt;lgl&gt; TRUE, FALSE, TRUE, TRUE, TRUE, FALSE, TRUE…\n$ is_oa_anywhere              &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE,…\n$ oa_status                   &lt;chr&gt; \"hybrid\", \"green\", \"bronze\", \"gold\", \"bron…\n$ oa_url                      &lt;chr&gt; \"https://www.karger.com/Article/Pdf/376585…\n$ any_repository_has_fulltext &lt;lgl&gt; FALSE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE…\n$ language                    &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", …\n$ grants                      &lt;list&gt; NA, NA, NA, &lt;\"https://openalex.org/F43203…\n$ cited_by_count              &lt;int&gt; 2657, 1375, 2568, 803, 3664, 1553, 2895, 9…\n$ counts_by_year              &lt;list&gt; [&lt;data.frame[11 x 2]&gt;], [&lt;data.frame[7 x …\n$ publication_year            &lt;int&gt; 2015, 2017, 2014, 2016, 2020, 2014, 2017, …\n$ cited_by_api_url            &lt;chr&gt; \"https://api.openalex.org/works?filter=cit…\n$ ids                         &lt;list&gt; &lt;\"https://openalex.org/W4293003987\", \"htt…\n$ doi                         &lt;chr&gt; \"https://doi.org/10.1159/000376585\", \"http…\n$ type                        &lt;chr&gt; \"article\", \"article\", \"article\", \"article\"…\n$ referenced_works            &lt;list&gt; &lt;\"https://openalex.org/W1492518593\", \"htt…\n$ related_works               &lt;list&gt; &lt;\"https://openalex.org/W3020194755\", \"htt…\n$ is_paratext                 &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ is_retracted                &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ concepts                    &lt;list&gt; [&lt;data.frame[7 x 5]&gt;], [&lt;data.frame[18 x …\n$ topics_i                    &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ topics_score                &lt;dbl&gt; 0.9926, 0.9050, 0.9995, 0.9987, 0.9999, 1.…\n$ topics_name                 &lt;chr&gt; \"field\", \"field\", \"field\", \"field\", \"field…\n$ topics_id                   &lt;chr&gt; \"https://openalex.org/fields/32\", \"https:/…\n$ topics_display_name         &lt;chr&gt; \"Psychology\", \"Social Sciences\", \"Psycholo…\n$ publication_year_fct        &lt;fct&gt; 2015, 2017, 2014, 2016, 2020, 2014, 2017, …\n$ type_fct                    &lt;fct&gt; article, article, article, article, articl…\n$ field                       &lt;fct&gt; Psychology, Social Sciences, Psychology, P…"
  },
  {
    "objectID": "slides/ms-slides-10.html#die-bisherige-transformation-der-daten-1",
    "href": "slides/ms-slides-10.html#die-bisherige-transformation-der-daten-1",
    "title": "Unsupervised Machine Learning (II)",
    "section": "Die (bisherige) Transformation der Daten",
    "text": "Die (bisherige) Transformation der Daten\nVon der Subsample bis zum (neuen) Modell: Document-Term-Matrix\n\n\n\n\n# Create corpus\nquanteda_corpus &lt;- review_subsample %&gt;% \n  quanteda::corpus(\n    docid_field = \"id\", \n    text_field = \"ab\"\n  )\n\n# Tokenize\nquanteda_token &lt;- quanteda_corpus %&gt;% \n  quanteda::tokens(\n    remove_punct = TRUE,\n    remove_symbols = TRUE, \n    remove_numbers = TRUE, \n    remove_url = TRUE, \n    split_tags = FALSE # keep hashtags and mentions\n  ) %&gt;% \n  quanteda::tokens_tolower() %&gt;% \n  quanteda::tokens_remove(\n    pattern = stopwords(\"en\")\n    )\n\n# Convert to Document-Feature-Matrix (DFM)\nquanteda_dfm &lt;- quanteda_token %&gt;% \n  quanteda::dfm()\n\n# Pruning\nquanteda_dfm_trim &lt;- quanteda_dfm %&gt;% \n  dfm_trim( \n    min_docfreq = 10/nrow(review_subsample),\n    max_docfreq = 0.99, \n    docfreq_type = \"prop\")\n\n# Convert for stm topic modeling\nquanteda_stm &lt;- quanteda_dfm_trim %&gt;% \n   convert(to = \"stm\")\n\n\n\n# Overview\nquanteda_stm %&gt;% summary  \n\n          Length Class      Mode     \ndocuments 36650  -none-     list     \nvocab     14322  -none-     character\nmeta         45  data.frame list"
  },
  {
    "objectID": "slides/ms-slides-10.html#die-bisherige-transformation-der-daten-2",
    "href": "slides/ms-slides-10.html#die-bisherige-transformation-der-daten-2",
    "title": "Unsupervised Machine Learning (II)",
    "section": "Die (bisherige) Transformation der Daten",
    "text": "Die (bisherige) Transformation der Daten\nVon der Subsample bis zum (neuen) Modell: STM\n\n\n\n\n# Estimate model\nstm_mdl_k20 &lt;- stm::stm(\n    documents = quanteda_stm$documents,\n    vocab = quanteda_stm$vocab, \n    prevalence =~ publication_year_fct + field, \n    K = 20, \n    seed = 42,\n    max.em.its = 1000,\n    data = quanteda_stm$meta,\n    init.type = \"Spectral\",\n    verbose = TRUE)\n\n\n\n# Overview\nstm_mdl_k20\n\nA topic model with 20 topics, 36650 documents and a 14322 word dictionary."
  },
  {
    "objectID": "slides/ms-slides-10.html#breaking-down-the-model",
    "href": "slides/ms-slides-10.html#breaking-down-the-model",
    "title": "Unsupervised Machine Learning (II)",
    "section": "Breaking down the model",
    "text": "Breaking down the model\nErweiterte Modellauswertung: Beta-Matrix\n\n# Create tidy beta matrix\ntd_beta &lt;- tidytext::tidy(stm_mdl_k20)\n\n# Output \ntd_beta\n\n# A tibble: 286,440 × 3\n   topic term      beta\n   &lt;int&gt; &lt;chr&gt;    &lt;dbl&gt;\n 1     1 #x0d  6.83e-44\n 2     2 #x0d  2.37e-30\n 3     3 #x0d  3.35e-45\n 4     4 #x0d  7.24e- 3\n 5     5 #x0d  1.45e-23\n 6     6 #x0d  5.23e-20\n 7     7 #x0d  2.70e-12\n 8     8 #x0d  3.32e-39\n 9     9 #x0d  8.78e- 4\n10    10 #x0d  2.58e-12\n# ℹ 286,430 more rows"
  },
  {
    "objectID": "slides/ms-slides-10.html#top-begriffe-nach-thema",
    "href": "slides/ms-slides-10.html#top-begriffe-nach-thema",
    "title": "Unsupervised Machine Learning (II)",
    "section": "Top Begriffe nach Thema",
    "text": "Top Begriffe nach Thema\nErweiterte Modellauswertung: Beta-Matrix\n\n\n# Create top terms\ntop_terms &lt;- td_beta %&gt;%\n  arrange(beta) %&gt;%\n  group_by(topic) %&gt;%\n  top_n(7, beta) %&gt;%\n  arrange(-beta) %&gt;%\n  select(topic, term) %&gt;%\n  summarise(terms = list(term)) %&gt;%\n  mutate(terms = map(terms, paste, collapse = \", \")) %&gt;% \n  unnest(cols = c(terms))\n\n# Output\ntop_terms %&gt;% \n  head(15)\n\n# A tibble: 15 × 2\n   topic terms                                                                 \n   &lt;int&gt; &lt;chr&gt;                                                                 \n 1     1 sleep, studies, eating, cognitive, weight, associated, duration       \n 2     2 health, women, gender, cultural, barriers, men, countries             \n 3     3 treatment, disorders, symptoms, disorder, depression, anxiety, therapy\n 4     4 patients, cancer, nurses, patient, music, nursing, pain               \n 5     5 literature, university, author, gt, lt, et, al                        \n 6     6 prevalence, covid-19, suicide, pandemic, studies, among, risk         \n 7     7 articles, review, study, systematic, results, search, science         \n 8     8 physical, activity, disabilities, exercise, cognitive, adults, body   \n 9     9 learning, education, students, educational, skills, teachers, teaching\n10    10 de, la, y, en, los, 的, el                                            \n11    11 work, study, development, can, public, management, factors            \n12    12 violence, sexual, use, abuse, risk, youth, h3                         \n13    13 health, mental, care, support, people, social, family                 \n14    14 social, use, media, digital, information, technology, communication   \n15    15 ci, effect, meta-analysis, p, studies, effects, trials"
  },
  {
    "objectID": "slides/ms-slides-10.html#break-down-the-modell-a-little-more",
    "href": "slides/ms-slides-10.html#break-down-the-modell-a-little-more",
    "title": "Unsupervised Machine Learning (II)",
    "section": "Break down the modell (a little more)",
    "text": "Break down the modell (a little more)\nErweiterte Modellauswertung: Gamma-Matrix\n\n# Create tidy beta matrix\ntd_gamma &lt;- tidy(\n  stm_mdl_k20, \n  matrix = \"gamma\", \n  document_names = names(quanteda_stm$documents)\n  )\n\n# Output \ntd_gamma\n\n# A tibble: 733,000 × 3\n   document                         topic    gamma\n   &lt;chr&gt;                            &lt;int&gt;    &lt;dbl&gt;\n 1 https://openalex.org/W4293003987     1 0.00473 \n 2 https://openalex.org/W2750168540     1 0.000270\n 3 https://openalex.org/W1998933811     1 0.00368 \n 4 https://openalex.org/W2547134104     1 0.00340 \n 5 https://openalex.org/W3047898105     1 0.00263 \n 6 https://openalex.org/W2149640470     1 0.00230 \n 7 https://openalex.org/W2740726397     1 0.0374  \n 8 https://openalex.org/W2974087526     1 0.00339 \n 9 https://openalex.org/W2195703978     1 0.00287 \n10 https://openalex.org/W2093916237     1 0.170   \n# ℹ 732,990 more rows"
  },
  {
    "objectID": "slides/ms-slides-10.html#häufigkeit-und-top-begriffe-der-themen",
    "href": "slides/ms-slides-10.html#häufigkeit-und-top-begriffe-der-themen",
    "title": "Unsupervised Machine Learning (II)",
    "section": "Häufigkeit und Top Begriffe der Themen",
    "text": "Häufigkeit und Top Begriffe der Themen\nErweiterte Modellauswertung: Gamma-Matrix\n\n\nExpand for full code\n# Create data\nprevalence &lt;- td_gamma %&gt;%\n  group_by(topic) %&gt;%\n  summarise(gamma = mean(gamma)) %&gt;%\n  arrange(desc(gamma)) %&gt;%\n  left_join(top_terms, by = \"topic\") %&gt;%\n  mutate(topic = paste0(\"Topic \",sprintf(\"%02d\", topic)),\n         topic = reorder(topic, gamma))\n\n# Create output\nprevalence %&gt;% \n  gt() %&gt;% \n  gt::tab_options(\n      table.width = gt::pct(80), \n      table.font.size = \"10px\", \n      data_row.padding = gt::px(1)\n  ) %&gt;% \n  fmt_number(\n    columns = c(gamma), \n    decimals = 2) %&gt;% \n  gtExtras::gt_theme_538()\n\n\n\n\n\n\n\n\ntopic\ngamma\nterms\n\n\n\n\nTopic 16\n0.12\nresearch, literature, review, future, findings, systematic, paper\n\n\nTopic 19\n0.09\nstudies, interventions, included, evidence, review, systematic, outcomes\n\n\nTopic 07\n0.08\narticles, review, study, systematic, results, search, science\n\n\nTopic 11\n0.07\nwork, study, development, can, public, management, factors\n\n\nTopic 09\n0.06\nlearning, education, students, educational, skills, teachers, teaching\n\n\nTopic 17\n0.06\nchildren, studies, factors, relationship, adolescents, review, associated\n\n\nTopic 13\n0.06\nhealth, mental, care, support, people, social, family\n\n\nTopic 14\n0.05\nsocial, use, media, digital, information, technology, communication\n\n\nTopic 15\n0.05\nci, effect, meta-analysis, p, studies, effects, trials\n\n\nTopic 03\n0.04\ntreatment, disorders, symptoms, disorder, depression, anxiety, therapy\n\n\nTopic 06\n0.04\nprevalence, covid-19, suicide, pandemic, studies, among, risk\n\n\nTopic 18\n0.04\nmeasures, assessment, used, studies, tools, measurement, quality\n\n\nTopic 20\n0.04\nprograms, school, training, interventions, outcomes, review, intervention\n\n\nTopic 02\n0.04\nhealth, women, gender, cultural, barriers, men, countries\n\n\nTopic 01\n0.03\nsleep, studies, eating, cognitive, weight, associated, duration\n\n\nTopic 05\n0.03\nliterature, university, author, gt, lt, et, al\n\n\nTopic 12\n0.03\nviolence, sexual, use, abuse, risk, youth, h3\n\n\nTopic 04\n0.03\npatients, cancer, nurses, patient, music, nursing, pain\n\n\nTopic 08\n0.03\nphysical, activity, disabilities, exercise, cognitive, adults, body\n\n\nTopic 10\n0.00\nde, la, y, en, los, 的, el"
  },
  {
    "objectID": "slides/ms-slides-10.html#visualisierung-des-stm-modells",
    "href": "slides/ms-slides-10.html#visualisierung-des-stm-modells",
    "title": "Unsupervised Machine Learning (II)",
    "section": "Visualisierung des STM-Modells",
    "text": "Visualisierung des STM-Modells\nKombination von Beta- und Gamma-Matrix\n\n\nExpand for full code\nprevalence %&gt;%\n  ggplot(aes(topic, gamma, label = terms, fill = topic)) +\n  geom_col(show.legend = FALSE) +\n  geom_text(hjust = 0, nudge_y = 0.0005, size = 3) +\n  coord_flip() +\n  scale_y_continuous(\n    expand = c(0,0),\n    limits = c(0, 0.2)) +\n  theme_pubr() +\n  theme(\n    plot.title = element_text(size = 16),\n    plot.subtitle = element_text(size = 13)) +\n  labs(\n    x = NULL, y = expression(gamma),\n    title = \"Topic Prevalence in the OpenAlex Corpus\",\n    subtitle = \"With the top seven words that contribute to each topic\")"
  },
  {
    "objectID": "slides/ms-slides-10.html#einfluss-von-publikationsjahr-und-forschungsfeld",
    "href": "slides/ms-slides-10.html#einfluss-von-publikationsjahr-und-forschungsfeld",
    "title": "Unsupervised Machine Learning (II)",
    "section": "Einfluss von Publikationsjahr und Forschungsfeld",
    "text": "Einfluss von Publikationsjahr und Forschungsfeld\nBerücksichtigung der Meta-Daten (Kovariaten)\n\n\n# Create estimation\neffects &lt;- estimateEffect(\n  1:20 ~ publication_year_fct + field, \n  stm_mdl_k20, \n  meta = quanteda_stm$meta)\n\n\n\n\n\n# Effects of covariates on Topic 16\neffects %&gt;% summary(topics = 16)\n\n\nCall:\nestimateEffect(formula = 1:20 ~ publication_year_fct + field, \n    stmobj = stm_mdl_k20, metadata = quanteda_stm$meta)\n\n\nTopic 16:\n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               0.063809   0.005169  12.345   &lt;2e-16 ***\npublication_year_fct2014  0.006100   0.006937   0.879   0.3792    \npublication_year_fct2015  0.013449   0.006732   1.998   0.0457 *  \npublication_year_fct2016  0.004191   0.006209   0.675   0.4998    \npublication_year_fct2017  0.007418   0.006225   1.192   0.2334    \npublication_year_fct2018  0.001410   0.005955   0.237   0.8129    \npublication_year_fct2019 -0.001950   0.006294  -0.310   0.7567    \npublication_year_fct2020  0.002573   0.005991   0.429   0.6676    \npublication_year_fct2021  0.002243   0.005543   0.405   0.6857    \npublication_year_fct2022 -0.001228   0.005427  -0.226   0.8209    \npublication_year_fct2023  0.008276   0.005533   1.496   0.1347    \nfieldSocial Sciences      0.086363   0.001806  47.824   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n# Effects of covariates on Topic 6\neffects %&gt;% summary(topics = 6) \n\n\nCall:\nestimateEffect(formula = 1:20 ~ publication_year_fct + field, \n    stmobj = stm_mdl_k20, metadata = quanteda_stm$meta)\n\n\nTopic 6:\n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               0.040449   0.004350   9.298  &lt; 2e-16 ***\npublication_year_fct2014 -0.002881   0.005654  -0.509   0.6104    \npublication_year_fct2015 -0.002268   0.005477  -0.414   0.6788    \npublication_year_fct2016 -0.003846   0.005160  -0.745   0.4562    \npublication_year_fct2017 -0.003835   0.005283  -0.726   0.4679    \npublication_year_fct2018 -0.005119   0.005154  -0.993   0.3206    \npublication_year_fct2019 -0.001962   0.004981  -0.394   0.6937    \npublication_year_fct2020  0.005633   0.004745   1.187   0.2351    \npublication_year_fct2021  0.021630   0.005019   4.309 1.64e-05 ***\npublication_year_fct2022  0.025143   0.004739   5.306 1.13e-07 ***\npublication_year_fct2023  0.012092   0.004771   2.534   0.0113 *  \nfieldSocial Sciences     -0.010577   0.001638  -6.458 1.07e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "slides/ms-slides-10.html#forschungsfeld-im-fokus",
    "href": "slides/ms-slides-10.html#forschungsfeld-im-fokus",
    "title": "Unsupervised Machine Learning (II)",
    "section": "Forschungsfeld im Fokus",
    "text": "Forschungsfeld im Fokus\nEinfluss des Forschungsfeldes auf Themenaufkommen\n\n\nExpand for full code\neffects %&gt;%\n  tidy() %&gt;% \n  filter(\n    term != \"(Intercept)\",\n    term == \"fieldSocial Sciences\") %&gt;% \n    select(-term) %&gt;% \n  gt() %&gt;% \n    fmt_number(\n      columns = -c(topic),\n      decimals = 3\n    ) %&gt;% \n  # Color social science topics \"blue\"\n  data_color(\n    columns = topic,\n    rows = estimate &gt; 0,\n    method = \"numeric\",\n    palette = c(\"#04316A\"),\n    alpha = 0.4\n  ) %&gt;% \n  # Color psychology topics \"yellow\"\n  data_color(\n    columns = topic,\n    rows = estimate &lt; 0,\n    method = \"numeric\",\n    palette = c(\"#D3A518\"),\n    alpha = 0.4\n  ) %&gt;% \n  # Color effect size for estimation\n  data_color(\n    columns = estimate,\n    method = \"numeric\",\n    palette = \"viridis\"\n  ) %&gt;% \n  # Color insignificant p-values\n  data_color(\n    rows = p.value &gt; 0.05,\n    method = \"numeric\",\n    palette = c(\"#C50F3C\")\n  ) %&gt;%\n  gtExtras::gt_theme_538() \n\n\n\n\n\n\n\n\ntopic\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n1\n−0.056\n0.001\n−40.952\n0.000\n\n\n2\n0.014\n0.001\n11.002\n0.000\n\n\n3\n−0.069\n0.001\n−47.812\n0.000\n\n\n4\n−0.014\n0.001\n−11.866\n0.000\n\n\n5\n0.027\n0.001\n18.350\n0.000\n\n\n6\n−0.011\n0.002\n−6.435\n0.000\n\n\n7\n0.014\n0.001\n10.788\n0.000\n\n\n8\n−0.015\n0.001\n−13.079\n0.000\n\n\n9\n0.052\n0.002\n29.180\n0.000\n\n\n10\n−0.001\n0.001\n−2.637\n0.008\n\n\n11\n0.091\n0.002\n53.529\n0.000\n\n\n12\n0.003\n0.001\n1.852\n0.064\n\n\n13\n−0.024\n0.001\n−18.915\n0.000\n\n\n14\n0.044\n0.001\n31.451\n0.000\n\n\n15\n−0.059\n0.002\n−37.655\n0.000\n\n\n16\n0.086\n0.002\n47.566\n0.000\n\n\n17\n−0.033\n0.001\n−23.428\n0.000\n\n\n18\n−0.012\n0.001\n−11.901\n0.000\n\n\n19\n−0.044\n0.001\n−31.870\n0.000\n\n\n20\n0.008\n0.001\n7.385\n0.000"
  },
  {
    "objectID": "slides/ms-slides-10.html#zusammenführung-der-daten",
    "href": "slides/ms-slides-10.html#zusammenführung-der-daten",
    "title": "Unsupervised Machine Learning (II)",
    "section": "Zusammenführung der Daten",
    "text": "Zusammenführung der Daten\nMatch Topic Modeling Ergebnisse mit OpenAlex-Daten\n\n\n\nExpand for full code\ngamma_export &lt;- stm_mdl_k20 %&gt;% \n  tidytext::tidy(\n    matrix = \"gamma\", \n    document_names = names(quanteda_stm$documents)) %&gt;%\n  dplyr::group_by(document) %&gt;% \n  dplyr::slice_max(gamma) %&gt;% \n  dplyr::mutate(main_topic = ifelse(gamma &gt; 0.5, topic, NA)) %&gt;% \n      rename(\n        top_topic = topic,\n        top_gamma = gamma) %&gt;% \n  dplyr::ungroup() %&gt;% \n  dplyr::left_join(review_subsample, by = c(\"document\" = \"id\")) %&gt;% \n  dplyr::rename(id = document) %&gt;% \n  dplyr::mutate(\n    stm_topic = as.factor(paste(\"Topic\", sprintf(\"%02d\", top_topic)))\n  )\n\n# Output\ngamma_export %&gt;% glimpse()\n\n\nRows: 36,650\nColumns: 50\n$ id                          &lt;chr&gt; \"https://openalex.org/W1000529773\", \"https…\n$ top_topic                   &lt;int&gt; 9, 16, 14, 14, 4, 16, 3, 5, 6, 17, 17, 12,…\n$ top_gamma                   &lt;dbl&gt; 0.6641380, 0.2635474, 0.4931637, 0.3960384…\n$ main_topic                  &lt;int&gt; 9, NA, NA, NA, NA, 16, 3, 5, NA, NA, NA, N…\n$ title                       &lt;chr&gt; \"A critical evaluation of the teaching of …\n$ display_name                &lt;chr&gt; \"A critical evaluation of the teaching of …\n$ author                      &lt;list&gt; [&lt;data.frame[1 x 12]&gt;], [&lt;data.frame[2 x …\n$ ab                          &lt;chr&gt; \"A Critical Evaluation of the Teaching of …\n$ publication_date            &lt;chr&gt; \"2014-01-16\", \"2015-05-26\", \"2014-05-22\", …\n$ relevance_score             &lt;dbl&gt; 4.012791, 27.896568, 32.074608, 23.670475,…\n$ so                          &lt;chr&gt; NA, \"Proceedings of the annual conference …\n$ so_id                       &lt;chr&gt; NA, \"https://openalex.org/S4306523984\", \"h…\n$ host_organization           &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, \"Taylor & …\n$ issn_l                      &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, \"1381-1118…\n$ url                         &lt;chr&gt; \"https://uwispace.sta.uwi.edu/dspace/bitst…\n$ pdf_url                     &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"h…\n$ license                     &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"c…\n$ version                     &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"p…\n$ first_page                  &lt;chr&gt; NA, \"76\", NA, NA, NA, NA, NA, NA, \"1\", \"11…\n$ last_page                   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, \"21\", \"122…\n$ volume                      &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, \"20\", \"78\"…\n$ issue                       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, \"1\", NA, \"…\n$ is_oa                       &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ is_oa_anywhere              &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ oa_status                   &lt;chr&gt; \"closed\", \"closed\", \"closed\", \"closed\", \"c…\n$ oa_url                      &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, \"https://e…\n$ any_repository_has_fulltext &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ language                    &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", …\n$ grants                      &lt;list&gt; NA, NA, NA, NA, NA, NA, NA, NA, &lt;\"https:/…\n$ cited_by_count              &lt;int&gt; 0, 1, 1, 1, 1, 0, 2, 0, 226, 159, 122, 31,…\n$ counts_by_year              &lt;list&gt; NA, [&lt;data.frame[1 x 2]&gt;], [&lt;data.frame[1…\n$ publication_year            &lt;int&gt; 2014, 2015, 2014, 2013, 2013, 2015, 2015, …\n$ cited_by_api_url            &lt;chr&gt; \"https://api.openalex.org/works?filter=cit…\n$ ids                         &lt;list&gt; &lt;\"https://openalex.org/W1000529773\", \"100…\n$ doi                         &lt;chr&gt; NA, \"https://doi.org/10.5555/2814058.28141…\n$ type                        &lt;chr&gt; \"article\", \"article\", \"article\", \"article\"…\n$ referenced_works            &lt;list&gt; NA, &lt;\"https://openalex.org/W1526029332\", …\n$ related_works               &lt;list&gt; &lt;\"https://openalex.org/W958254955\", \"http…\n$ is_paratext                 &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ is_retracted                &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ concepts                    &lt;list&gt; [&lt;data.frame[7 x 5]&gt;], [&lt;data.frame[16 x …\n$ topics_i                    &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ topics_score                &lt;dbl&gt; 0.9566, 0.9812, 0.9894, 0.9999, 0.9752, 0.…\n$ topics_name                 &lt;chr&gt; \"field\", \"field\", \"field\", \"field\", \"field…\n$ topics_id                   &lt;chr&gt; \"https://openalex.org/fields/32\", \"https:/…\n$ topics_display_name         &lt;chr&gt; \"Psychology\", \"Social Sciences\", \"Psycholo…\n$ publication_year_fct        &lt;fct&gt; 2014, 2015, 2014, 2013, 2013, 2015, 2015, …\n$ type_fct                    &lt;fct&gt; article, article, article, article, articl…\n$ field                       &lt;fct&gt; Psychology, Social Sciences, Psychology, S…\n$ stm_topic                   &lt;fct&gt; Topic 09, Topic 16, Topic 14, Topic 14, To…"
  },
  {
    "objectID": "slides/ms-slides-10.html#themenhäufig-abstracthäufigkeit",
    "href": "slides/ms-slides-10.html#themenhäufig-abstracthäufigkeit",
    "title": "Unsupervised Machine Learning (II)",
    "section": "Themenhäufig ≠ Abstracthäufigkeit",
    "text": "Themenhäufig ≠ Abstracthäufigkeit\nÜberblick über Anzahl der Abstracts nach Thema\n\ngamma_export %&gt;% \n  ggplot(aes(x = fct_rev(fct_infreq(stm_topic)))) +\n  geom_bar() +\n  coord_flip() +\n  theme_pubr()"
  },
  {
    "objectID": "slides/ms-slides-10.html#verschiedene-schwerpunkte-in-verschiedenen-feldern",
    "href": "slides/ms-slides-10.html#verschiedene-schwerpunkte-in-verschiedenen-feldern",
    "title": "Unsupervised Machine Learning (II)",
    "section": "Verschiedene Schwerpunkte in verschiedenen Feldern",
    "text": "Verschiedene Schwerpunkte in verschiedenen Feldern\nÜberblick über die Anzahl der Abstracts nach Thema und Feld\n\ngamma_export %&gt;% \n  gtsummary::tbl_cross(\n    row = stm_topic, \n    col = field,\n    percent = \"row\",\n    )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfield\nTotal\n\n\nPsychology\nSocial Sciences\n\n\n\n\nstm_topic\n\n\n\n\n\n\n\n\n    Topic 01\n1,470 (94%)\n87 (5.6%)\n1,557 (100%)\n\n\n    Topic 02\n649 (50%)\n652 (50%)\n1,301 (100%)\n\n\n    Topic 03\n1,481 (95%)\n71 (4.6%)\n1,552 (100%)\n\n\n    Topic 04\n459 (60%)\n306 (40%)\n765 (100%)\n\n\n    Topic 05\n301 (29%)\n734 (71%)\n1,035 (100%)\n\n\n    Topic 06\n1,116 (60%)\n745 (40%)\n1,861 (100%)\n\n\n    Topic 07\n825 (44%)\n1,051 (56%)\n1,876 (100%)\n\n\n    Topic 08\n540 (70%)\n227 (30%)\n767 (100%)\n\n\n    Topic 09\n821 (27%)\n2,260 (73%)\n3,081 (100%)\n\n\n    Topic 10\n114 (67%)\n57 (33%)\n171 (100%)\n\n\n    Topic 11\n303 (12%)\n2,244 (88%)\n2,547 (100%)\n\n\n    Topic 12\n439 (41%)\n630 (59%)\n1,069 (100%)\n\n\n    Topic 13\n1,477 (67%)\n722 (33%)\n2,199 (100%)\n\n\n    Topic 14\n509 (28%)\n1,277 (72%)\n1,786 (100%)\n\n\n    Topic 15\n1,815 (86%)\n299 (14%)\n2,114 (100%)\n\n\n    Topic 16\n1,369 (27%)\n3,742 (73%)\n5,111 (100%)\n\n\n    Topic 17\n1,672 (75%)\n566 (25%)\n2,238 (100%)\n\n\n    Topic 18\n737 (65%)\n396 (35%)\n1,133 (100%)\n\n\n    Topic 19\n2,552 (70%)\n1,118 (30%)\n3,670 (100%)\n\n\n    Topic 20\n342 (42%)\n475 (58%)\n817 (100%)\n\n\nTotal\n18,991 (52%)\n17,659 (48%)\n36,650 (100%)"
  },
  {
    "objectID": "slides/ms-slides-10.html#a-closer-look",
    "href": "slides/ms-slides-10.html#a-closer-look",
    "title": "Unsupervised Machine Learning (II)",
    "section": "A closer look",
    "text": "A closer look\nFokus auf die Top-Abstracts von Thema 16\n\n\nExpand for full code\ngamma_export %&gt;% \n  filter(stm_topic == \"Topic 16\") %&gt;%\n  arrange(-top_gamma) %&gt;%\n  select(title, so, top_gamma, type, ab) %&gt;%\n  slice_head(n = 3) %&gt;% \n  gt() %&gt;% \n  fmt_number(\n    columns = vars(top_gamma), \n    decimals = 2) %&gt;%\n  gtExtras::gt_theme_538()\n\n\n\n\n\n\n\n\ntitle\nso\ntop_gamma\ntype\nab\n\n\n\n\nTheory of Knowledge for Literature Reviews: An Epistemological Model, Taxonomy and Empirical Analysis of IS Literature\nNA\n0.93\narticle\nLiterature reviews play an important role in the development of knowledge. Yet, we observe a lack of theoretical underpinning of and epistemological insights into how literature reviews can contribute to knowledge creation and have actually contributed in the IS discipline. To address these theoretical and empirical research gaps, we suggest a novel epistemological model of literature reviews. This model allows us to align different contributions of literature reviews with their underlying knowledge conversions - thereby building a bridge between the previously largely unconnected fields of literature reviews and epistemology. We evaluate the appropriateness of the model by conducting an empirical analysis of 173 IS literature reviews which were published in 39 pertinent IS journals between 2000 and 2014. Based on this analysis, we derive an epistemological taxonomy of IS literature reviews, which complements previously suggested typologies.\n\n\nTheory of Knowledge for Literature Reviews: An Epistemological Model, Taxonomy and Empirical Analysis of IS Literature Completed Research Paper\nNA\n0.93\narticle\nLiterature reviews play an important role in the development of knowledge. Yet, we observe a lack of theoretical underpinning of and epistemological insights into how literature reviews can contribute to knowledge creation and have actually contributed in the IS discipline. To address these theoretical and empirical research gaps, we suggest a novel epistemological model of literature reviews. This model allows us to align different contributions of literature reviews with their underlying knowledge conversions - thereby building a bridge between the previously largely unconnected fields of literature reviews and epistemology. We evaluate the appropriateness of the model by conducting an empirical analysis of 173 IS literature reviews which were published in 39 pertinent IS journals between 2000 and 2014. Based on this analysis, we derive an epistemological taxonomy of IS literature reviews, which complements previously suggested typologies.\n\n\nRelationality in negotiations: a systematic review and propositions for future research\n˜The œinternational journal of conflict management/International journal of conflict management\n0.93\narticle\nPurpose The purpose of this paper is to systematically review and analyze the important, yet under-researched, topic of relationality in negotiations and propose new directions for future negotiation research. Design/methodology/approach This paper conducts a systematic review of negotiation literature related to relationality from multiple disciplines. Thirty-nine leading and topical academic journals are selected and 574 papers on negotiation are reviewed from 1990 to 2014. Based on the systematic review, propositions regarding the rationales for relationality in negotiations are developed and future research avenues in this area are discussed. Findings Of 574 papers on negotiations published in 39 peer-reviewed journals between 1990 and 2014, only 18 papers have studied and discussed relationality in negotiations. This suggests that relationality as a theoretical theme has long been under-researched in negotiation research. For future research, this paper proposes to incorporate the dynamic, cultural and mechanism perspectives, and to use a qualitative approach to study relationality in negotiations. Originality/value This paper presents the first systematic review of the negotiation literature on relationality, and identifies new research topics on relationality in negotiations. In so doing, this research opens new avenues for future negotiation research on relationality."
  },
  {
    "objectID": "slides/ms-slides-10.html#validieren-validieren-validieren",
    "href": "slides/ms-slides-10.html#validieren-validieren-validieren",
    "title": "Unsupervised Machine Learning (II)",
    "section": "Validieren, Validieren, Validieren",
    "text": "Validieren, Validieren, Validieren\nFokus auf die Top-Abstracts von Thema 6\n\n\nExpand for full code\ngamma_export %&gt;% \n  filter(stm_topic == \"Topic 06\") %&gt;%\n  arrange(-top_gamma) %&gt;%\n  select(title, so, top_gamma, type, ab) %&gt;%\n  slice_head(n = 3) %&gt;% \n  gt() %&gt;% \n  fmt_number(\n    columns = vars(top_gamma), \n    decimals = 2) %&gt;%\n  gtExtras::gt_theme_538()\n\n\n\n\n\n\n\n\ntitle\nso\ntop_gamma\ntype\nab\n\n\n\n\nThe Acceptance of COVID-19 Vaccine: A Global Rapid Systematic Review and Meta-Analysis\nSocial Science Research Network\n0.89\narticle\nBackground: Vaccination seems to be the most effective way to prevent and control the spread of COVID-19, a disease that has been playing havoc with the lives of over 7 billion people across the globe. Vaccine hesitancy is probably the most common problem worldwide. This study aims to inspect the COVID-19 vaccine acceptance rates worldwide among the general population and healthcare workers. In addition, it compares the vaccine acceptance rates between the pre-and post-vaccine approval periods.Method: A systematic search was conducted on April 25, 2021, through PubMed, MEDLINE, Web of Science, and GOOGLE SCHOLAR databases using PRISMA and MOOSE statements. Q-test, and statistics were used to search for heterogeneity, and Eggers's test and funnel plot were applied to assess the publication bias. The random-effects model was used to estimate the pooled acceptance rates of the COVID-19 vaccines.Results: The combined COVID-19 vaccine acceptance rate among the general population and healthcare workers (n=1,581,562) was estimated at 61.74%. The vaccine acceptance rate among the general population was 62.66% and the rate among healthcare workers was 57.89%. The acceptance rate decreased from 67.21% to 53.44% among the general population and remained constant among healthcare workers during the pre and post-vaccine approval periods. The acceptance rates also vary in different regions of the world. The highest acceptance rate was found in Western Pacific Region (67.85%) and the lowest was found in African Region (39.51%).Conclusion: Low COVID-19 vaccine acceptance rate might be a massive barrier to getting rid of the pandemic. More researches are needed to address the responsible factors influencing the global rate of COVID-19 vaccine acceptance. Integrated global efforts are required to remove the barriers.\n\n\nPrevalence of Suicidal Behavior Among Students in South-East Asia: A Systematic Review and Meta-Analysis\nArchives of Suicide Research\n0.88\narticle\nEstimation of rates of suicidal behaviors (ideation, plan, and attempt) would help to understand the burden and prioritize prevention strategies. However, no attempt to assess suicidal behavior among students was identified in South-East Asia (SEA). We aimed to assess the prevalence of suicidal behavior (ideation, plan, and attempt) among students in SEA.We followed PRISMA 2020 guidelines and registered the protocol in PROSPERO (CRD42022353438). We searched in Medline, Embase, and PsycINFO and performed meta-analyses to pool the lifetime, 1-year, and point prevalence rates for suicidal ideation, plans, and attempts. We considered the duration of a month for point prevalence.The search identified 40 separate populations from which 46 were included in the analyses, as some studies included samples from multiple countries. The pooled prevalence of suicidal ideation was 17.4% (confidence interval [95% CI], 12.4%-23.9%) for lifetime, 9.33% (95% CI, 7.2%-12%) for the past year, and 4.8% (95% CI, 3.6%-6.4%) for the present time. The pooled prevalence of suicide plans was 9% (95% CI, 6.2%-12.9%) for lifetime, 7.3% (95% CI, 5.1%-10.3%) for the past year, and 2.3% (95% CI, 0.8%-6.7%) for the present time. The pooled prevalence of suicide attempts was 5.2% (95% CI, 3.5%-7.8%) for lifetime and 4.5% (95% CI, 3.4%-5.8%) for the past year. Higher rates of suicide attempts in the lifetime were noted in Nepal (10%) and Bangladesh (9%), while lower rates were reported in India (4%) and Indonesia (5%).Suicidal behaviors are a common phenomenon among students in the SEA region. These findings call for integrated, multisectoral efforts to prevent suicidal behaviors in this group.\n\n\nFirst COVID-19 Booster Dose in the General Population: A Systematic Review and Meta-Analysis of Willingness and Its Predictors\nVaccines\n0.88\narticle\nThe emergence of breakthrough infections and new highly contagious variants of SARS-CoV-2 threaten the immunization in individuals who had completed the primary COVID-19 vaccination. This systematic review and meta-analysis investigated, for the first time, acceptance of the first COVID-19 booster dose and its associated factors among fully vaccinated individuals. We followed the PRISMA guidelines. We searched Scopus, Web of Science, Medline, PubMed, ProQuest, CINAHL and medrxiv from inception to 21 May 2022. We found 14 studies including 104,047 fully vaccinated individuals. The prevalence of individuals who intend to accept a booster was 79.0%, while the prevalence of unsure individuals was 12.6%, and the prevalence of individuals that intend to refuse a booster was 14.3%. The main predictors of willingness were older age, flu vaccination in the previous season, and confidence in COVID-19 vaccination. The most important reasons for decline were adverse reactions and discomfort experienced after previous COVID-19 vaccine doses and concerns for serious adverse reactions to COVID-19 booster doses. Considering the burden of COVID-19, a high acceptance rate of booster doses could be critical in controlling the pandemic. Our findings are innovative and could help policymakers to design and implement specific COVID-19 vaccination programs in order to decrease booster vaccine hesitancy."
  },
  {
    "objectID": "slides/ms-slides-10.html#die-suche-nach-dem-optimalen-k",
    "href": "slides/ms-slides-10.html#die-suche-nach-dem-optimalen-k",
    "title": "Unsupervised Machine Learning (II)",
    "section": "Die Suche nach dem optimalen k",
    "text": "Die Suche nach dem optimalen k\nDie wichtigste Frage bei der Modellauswahl\n\nDie Wahl von K (ob das Modell 5, 15 oder 100 Themen identifizieren soll), hat einen erheblichen Einfluss auf die Ergebnisse:\n\nje kleiner K, desto feinkörniger und in der Regel exklusiver die Themen;\nje größer K, desto deutlicher identifizieren die Themen einzelne Ereignisse oder Themen.\n\nDas Paket stm (Roberts et al., 2019) verfügt über zwei eingebaute Lösungen, um das optimale K zu finden\n\nsearchK()-Funktion\nEinstellung von K = 0 bei der Schätzung des Modells\n\nEmpfehlung für stm: (Manuelles) Training und Auswertung!"
  },
  {
    "objectID": "slides/ms-slides-10.html#training-und-evaluation-der-modelle",
    "href": "slides/ms-slides-10.html#training-und-evaluation-der-modelle",
    "title": "Unsupervised Machine Learning (II)",
    "section": "Training und Evaluation der Modelle",
    "text": "Training und Evaluation der Modelle\nDie bessere Version von searchK(): Manuelle Exploration\n\n\n\n\n# Define parameters\nfuture::plan(future::multisession()) # use multiple sessions\ntopic_range &lt;- seq(from = 10, to = 100, by = 10) \n\n# Estimate models\nstm_search  &lt;- tibble(k = topic_range) %&gt;%\n  mutate(\n    mdl = furrr::future_map(\n      k, \n      ~stm::stm(\n        documents = quanteda_stm$documents,\n        vocab = quanteda_stm$vocab, \n        prevalence =~ publication_year_fct + field,\n        K = ., \n        seed = 42,\n        max.em.its = 1000,\n        data = quanteda_stm$meta,\n        init.type = \"Spectral\",\n        verbose = FALSE),\n      .options = furrr::furrr_options(seed = 42)\n      )\n    )\n\n\n\n# Overview\nstm_search$mdl\n\n[[1]]\nA topic model with 10 topics, 36650 documents and a 14322 word dictionary.\n\n[[2]]\nA topic model with 20 topics, 36650 documents and a 14322 word dictionary.\n\n[[3]]\nA topic model with 30 topics, 36650 documents and a 14322 word dictionary.\n\n[[4]]\nA topic model with 40 topics, 36650 documents and a 14322 word dictionary.\n\n[[5]]\nA topic model with 50 topics, 36650 documents and a 14322 word dictionary.\n\n[[6]]\nA topic model with 60 topics, 36650 documents and a 14322 word dictionary.\n\n[[7]]\nA topic model with 70 topics, 36650 documents and a 14322 word dictionary.\n\n[[8]]\nA topic model with 80 topics, 36650 documents and a 14322 word dictionary.\n\n[[9]]\nA topic model with 90 topics, 36650 documents and a 14322 word dictionary.\n\n[[10]]\nA topic model with 100 topics, 36650 documents and a 14322 word dictionary."
  },
  {
    "objectID": "slides/ms-slides-10.html#trainings--und-validierungsdatensatz",
    "href": "slides/ms-slides-10.html#trainings--und-validierungsdatensatz",
    "title": "Unsupervised Machine Learning (II)",
    "section": "Trainings- und Validierungsdatensatz",
    "text": "Trainings- und Validierungsdatensatz\nVergleich der verschiedenen Modelle anhand verschiedener Metriken\n\nheldout &lt;- make.heldout(\n  documents = quanteda_stm$documents,\n  vocab = quanteda_stm$vocab,\n  seed = 42)\n\n\nstm_search$results &lt;- stm_search %&gt;%\n  mutate(\n    exclusivity = map(mdl, exclusivity),\n    semantic_coherence = map(mdl, semanticCoherence, quanteda_stm$documents),\n    eval_heldout = map(mdl, eval.heldout, heldout$missing),\n    residual = map(mdl, checkResiduals, quanteda_stm$documents),\n    bound =  map_dbl(mdl, function(x) max(x$convergence$bound)),\n    lfact = map_dbl(mdl, function(x) lfactorial(x$settings$dim$K)),\n    lbound = bound + lfact,\n    iterations = map_dbl(mdl, function(x) length(x$convergence$bound)))"
  },
  {
    "objectID": "slides/ms-slides-10.html#kurzer-crashkurs",
    "href": "slides/ms-slides-10.html#kurzer-crashkurs",
    "title": "Unsupervised Machine Learning (II)",
    "section": "Kurzer Crashkurs",
    "text": "Kurzer Crashkurs\nÜberblick über die verschiedenen Evaluationskritierien\n\nHeld-Out Likelihood misst, wie gut ein Modell ungesehene Daten vorhersagt (ABER: kein allgemeingültiger Schwellenwert, nur Vergleich identischer Daten). Höhere Werte weisen auf eine bessere Vorhersageleistung hin.\nLower bound ist eine Annäherung an die Log-Likelihood des Modells. Ein höherer Wert deutet auf eine bessere Anpassung an die Daten hin.\nResiduen geben die Differenz zwischen den beobachteten und den vorhergesagten Werten an. Kleinere Residuen deuten auf eine bessere Modellanpassung hin. Im Idealfall sollten die Residuen so klein wie möglich sein.\nSemantische Kohärenz misst, wie semantisch verwandt die wichtigsten Wörter eines Themas sind, wobei höhere Werte auf kohärentere Themen hinweisen."
  },
  {
    "objectID": "slides/ms-slides-10.html#the-best-of-the-not-so-optimal-models",
    "href": "slides/ms-slides-10.html#the-best-of-the-not-so-optimal-models",
    "title": "Unsupervised Machine Learning (II)",
    "section": "The best of the not so optimal models",
    "text": "The best of the not so optimal models\nÜberblick über die verschiedenen Evaluationskritierien\n\n\nExpand for full code\nstm_search$results %&gt;% \n  # Create data for graph\n  transmute(\n    k, \n    `Lower bound` = lbound,\n    Residuals = map_dbl(residual, \"dispersion\"),\n    `Semantic coherence` = map_dbl(semantic_coherence, mean),\n    `Held-out likelihood` = map_dbl(eval_heldout, \"expected.heldout\")\n    ) %&gt;%   \n  gather(Metric, Value, -k) %&gt;%\n  # Create graph\n  ggplot(aes(k, Value, color = Metric)) +\n    geom_line(linewidth = 1.5, alpha = 0.7, show.legend = FALSE) +\n    geom_point(size = 3) +\n    # Add marker\n    geom_vline(aes(xintercept = 20), color = \"#C77CFF\", alpha = .5) +\n    geom_vline(aes(xintercept = 40), color = \"#00BFC4\", alpha = .5) +\n    geom_vline(aes(xintercept = 60), color = \"#C77CFF\", alpha = .5) +\n    geom_vline(aes(xintercept = 70), color = \"#00BFC4\", alpha = .5) +  \n    scale_x_continuous(breaks = seq(from = 10, to = 100, by = 10)) +\n    facet_wrap(~Metric, scales = \"free_y\") +\n    labs(x = \"K (number of topics)\",\n        y = NULL,\n        title = \"Model diagnostics by number of topics\"\n        ) +\n    theme_pubr()"
  },
  {
    "objectID": "slides/ms-slides-10.html#kohärenz-nur-mit-exklusivität",
    "href": "slides/ms-slides-10.html#kohärenz-nur-mit-exklusivität",
    "title": "Unsupervised Machine Learning (II)",
    "section": "Kohärenz nur mit Exklusivität",
    "text": "Kohärenz nur mit Exklusivität\nVergleich verschiedener potentieller “optimaler” Modelle\n\n\nExpand for full code\nstm_search$results %&gt;% \n  select(k, exclusivity, semantic_coherence) %&gt;% \n  filter(k %in% c(20, 40, 70)) %&gt;%\n  unnest(cols = c(exclusivity, semantic_coherence)) %&gt;%\n  mutate(k = as.factor(k)) %&gt;%\n  ggplot(aes(semantic_coherence, exclusivity, color = k)) +\n  geom_point(size = 2, alpha = 0.7) +\n  labs(x = \"Semantic coherence\",\n       y = \"Exclusivity\",\n       title = \"Comparing exclusivity and semantic coherence\",\n       subtitle = \"Models with fewer topics have higher semantic coherence for more topics, but lower exclusivity\"\n       ) +\n  theme_minimal()"
  },
  {
    "objectID": "slides/ms-slides-10.html#and-now-you-textanalyse-mit-r",
    "href": "slides/ms-slides-10.html#and-now-you-textanalyse-mit-r",
    "title": "Unsupervised Machine Learning (II)",
    "section": "🧪 And now … you: Textanalyse mit R",
    "text": "🧪 And now … you: Textanalyse mit R\nNext Steps: Wiederholung der Inhalte\n\nLaden Sie die auf StudOn bereitgestellten Dateien für die Sitzungen herunter\nLaden Sie die .zip-Datei in Ihren RStudio Workspace\nNavigieren Sie zu dem Ordner, in dem die Datei ps_24_binder.Rproj liegt. Öffnen Sie diese Datei mit einem Doppelklick. Nur dadurch ist gewährleistet, dass alle Dependencies korrekt funktionieren.\nÖffnen Sie die Datei exercise-10.qmd im Ordner exercises und lesen Sie sich gründlich die Anweisungen durch.\nTipp: Sie finden alle in den Folien verwendeten Code-Bausteine in der Datei showcase.qmd (für den “rohen” Code) oder showcase.html (mit gerenderten Ausgaben)."
  },
  {
    "objectID": "slides/ms-slides-10.html#references",
    "href": "slides/ms-slides-10.html#references",
    "title": "Unsupervised Machine Learning (II)",
    "section": "References",
    "text": "References\n\n\nRoberts, M. E., Stewart, B. M., & Tingley, D. (2019). stm: An R Package for Structural Topic Models. Journal of Statistical Software, 91(1), 1–40. https://doi.org/10.18637/jss.v091.i02\n\n\n\n\n\n\nHome"
  },
  {
    "objectID": "slides/ms-slides-08.html#seminarplan",
    "href": "slides/ms-slides-08.html#seminarplan",
    "title": "Text processing",
    "section": "Seminarplan",
    "text": "Seminarplan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSitzung\nDatum\nThema (synchron)\nÜbung (asynchron)\nDozent:in\n\n\n\n\n1\n18.04.2024\nEinführung & Überblick\n\nAM & CA\n\n\n\n📚\nTeil 1: Systematic Review\n\n\n\n\n2\n25.04.2024\nEinführung in Systematic Reviews I\nR-Einführung\nAM\n\n\n3\n02.05.2024\nEinführung in Systematic Reviews II\nR-Einführung\nAM\n\n\n\n09.05.2024\n🏖️ Feiertag\nR-Einführung\n\n\n\n4\n16.05.2024\nAutomatisierung von SRs & KI-Tools\nR-Einführung\nAM\n\n\n\n23.05.2024\n🍻 WiSo-Projekt-Woche\nR-Einführung\n\n\n\n5\n04.06.2024\n🍕 Gastvortrag: Prof. Dr. Emese Domahidi\nR-Einführung\nED\n\n\n6\n06.06.2024\nAutomatisierung von SRs & KI-Tools\nR-Einführung\nAM\n\n\n\n💻\nTeil 2: Text as Data & Unsupervised Machine Learning\n\n\n\n\n7\n13.06.2024\nIntroduction to Text as Data\nzur Sitzung\nCA\n\n\n8\n20.06.2024\nText processing\nzur Sitzung\nCA\n\n\n9\n27.06.2024\nUnsupervised Machine Learning I\nzur Sitzung\nCA\n\n\n10\n04.07.2024\nUnsupervised Machine Learning II\nzur Sitzung\nCA & AM\n\n\n11\n11.07.2024\nRecap & Ausblick\nzur Sitzung\nCA & AM\n\n\n12\n18.07.2024\n🏁 Semesterabschluss\nzur Sitzung\nCA & AM"
  },
  {
    "objectID": "slides/ms-slides-08.html#building-a-shared-vocabulary",
    "href": "slides/ms-slides-08.html#building-a-shared-vocabulary",
    "title": "Text processing",
    "section": "Building a shared vocabulary",
    "text": "Building a shared vocabulary\nWichtige Begriffe und Konzepte\n\n by Analytics Vidhya\nToken: A token is a string with a known meaning, and a token may be a word, number or just characters like punctuation. “Hello”, “123”, and “-” are some examples of tokens.\nSentence: A sentence is a group of tokens that is complete in meaning. “The weather looks good” is an example of a sentence, and the tokens of the sentence are [“The”, “weather”, “looks”, “good].\nParagraph: A paragraph is a collection of sentences or phrases, and a sentence can alternatively be viewed as a token of a paragraph.\nDocuments: A document might be a sentence, a paragraph, or a set of paragraphs. A text message sent to an individual is an example of a document.\nCorpus: A corpus is typically an extensive collection of documents as a Bag-of-words. A corpus comprises each word’s id and frequency count in each record. An example of a corpus is a collection of emails or text messages sent to a particular person."
  },
  {
    "objectID": "slides/ms-slides-08.html#a-bag-of-words",
    "href": "slides/ms-slides-08.html#a-bag-of-words",
    "title": "Text processing",
    "section": "A “bag of words”",
    "text": "A “bag of words”\nEinfache Technik im Natural Language Processing (NLP)\n\n by Shubham Gandhi\na collection of words, disregarding grammar, word order, and context."
  },
  {
    "objectID": "slides/ms-slides-08.html#digitales-wörterbuch-der-deutschen-sprache",
    "href": "slides/ms-slides-08.html#digitales-wörterbuch-der-deutschen-sprache",
    "title": "Text processing",
    "section": "Digitales Wörterbuch der deutschen Sprache",
    "text": "Digitales Wörterbuch der deutschen Sprache\nGroßes, frei verfügbares & deutschsprachiges Textkorpora"
  },
  {
    "objectID": "slides/ms-slides-08.html#vom-korpus-zum-token",
    "href": "slides/ms-slides-08.html#vom-korpus-zum-token",
    "title": "Text processing",
    "section": "Vom Korpus zum Token",
    "text": "Vom Korpus zum Token\nEinfaches Beispiel zur Darstellung der verschiedenen Konzepte\n\n by Mina Ghashami"
  },
  {
    "objectID": "slides/ms-slides-08.html#vom-korpus-zum-token-zum-model",
    "href": "slides/ms-slides-08.html#vom-korpus-zum-token-zum-model",
    "title": "Text processing",
    "section": "Vom Korpus zum Token zum Model",
    "text": "Vom Korpus zum Token zum Model\nKomplexer Prozess der Textverarbeitung\n\n by Jiawei Hu"
  },
  {
    "objectID": "slides/ms-slides-08.html#sätze-token-lemma-pos",
    "href": "slides/ms-slides-08.html#sätze-token-lemma-pos",
    "title": "Text processing",
    "section": "Sätze ➜ Token ➜ Lemma ➜ POS",
    "text": "Sätze ➜ Token ➜ Lemma ➜ POS\nBeispielhafte Darstellung des Text Preprocessing\n\n\n\n\n\n\n\n1. Satzerkennung\n\n\nWas gibt’s in New York zu sehen?\n\n\n\n\n\n\n\n\n\n2. Tokenisierung\n\n\nwas; gibt; `s; in; new; york; zu; sehen; ?\n\n\n\n\n\n\n\n\n\n3. Lemmatisierung\n\n\nwas; geben; `s; in; new; york; zu; sehen; ?\n\n\n\n\n\n\n\n\n\n4. Part-Of-Speech (POS) Tagging\n\n\n&gt;Was/PWS &gt;gibt/VVFIN &gt;’s/PPER &gt;in/APPR &gt;New/NE &gt;York/NE &gt;zu/PTKZU &gt;sehen/VVINF\n\n\n\n\nSatzerkennung: Auflösung der Satzstruktur; Aber: Probleme mit Datumsangaben, Uhrzeit, Abkürzungen, URLS\nTokenisierung: Zerteilung in kleinste Einheiten, Abtrennung von Satzzeichen; Fragen: Umgang mit Zeichen, Symbolen, Zahlen, N-Gramme …\nDefinition Lemmatisierung: Grundform eines Worters, als diejenige Form, unter dem an einen Begriff in einem Nachschlagewerk findet / Rückführung auf die „Vollfrom”\nDefinition POS: Zuordnung von Wörtern und Satzzeichen eines Textes zu Wortarten"
  },
  {
    "objectID": "slides/ms-slides-08.html#von-bow-zu-dfm",
    "href": "slides/ms-slides-08.html#von-bow-zu-dfm",
    "title": "Text processing",
    "section": "Von BoW zu DFM",
    "text": "Von BoW zu DFM\nTransformation des Bag-of-Words (BOW) zur Document-Feature-Matrix (DFM)\n\n by OpenClassrooms\nBag-of-Words-Modell: es zählt lediglich die Worthäufigkeit je Dokument, die syntaktischen und grammatikalischen Zusammenhänge zwischen einzelnen Wörtern werden ignoriert."
  },
  {
    "objectID": "slides/ms-slides-08.html#what-we-did-so-far",
    "href": "slides/ms-slides-08.html#what-we-did-so-far",
    "title": "Text processing",
    "section": "What we did so far",
    "text": "What we did so far\nInformationen zur Datengrundlage und -quelle\n\nSuche nach Literatur zur (Sytematischen) Literaturüberblicken auf OpenAlex\nDownload von knapp 100.000 Literaturverweisen via API mit openalexR (Aria et al., 2024)\nDeskriptive Auswertung der Daten (“Rekonstruktion” des OpenAlex Web-Dashboards) mit R\n\nHeutige Ziele:\n\nEingrenzung der Datenbasis für weiterführende Analysen\nAnwendung einfacher Textanalyseverfahren zur Untersuchung der Abstracts"
  },
  {
    "objectID": "slides/ms-slides-08.html#euer-input-ist-gefragt",
    "href": "slides/ms-slides-08.html#euer-input-ist-gefragt",
    "title": "Text processing",
    "section": "Euer Input ist gefragt!",
    "text": "Euer Input ist gefragt!\nWie sollen die Daten weiter eingegrenzt werden?\n\n\n\nBitte scannt den QR-Code oder nutzt den folgenden Link für die Teilnahme an einer kurzen Umfrage:\n\nhttps://www.menti.com/albpi1xur7et\nTemporary Access Code: 3332 2971\n\n\n\n\n \n\n    \n\n\n\n\n−+\n01:00"
  },
  {
    "objectID": "slides/ms-slides-08.html#ergebnis",
    "href": "slides/ms-slides-08.html#ergebnis",
    "title": "Text processing",
    "section": "Ergebnis",
    "text": "Ergebnis"
  },
  {
    "objectID": "slides/ms-slides-08.html#build-the-subsample",
    "href": "slides/ms-slides-08.html#build-the-subsample",
    "title": "Text processing",
    "section": "Build the subsample",
    "text": "Build the subsample\nFokus auf englische Artikel aus den Sozialwissenschaften und der Psychologie\n\n\n\nreview_subsample &lt;- review_works_correct %&gt;% \n  # Eingrenzung: Sprache und Typ\n  filter(language == \"en\") %&gt;% \n  filter(type == \"article\") %&gt;%\n  # Datentranformation\n  unnest(topics, names_sep = \"_\") %&gt;%\n  filter(topics_name == \"field\") %&gt;% \n  filter(topics_i == \"1\") %&gt;% \n  # Eingrenzung: Forschungsfeldes\n  filter(\n   topics_display_name == \"Social Sciences\"|\n   topics_display_name == \"Psychology\"\n    )\n\n\n\n\nExpand for full code\nreview_works_correct %&gt;% \n  mutate(\n    included = ifelse(id %in% review_subsample$id, \"Ja\", \"Nein\"),\n    included = factor(included, levels = c(\"Nein\", \"Ja\"))\n    ) %&gt;%\n  ggplot(aes(x = publication_year_fct, fill = included)) +\n    geom_bar() +\n    labs(\n      x = \"\",\n      y = \"Anzahl der Einträge\", \n      fill = \"In Subsample enthalten?\"\n     ) +\n    scale_fill_manual(values = c(\"#A0ACBD50\", \"#FF707F\")) +\n    theme_pubr()"
  },
  {
    "objectID": "slides/ms-slides-08.html#explore-abstracts",
    "href": "slides/ms-slides-08.html#explore-abstracts",
    "title": "Text processing",
    "section": "Explore abstracts",
    "text": "Explore abstracts\nTidy data principles als Grundlage für Analyse-Workflow\n\n(Silge & Robinson, 2017)\nTidy data struture (each variable is column, each observation a row, each value is a cell, each type of observaional unit is a table) results in a table with one-token-per-row (Silge & Robinson, 2017)."
  },
  {
    "objectID": "slides/ms-slides-08.html#tokenization-der-abstracts",
    "href": "slides/ms-slides-08.html#tokenization-der-abstracts",
    "title": "Text processing",
    "section": "Tokenization der Abstracts",
    "text": "Tokenization der Abstracts\nTransform data to tidy text\n\n# Create tidy data\nreview_tidy &lt;- review_subsample %&gt;% \n    # Tokenization\n    tidytext::unnest_tokens(\"text\", ab) %&gt;% \n    # Remove stopwords\n    filter(!text %in% tidytext::stop_words$word)\n\n# Preview\nreview_tidy %&gt;% \n  select(id, text) %&gt;% \n  print(n = 10)\n\n# A tibble: 4,880,965 × 2\n   id                               text          \n   &lt;chr&gt;                            &lt;chr&gt;         \n 1 https://openalex.org/W4293003987 5             \n 2 https://openalex.org/W4293003987 item          \n 3 https://openalex.org/W4293003987 world         \n 4 https://openalex.org/W4293003987 health        \n 5 https://openalex.org/W4293003987 organization  \n 6 https://openalex.org/W4293003987 index         \n 7 https://openalex.org/W4293003987 5             \n 8 https://openalex.org/W4293003987 widely        \n 9 https://openalex.org/W4293003987 questionnaires\n10 https://openalex.org/W4293003987 assessing     \n# ℹ 4,880,955 more rows"
  },
  {
    "objectID": "slides/ms-slides-08.html#before-and-after-the-transformation",
    "href": "slides/ms-slides-08.html#before-and-after-the-transformation",
    "title": "Text processing",
    "section": "Before and after the transformation",
    "text": "Before and after the transformation\nVergleich eines Abstraktes in Rohform und nach Tokenisierung\n\n\n\nreview_subsample$ab[[1]]\n\n[1] \"The 5-item World Health Organization Well-Being Index (WHO-5) is among the most widely used questionnaires assessing subjective psychological well-being. Since its first publication in 1998, the WHO-5 has been translated into more than 30 languages and has been used in research studies all over the world. We now provide a systematic review of the literature on the WHO-5.We conducted a systematic search for literature on the WHO-5 in PubMed and PsycINFO in accordance with the PRISMA guidelines. In our review of the identified articles, we focused particularly on the following aspects: (1) the clinimetric validity of the WHO-5; (2) the responsiveness/sensitivity of the WHO-5 in controlled clinical trials; (3) the potential of the WHO-5 as a screening tool for depression, and (4) the applicability of the WHO-5 across study fields.A total of 213 articles met the predefined criteria for inclusion in the review. The review demonstrated that the WHO-5 has high clinimetric validity, can be used as an outcome measure balancing the wanted and unwanted effects of treatments, is a sensitive and specific screening tool for depression and its applicability across study fields is very high.The WHO-5 is a short questionnaire consisting of 5 simple and non-invasive questions, which tap into the subjective well-being of the respondents. The scale has adequate validity both as a screening tool for depression and as an outcome measure in clinical trials and has been applied successfully across a wide range of study fields.\"\n\n\n\n\nreview_tidy %&gt;% \n  filter(id == \"https://openalex.org/W4293003987\") %&gt;% \n  pull(text) %&gt;% \n  paste(collapse = \" \")\n\n[1] \"5 item world health organization index 5 widely questionnaires assessing subjective psychological publication 1998 5 translated 30 languages research studies world provide systematic review literature 5 conducted systematic search literature 5 pubmed psycinfo accordance prisma guidelines review identified articles focused aspects 1 clinimetric validity 5 2 responsiveness sensitivity 5 controlled clinical trials 3 potential 5 screening tool depression 4 applicability 5 study fields.a total 213 articles met predefined criteria inclusion review review demonstrated 5 clinimetric validity outcome measure balancing unwanted effects treatments sensitive specific screening tool depression applicability study fields high.the 5 short questionnaire consisting 5 simple invasive questions tap subjective respondents scale adequate validity screening tool depression outcome measure clinical trials applied successfully wide range study fields\""
  },
  {
    "objectID": "slides/ms-slides-08.html#count-token-frequency",
    "href": "slides/ms-slides-08.html#count-token-frequency",
    "title": "Text processing",
    "section": "Count token frequency",
    "text": "Count token frequency\nSummarize all tokens over all tweets\n\n\n# Create summarized data\nreview_summarized &lt;- review_tidy %&gt;% \n  count(text, sort = TRUE) \n\n# Preview Top 15 token\nreview_summarized %&gt;% \n    print(n = 15)\n\n\n# A tibble: 122,148 × 2\n   text              n\n   &lt;chr&gt;         &lt;int&gt;\n 1 studies       73398\n 2 review        57878\n 3 research      42689\n 4 health        35108\n 5 systematic    32431\n 6 literature    31374\n 7 study         29012\n 8 interventions 22731\n 9 included      21987\n10 social        21528\n11 articles      20631\n12 results       20166\n13 analysis      19624\n14 based         18929\n15 evidence      18545\n# ℹ 122,133 more rows"
  },
  {
    "objectID": "slides/ms-slides-08.html#the-unavoidable-word-cloud",
    "href": "slides/ms-slides-08.html#the-unavoidable-word-cloud",
    "title": "Text processing",
    "section": "The (Unavoidable) Word Cloud",
    "text": "The (Unavoidable) Word Cloud\nVisualization of Top 50 token\n\nreview_summarized %&gt;% \n    top_n(50) %&gt;% \n    ggplot(aes(label = text, size = n)) +\n    ggwordcloud::geom_text_wordcloud() +\n    scale_size_area(max_size = 20) +\n    theme_minimal()"
  },
  {
    "objectID": "slides/ms-slides-08.html#mehr-als-nur-ein-wort",
    "href": "slides/ms-slides-08.html#mehr-als-nur-ein-wort",
    "title": "Text processing",
    "section": "Mehr als nur ein Wort",
    "text": "Mehr als nur ein Wort\nModellierung von Wortzusammenhängen: n-grams and correlations\n\n\nViele der wirklich interessanten Ergebnisse von Textanalysen basieren auf den Beziehungen zwischen Wörtern, z.B.\n\nwelche Wörter dazu “neigen”, unmittelbar auf einander zu folgen (n-grams),\noder innerhalb desselben Dokuments gemeinsam aufzutreten (Korrelation)\n\n\n\n\n\n(Silge & Robinson, 2017)"
  },
  {
    "objectID": "slides/ms-slides-08.html#häufige-wortpaare",
    "href": "slides/ms-slides-08.html#häufige-wortpaare",
    "title": "Text processing",
    "section": "Häufige Wortpaare",
    "text": "Häufige Wortpaare\nWortkombinationen (n-grams) im FokusH\n\n\n# Create word paris\nreview_word_pairs &lt;- review_tidy %&gt;% \n    widyr::pairwise_count(\n        text,\n        id,\n        sort = TRUE)\n\n# Preview\nreview_word_pairs %&gt;% \n    print(n = 14)\n\n\n# A tibble: 114,446,724 × 3\n   item1      item2          n\n   &lt;chr&gt;      &lt;chr&gt;      &lt;dbl&gt;\n 1 review     studies    20494\n 2 studies    review     20494\n 3 review     systematic 20266\n 4 systematic review     20266\n 5 review     research   16902\n 6 research   review     16902\n 7 literature review     16754\n 8 review     literature 16754\n 9 systematic studies    16097\n10 studies    systematic 16097\n11 study      review     13391\n12 review     study      13391\n13 studies    research   13173\n14 research   studies    13173\n# ℹ 114,446,710 more rows"
  },
  {
    "objectID": "slides/ms-slides-08.html#häufig-zusammen-selten-allein",
    "href": "slides/ms-slides-08.html#häufig-zusammen-selten-allein",
    "title": "Text processing",
    "section": "Häufig zusammen, selten allein",
    "text": "Häufig zusammen, selten allein\nWortkorrelationen im Fokus\n\n\n# Create word correlation\nreview_pairs_corr &lt;- review_tidy %&gt;% \n    group_by(text) %&gt;% \n    filter(n() &gt;= 300) %&gt;% \n    pairwise_cor(\n        text, \n        id, \n        sort = TRUE)\n\n# Preview\nreview_pairs_corr %&gt;% \n    print(n = 15)\n\n\n# A tibble: 5,529,552 × 3\n   item1      item2      correlation\n   &lt;chr&gt;      &lt;chr&gt;            &lt;dbl&gt;\n 1 ottawa     newcastle        0.977\n 2 newcastle  ottawa           0.977\n 3 briggs     joanna           0.967\n 4 joanna     briggs           0.967\n 5 scholar    google           0.938\n 6 google     scholar          0.938\n 7 obsessive  compulsive       0.929\n 8 compulsive obsessive        0.929\n 9 nervosa    anorexia         0.893\n10 anorexia   nervosa          0.893\n11 ci         95               0.887\n12 95         ci               0.887\n13 las        los              0.886\n14 los        las              0.886\n15 gay        bisexual         0.861\n# ℹ 5,529,537 more rows"
  },
  {
    "objectID": "slides/ms-slides-08.html#spezifische-partner-in-spezifischen-umgebungen",
    "href": "slides/ms-slides-08.html#spezifische-partner-in-spezifischen-umgebungen",
    "title": "Text processing",
    "section": "Spezifische “Partner” in spezifischen Umgebungen",
    "text": "Spezifische “Partner” in spezifischen Umgebungen\nHäufig auftretenden Wörter in der Umgebung von review, literature, systematic\n\n\nreview_pairs_corr %&gt;% \n  filter(\n    item1 %in% c(\n      \"review\",\n      \"literature\",\n      \"systematic\")\n    ) %&gt;% \n  group_by(item1) %&gt;% \n  slice_max(correlation, n = 5) %&gt;% \n  ungroup() %&gt;% \n  mutate(\n    item2 = reorder(item2, correlation)\n    ) %&gt;% \n  ggplot(\n    aes(item2, correlation, fill = item1)\n    ) +\n  geom_bar(stat = \"identity\") +\n  facet_wrap(~ item1, scales = \"free_y\") +\n  coord_flip() +\n  scale_fill_manual(\n    values = c(\n      \"#04316A\",\n      \"#C50F3C\",\n      \"#00B2D1\")) +\n  theme_pubr()"
  },
  {
    "objectID": "slides/ms-slides-08.html#lets-talk-about-sentiments",
    "href": "slides/ms-slides-08.html#lets-talk-about-sentiments",
    "title": "Text processing",
    "section": "Let’s talk about sentiments",
    "text": "Let’s talk about sentiments\nDictionary based approach of text analysis\n\n(Silge & Robinson, 2017)\n\n\n\n\n\nAtteveldt et al. (2021) argue that sentiment, in fact, are quite a complex concepts that are often hard to capture with dictionaries."
  },
  {
    "objectID": "slides/ms-slides-08.html#über-die-bedeutung-von-positivnegativ",
    "href": "slides/ms-slides-08.html#über-die-bedeutung-von-positivnegativ",
    "title": "Text processing",
    "section": "Über die Bedeutung von “positiv;negativ”",
    "text": "Über die Bedeutung von “positiv;negativ”\nDie häufigsten “positiven” und “negativen” Wörter in den Abstracts\n\n\nreview_sentiment_count &lt;- review_tidy %&gt;% \n  inner_join(\n     get_sentiments(\"bing\"),\n     by = c(\"text\" = \"word\"),\n     relationship = \"many-to-many\") %&gt;% \n  count(text, sentiment)\n  \n# Preview\nreview_sentiment_count %&gt;% \n  group_by(sentiment) %&gt;%\n  slice_max(n, n = 10) %&gt;% \n  ungroup() %&gt;% \n  mutate(text = reorder(text, n)) %&gt;%\n  ggplot(aes(n, text, fill = sentiment)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(\n    ~sentiment, scales = \"free_y\") +\n  labs(x = \"Contribution to sentiment\",\n       y = NULL) +\n  scale_fill_manual(\n    values = c(\"#C50F3C\", \"#007900\")) +\n  theme_pubr()"
  },
  {
    "objectID": "slides/ms-slides-08.html#anreicherung-der-daten",
    "href": "slides/ms-slides-08.html#anreicherung-der-daten",
    "title": "Text processing",
    "section": "Anreicherung der Daten",
    "text": "Anreicherung der Daten\nVerknüpfung des Sentiemnt (Scores) mit den Abstracts\n\nreview_sentiment &lt;- review_tidy %&gt;% \n  inner_join(\n     get_sentiments(\"bing\"),\n     by = c(\"text\" = \"word\"),\n     relationship = \"many-to-many\") %&gt;% \n  count(id, sentiment) %&gt;% \n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %&gt;% \n  mutate(sentiment = positive - negative)\n  \n# Check\nreview_sentiment \n\n# A tibble: 35,710 × 4\n   id                               negative positive sentiment\n   &lt;chr&gt;                               &lt;int&gt;    &lt;int&gt;     &lt;int&gt;\n 1 https://openalex.org/W1000529773        2        2         0\n 2 https://openalex.org/W1006561082        0        1         1\n 3 https://openalex.org/W100685805         4       15        11\n 4 https://openalex.org/W1007410967        0        7         7\n 5 https://openalex.org/W1008209175        8        1        -7\n 6 https://openalex.org/W1009104829        2        4         2\n 7 https://openalex.org/W1009607471       15        8        -7\n 8 https://openalex.org/W1031503832       13        6        -7\n 9 https://openalex.org/W1035654938       10        5        -5\n10 https://openalex.org/W1044055445        5        0        -5\n# ℹ 35,700 more rows"
  },
  {
    "objectID": "slides/ms-slides-08.html#neutral-mit-einem-leicht-negativen-unterton",
    "href": "slides/ms-slides-08.html#neutral-mit-einem-leicht-negativen-unterton",
    "title": "Text processing",
    "section": "Neutral, mit einem leicht “negativen” Unterton",
    "text": "Neutral, mit einem leicht “negativen” Unterton\nVerteilung des Sentiment (Scores) in den Abstracts\n\n\n[1] 0.4858737\n\n\n\nreview_sentiment %&gt;% \n  ggplot(aes(sentiment)) +\n  geom_histogram(binwidth = 0.5, fill = \"#FF707F\") +\n  labs(\n    x = \"Sentiment (Score) des Abstracts\", \n    y = \"Anzahl der Einträge\"\n  ) +\n  theme_pubr() +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))"
  },
  {
    "objectID": "slides/ms-slides-08.html#keep-it-neutral",
    "href": "slides/ms-slides-08.html#keep-it-neutral",
    "title": "Text processing",
    "section": "Keep it neutral",
    "text": "Keep it neutral\nEntwicklung des Sentiment (Scores) der Abstracts im Zeitverlauf\n\n\nExpand for full code\n# Create first graph\ng1 &lt;- review_works_correct %&gt;% \n  filter(id %in% review_sentiment$id) %&gt;% \n  left_join(review_sentiment, by = join_by(id)) %&gt;% \n  sjmisc::rec(\n    sentiment,\n    rec = \"min:-2=negative; -1:1=neutral; 2:max=positive\") %&gt;% \n  ggplot(aes(x = publication_year_fct, fill = as.factor(sentiment_r))) +\n    geom_bar() +\n    labs(\n      x = \"\",\n      y = \"Anzahl der Einträge\", \n      fill = \"Sentiment (Score)\") +\n    scale_fill_manual(values = c(\"#C50F3C\", \"#90A0AF\", \"#007900\")) +\n    theme_pubr() \n    #theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n# Create second graph\ng2 &lt;- review_works_correct %&gt;% \n  filter(id %in% review_sentiment$id) %&gt;% \n  left_join(review_sentiment, by = join_by(id)) %&gt;% \n  sjmisc::rec(\n    sentiment,\n    rec = \"min:-2=negative; -1:1=neutral; 2:max=positive\") %&gt;% \n  ggplot(aes(x = publication_year_fct, fill = as.factor(sentiment_r))) +\n    geom_bar(position = \"fill\") +\n    labs(\n      x = \"\",\n      y = \"Anteil der Einträge\", \n      fill = \"Sentiment (Score)\") +\n    scale_fill_manual(values = c(\"#C50F3C\", \"#90A0AF\", \"#007D29\")) +\n    theme_pubr() \n\n# COMBINE GRPAHS\nggarrange(g1, g2,\n          nrow = 1, ncol = 2, \n          align = \"hv\",\n          common.legend = TRUE)"
  },
  {
    "objectID": "slides/ms-slides-08.html#and-now-you-wiederholung",
    "href": "slides/ms-slides-08.html#and-now-you-wiederholung",
    "title": "Text processing",
    "section": "🧪 And now … you: Wiederholung",
    "text": "🧪 And now … you: Wiederholung\nNext Steps: Wiederholung der R-Grundlagen an OpenAlex-Daten\n\nLaden Sie die auf StudOn bereitgestellten Dateien für die Sitzungen herunter\nLaden Sie die .zip-Datei in Ihren RStudio Workspace\nNavigieren Sie zu dem Ordner, in dem die Datei ps_24_binder.Rproj liegt. Öffnen Sie diese Datei mit einem Doppelklick. Nur dadurch ist gewährleistet, dass alle Dependencies korrekt funktionieren.\nÖffnen Sie die Datei exercise-08.qmd im Ordner exercises und lesen Sie sich gründlich die Anweisungen durch.\nTipp: Sie finden alle in den Folien verwendeten Code-Bausteine in der Datei showcase.qmd (für den “rohen” Code) oder showcase.html (mit gerenderten Ausgaben)."
  },
  {
    "objectID": "slides/ms-slides-08.html#literatur",
    "href": "slides/ms-slides-08.html#literatur",
    "title": "Text processing",
    "section": "Literatur",
    "text": "Literatur\n\n\nAria, M., Le, T., Cuccurullo, C., Belfiore, A., & Choe, J. (2024). openalexR: An R-Tool for Collecting Bibliometric Data from OpenAlex. The R Journal, 15(4), 167–180. https://doi.org/10.32614/rj-2023-089\n\n\nAtteveldt, W. van, Trilling, D., & Arcíla, C. (2021). Computational analysis of communication: A practical introduction to the analysis of texts, networks, and images with code examples in python and r. John Wiley & Sons.\n\n\nSilge, J., & Robinson, D. (2017). Text mining with r: A tidy approach (First edition). O’Reilly.\n\n\n\n\n\n\nHome"
  },
  {
    "objectID": "sessions/ms-session-08.html",
    "href": "sessions/ms-session-08.html",
    "title": "Text processing in R",
    "section": "",
    "text": "🖥️ Slides\n📋 Showcase"
  },
  {
    "objectID": "sessions/ms-session-08.html#participate",
    "href": "sessions/ms-session-08.html#participate",
    "title": "Text processing in R",
    "section": "",
    "text": "🖥️ Slides\n📋 Showcase"
  },
  {
    "objectID": "sessions/ms-session-08.html#practice",
    "href": "sessions/ms-session-08.html#practice",
    "title": "Text processing in R",
    "section": "Practice",
    "text": "Practice\n✍️ Exercise"
  },
  {
    "objectID": "sessions/ms-session-08.html#suggested-readings",
    "href": "sessions/ms-session-08.html#suggested-readings",
    "title": "Text processing in R",
    "section": "Suggested readings",
    "text": "Suggested readings\n\nAtteveldt, W. van, Trilling, D., & Arcíla, C. (2021). Computational analysis of communication: A practical introduction to the analysis of texts, networks, and images with code examples in python and r. John Wiley & Sons.\nSilge, J., & Hvitfeldt, E. (n.d.). Supervised machine learning for text analysis in r. https://smltar.com/\nSilge, J., & Robinson, D. (2017). Text mining with r: A tidy approach (First edition). O’Reilly.\nWelbers, K., Van Atteveldt, W., & Benoit, K. (2017). Text Analysis in R. Communication Methods and Measures, 11(4), 245–265. https://doi.org/10.1080/19312458.2017.1387238"
  },
  {
    "objectID": "sessions/ms-session-08.html#useful-resources",
    "href": "sessions/ms-session-08.html#useful-resources",
    "title": "Text processing in R",
    "section": "Useful resources",
    "text": "Useful resources\n\nTutorials des CCS-Amsterdam zu “tidytext-based” Textanalyse:\n\n📖 TidyText basics\n📖 Dictionary analysis with TidyText\n\n\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "sessions/ms-session-07.html",
    "href": "sessions/ms-session-07.html",
    "title": "Introduction to Text as Data",
    "section": "",
    "text": "🖥️ Slides\n📋 Showcase"
  },
  {
    "objectID": "sessions/ms-session-07.html#participate",
    "href": "sessions/ms-session-07.html#participate",
    "title": "Introduction to Text as Data",
    "section": "",
    "text": "🖥️ Slides\n📋 Showcase"
  },
  {
    "objectID": "sessions/ms-session-07.html#practice",
    "href": "sessions/ms-session-07.html#practice",
    "title": "Introduction to Text as Data",
    "section": "Practice",
    "text": "Practice\n✍️ Exercise"
  },
  {
    "objectID": "sessions/ms-session-07.html#suggested-readings",
    "href": "sessions/ms-session-07.html#suggested-readings",
    "title": "Introduction to Text as Data",
    "section": "Suggested readings",
    "text": "Suggested readings\n\nBenoit, K. (2020). Text as Data: An Overview (L. Curini & R. Franzese, Eds.). SAGE Publications Ltd. https://methods.sagepub.com/book/research-methods-in-political-science-and-international-relations\nGrimmer, J., Roberts, M. E., & Stewart, B. M. (2022). Text as data: A new framework for machine learning and the social sciences. Princeton University Press.\nGrimmer, J., & Stewart, B. M. (2013). Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts. Political Analysis, 21(3), 267–297. https://doi.org/f458q9\n\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "exercises/ms-showcase-08.html",
    "href": "exercises/ms-showcase-08.html",
    "title": "Text processing in R",
    "section": "",
    "text": "Link to slides"
  },
  {
    "objectID": "exercises/ms-showcase-08.html#preparation",
    "href": "exercises/ms-showcase-08.html#preparation",
    "title": "Text processing in R",
    "section": "Preparation",
    "text": "Preparation\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n    here, qs, # file management\n    magrittr, janitor, # data wrangling\n    easystats, sjmisc, # data analysis\n    gt, gtExtras, # table visualization\n    ggpubr, ggwordcloud, # visualization\n    tidytext, widyr, # text analysis    \n    openalexR, \n    tidyverse # load last to avoid masking issues\n  )"
  },
  {
    "objectID": "exercises/ms-showcase-08.html#codechunks-aus-der-sitzung",
    "href": "exercises/ms-showcase-08.html#codechunks-aus-der-sitzung",
    "title": "Text processing in R",
    "section": "Codechunks aus der Sitzung",
    "text": "Codechunks aus der Sitzung\n\nErstelle Subsample\n\nreview_subsample &lt;- review_works_correct %&gt;% \n  # Eingrenzung: Sprache und Typ\n  filter(language == \"en\") %&gt;% \n  filter(type == \"article\") %&gt;%\n  # Datentranformation\n  unnest(topics, names_sep = \"_\") %&gt;%\n  filter(topics_name == \"field\") %&gt;% \n  filter(topics_i == \"1\") %&gt;% \n  # Eingrenzung: Forschungsfeldes\n  filter(\n   topics_display_name == \"Social Sciences\"|\n   topics_display_name == \"Psychology\"\n    )\n\n\n\nSubsample im Zeitverlauf\n\nreview_works_correct %&gt;% \n  mutate(\n    included = ifelse(id %in% review_subsample$id, \"Ja\", \"Nein\"),\n    included = factor(included, levels = c(\"Nein\", \"Ja\"))\n    ) %&gt;%\n  ggplot(aes(x = publication_year_fct, fill = included)) +\n    geom_bar() +\n    labs(\n      x = \"\",\n      y = \"Anzahl der Einträge\", \n      fill = \"In Subsample enthalten?\"\n     ) +\n    scale_fill_manual(values = c(\"#A0ACBD50\", \"#FF707F\")) +\n    theme_pubr() \n\n\n\n\n\n\n\n\n\n\nTokenization der Abstracts\n\n# Create tidy data\nreview_tidy &lt;- review_subsample %&gt;% \n    # Tokenization\n    tidytext::unnest_tokens(\"text\", ab) %&gt;% \n    # Remove stopwords\n    filter(!text %in% tidytext::stop_words$word)\n\n# Preview\nreview_tidy %&gt;% \n  select(id, text) %&gt;% \n  print(n = 10)\n\n# A tibble: 4,880,965 × 2\n   id                               text          \n   &lt;chr&gt;                            &lt;chr&gt;         \n 1 https://openalex.org/W4293003987 5             \n 2 https://openalex.org/W4293003987 item          \n 3 https://openalex.org/W4293003987 world         \n 4 https://openalex.org/W4293003987 health        \n 5 https://openalex.org/W4293003987 organization  \n 6 https://openalex.org/W4293003987 index         \n 7 https://openalex.org/W4293003987 5             \n 8 https://openalex.org/W4293003987 widely        \n 9 https://openalex.org/W4293003987 questionnaires\n10 https://openalex.org/W4293003987 assessing     \n# ℹ 4,880,955 more rows\n\n\n\nVergleich eines Abstraktes in Rohform und nach Tokenisierung\n\nreview_subsample$ab[[1]]\n\n[1] \"The 5-item World Health Organization Well-Being Index (WHO-5) is among the most widely used questionnaires assessing subjective psychological well-being. Since its first publication in 1998, the WHO-5 has been translated into more than 30 languages and has been used in research studies all over the world. We now provide a systematic review of the literature on the WHO-5.We conducted a systematic search for literature on the WHO-5 in PubMed and PsycINFO in accordance with the PRISMA guidelines. In our review of the identified articles, we focused particularly on the following aspects: (1) the clinimetric validity of the WHO-5; (2) the responsiveness/sensitivity of the WHO-5 in controlled clinical trials; (3) the potential of the WHO-5 as a screening tool for depression, and (4) the applicability of the WHO-5 across study fields.A total of 213 articles met the predefined criteria for inclusion in the review. The review demonstrated that the WHO-5 has high clinimetric validity, can be used as an outcome measure balancing the wanted and unwanted effects of treatments, is a sensitive and specific screening tool for depression and its applicability across study fields is very high.The WHO-5 is a short questionnaire consisting of 5 simple and non-invasive questions, which tap into the subjective well-being of the respondents. The scale has adequate validity both as a screening tool for depression and as an outcome measure in clinical trials and has been applied successfully across a wide range of study fields.\"\n\n\n\nreview_tidy %&gt;% \n  filter(id == \"https://openalex.org/W4293003987\") %&gt;% \n  pull(text) %&gt;% \n  paste(collapse = \" \")\n\n[1] \"5 item world health organization index 5 widely questionnaires assessing subjective psychological publication 1998 5 translated 30 languages research studies world provide systematic review literature 5 conducted systematic search literature 5 pubmed psycinfo accordance prisma guidelines review identified articles focused aspects 1 clinimetric validity 5 2 responsiveness sensitivity 5 controlled clinical trials 3 potential 5 screening tool depression 4 applicability 5 study fields.a total 213 articles met predefined criteria inclusion review review demonstrated 5 clinimetric validity outcome measure balancing unwanted effects treatments sensitive specific screening tool depression applicability study fields high.the 5 short questionnaire consisting 5 simple invasive questions tap subjective respondents scale adequate validity screening tool depression outcome measure clinical trials applied successfully wide range study fields\"\n\n\n\n\n\nCount token frequency\n\n# Create summarized data\nreview_summarized &lt;- review_tidy %&gt;% \n  count(text, sort = TRUE) \n\n# Preview Top 15 token\nreview_summarized %&gt;% \n    print(n = 15)\n\n# A tibble: 122,148 × 2\n   text              n\n   &lt;chr&gt;         &lt;int&gt;\n 1 studies       73398\n 2 review        57878\n 3 research      42689\n 4 health        35108\n 5 systematic    32431\n 6 literature    31374\n 7 study         29012\n 8 interventions 22731\n 9 included      21987\n10 social        21528\n11 articles      20631\n12 results       20166\n13 analysis      19624\n14 based         18929\n15 evidence      18545\n# ℹ 122,133 more rows"
  },
  {
    "objectID": "exercises/ms-showcase-08.html#the-unavoidable-word-cloud",
    "href": "exercises/ms-showcase-08.html#the-unavoidable-word-cloud",
    "title": "Text processing in R",
    "section": "The (Unavoidable) Word Cloud",
    "text": "The (Unavoidable) Word Cloud\n\nreview_summarized %&gt;% \n    top_n(50) %&gt;% \n    ggplot(aes(label = text, size = n)) +\n    ggwordcloud::geom_text_wordcloud() +\n    scale_size_area(max_size = 20) +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\nWortkombinationen (n-grams)\n\n# Create word paris\nreview_word_pairs &lt;- review_tidy %&gt;% \n    widyr::pairwise_count(\n        text,\n        id,\n        sort = TRUE)\n\n# Preview\nreview_word_pairs %&gt;% \n    print(n = 14)\n\n# A tibble: 114,446,724 × 3\n   item1      item2          n\n   &lt;chr&gt;      &lt;chr&gt;      &lt;dbl&gt;\n 1 review     studies    20494\n 2 studies    review     20494\n 3 review     systematic 20266\n 4 systematic review     20266\n 5 review     research   16902\n 6 research   review     16902\n 7 literature review     16754\n 8 review     literature 16754\n 9 systematic studies    16097\n10 studies    systematic 16097\n11 study      review     13391\n12 review     study      13391\n13 studies    research   13173\n14 research   studies    13173\n# ℹ 114,446,710 more rows\n\n\n\n\nWortkorrelationen\n\n# Create word correlation\nreview_pairs_corr &lt;- review_tidy %&gt;% \n    group_by(text) %&gt;% \n    filter(n() &gt;= 300) %&gt;% \n    pairwise_cor(\n        text, \n        id, \n        sort = TRUE)\n\n# Preview\nreview_pairs_corr %&gt;% \n    print(n = 15)\n\n# A tibble: 5,529,552 × 3\n   item1      item2      correlation\n   &lt;chr&gt;      &lt;chr&gt;            &lt;dbl&gt;\n 1 ottawa     newcastle        0.977\n 2 newcastle  ottawa           0.977\n 3 briggs     joanna           0.967\n 4 joanna     briggs           0.967\n 5 scholar    google           0.938\n 6 google     scholar          0.938\n 7 obsessive  compulsive       0.929\n 8 compulsive obsessive        0.929\n 9 nervosa    anorexia         0.893\n10 anorexia   nervosa          0.893\n11 ci         95               0.887\n12 95         ci               0.887\n13 las        los              0.886\n14 los        las              0.886\n15 gay        bisexual         0.861\n# ℹ 5,529,537 more rows\n\n\n\n\nSpezifische “Partner” in spezifischen Umgebungen\n\nreview_pairs_corr %&gt;% #| \n  filter(\n    item1 %in% c(\n      \"review\",\n      \"literature\",\n      \"systematic\")\n    ) %&gt;% \n  group_by(item1) %&gt;% \n  slice_max(correlation, n = 5) %&gt;% \n  ungroup() %&gt;% \n  mutate(\n    item2 = reorder(item2, correlation)\n    ) %&gt;% \n  ggplot(\n    aes(item2, correlation, fill = item1)\n    ) +\n  geom_bar(stat = \"identity\") +\n  facet_wrap(~ item1, scales = \"free_y\") +\n  coord_flip() +\n  scale_fill_manual(\n    values = c(\n      \"#04316A\",\n      \"#C50F3C\",\n      \"#00B2D1\")) +\n  theme_pubr()\n\n\n\n\n\n\n\n\n\n\nDie häufigsten “positiven” und “negativen” Wörter in den Abstracts\n\nreview_sentiment_count &lt;- review_tidy %&gt;% \n  inner_join(\n     get_sentiments(\"bing\"),\n     by = c(\"text\" = \"word\"),\n     relationship = \"many-to-many\") %&gt;% \n  count(text, sentiment)\n  \n# Preview\nreview_sentiment_count %&gt;% \n  group_by(sentiment) %&gt;%\n  slice_max(n, n = 10) %&gt;% \n  ungroup() %&gt;% \n  mutate(text = reorder(text, n)) %&gt;%\n  ggplot(aes(n, text, fill = sentiment)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(\n    ~sentiment, scales = \"free_y\") +\n  labs(x = \"Contribution to sentiment\",\n       y = NULL) +\n  scale_fill_manual(\n    values = c(\"#C50F3C\", \"#007900\")) +\n  theme_pubr()\n\n\n\n\n\n\n\n\n\n\nVerknüpfung des Sentiemnt (“Scores”) mit den Abstracts\n\nreview_sentiment &lt;- review_tidy %&gt;% \n  inner_join(\n     get_sentiments(\"bing\"),\n     by = c(\"text\" = \"word\"),\n     relationship = \"many-to-many\") %&gt;% \n  count(id, sentiment) %&gt;% \n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %&gt;% \n  mutate(sentiment = positive - negative)\n  \n# Check\nreview_sentiment \n\n# A tibble: 35,710 × 4\n   id                               negative positive sentiment\n   &lt;chr&gt;                               &lt;int&gt;    &lt;int&gt;     &lt;int&gt;\n 1 https://openalex.org/W1000529773        2        2         0\n 2 https://openalex.org/W1006561082        0        1         1\n 3 https://openalex.org/W100685805         4       15        11\n 4 https://openalex.org/W1007410967        0        7         7\n 5 https://openalex.org/W1008209175        8        1        -7\n 6 https://openalex.org/W1009104829        2        4         2\n 7 https://openalex.org/W1009607471       15        8        -7\n 8 https://openalex.org/W1031503832       13        6        -7\n 9 https://openalex.org/W1035654938       10        5        -5\n10 https://openalex.org/W1044055445        5        0        -5\n# ℹ 35,700 more rows\n\n\n\n\nVerteilung des Sentiment (Scores) in den Abstracts\n\n\n[1] 0.4858737\n\n\n\nreview_sentiment %&gt;% \n  ggplot(aes(sentiment)) +\n  geom_histogram(binwidth = 0.5, fill = \"#FF707F\") +\n  labs(\n    x = \"Sentiment (Score) des Abstracts\", \n    y = \"Anzahl der Einträge\"\n  ) +\n  theme_pubr() +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\n\n\n\n\n\n\n\n\nEntwicklung des Sentiment (Scores) der Abstracts im Zeitverlauf\n\n# Create first graph\ng1 &lt;- review_works_correct %&gt;% \n  filter(id %in% review_sentiment$id) %&gt;% \n  left_join(review_sentiment, by = join_by(id)) %&gt;% \n  sjmisc::rec(\n    sentiment,\n    rec = \"min:-2=negative; -1:1=neutral; 2:max=positive\") %&gt;% \n  ggplot(aes(x = publication_year_fct, fill = as.factor(sentiment_r))) +\n    geom_bar() +\n    labs(\n      x = \"\",\n      y = \"Anzahl der Einträge\", \n      fill = \"Sentiment (Score)\") +\n    scale_fill_manual(values = c(\"#C50F3C\", \"#90A0AF\", \"#007900\")) +\n    theme_pubr() \n    #theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n# Create second graph\ng2 &lt;- review_works_correct %&gt;% \n  filter(id %in% review_sentiment$id) %&gt;% \n  left_join(review_sentiment, by = join_by(id)) %&gt;% \n  sjmisc::rec(\n    sentiment,\n    rec = \"min:-2=negative; -1:1=neutral; 2:max=positive\") %&gt;% \n  ggplot(aes(x = publication_year_fct, fill = as.factor(sentiment_r))) +\n    geom_bar(position = \"fill\") +\n    labs(\n      x = \"\",\n      y = \"Anteil der Einträge\", \n      fill = \"Sentiment (Score)\") +\n    scale_fill_manual(values = c(\"#C50F3C\", \"#90A0AF\", \"#007D29\")) +\n    theme_pubr() \n\n# COMBINE GRPAHS\nggarrange(g1, g2,\n          nrow = 1, ncol = 2, \n          align = \"hv\",\n          common.legend = TRUE)"
  },
  {
    "objectID": "exercises/ms-exercise-08_solution.html",
    "href": "exercises/ms-exercise-08_solution.html",
    "title": "Text processing with R",
    "section": "",
    "text": "Link to source file"
  },
  {
    "objectID": "exercises/ms-exercise-08_solution.html#background",
    "href": "exercises/ms-exercise-08_solution.html#background",
    "title": "Text processing with R",
    "section": "Background",
    "text": "Background\n\n\n\n\n\n\nTodays’s data basis: OpenAlex\n\n\n\n\nVia API bzw. openalexR (Aria et al. 2024) gesammelte “works” der Datenbank OpenAlex mit Bezug zu Literaturriews in den Sozialwissenschaften zwischen 2013 und 2023\nDetaillierte Informationen und Ergebnisse zur Suchquery finden Sie hier."
  },
  {
    "objectID": "exercises/ms-exercise-08_solution.html#preparation",
    "href": "exercises/ms-exercise-08_solution.html#preparation",
    "title": "Text processing with R",
    "section": "Preparation",
    "text": "Preparation\n\n\n\n\n\n\nWichtige Information\n\n\n\n\nBitte stellen Sie sicher, dass Sie das jeweilige R-Studio Projekt zur Übung geöffnet haben. Nur so funktionieren alle Dependencies korrekt.\nUm den einwandfreien Ablauf der Übung zu gewährleisten, wird für die Aufgaben auf eine eigenständige Datenerhebung verzichtet und ein Übungsdatensatz zu verfügung gestelt.\n\n\n\n\nPackages\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n    here, qs, # file management\n    magrittr, janitor, # data wrangling\n    easystats, sjmisc, # data analysis\n    gt, gtExtras, # table visualization\n    ggpubr, ggwordcloud, # visualization\n    tidytext, widyr, # text analysis    \n    openalexR, \n    tidyverse # load last to avoid masking issues\n  )\n\n\n\nImport und Vorverarbeitung der Daten\n\n# Import from local\nreview_works &lt;- qs::qread(here(\"data/session-07/openalex-review_works-2013_2023.qs\"))\n\n# Create correct data\nreview_works_correct &lt;- review_works %&gt;% \n    mutate(\n        # Create additional factor variables\n        publication_year_fct = as.factor(publication_year), \n        type_fct = as.factor(type)\n        )"
  },
  {
    "objectID": "exercises/ms-exercise-08_solution.html#praktische-anwendung",
    "href": "exercises/ms-exercise-08_solution.html#praktische-anwendung",
    "title": "Text processing with R",
    "section": "🛠️ Praktische Anwendung",
    "text": "🛠️ Praktische Anwendung\n\n\n\n\n\n\nAchtung, bitte lesen!\n\n\n\n\nBevor Sie mit der Arbeit an den folgenden 📋 Exercises beginnen, stellen Sie bitte sicher, dass Sie alle Chunks des Abschnitts Preparation gerendert haben. Das können Sie tun, indem Sie den “Run all chunks above”-Knopf des nächsten Chunks benutzen.\nBei Fragen zum Code lohnt sich ein Blick in den Showcase (.qmd oder .html). Beim Showcase handelt es sich um eine kompakte Darstellung des in der Präsentation verwenden R-Codes. Sie können das Showcase also nutzen, um sich die Code-Bausteine anzusehen, die für die R-Outputs auf den Slides benutzt wurden.\n\n\n\n\n📋 Exercise 1: Neues Subsample\n\n\n\n\n\n\nZiel der Aufgabe\n\n\n\n\nErstellung eines neuen Datensatzes review_subsample_new, der sich auf englischsprachig Bücher bzw. Buchrartikel beschränkt.\n\n\n\n\nErstellen Sie einen neuen Datensatz review_subsample_new\n\nBasierend auf dem Datensatzes review_works_correct:\n\nNutzen Sie die filter()-Funktion, um\n\nnur englischsprachige (language),\nBücher und Buchkapitel (type) herauszufiltern.\n\nSpeichern Sie diese Umwandlung in einem neuen Datensatz mit dem Namen review_subsample_new\n\n\nÜberprüfen Sie die Transformation mit Hilfe der glimpse()-Funktion.\n✍️ Notieren Sie, wie viele Artikel im neuen Subsample enthalten sind.\n\n\n\nLösung anzeigen\n# Erstellung Subsample\nreview_subsample_new &lt;- review_works_correct %&gt;% \n  # Eingrenzung: Sprache und Typ\n  filter(language == \"en\") %&gt;% \n  filter(type == \"preprint\")\n\n# Überprüfung\nreview_subsample_new %&gt;% glimpse\n\n\nRows: 3,547\nColumns: 41\n$ id                          &lt;chr&gt; \"https://openalex.org/W4236476849\", \"https…\n$ title                       &lt;chr&gt; \"The PRISMA 2020 statement: an updated gui…\n$ display_name                &lt;chr&gt; \"The PRISMA 2020 statement: an updated gui…\n$ author                      &lt;list&gt; [&lt;data.frame[26 x 12]&gt;], [&lt;data.frame[3 x…\n$ ab                          &lt;chr&gt; \"Background: The Preferred Reporting Items…\n$ publication_date            &lt;chr&gt; \"2020-09-14\", \"2019-07-01\", \"2017-04-01\", …\n$ relevance_score             &lt;dbl&gt; 584.98030, 253.79811, 214.51546, 199.14885…\n$ so                          &lt;chr&gt; NA, \"Technological forecasting & social ch…\n$ so_id                       &lt;chr&gt; NA, \"https://openalex.org/S39307421\", \"htt…\n$ host_organization           &lt;chr&gt; NA, \"Elsevier BV\", \"Elsevier BV\", \"Faculty…\n$ issn_l                      &lt;chr&gt; NA, \"0040-1625\", \"0959-6526\", \"2046-1402\",…\n$ url                         &lt;chr&gt; \"https://doi.org/10.31222/osf.io/v7gm2\", \"…\n$ pdf_url                     &lt;chr&gt; \"https://osf.io/v7gm2/download\", NA, NA, \"…\n$ license                     &lt;chr&gt; NA, NA, NA, \"cc-by\", NA, NA, \"cc-by\", NA, …\n$ version                     &lt;chr&gt; \"submittedVersion\", NA, NA, \"publishedVers…\n$ first_page                  &lt;chr&gt; NA, \"251\", \"1278\", \"588\", \"281\", \"113113\",…\n$ last_page                   &lt;chr&gt; NA, \"269\", \"1302\", \"588\", \"312\", \"113113\",…\n$ volume                      &lt;chr&gt; NA, \"144\", \"149\", \"6\", \"32\", \"125\", \"6\", N…\n$ issue                       &lt;chr&gt; NA, NA, NA, NA, \"3-4\", NA, NA, NA, NA, NA,…\n$ is_oa                       &lt;lgl&gt; TRUE, FALSE, FALSE, TRUE, FALSE, FALSE, TR…\n$ is_oa_anywhere              &lt;lgl&gt; TRUE, FALSE, FALSE, TRUE, FALSE, FALSE, TR…\n$ oa_status                   &lt;chr&gt; \"green\", \"closed\", \"closed\", \"gold\", \"clos…\n$ oa_url                      &lt;chr&gt; \"https://osf.io/v7gm2/download\", NA, NA, \"…\n$ any_repository_has_fulltext &lt;lgl&gt; TRUE, FALSE, FALSE, TRUE, FALSE, FALSE, TR…\n$ language                    &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", …\n$ grants                      &lt;list&gt; NA, NA, &lt;\"https://openalex.org/F432032109…\n$ cited_by_count              &lt;int&gt; 3320, 222, 153, 190, 105, 140, 132, 57, 95…\n$ counts_by_year              &lt;list&gt; [&lt;data.frame[5 x 2]&gt;], [&lt;data.frame[7 x 2…\n$ publication_year            &lt;int&gt; 2020, 2019, 2017, 2017, 2019, 2019, 2017, …\n$ cited_by_api_url            &lt;chr&gt; \"https://api.openalex.org/works?filter=cit…\n$ ids                         &lt;list&gt; &lt;\"https://openalex.org/W4236476849\", \"htt…\n$ doi                         &lt;chr&gt; \"https://doi.org/10.31222/osf.io/v7gm2\", \"…\n$ type                        &lt;chr&gt; \"preprint\", \"preprint\", \"preprint\", \"prepr…\n$ referenced_works            &lt;list&gt; &lt;\"https://openalex.org/W2022190222\", \"htt…\n$ related_works               &lt;list&gt; &lt;\"https://openalex.org/W2921208823\", \"htt…\n$ is_paratext                 &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ is_retracted                &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ concepts                    &lt;list&gt; [&lt;data.frame[20 x 5]&gt;], [&lt;data.frame[20 x…\n$ topics                      &lt;list&gt; [&lt;tbl_df[4 x 5]&gt;], [&lt;tbl_df[12 x 5]&gt;], [&lt;…\n$ publication_year_fct        &lt;fct&gt; 2020, 2019, 2017, 2017, 2019, 2019, 2017, …\n$ type_fct                    &lt;fct&gt; preprint, preprint, preprint, preprint, pr…\n\n\nLösung anzeigen\n# Notiz:\n# Subsample enthält 3547 Einträge\n\n\n\n\n📋 Exercise 2: Umwandlung zu ‘tidy text’\n\nErstellen Sie einen neuen Datensatz subsample_new_tidy,\n\nBasierend auf dem Datensatz review_subsample_new, mit folgenden Schritten:\n\nTokenisierung der Abstracts (ab) mit der Funktion unnest_tokens.\nAusschluss von Stoppwörter mit filter und stopwords$words heraus.\nSpeichern Sie diese Umwandlung, indem Sie einen neuen Datensatz mit dem Namen subsample_new_tidy erstellen.\n\n\nPrüfen Sie, ob die Umwandlung erfolgreich war (z.B. mit der Funktion glimpse())\n✍️ Notieren Sie, wie viele Token im neuen Datensatz subsample_new_tidy enthalten sind.\n\n\n\nLösung anzeigen\n# Erstellung des neuen Datensatzes `subsample_new_tidy`\nsubsample_new_tidy &lt;- review_subsample_new %&gt;% \n  tidytext::unnest_tokens(\"text\", ab) %&gt;% \n   filter(!text %in% tidytext::stop_words$word)\n\n# Überprüfung\nsubsample_new_tidy %&gt;% print()\n\n\n# A tibble: 498,535 × 41\n   id     title display_name author publication_date relevance_score so    so_id\n   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;        &lt;list&gt; &lt;chr&gt;                      &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n 1 https… The … The PRISMA … &lt;df&gt;   2020-09-14                  585. &lt;NA&gt;  &lt;NA&gt; \n 2 https… The … The PRISMA … &lt;df&gt;   2020-09-14                  585. &lt;NA&gt;  &lt;NA&gt; \n 3 https… The … The PRISMA … &lt;df&gt;   2020-09-14                  585. &lt;NA&gt;  &lt;NA&gt; \n 4 https… The … The PRISMA … &lt;df&gt;   2020-09-14                  585. &lt;NA&gt;  &lt;NA&gt; \n 5 https… The … The PRISMA … &lt;df&gt;   2020-09-14                  585. &lt;NA&gt;  &lt;NA&gt; \n 6 https… The … The PRISMA … &lt;df&gt;   2020-09-14                  585. &lt;NA&gt;  &lt;NA&gt; \n 7 https… The … The PRISMA … &lt;df&gt;   2020-09-14                  585. &lt;NA&gt;  &lt;NA&gt; \n 8 https… The … The PRISMA … &lt;df&gt;   2020-09-14                  585. &lt;NA&gt;  &lt;NA&gt; \n 9 https… The … The PRISMA … &lt;df&gt;   2020-09-14                  585. &lt;NA&gt;  &lt;NA&gt; \n10 https… The … The PRISMA … &lt;df&gt;   2020-09-14                  585. &lt;NA&gt;  &lt;NA&gt; \n# ℹ 498,525 more rows\n# ℹ 33 more variables: host_organization &lt;chr&gt;, issn_l &lt;chr&gt;, url &lt;chr&gt;,\n#   pdf_url &lt;chr&gt;, license &lt;chr&gt;, version &lt;chr&gt;, first_page &lt;chr&gt;,\n#   last_page &lt;chr&gt;, volume &lt;chr&gt;, issue &lt;chr&gt;, is_oa &lt;lgl&gt;,\n#   is_oa_anywhere &lt;lgl&gt;, oa_status &lt;chr&gt;, oa_url &lt;chr&gt;,\n#   any_repository_has_fulltext &lt;lgl&gt;, language &lt;chr&gt;, grants &lt;list&gt;,\n#   cited_by_count &lt;int&gt;, counts_by_year &lt;list&gt;, publication_year &lt;int&gt;, …\n\n\nLösung anzeigen\n# Notiz:\n# Der neue Datensatz enthält 498535 Token. \n\n\n\n\n📋 Exercise 3: Auswertung der Token\n\nErstellen Sie einen neuen Datensatz subsample_new_summarized,\n\nFassen Sie auf der Grundlage des Datensatzes subsample_new_tidy die Häufigkeit der einzelnen Token zusammen, indem Sie die Funktion count() auf die Variable text anwenden. Verwenden Sie das Argument sort = TRUE, um den Datensatz nach absteigender Häufigkeit der Token zu sortieren.\nSpeichern Sie diese Transformation, indem Sie einen neuen Datensatz mit dem Namen subsample_new_summarized erstellen.\n\nPrüfen Sie, ob die Umwandlung erfolgreich war, indem Sie die Funktion print() verwenden.\n\nVerwenden Sie das Argument n = 50, um die 50 wichtigsten Token anzuzeigen (nur möglich, wenn das Argument sort = TRUE bei der Ausführung der Funktion count() verwendet wurde)\n\nVerteilung der Token prüfen\n\nVerwenden Sie die Funktion datawizard::describe_distribution(), um verschiedene Verteilungsparameter des neuen Datensatzes zu überprüfen\n✍️ Notieren Sie, wie viele Token ein Abstract durchschnittlich enthält.\n\n\n\nOptional: Ergebnisse mit einer Wortwolke überprüfen\n\nBasierend auf dem sortierten Datensatz subsample_new_summarized\n\nAuswahl der 50 häufigsten Token mit Hilfe der Funktion top_n()\nErstellen Sie eine ggplot()-Basis mit label = text und size = n als aes() und\nBenutze ggwordcloud::geom_text_wordclout() um die Wortwolke zu erstellen.\nVerwenden Sie scale_size_are(), um die Skalierung der Wortwolke zu übernehmen.\nVerwenden Sie theme_minimal() für eine saubere Visualisierung.\n\n\n\n\n\nLösung anzeigen\n# Erstellung des neuen Datensatzes `subsample_new_summmarized`\nsubsample_new_summmarized &lt;- subsample_new_tidy %&gt;% \n  count(text, sort = TRUE) \n\n# Preview Top 50 token\nsubsample_new_summmarized %&gt;% \n    print(n = 50)\n\n\n# A tibble: 22,001 × 2\n   text              n\n   &lt;chr&gt;         &lt;int&gt;\n 1 studies        7115\n 2 review         5857\n 3 title          5223\n 4 sec            5133\n 5 health         4591\n 6 systematic     4197\n 7 research       3597\n 8 results        2981\n 9 study          2949\n10 literature     2881\n11 data           2764\n12 analysis       2600\n13 interventions  2532\n14 included       2491\n15 methods        2469\n16 evidence       2229\n17 meta           2122\n18 quality        2069\n19 based          2017\n20 mental         1998\n21 reviews        1812\n22 social         1752\n23 articles       1734\n24 risk           1653\n25 search         1540\n26 outcomes       1515\n27 conducted      1498\n28 lt             1481\n29 background     1434\n30 19             1426\n31 identified     1420\n32 factors        1307\n33 effects        1300\n34 covid          1279\n35 findings       1254\n36 care           1248\n37 95             1247\n38 gt             1245\n39 published      1241\n40 related        1234\n41 abstract       1214\n42 databases      1211\n43 reported       1191\n44 2              1138\n45 conclusions    1133\n46 effect         1116\n47 ci             1111\n48 intervention   1110\n49 criteria       1077\n50 support        1065\n# ℹ 21,951 more rows\n\n\nLösung anzeigen\n# Check distribution parameters \nsubsample_new_summmarized %&gt;%\n  datawizard::describe_distribution()\n\n\nVariable |  Mean |     SD | IQR |           Range | Skewness | Kurtosis |     n | n_Missing\n-------------------------------------------------------------------------------------------\nn        | 22.66 | 136.91 |   7 | [1.00, 7115.00] |    24.15 |   858.87 | 22001 |         0\n\n\nLösung anzeigen\n# Notiz:\n# Ein Absatz enthält durchschnittlich 22 Token. \n\n# Optional: Check results with a wordcloud\nsubsample_new_summmarized %&gt;% \n    top_n(50) %&gt;% \n    ggplot(aes(label = text, size = n)) +\n    ggwordcloud::geom_text_wordcloud() +\n    scale_size_area(max_size = 15) +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n📋 Exercise 4: Wortbeziehungen im Fokus\n\n4.1 Couting word pairs\n\nZählen von häufigen Wortpaaren\n\nZählen Sie auf der Grundlage des Datensatzes subsample_new_tidy Wortpaare mit widyr::pairwise_count(), mit den Argumenten item = text, feature = id und sort = TRUE.\nSpeichern Sie diese Transformation, indem Sie einen neuen Datensatz mit dem Namen subsample_new_word_pairs erstellen.\n\nPrüfen Sie, ob die Umwandlung erfolgreich war, indem Sie die Funktion print() verwenden.\n\nVerwenden Sie das Argument n = 50, um die 50 wichtigsten Token anzuzeigen (nur möglich, wenn bei der Ausführung der Funktion count() das Argument sort = TRUE verwendet wurde)\n\n\n\n\nLösung anzeigen\n# Couting word pairs among sections\nsubsample_new_word_pairs &lt;- subsample_new_tidy %&gt;% \n  widyr::pairwise_count(\n    item = text,\n    feature = id,\n    sort = TRUE)\n\n# Check \nsubsample_new_word_pairs %&gt;% print(n = 50)\n\n\n# A tibble: 12,662,794 × 3\n   item1      item2          n\n   &lt;chr&gt;      &lt;chr&gt;      &lt;dbl&gt;\n 1 review     systematic  2068\n 2 systematic review      2068\n 3 studies    review      1909\n 4 review     studies     1909\n 5 studies    systematic  1668\n 6 results    review      1668\n 7 review     results     1668\n 8 systematic studies     1668\n 9 studies    results     1502\n10 results    studies     1502\n11 research   review      1483\n12 review     research    1483\n13 results    systematic  1478\n14 systematic results     1478\n15 literature review      1471\n16 review     literature  1471\n17 methods    review      1453\n18 review     methods     1453\n19 methods    results     1387\n20 results    methods     1387\n21 study      review      1336\n22 review     study       1336\n23 methods    systematic  1320\n24 systematic methods     1320\n25 studies    methods     1306\n26 methods    studies     1306\n27 included   review      1236\n28 review     included    1236\n29 review     background  1227\n30 background review      1227\n31 methods    background  1224\n32 background methods     1224\n33 research   systematic  1216\n34 systematic research    1216\n35 results    background  1204\n36 background results     1204\n37 included   studies     1198\n38 studies    included    1198\n39 study      studies     1197\n40 studies    study       1197\n41 study      systematic  1195\n42 systematic study       1195\n43 research   studies     1178\n44 studies    research    1178\n45 health     review      1153\n46 review     health      1153\n47 analysis   review      1152\n48 review     analysis    1152\n49 studies    background  1148\n50 background studies     1148\n# ℹ 12,662,744 more rows\n\n\n\n\n4.2 Pairwise correlation\n\nErmittlung der paarweisen Korrelation\n\nBasierend auf dem Datensatz subsample_new_tidy,\ngruppieren Sie die Daten mit der Funktion group_by() nach der Variable text und\nverwenden Sie filter(n() &gt;= X), um nur Token zu verwenden, die mindestens in einer bestimmte Anzahl (X) vorkommen; Sie können für X einen Wert Ihrer Wahl wählen, ich würde jedoch dringend empfehlen, ein X &gt; 100 zu wählen, da die folgende Funktion sonst möglicherweise nicht in der Lage ist, die Berechnung durchzuführen.\nErstellen Sie Wortkorrelationen mit widyr::pairwise_cor(), mit den Argumenten item = text,feature = id und sort = TRUE.\nSpeichern Sie diese Transformation, indem Sie einen neuen Datensatz mit dem Namen subsample_new_corr erstellen.\n\nPrüfen Sie die Paare mit der höchsten Korrelation mit der Funktion print()..\n\n\n\nLösung anzeigen\n# Getting pairwise correlation \nsubsample_new_corr &lt;- subsample_new_tidy %&gt;% \n  group_by(text) %&gt;% \n  filter(n() &gt;= 250) %&gt;% \n  pairwise_cor(text, id, sort = TRUE)\n\n# Check pairs with highest correlation\nsubsample_new_corr %&gt;% print(n = 50)\n\n\n# A tibble: 131,406 × 3\n   item1        item2        correlation\n   &lt;chr&gt;        &lt;chr&gt;              &lt;dbl&gt;\n 1 sec          title              0.935\n 2 title        sec                0.935\n 3 ci           95                 0.881\n 4 95           ci                 0.881\n 5 19           covid              0.844\n 6 covid        19                 0.844\n 7 items        preferred          0.835\n 8 preferred    items              0.835\n 9 pandemic     covid              0.812\n10 covid        pandemic           0.812\n11 vaccination  vaccine            0.796\n12 vaccine      vaccination        0.796\n13 trials       controlled         0.763\n14 controlled   trials             0.763\n15 web          science            0.755\n16 science      web                0.755\n17 sec          objective          0.715\n18 objective    sec                0.715\n19 srs          sr                 0.710\n20 sr           srs                0.710\n21 randomized   controlled         0.708\n22 controlled   randomized         0.708\n23 pandemic     19                 0.695\n24 19           pandemic           0.695\n25 objective    title              0.669\n26 title        objective          0.669\n27 gt           lt                 0.662\n28 lt           gt                 0.662\n29 trials       randomized         0.658\n30 randomized   trials             0.658\n31 depression   anxiety            0.650\n32 anxiety      depression         0.650\n33 bold         ns4                0.636\n34 ns4          bold               0.636\n35 methods      background         0.613\n36 background   methods            0.613\n37 inclusion    criteria           0.607\n38 criteria     inclusion          0.607\n39 reporting    preferred          0.599\n40 preferred    reporting          0.599\n41 items        reporting          0.599\n42 reporting    items              0.599\n43 peer         reviewed           0.592\n44 reviewed     peer               0.592\n45 1            2                  0.585\n46 2            1                  0.585\n47 sec          conclusions        0.577\n48 conclusions  sec                0.577\n49 registration prospero           0.574\n50 prospero     registration       0.574\n# ℹ 131,356 more rows\n\n\n\n\n\n📋 Exercise 5: Inhaltlicher Vergleich\n\nVergleichen Sie die Ergebnisse der Übung mit den Auswertungen der Folien:\n\nWie unterscheiden sich die Ergebnisse?\nWürden Sie die Bücher bzw. Buchabschnitte mit in die Untersuchung integrieren?"
  },
  {
    "objectID": "exercises/ms-exercise-08.html",
    "href": "exercises/ms-exercise-08.html",
    "title": "Text processing with R",
    "section": "",
    "text": "Link to source file"
  },
  {
    "objectID": "exercises/ms-exercise-08.html#background",
    "href": "exercises/ms-exercise-08.html#background",
    "title": "Text processing with R",
    "section": "Background",
    "text": "Background\n\n\n\n\n\n\nTodays’s data basis: OpenAlex\n\n\n\n\nVia API bzw. openalexR (Aria et al. 2024) gesammelte “works” der Datenbank OpenAlex mit Bezug zu Literaturriews in den Sozialwissenschaften zwischen 2013 und 2023\nDetaillierte Informationen und Ergebnisse zur Suchquery finden Sie hier."
  },
  {
    "objectID": "exercises/ms-exercise-08.html#preparation",
    "href": "exercises/ms-exercise-08.html#preparation",
    "title": "Text processing with R",
    "section": "Preparation",
    "text": "Preparation\n\n\n\n\n\n\nWichtige Information\n\n\n\n\nBitte stellen Sie sicher, dass Sie das jeweilige R-Studio Projekt zur Übung geöffnet haben. Nur so funktionieren alle Dependencies korrekt.\nUm den einwandfreien Ablauf der Übung zu gewährleisten, wird für die Aufgaben auf eine eigenständige Datenerhebung verzichtet und ein Übungsdatensatz zu verfügung gestelt.\n\n\n\n\nPackages\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n    here, qs, # file management\n    magrittr, janitor, # data wrangling\n    easystats, sjmisc, # data analysis\n    gt, gtExtras, # table visualization\n    ggpubr, ggwordcloud, # visualization\n    tidytext, widyr, # text analysis    \n    openalexR, \n    tidyverse # load last to avoid masking issues\n  )\n\n\n\nImport und Vorverarbeitung der Daten\n\n# Import from local\nreview_works &lt;- qs::qread(here(\"data/session-07/openalex-review_works-2013_2023.qs\"))\n\n# Create correct data\nreview_works_correct &lt;- review_works %&gt;% \n    mutate(\n        # Create additional factor variables\n        publication_year_fct = as.factor(publication_year), \n        type_fct = as.factor(type)\n        )"
  },
  {
    "objectID": "exercises/ms-exercise-08.html#praktische-anwendung",
    "href": "exercises/ms-exercise-08.html#praktische-anwendung",
    "title": "Text processing with R",
    "section": "🛠️ Praktische Anwendung",
    "text": "🛠️ Praktische Anwendung\n\n\n\n\n\n\nAchtung, bitte lesen!\n\n\n\n\nBevor Sie mit der Arbeit an den folgenden 📋 Exercises beginnen, stellen Sie bitte sicher, dass Sie alle Chunks des Abschnitts Preparation gerendert haben. Das können Sie tun, indem Sie den “Run all chunks above”-Knopf des nächsten Chunks benutzen.\nBei Fragen zum Code lohnt sich ein Blick in den Showcase (.qmd oder .html). Beim Showcase handelt es sich um eine kompakte Darstellung des in der Präsentation verwenden R-Codes. Sie können das Showcase also nutzen, um sich die Code-Bausteine anzusehen, die für die R-Outputs auf den Slides benutzt wurden.\n\n\n\n\n📋 Exercise 1: Neues Subsample\n\n\n\n\n\n\nZiel der Aufgabe\n\n\n\n\nErstellung eines neuen Datensatzes review_subsample_new, der sich auf englischsprachig Bücher bzw. Buchrartikel beschränkt.\n\n\n\n\nErstellen Sie einen neuen Datensatz review_subsample_new\n\nBasierend auf dem Datensatzes review_works_correct:\n\nNutzen Sie die filter()-Funktion, um\n\nnur englischsprachige (language),\nBücher und Buchkapitel (type) herauszufiltern.\n\nSpeichern Sie diese Umwandlung in einem neuen Datensatz mit dem Namen review_subsample_new\n\n\nÜberprüfen Sie die Transformation mit Hilfe der glimpse()-Funktion.\n✍️ Notieren Sie, wie viele Artikel im neuen Subsample enthalten sind.\n\n\n# Erstellung Subsample\n\n# Überprüfung\n\n# Notiz:\n# Subsample enthält ... Einträge\n\n\n\n📋 Exercise 2: Umwandlung zu ‘tidy text’\n\nErstellen Sie einen neuen Datensatz subsample_new_tidy,\n\nBasierend auf dem Datensatz review_subsample_new, mit folgenden Schritten:\n\nTokenisierung der Abstracts (ab) mit der Funktion unnest_tokens.\nAusschluss von Stoppwörter mit filter und stopwords$words heraus.\nSpeichern Sie diese Umwandlung, indem Sie einen neuen Datensatz mit dem Namen subsample_new_tidy erstellen.\n\n\nPrüfen Sie, ob die Umwandlung erfolgreich war (z.B. mit der Funktion glimpse())\n✍️ Notieren Sie, wie viele Token im neuen Datensatz subsample_new_tidy enthalten sind.\n\n\n# Erstellung des neuen Datensatzes `subsample_new_tidy`\n\n# Überprüfung\n\n# Notiz:\n# Der neue Datensatz enthält ... Token. \n\n\n\n📋 Exercise 3: Auswertung der Token\n\nErstellen Sie einen neuen Datensatz subsample_new_summarized,\n\nFassen Sie auf der Grundlage des Datensatzes subsample_new_tidy die Häufigkeit der einzelnen Token zusammen, indem Sie die Funktion count() auf die Variable text anwenden. Verwenden Sie das Argument sort = TRUE, um den Datensatz nach absteigender Häufigkeit der Token zu sortieren.\nSpeichern Sie diese Transformation, indem Sie einen neuen Datensatz mit dem Namen subsample_new_summarized erstellen.\n\nPrüfen Sie, ob die Umwandlung erfolgreich war, indem Sie die Funktion print() verwenden.\n\nVerwenden Sie das Argument n = 50, um die 50 wichtigsten Token anzuzeigen (nur möglich, wenn das Argument sort = TRUE bei der Ausführung der Funktion count() verwendet wurde)\n\nVerteilung der Token prüfen\n\nVerwenden Sie die Funktion datawizard::describe_distribution(), um verschiedene Verteilungsparameter des neuen Datensatzes zu überprüfen\n✍️ Notieren Sie, wie viele Token ein Abstract durchschnittlich enthält.\n\n\n\nOptional: Ergebnisse mit einer Wortwolke überprüfen\n\nBasierend auf dem sortierten Datensatz subsample_new_summarized\n\nAuswahl der 50 häufigsten Token mit Hilfe der Funktion top_n()\nErstellen Sie eine ggplot()-Basis mit label = text und size = n als aes() und\nBenutze ggwordcloud::geom_text_wordclout() um die Wortwolke zu erstellen.\nVerwenden Sie scale_size_are(), um die Skalierung der Wortwolke zu übernehmen.\nVerwenden Sie theme_minimal() für eine saubere Visualisierung.\n\n\n\n\n# Erstellung des neuen Datensatzes `subsample_new_summmarized`\n\n# Preview Top 50 token\n\n# Check distribution parameters \n\n# Notiz:\n# Ein Absatz enthält durchschnittlich ... Token. \n\n\n# Optional: Check results with a wordcloud\n\n\n\n📋 Exercise 4: Wortbeziehungen im Fokus\n\n4.1 Couting word pairs\n\nZählen von häufigen Wortpaaren\n\nZählen Sie auf der Grundlage des Datensatzes subsample_new_tidy Wortpaare mit widyr::pairwise_count(), mit den Argumenten item = text, feature = id und sort = TRUE.\nSpeichern Sie diese Transformation, indem Sie einen neuen Datensatz mit dem Namen subsample_new_word_pairs erstellen.\n\nPrüfen Sie, ob die Umwandlung erfolgreich war, indem Sie die Funktion print() verwenden.\n\nVerwenden Sie das Argument n = 50, um die 50 wichtigsten Token anzuzeigen (nur möglich, wenn bei der Ausführung der Funktion count() das Argument sort = TRUE verwendet wurde)\n\n\n\n# Couting word pairs among sections\n\n# Überprüfung \n\n\n\n4.2 Pairwise correlation\n\nErmittlung der paarweisen Korrelation\n\nBasierend auf dem Datensatz subsample_new_tidy,\ngruppieren Sie die Daten mit der Funktion group_by() nach der Variable text und\nverwenden Sie filter(n() &gt;= X), um nur Token zu verwenden, die mindestens in einer bestimmte Anzahl (X) vorkommen; Sie können für X einen Wert Ihrer Wahl wählen, ich würde jedoch dringend empfehlen, ein X &gt; 100 zu wählen, da die folgende Funktion sonst möglicherweise nicht in der Lage ist, die Berechnung durchzuführen.\nErstellen Sie Wortkorrelationen mit widyr::pairwise_cor(), mit den Argumenten item = text,feature = id und sort = TRUE.\nSpeichern Sie diese Transformation, indem Sie einen neuen Datensatz mit dem Namen subsample_new_corr erstellen.\n\nPrüfen Sie die Paare mit der höchsten Korrelation mit der Funktion print()..\n\n\n# Getting pairwise correlation \n\n# Check pairs with highest correlation\n\n\n\n\n📋 Exercise 5: Inhaltlicher Vergleich\n\nVergleichen Sie die Ergebnisse der Übung mit den Auswertungen der Folien:\n\nWie unterscheiden sich die Ergebnisse?\nWürden Sie die Bücher bzw. Buchabschnitte mit in die Untersuchung integrieren?"
  },
  {
    "objectID": "exercises/ms-exercise-07.html",
    "href": "exercises/ms-exercise-07.html",
    "title": "API mining and data wrangling with R",
    "section": "",
    "text": "Link to source file"
  },
  {
    "objectID": "exercises/ms-exercise-07.html#background",
    "href": "exercises/ms-exercise-07.html#background",
    "title": "API mining and data wrangling with R",
    "section": "Background",
    "text": "Background\n\n\n\n\n\n\nTodays’s data basis: OpenAlex\n\n\n\nOpenAlex is a free and open catalog of the global research system. It’s named after the ancient Library of Alexandria and made by the nonprofit OurResearch.\n\n\n\n\n\n\n\n\nAt the heart of OpenAlex is our dataset—a catalog of works. A work is any sort of scholarly output. A research article is one kind of work, but there are others such as datasets, books, and dissertations. We keep track of these works—their titles (and abstracts and full text in many cases), when they were created, etc. But that’s not all we do. We also keep track of the connections between these works, finding associations through things like journals, authors, institutional affiliations, citations, topics, and funders. There are hundreds of millions of works out there, and tens of thousands more being created every day, so it’s important that we have these relationships to help us make sense of research at a large scale."
  },
  {
    "objectID": "exercises/ms-exercise-07.html#preparation",
    "href": "exercises/ms-exercise-07.html#preparation",
    "title": "API mining and data wrangling with R",
    "section": "Preparation",
    "text": "Preparation\n\n\n\n\n\n\nWichtige Information\n\n\n\n\nBitte stellen Sie sicher, dass Sie das jeweilige R-Studio Projekt zur Übung geöffnet haben. Nur so funktionieren alle Dependencies korrekt.\nUm den einwandfreien Ablauf der Übung zu gewährleisten, wird für die Aufgaben auf eine eigenständige Datenerhebung verzichtet und ein Übungsdatensatz zu verfügung gestelt.\n\n\n\n\nPackages\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  here, qs, # file management\n  magrittr, janitor, # data wrangling\n  easystats, sjmisc, # data analysis\n  ggpubr, # visualization\n  openalexR, \n  tidyverse # load last to avoid masking issues\n  )\n\n\n\nImport und Vorverarbeitung der Daten\n\n# Import from local\nreview_works &lt;- qs::qread(here(\"data/session-07/openalex-review_works-2013_2023.qs\"))\n\n# Create correct data\nreview_works_correct &lt;- review_works %&gt;% \n    mutate(\n        # Create additional factor variables\n        publication_year_fct = as.factor(publication_year), \n        type_fct = as.factor(type)\n        )"
  },
  {
    "objectID": "exercises/ms-exercise-07.html#praktische-anwendung",
    "href": "exercises/ms-exercise-07.html#praktische-anwendung",
    "title": "API mining and data wrangling with R",
    "section": "🛠️ Praktische Anwendung",
    "text": "🛠️ Praktische Anwendung\n\n\n\n\n\n\nAchtung, bitte lesen!\n\n\n\n\nBevor Sie mit der Arbeit an den folgenden 📋 Exercises beginnen, stellen Sie bitte sicher, dass Sie alle Chunks des Abschnitts Preparation gerendert haben. Das können Sie tun, indem Sie den “Run all chunks above”-Knopf des nächsten Chunks benutzen.\nBei Fragen zum Code lohnt sich ein Blick in den Showcase (.qmd oder .html). Beim Showcase handelt es sich um eine kompakte Darstellung des in der Präsentation verwenden R-Codes. Sie können das Showcase also nutzen, um sich die Code-Bausteine anzusehen, die für die R-Outputs auf den Slides benutzt wurden.\n\n\n\n\n📋 Exercise 1: Sprache der Publikationen\n\n\n\n\n\n\nZiel der Aufgabe\n\n\n\n\nIdentifizieren Sie die für die Untersuchung relevanten Artikel auf Basis von deren Sprache (language)\nHintergrundinformation zur Variable language finden Sie in der API-Dokumentation von OpenAlex.\n\n\n\n\nNutzen Sie die Funktion sjmisc::frq() und schauen Sie sich im Datensatz review_works_correct die Variablen language an. Verwenden Sie das Argument sort.frq = \"desc\", um die Häufigkeit der Sprachen absteigend zu sortieren.\nNotieren Sie sich den jeweilgen ISO 639-1 language code, um Ihn später bei 📋 Exercise 4: Erstellung Subsample als Filter zu nutzen.\n\n\n# Create frequency table for the variable language\n...\n\n\n\n📋 Exercise 2: Typ der Publikationen\n\n\n\n\n\n\nZiel der Aufgabe\n\n\n\n\nIdentifizieren Sie die für die Untersuchung relevanten Artikel auf Basis deres Typen (type).\nHintergrundinformation zur Variable type finden Sie in der API-Dokumentation von OpenAlex.\n\n\n\n\nNutzen Sie die Funktion sjmisc::frq() und schauen Sie sich im Datensatz review_works_correct die Variablen type an. Verwenden Sie das Argument sort.frq = \"desc\", um die Typen in Abhängigkeit Ihrer Häufigkeit absteigend zu sortieren.\nNotieren Sie sich die Ausprägungen der Variable type, die aus Ihrer Sicht später bei 📋 Exercise 4: Erstellung Subsample als Filter genutzt werden soll.\n\n\n# Create frequency table for the variable type\n...\n\n\n\n📋 Exercise 3: Forschungsfeld der Publikationen\n\n\n\n\n\n\nZiel der Aufgabe\n\n\n\n\nIdentifizieren Sie die für die Untersuchung relevanten Artikel auf Basis des von OpenAlex dem Artikel zugeordnenten Foschungsfeldes (field)\nHintergrundinformation zur Variable field finden Sie in der API-Dokumentation von OpenAlex.\n\n\n\n\nBasierend auf dem Datensatz review_works_correct\n\nnutzen Sie die Funktion unnest() um die Variablen der topics-Liste zu extrahieren. Verwenden Sie dabei das Argument names_sep = \"_\". um doppelte Variablennamen durch Hinzufügen des Prefixes topics_ zu verhindern.\nfiltern Sie anschließen mit Hilfe der Funktion filter und der Variable bzw. dem Argument topics_name == \"field\" nur die Informationen zum Forschungsfeld, sowie mit der Variable bzw. dem Argument topics_i == \"1\" nur die erste Zuordnung.\nNutzen Sie die Funktion sjmisc::frq() und schauen Sie sich die Variablen topics_display_name an. Verwenden Sie das Argument sort.frq = \"desc\", um die Forschungsfelder in Abhängigkeit Ihrer Häufigkeit absteigend zu sortieren.\n\nNotieren Sie sich die Ausprägungen der Variable topics_display_name, die aus Ihrer Sicht später bei 📋 Exercise 4: Erstellung Subsample als Filter genutzt werden soll.\n\n\n# Unnest topis variable and create frequency table for the variable topics_display_name\n...\n\n\n\n📋 Exercise 4: Erstellung Subsample\n\n\n\n\n\n\nZiel der Aufgabe\n\n\n\n\nErstellung sie den Datensatz review_subsample, in dem Sie mit Hilfe der Funktionen select() und/oder filter() das Datenmaterial weiter eingrenzen. Sie können sich sowohl auf die Variablen aus der Übung, als auch auf die aus der Sitzung (bzw. den Slides) beziehen.\nDer Code dieses Chunks wird in der nächsten Sitzung benötigt bzw. besprochen, halten Sie diesen deshalb bitte bereit.\n\n\n\n\nreview_subsample &lt;- review_works_correct %&gt;% \n    filter(...)"
  },
  {
    "objectID": "exercises/ms-showcase-10.html",
    "href": "exercises/ms-showcase-10.html",
    "title": "Unsupervised Machine Learning II",
    "section": "",
    "text": "Link to slides"
  },
  {
    "objectID": "exercises/ms-showcase-10.html#preparation",
    "href": "exercises/ms-showcase-10.html#preparation",
    "title": "Unsupervised Machine Learning II",
    "section": "Preparation",
    "text": "Preparation\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n    here, qs, # file management\n    magrittr, janitor, # data wrangling\n    easystats, sjmisc, # data analysis\n    gt, gtExtras, # table visualization\n    ggpubr, ggwordcloud, # visualization\n    # text analysis    \n    tidytext, widyr, # based on tidytext\n    quanteda, # based on quanteda\n    quanteda.textmodels, quanteda.textplots, quanteda.textstats, \n    stm, # structural topic modeling\n    openalexR, pushoverr, tictoc, \n    tidyverse # load last to avoid masking issues\n  )\n\n\nreview_works &lt;- qs::qread(here(\"data/session-07/openalex-review_works-2013_2023.qs\"))\n\n# Create correct data\nreview_subsample &lt;- review_works %&gt;% \n    # Create additional factor variables\n    mutate(\n        publication_year_fct = as.factor(publication_year), \n        type_fct = as.factor(type)\n        ) %&gt;%\n    # Eingrenzung: Sprache und Typ\n    filter(language == \"en\") %&gt;% \n    filter(type == \"article\") %&gt;%\n    # Datentranformation\n    unnest(topics, names_sep = \"_\") %&gt;%\n    filter(topics_name == \"field\") %&gt;% \n    filter(topics_i == \"1\") %&gt;% \n    # Eingrenzung: Forschungsfeldes\n    filter(\n    topics_display_name == \"Social Sciences\"|\n    topics_display_name == \"Psychology\"\n    ) %&gt;% \n    mutate(\n        field = as.factor(topics_display_name)\n    ) %&gt;% \n    # Eingrenzung: Keine Einträge ohne Abstract\n    filter(!is.na(ab))\n\n\n# Create corpus\nquanteda_corpus &lt;- review_subsample %&gt;% \n  quanteda::corpus(\n    docid_field = \"id\", \n    text_field = \"ab\"\n  )\n\n# Tokenize\nquanteda_token &lt;- quanteda_corpus %&gt;% \n  quanteda::tokens(\n    remove_punct = TRUE,\n    remove_symbols = TRUE, \n    remove_numbers = TRUE, \n    remove_url = TRUE, \n    split_tags = FALSE # keep hashtags and mentions\n  ) %&gt;% \n  quanteda::tokens_tolower() %&gt;% \n  quanteda::tokens_remove(\n    pattern = stopwords(\"en\")\n    )\n\n# Convert to Document-Feature-Matrix (DFM)\nquanteda_dfm &lt;- quanteda_token %&gt;% \n  quanteda::dfm()\n\n# Pruning\nquanteda_dfm_trim &lt;- quanteda_dfm %&gt;% \n  dfm_trim( \n    min_docfreq = 10/nrow(review_subsample),\n    max_docfreq = 0.99, \n    docfreq_type = \"prop\")\n\n# Convert for stm topic modeling\nquanteda_stm &lt;- quanteda_dfm_trim %&gt;% \n   convert(to = \"stm\")"
  },
  {
    "objectID": "exercises/ms-showcase-10.html#codechunks-aus-der-sitzung",
    "href": "exercises/ms-showcase-10.html#codechunks-aus-der-sitzung",
    "title": "Unsupervised Machine Learning II",
    "section": "Codechunks aus der Sitzung",
    "text": "Codechunks aus der Sitzung\n\n(Re-)Estimation of k = 20 with metadata\n\n# Estimate model\nstm_mdl_k20 &lt;- stm::stm(\n    documents = quanteda_stm$documents,\n    vocab = quanteda_stm$vocab, \n    prevalence =~ publication_year_fct + field, \n    K = 20, \n    seed = 42,\n    max.em.its = 1000,\n    data = quanteda_stm$meta,\n    init.type = \"Spectral\",\n    verbose = TRUE)\n\n\n# Overview\nstm_mdl_k20\n\nA topic model with 20 topics, 36650 documents and a 14322 word dictionary.\n\n\n\n\nErweiterte Auswertungen\n\nBeta-Matrix\n\n# Create tidy beta matrix\ntd_beta &lt;- tidy(stm_mdl_k20)\n\n# Output \ntd_beta\n\n# A tibble: 286,440 × 3\n   topic term      beta\n   &lt;int&gt; &lt;chr&gt;    &lt;dbl&gt;\n 1     1 #x0d  6.83e-44\n 2     2 #x0d  2.37e-30\n 3     3 #x0d  3.35e-45\n 4     4 #x0d  7.24e- 3\n 5     5 #x0d  1.45e-23\n 6     6 #x0d  5.23e-20\n 7     7 #x0d  2.70e-12\n 8     8 #x0d  3.32e-39\n 9     9 #x0d  8.78e- 4\n10    10 #x0d  2.58e-12\n# ℹ 286,430 more rows\n\n\n\n# Create top terms\ntop_terms &lt;- td_beta %&gt;%\n  arrange(beta) %&gt;%\n  group_by(topic) %&gt;%\n  top_n(7, beta) %&gt;%\n  arrange(-beta) %&gt;%\n  select(topic, term) %&gt;%\n  summarise(terms = list(term)) %&gt;%\n  mutate(terms = map(terms, paste, collapse = \", \")) %&gt;% \n  unnest(cols = c(terms))\n\n# Output\ntop_terms\n\n# A tibble: 20 × 2\n   topic terms                                                                  \n   &lt;int&gt; &lt;chr&gt;                                                                  \n 1     1 sleep, studies, eating, cognitive, weight, associated, duration        \n 2     2 health, women, gender, cultural, barriers, men, countries              \n 3     3 treatment, disorders, symptoms, disorder, depression, anxiety, therapy \n 4     4 patients, cancer, nurses, patient, music, nursing, pain                \n 5     5 literature, university, author, gt, lt, et, al                         \n 6     6 prevalence, covid-19, suicide, pandemic, studies, among, risk          \n 7     7 articles, review, study, systematic, results, search, science          \n 8     8 physical, activity, disabilities, exercise, cognitive, adults, body    \n 9     9 learning, education, students, educational, skills, teachers, teaching \n10    10 de, la, y, en, los, 的, el                                             \n11    11 work, study, development, can, public, management, factors             \n12    12 violence, sexual, use, abuse, risk, youth, h3                          \n13    13 health, mental, care, support, people, social, family                  \n14    14 social, use, media, digital, information, technology, communication    \n15    15 ci, effect, meta-analysis, p, studies, effects, trials                 \n16    16 research, literature, review, future, findings, systematic, paper      \n17    17 children, studies, factors, relationship, adolescents, review, associa…\n18    18 measures, assessment, used, studies, tools, measurement, quality       \n19    19 studies, interventions, included, evidence, review, systematic, outcom…\n20    20 programs, school, training, interventions, outcomes, review, intervent…\n\n\n\ntd_beta %&gt;%\n    group_by(topic) %&gt;%\n    slice_max(beta, n = 10) %&gt;%\n    ungroup() %&gt;%\n    ggplot(aes(beta, term)) +\n    geom_col() +\n    facet_wrap(~ topic, scales = \"free\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGamma-Matrix\n\n# Create tidy gamma matrix\ntd_gamma &lt;- tidy(\n  stm_mdl_k20, \n  matrix = \"gamma\", \n  document_names = names(quanteda_stm$documents)\n  )\n\n# Output \ntd_gamma\n\n# A tibble: 733,000 × 3\n   document                         topic    gamma\n   &lt;chr&gt;                            &lt;int&gt;    &lt;dbl&gt;\n 1 https://openalex.org/W4293003987     1 0.00473 \n 2 https://openalex.org/W2750168540     1 0.000270\n 3 https://openalex.org/W1998933811     1 0.00368 \n 4 https://openalex.org/W2547134104     1 0.00340 \n 5 https://openalex.org/W3047898105     1 0.00263 \n 6 https://openalex.org/W2149640470     1 0.00230 \n 7 https://openalex.org/W2740726397     1 0.0374  \n 8 https://openalex.org/W2974087526     1 0.00339 \n 9 https://openalex.org/W2195703978     1 0.00287 \n10 https://openalex.org/W2093916237     1 0.170   \n# ℹ 732,990 more rows\n\n\n\n\nHäufigkeit und Top Begriffe der Themen\n\nprevalence &lt;- td_gamma %&gt;%\n  group_by(topic) %&gt;%\n  summarise(gamma = mean(gamma)) %&gt;%\n  arrange(desc(gamma)) %&gt;%\n  left_join(top_terms, by = \"topic\") %&gt;%\n  mutate(topic = paste0(\"Topic \",sprintf(\"%02d\", topic)),\n         topic = reorder(topic, gamma))\n\n\nTable\n\nprevalence %&gt;% \n  gt() %&gt;% \n  fmt_number(\n    columns = c(gamma), \n    decimals = 2) %&gt;% \n  gtExtras::gt_theme_538()\n\n\n\n\n\n\n\ntopic\ngamma\nterms\n\n\n\n\nTopic 16\n0.12\nresearch, literature, review, future, findings, systematic, paper\n\n\nTopic 19\n0.09\nstudies, interventions, included, evidence, review, systematic, outcomes\n\n\nTopic 07\n0.08\narticles, review, study, systematic, results, search, science\n\n\nTopic 11\n0.07\nwork, study, development, can, public, management, factors\n\n\nTopic 09\n0.06\nlearning, education, students, educational, skills, teachers, teaching\n\n\nTopic 17\n0.06\nchildren, studies, factors, relationship, adolescents, review, associated\n\n\nTopic 13\n0.06\nhealth, mental, care, support, people, social, family\n\n\nTopic 14\n0.05\nsocial, use, media, digital, information, technology, communication\n\n\nTopic 15\n0.05\nci, effect, meta-analysis, p, studies, effects, trials\n\n\nTopic 03\n0.04\ntreatment, disorders, symptoms, disorder, depression, anxiety, therapy\n\n\nTopic 06\n0.04\nprevalence, covid-19, suicide, pandemic, studies, among, risk\n\n\nTopic 18\n0.04\nmeasures, assessment, used, studies, tools, measurement, quality\n\n\nTopic 20\n0.04\nprograms, school, training, interventions, outcomes, review, intervention\n\n\nTopic 02\n0.04\nhealth, women, gender, cultural, barriers, men, countries\n\n\nTopic 01\n0.03\nsleep, studies, eating, cognitive, weight, associated, duration\n\n\nTopic 05\n0.03\nliterature, university, author, gt, lt, et, al\n\n\nTopic 12\n0.03\nviolence, sexual, use, abuse, risk, youth, h3\n\n\nTopic 04\n0.03\npatients, cancer, nurses, patient, music, nursing, pain\n\n\nTopic 08\n0.03\nphysical, activity, disabilities, exercise, cognitive, adults, body\n\n\nTopic 10\n0.00\nde, la, y, en, los, 的, el\n\n\n\n\n\n\n\n\n\nVisualization\n\nprevalence %&gt;%\n  ggplot(aes(topic, gamma, label = terms, fill = topic)) +\n  geom_col(show.legend = FALSE) +\n  geom_text(hjust = 0, nudge_y = 0.0005, size = 3) +\n  coord_flip() +\n  scale_y_continuous(\n    expand = c(0,0),\n    limits = c(0, 0.18)) +\n  theme_pubr() +\n  theme(\n    plot.title = element_text(size = 16),\n    plot.subtitle = element_text(size = 13)) +\n  labs(\n    x = NULL, y = expression(gamma),\n    title = \"Topic Prevalence in the OpenAlex Corpus\",\n    subtitle = \"With the top seven words that contribute to each topic\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEffect metadata\n\neffects &lt;- estimateEffect(\n  1:20 ~ publication_year_fct + field, \n  stm_mdl_k20, \n  meta = quanteda_stm$meta)\n\n\n# Comparison\n# Effects of covariates on Topic 6\neffects %&gt;% summary(topics = 6)\n\n\nCall:\nestimateEffect(formula = 1:20 ~ publication_year_fct + field, \n    stmobj = stm_mdl_k20, metadata = quanteda_stm$meta)\n\n\nTopic 6:\n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               0.039252   0.004311   9.106  &lt; 2e-16 ***\npublication_year_fct2014 -0.001077   0.005325  -0.202  0.83966    \npublication_year_fct2015 -0.000671   0.005540  -0.121  0.90359    \npublication_year_fct2016 -0.002200   0.005259  -0.418  0.67572    \npublication_year_fct2017 -0.002814   0.005098  -0.552  0.58098    \npublication_year_fct2018 -0.003452   0.005190  -0.665  0.50595    \npublication_year_fct2019 -0.001256   0.004941  -0.254  0.79935    \npublication_year_fct2020  0.006933   0.004570   1.517  0.12931    \npublication_year_fct2021  0.023013   0.004810   4.784 1.73e-06 ***\npublication_year_fct2022  0.025927   0.004562   5.683 1.33e-08 ***\npublication_year_fct2023  0.013690   0.004672   2.930  0.00339 ** \nfieldSocial Sciences     -0.010761   0.001468  -7.333 2.30e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Effects of covariates on Topic 16\neffects %&gt;% summary(topics = 16)\n\n\nCall:\nestimateEffect(formula = 1:20 ~ publication_year_fct + field, \n    stmobj = stm_mdl_k20, metadata = quanteda_stm$meta)\n\n\nTopic 16:\n\nCoefficients:\n                           Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               0.0633442  0.0054302  11.665   &lt;2e-16 ***\npublication_year_fct2014  0.0063400  0.0073414   0.864   0.3878    \npublication_year_fct2015  0.0140646  0.0069142   2.034   0.0419 *  \npublication_year_fct2016  0.0053490  0.0067670   0.790   0.4293    \npublication_year_fct2017  0.0073625  0.0063991   1.151   0.2499    \npublication_year_fct2018  0.0022709  0.0064811   0.350   0.7261    \npublication_year_fct2019 -0.0012180  0.0059953  -0.203   0.8390    \npublication_year_fct2020  0.0029234  0.0060136   0.486   0.6269    \npublication_year_fct2021  0.0025968  0.0058262   0.446   0.6558    \npublication_year_fct2022 -0.0007279  0.0059001  -0.123   0.9018    \npublication_year_fct2023  0.0087818  0.0058483   1.502   0.1332    \nfieldSocial Sciences      0.0862253  0.0018186  47.414   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\neffects %&gt;%\n  tidy() %&gt;% \n  filter(\n    term != \"(Intercept)\",\n    term == \"fieldSocial Sciences\") %&gt;% \n    select(-term) %&gt;% \n  gt() %&gt;% \n    fmt_number(\n      columns = -c(topic),\n      decimals = 3\n    ) %&gt;% \n  # Color social science topics \"blue\"\n  data_color(\n    columns = topic,\n    rows = estimate &gt; 0,\n    method = \"numeric\",\n    palette = c(\"#04316A\"),\n    alpha = 0.4\n  ) %&gt;% \n  # Color psychology topics \"yellow\"\n  data_color(\n    columns = topic,\n    rows = estimate &lt; 0,\n    method = \"numeric\",\n    palette = c(\"#D3A518\"),\n    alpha = 0.4\n  ) %&gt;% \n  # # Color effect size for estimation\n  # data_color(\n  #   columns = estimate,\n  #   method = \"numeric\",\n  #   palette = \"viridis\"\n  # ) %&gt;% \n  # # Color insignificant p-values\n  # data_color(\n  #   columns = p.value,\n  #   rows = p.value &gt; 0.05,\n  #   method = \"numeric\",\n  #   palette = c(\"#C50F3C\", \"#C50F3C\")\n  # ) %&gt;% \n  gtExtras::gt_theme_538()    \n\n\n\n\n\n\n\ntopic\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n1\n−0.056\n0.001\n−40.176\n0.000\n\n\n2\n0.014\n0.001\n10.300\n0.000\n\n\n3\n−0.069\n0.001\n−51.304\n0.000\n\n\n4\n−0.014\n0.001\n−11.136\n0.000\n\n\n5\n0.027\n0.001\n20.803\n0.000\n\n\n6\n−0.011\n0.001\n−7.202\n0.000\n\n\n7\n0.014\n0.001\n11.737\n0.000\n\n\n8\n−0.015\n0.001\n−14.155\n0.000\n\n\n9\n0.052\n0.002\n28.570\n0.000\n\n\n10\n−0.001\n0.001\n−2.303\n0.021\n\n\n11\n0.090\n0.002\n55.755\n0.000\n\n\n12\n0.003\n0.001\n2.241\n0.025\n\n\n13\n−0.025\n0.001\n−16.825\n0.000\n\n\n14\n0.044\n0.002\n27.583\n0.000\n\n\n15\n−0.059\n0.001\n−40.525\n0.000\n\n\n16\n0.086\n0.002\n47.001\n0.000\n\n\n17\n−0.033\n0.001\n−23.153\n0.000\n\n\n18\n−0.012\n0.001\n−11.614\n0.000\n\n\n19\n−0.044\n0.001\n−32.785\n0.000\n\n\n20\n0.008\n0.001\n7.517\n0.000\n\n\n\n\n\n\n\n\n\n\nZusammenführung der Daten\n\nMerge mit Stammdaten\n\ngamma_export &lt;- stm_mdl_k20 %&gt;% \n  tidytext::tidy(\n    matrix = \"gamma\", \n    document_names = names(quanteda_stm$documents)) %&gt;%\n  dplyr::group_by(document) %&gt;% \n  dplyr::slice_max(gamma) %&gt;% \n  dplyr::mutate(main_topic = ifelse(gamma &gt; 0.5, topic, NA)) %&gt;% \n      rename(\n        top_topic = topic,\n        top_gamma = gamma) %&gt;% \n  dplyr::ungroup() %&gt;% \n  dplyr::left_join(review_subsample, by = c(\"document\" = \"id\")) %&gt;% \n  dplyr::rename(id = document) %&gt;% \n  dplyr::mutate(\n    stm_topic = as.factor(paste(\"Topic\", sprintf(\"%02d\", top_topic)))\n  )\n\n\n\nAnzahl der Abstracts nach Thema\n\ngamma_export %&gt;% \n  ggplot(aes(x = fct_rev(fct_infreq(stm_topic)))) +\n  geom_bar() +\n  coord_flip() +\n  theme_pubr()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnzahl der Abstracts nach Thema und Feld\n\ngamma_export %&gt;% \n  gtsummary::tbl_cross(\n    row = stm_topic, \n    col = field,\n    percent = \"row\",\n    )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfield\nTotal\n\n\nPsychology\nSocial Sciences\n\n\n\n\nstm_topic\n\n\n\n\n\n\n\n\n    Topic 01\n1,470 (94%)\n87 (5.6%)\n1,557 (100%)\n\n\n    Topic 02\n649 (50%)\n652 (50%)\n1,301 (100%)\n\n\n    Topic 03\n1,481 (95%)\n71 (4.6%)\n1,552 (100%)\n\n\n    Topic 04\n459 (60%)\n306 (40%)\n765 (100%)\n\n\n    Topic 05\n301 (29%)\n734 (71%)\n1,035 (100%)\n\n\n    Topic 06\n1,116 (60%)\n745 (40%)\n1,861 (100%)\n\n\n    Topic 07\n825 (44%)\n1,051 (56%)\n1,876 (100%)\n\n\n    Topic 08\n540 (70%)\n227 (30%)\n767 (100%)\n\n\n    Topic 09\n821 (27%)\n2,260 (73%)\n3,081 (100%)\n\n\n    Topic 10\n114 (67%)\n57 (33%)\n171 (100%)\n\n\n    Topic 11\n303 (12%)\n2,244 (88%)\n2,547 (100%)\n\n\n    Topic 12\n439 (41%)\n630 (59%)\n1,069 (100%)\n\n\n    Topic 13\n1,477 (67%)\n722 (33%)\n2,199 (100%)\n\n\n    Topic 14\n509 (28%)\n1,277 (72%)\n1,786 (100%)\n\n\n    Topic 15\n1,815 (86%)\n299 (14%)\n2,114 (100%)\n\n\n    Topic 16\n1,369 (27%)\n3,742 (73%)\n5,111 (100%)\n\n\n    Topic 17\n1,672 (75%)\n566 (25%)\n2,238 (100%)\n\n\n    Topic 18\n737 (65%)\n396 (35%)\n1,133 (100%)\n\n\n    Topic 19\n2,552 (70%)\n1,118 (30%)\n3,670 (100%)\n\n\n    Topic 20\n342 (42%)\n475 (58%)\n817 (100%)\n\n\nTotal\n18,991 (52%)\n17,659 (48%)\n36,650 (100%)\n\n\n\n\n\n\n\n\n\nFokus: Thema 16\n\ngamma_export %&gt;% \n  filter(stm_topic == \"Topic 16\") %&gt;%\n  arrange(-top_gamma) %&gt;%\n  select(title, so, top_gamma, type, ab) %&gt;%\n  slice_head(n = 3) %&gt;% \n  gt() %&gt;% \n  fmt_number(\n    columns = c(top_gamma), \n    decimals = 2) %&gt;% \n  gtExtras::gt_theme_538()\n\n\n\n\n\n\n\ntitle\nso\ntop_gamma\ntype\nab\n\n\n\n\nTheory of Knowledge for Literature Reviews: An Epistemological Model, Taxonomy and Empirical Analysis of IS Literature\nNA\n0.93\narticle\nLiterature reviews play an important role in the development of knowledge. Yet, we observe a lack of theoretical underpinning of and epistemological insights into how literature reviews can contribute to knowledge creation and have actually contributed in the IS discipline. To address these theoretical and empirical research gaps, we suggest a novel epistemological model of literature reviews. This model allows us to align different contributions of literature reviews with their underlying knowledge conversions - thereby building a bridge between the previously largely unconnected fields of literature reviews and epistemology. We evaluate the appropriateness of the model by conducting an empirical analysis of 173 IS literature reviews which were published in 39 pertinent IS journals between 2000 and 2014. Based on this analysis, we derive an epistemological taxonomy of IS literature reviews, which complements previously suggested typologies.\n\n\nTheory of Knowledge for Literature Reviews: An Epistemological Model, Taxonomy and Empirical Analysis of IS Literature Completed Research Paper\nNA\n0.93\narticle\nLiterature reviews play an important role in the development of knowledge. Yet, we observe a lack of theoretical underpinning of and epistemological insights into how literature reviews can contribute to knowledge creation and have actually contributed in the IS discipline. To address these theoretical and empirical research gaps, we suggest a novel epistemological model of literature reviews. This model allows us to align different contributions of literature reviews with their underlying knowledge conversions - thereby building a bridge between the previously largely unconnected fields of literature reviews and epistemology. We evaluate the appropriateness of the model by conducting an empirical analysis of 173 IS literature reviews which were published in 39 pertinent IS journals between 2000 and 2014. Based on this analysis, we derive an epistemological taxonomy of IS literature reviews, which complements previously suggested typologies.\n\n\nRelationality in negotiations: a systematic review and propositions for future research\n˜The œinternational journal of conflict management/International journal of conflict management\n0.93\narticle\nPurpose The purpose of this paper is to systematically review and analyze the important, yet under-researched, topic of relationality in negotiations and propose new directions for future negotiation research. Design/methodology/approach This paper conducts a systematic review of negotiation literature related to relationality from multiple disciplines. Thirty-nine leading and topical academic journals are selected and 574 papers on negotiation are reviewed from 1990 to 2014. Based on the systematic review, propositions regarding the rationales for relationality in negotiations are developed and future research avenues in this area are discussed. Findings Of 574 papers on negotiations published in 39 peer-reviewed journals between 1990 and 2014, only 18 papers have studied and discussed relationality in negotiations. This suggests that relationality as a theoretical theme has long been under-researched in negotiation research. For future research, this paper proposes to incorporate the dynamic, cultural and mechanism perspectives, and to use a qualitative approach to study relationality in negotiations. Originality/value This paper presents the first systematic review of the negotiation literature on relationality, and identifies new research topics on relationality in negotiations. In so doing, this research opens new avenues for future negotiation research on relationality.\n\n\n\n\n\n\n\n\n\nFokus: Thema 6\n\ngamma_export %&gt;% \n  filter(stm_topic == \"Topic 06\") %&gt;%\n  arrange(-top_gamma) %&gt;%\n  select(title, so, top_gamma, type, ab) %&gt;%\n  slice_head(n = 3) %&gt;% \n  gt() %&gt;% \n  fmt_number(\n    columns = c(top_gamma), \n    decimals = 2) %&gt;% \n  gtExtras::gt_theme_538()\n\n\n\n\n\n\n\ntitle\nso\ntop_gamma\ntype\nab\n\n\n\n\nThe Acceptance of COVID-19 Vaccine: A Global Rapid Systematic Review and Meta-Analysis\nSocial Science Research Network\n0.89\narticle\nBackground: Vaccination seems to be the most effective way to prevent and control the spread of COVID-19, a disease that has been playing havoc with the lives of over 7 billion people across the globe. Vaccine hesitancy is probably the most common problem worldwide. This study aims to inspect the COVID-19 vaccine acceptance rates worldwide among the general population and healthcare workers. In addition, it compares the vaccine acceptance rates between the pre-and post-vaccine approval periods.Method: A systematic search was conducted on April 25, 2021, through PubMed, MEDLINE, Web of Science, and GOOGLE SCHOLAR databases using PRISMA and MOOSE statements. Q-test, and statistics were used to search for heterogeneity, and Eggers's test and funnel plot were applied to assess the publication bias. The random-effects model was used to estimate the pooled acceptance rates of the COVID-19 vaccines.Results: The combined COVID-19 vaccine acceptance rate among the general population and healthcare workers (n=1,581,562) was estimated at 61.74%. The vaccine acceptance rate among the general population was 62.66% and the rate among healthcare workers was 57.89%. The acceptance rate decreased from 67.21% to 53.44% among the general population and remained constant among healthcare workers during the pre and post-vaccine approval periods. The acceptance rates also vary in different regions of the world. The highest acceptance rate was found in Western Pacific Region (67.85%) and the lowest was found in African Region (39.51%).Conclusion: Low COVID-19 vaccine acceptance rate might be a massive barrier to getting rid of the pandemic. More researches are needed to address the responsible factors influencing the global rate of COVID-19 vaccine acceptance. Integrated global efforts are required to remove the barriers.\n\n\nPrevalence of Suicidal Behavior Among Students in South-East Asia: A Systematic Review and Meta-Analysis\nArchives of Suicide Research\n0.88\narticle\nEstimation of rates of suicidal behaviors (ideation, plan, and attempt) would help to understand the burden and prioritize prevention strategies. However, no attempt to assess suicidal behavior among students was identified in South-East Asia (SEA). We aimed to assess the prevalence of suicidal behavior (ideation, plan, and attempt) among students in SEA.We followed PRISMA 2020 guidelines and registered the protocol in PROSPERO (CRD42022353438). We searched in Medline, Embase, and PsycINFO and performed meta-analyses to pool the lifetime, 1-year, and point prevalence rates for suicidal ideation, plans, and attempts. We considered the duration of a month for point prevalence.The search identified 40 separate populations from which 46 were included in the analyses, as some studies included samples from multiple countries. The pooled prevalence of suicidal ideation was 17.4% (confidence interval [95% CI], 12.4%-23.9%) for lifetime, 9.33% (95% CI, 7.2%-12%) for the past year, and 4.8% (95% CI, 3.6%-6.4%) for the present time. The pooled prevalence of suicide plans was 9% (95% CI, 6.2%-12.9%) for lifetime, 7.3% (95% CI, 5.1%-10.3%) for the past year, and 2.3% (95% CI, 0.8%-6.7%) for the present time. The pooled prevalence of suicide attempts was 5.2% (95% CI, 3.5%-7.8%) for lifetime and 4.5% (95% CI, 3.4%-5.8%) for the past year. Higher rates of suicide attempts in the lifetime were noted in Nepal (10%) and Bangladesh (9%), while lower rates were reported in India (4%) and Indonesia (5%).Suicidal behaviors are a common phenomenon among students in the SEA region. These findings call for integrated, multisectoral efforts to prevent suicidal behaviors in this group.\n\n\nFirst COVID-19 Booster Dose in the General Population: A Systematic Review and Meta-Analysis of Willingness and Its Predictors\nVaccines\n0.88\narticle\nThe emergence of breakthrough infections and new highly contagious variants of SARS-CoV-2 threaten the immunization in individuals who had completed the primary COVID-19 vaccination. This systematic review and meta-analysis investigated, for the first time, acceptance of the first COVID-19 booster dose and its associated factors among fully vaccinated individuals. We followed the PRISMA guidelines. We searched Scopus, Web of Science, Medline, PubMed, ProQuest, CINAHL and medrxiv from inception to 21 May 2022. We found 14 studies including 104,047 fully vaccinated individuals. The prevalence of individuals who intend to accept a booster was 79.0%, while the prevalence of unsure individuals was 12.6%, and the prevalence of individuals that intend to refuse a booster was 14.3%. The main predictors of willingness were older age, flu vaccination in the previous season, and confidence in COVID-19 vaccination. The most important reasons for decline were adverse reactions and discomfort experienced after previous COVID-19 vaccine doses and concerns for serious adverse reactions to COVID-19 booster doses. Considering the burden of COVID-19, a high acceptance rate of booster doses could be critical in controlling the pandemic. Our findings are innovative and could help policymakers to design and implement specific COVID-19 vaccination programs in order to decrease booster vaccine hesitancy.\n\n\n\n\n\n\n\n\n\n\nDie Suche nach dem optimalen k\n\n# Define parameters\nfuture::plan(future::multisession()) # use multiple sessions\ntopic_range &lt;- seq(from = 10, to = 100, by = 10) \n\n# Initiate notifications & time tracking\ntic(\"STM extended search\")\n\n# Estimate models\nstm_search  &lt;- tibble(k = topic_range) %&gt;%\n    mutate(\n        mdl = furrr::future_map(\n            k, \n            ~stm::stm(\n                documents = quanteda_stm$documents,\n                vocab = quanteda_stm$vocab, \n                prevalence =~ publication_year_fct + field,\n                K = ., \n                seed = 42,\n                max.em.its = 1000,\n                data = quanteda_stm$meta,\n                init.type = \"Spectral\",\n                verbose = FALSE),\n            .options = furrr::furrr_options(seed = 42)\n            )\n    )\n\n# Sent status update and finish time tracking\ntoc(log = TRUE)\n\n\n\nErstellung des “Heldouts”\n\nheldout &lt;- make.heldout(\n  documents = quanteda_stm$documents,\n  vocab = quanteda_stm$vocab,\n  seed = 42)\n\n\n\nEvaluation der Modelle\n\nstm_search$results &lt;- stm_search %&gt;%\n  mutate(\n    exclusivity = map(mdl, exclusivity),\n    semantic_coherence = map(mdl, semanticCoherence, quanteda_stm$documents),\n    eval_heldout = map(mdl, eval.heldout, heldout$missing),\n    residual = map(mdl, checkResiduals, quanteda_stm$documents),\n    bound =  map_dbl(mdl, function(x) max(x$convergence$bound)),\n    lfact = map_dbl(mdl, function(x) lfactorial(x$settings$dim$K)),\n    lbound = bound + lfact,\n    iterations = map_dbl(mdl, function(x) length(x$convergence$bound)))\n\n\nVergleich verschiedener Statistiken\n\nstm_search$results %&gt;% \n  # Create data for graph\n  transmute(\n    k, \n    `Lower bound` = lbound,\n    Residuals = map_dbl(residual, \"dispersion\"),\n    `Semantic coherence` = map_dbl(semantic_coherence, mean),\n    `Held-out likelihood` = map_dbl(eval_heldout, \"expected.heldout\")\n    ) %&gt;%   \n  gather(Metric, Value, -k) %&gt;%\n  # Create graph\n  ggplot(aes(k, Value, color = Metric)) +\n    geom_line(linewidth = 1.5, alpha = 0.7, show.legend = FALSE) +\n    geom_point(size = 3) +\n    # Add marker\n    geom_vline(aes(xintercept = 20), color = \"#C77CFF\", alpha = .5) +\n    geom_vline(aes(xintercept = 40), color = \"#00BFC4\", alpha = .5) +\n    geom_vline(aes(xintercept = 60), color = \"#C77CFF\", alpha = .5) +\n    geom_vline(aes(xintercept = 70), color = \"#00BFC4\", alpha = .5) +  \n    scale_x_continuous(breaks = seq(from = 10, to = 100, by = 10)) +\n    facet_wrap(~Metric, scales = \"free_y\") +\n    labs(x = \"K (number of topics)\",\n        y = NULL,\n        title = \"Model diagnostics by number of topics\"\n        ) +\n    theme_pubr()\n\n\n\n\n\n\n\n\n\n\n\n\n\nstm_search$results %&gt;% \n  select(k, exclusivity, semantic_coherence) %&gt;% \n  filter(k %in% c(20, 40, 70)) %&gt;%\n  unnest(cols = c(exclusivity, semantic_coherence)) %&gt;%\n  mutate(k = as.factor(k)) %&gt;%\n  ggplot(aes(semantic_coherence, exclusivity, color = k)) +\n  geom_point(size = 2, alpha = 0.7) +\n  labs(x = \"Semantic coherence\",\n       y = \"Exclusivity\",\n       title = \"Comparing exclusivity and semantic coherence\",\n       subtitle = \"Models with fewer topics have higher semantic coherence for more topics, but lower exclusivity\"\n       ) +\n  theme_minimal()"
  },
  {
    "objectID": "exercises/ms-exercise-07_solution.html",
    "href": "exercises/ms-exercise-07_solution.html",
    "title": "API mining and data wrangling with R",
    "section": "",
    "text": "Link to source file"
  },
  {
    "objectID": "exercises/ms-exercise-07_solution.html#background",
    "href": "exercises/ms-exercise-07_solution.html#background",
    "title": "API mining and data wrangling with R",
    "section": "Background",
    "text": "Background\n\n\n\n\n\n\nTodays’s data basis: OpenAlex\n\n\n\nOpenAlex is a free and open catalog of the global research system. It’s named after the ancient Library of Alexandria and made by the nonprofit OurResearch.\n\n\n\n\n\n\n\n\nAt the heart of OpenAlex is our dataset—a catalog of works. A work is any sort of scholarly output. A research article is one kind of work, but there are others such as datasets, books, and dissertations. We keep track of these works—their titles (and abstracts and full text in many cases), when they were created, etc. But that’s not all we do. We also keep track of the connections between these works, finding associations through things like journals, authors, institutional affiliations, citations, topics, and funders. There are hundreds of millions of works out there, and tens of thousands more being created every day, so it’s important that we have these relationships to help us make sense of research at a large scale."
  },
  {
    "objectID": "exercises/ms-exercise-07_solution.html#preparation",
    "href": "exercises/ms-exercise-07_solution.html#preparation",
    "title": "API mining and data wrangling with R",
    "section": "Preparation",
    "text": "Preparation\n\n\n\n\n\n\nWichtige Information\n\n\n\n\nBitte stellen Sie sicher, dass Sie das jeweilige R-Studio Projekt zur Übung geöffnet haben. Nur so funktionieren alle Dependencies korrekt.\nUm den einwandfreien Ablauf der Übung zu gewährleisten, wird für die Aufgaben auf eine eigenständige Datenerhebung verzichtet und ein Übungsdatensatz zu verfügung gestelt.\n\n\n\n\nPackages\n\nZum Laden der Pakete wird das Paket pacman::pload() genutzt, dass gegenüber der herkömmlichen Methode mit library() eine Reihe an Vorteile hat:\n\nPrägnante Syntax\nAutomatische Installation (wenn Paket noch nicht vorhanden)\nLaden mehrerer Pakete auf einmal\nAutomatische Suche nach dependencies\n\n\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  here, qs, # file management\n  magrittr, janitor, # data wrangling\n  easystats, sjmisc, # data analysis\n  ggpubr, # visualization\n  openalexR, \n  tidyverse # load last to avoid masking issues\n  )\n\n\n\nImport und Vorverarbeitung der Daten\n\n# Import from local\nreview_works &lt;- qs::qread(here(\"data/session-07/openalex-review_works-2013_2023.qs\"))\n\n# Create correct data\nreview_works_correct &lt;- review_works %&gt;% \n    mutate(\n        # Create additional factor variables\n        publication_year_fct = as.factor(publication_year), \n        type_fct = as.factor(type)\n        )"
  },
  {
    "objectID": "exercises/ms-exercise-07_solution.html#praktische-anwendung",
    "href": "exercises/ms-exercise-07_solution.html#praktische-anwendung",
    "title": "API mining and data wrangling with R",
    "section": "🛠️ Praktische Anwendung",
    "text": "🛠️ Praktische Anwendung\n\n\n\n\n\n\nAchtung, bitte lesen!\n\n\n\n\nBevor Sie mit der Arbeit an den folgenden 📋 Exercises beginnen, stellen Sie bitte sicher, dass Sie alle Chunks des Abschnitts Preparation gerendert haben. Das können Sie tun, indem Sie den “Run all chunks above”-Knopf  des nächsten Chunks benutzen.\nBei Fragen zum Code lohnt sich ein Blick in den Showcase (.qmd oder .html). Beim Showcase handelt es sich um eine kompakte Darstellung des in der Präsentation verwenden R-Codes. Sie können das Showcase also nutzen, um sich die Code-Bausteine anzusehen, die für die R-Outputs auf den Slides benutzt wurden.\n\n\n\n\n📋 Exercise 1: Sprache der Publikationen\n\n\n\n\n\n\nZiel der Aufgabe\n\n\n\n\nIdentifizieren Sie die für die Untersuchung relevanten Artikel auf Basis von deren Sprache (language)\nHintergrundinformation zur Variable language finden Sie in der API-Dokumentation von OpenAlex.\n\n\n\n\nNutzen Sie die Funktion sjmisc::frq() und schauen Sie sich im Datensatz review_works_correct die Variablen language an. Verwenden Sie das Argument sort.frq = \"desc\", um die Häufigkeit der Sprachen absteigend zu sortieren.\nNotieren Sie sich den jeweilgen ISO 639-1 language code, um Ihn später bei 📋 Exercise 4: Erstellung Subsample als Filter zu nutzen.\n\n\n\nLösung anzeigen\nreview_works_correct %&gt;% \n    sjmisc::frq(\"language\", sort.frq = \"desc\")\n\n\nlanguage &lt;character&gt; \n# total N=93655 valid N=93476 mean=10.33 sd=2.27\n\nValue |     N | Raw % | Valid % | Cum. %\n----------------------------------------\nen    | 90653 | 96.79 |   96.98 |  96.98\nid    |  1118 |  1.19 |    1.20 |  98.18\npt    |   521 |  0.56 |    0.56 |  98.73\nes    |   309 |  0.33 |    0.33 |  99.06\ntr    |   182 |  0.19 |    0.19 |  99.26\nko    |   130 |  0.14 |    0.14 |  99.40\nfr    |   127 |  0.14 |    0.14 |  99.53\nru    |    72 |  0.08 |    0.08 |  99.61\nde    |    59 |  0.06 |    0.06 |  99.67\nit    |    43 |  0.05 |    0.05 |  99.72\npl    |    36 |  0.04 |    0.04 |  99.76\nfa    |    20 |  0.02 |    0.02 |  99.78\nro    |    20 |  0.02 |    0.02 |  99.80\nja    |    19 |  0.02 |    0.02 |  99.82\nca    |    16 |  0.02 |    0.02 |  99.84\nuk    |    16 |  0.02 |    0.02 |  99.86\nar    |    15 |  0.02 |    0.02 |  99.87\nth    |    14 |  0.01 |    0.01 |  99.89\nnl    |    13 |  0.01 |    0.01 |  99.90\nsv    |    13 |  0.01 |    0.01 |  99.91\nhr    |    11 |  0.01 |    0.01 |  99.93\nhu    |    11 |  0.01 |    0.01 |  99.94\nsl    |    10 |  0.01 |    0.01 |  99.95\naf    |     8 |  0.01 |    0.01 |  99.96\ncs    |     6 |  0.01 |    0.01 |  99.96\nda    |     5 |  0.01 |    0.01 |  99.97\nel    |     5 |  0.01 |    0.01 |  99.97\nbg    |     4 |  0.00 |    0.00 |  99.98\nzh-cn |     4 |  0.00 |    0.00 |  99.98\net    |     3 |  0.00 |    0.00 |  99.99\nlt    |     3 |  0.00 |    0.00 |  99.99\nfi    |     2 |  0.00 |    0.00 |  99.99\nmk    |     2 |  0.00 |    0.00 |  99.99\nno    |     2 |  0.00 |    0.00 | 100.00\nta    |     2 |  0.00 |    0.00 | 100.00\ncy    |     1 |  0.00 |    0.00 | 100.00\nhi    |     1 |  0.00 |    0.00 | 100.00\n&lt;NA&gt;  |   179 |  0.19 |    &lt;NA&gt; |   &lt;NA&gt;\n\n\nLösung anzeigen\n# Notiz(en)\n# - Überwiegende Anzahl der heruntergeladenen Datenbankbeiträge ist in englischer Sprache (en) verfasst.\n# -Die Sprache en wird als Filterkriterium für die Erstellung des Subsamples genutzt, auch um spätere Probleme bei der Analyse durch multi-linguale Texte zu vermeiden.\n\n\n\n\n📋 Exercise 2: Typ der Publikationen\n\n\n\n\n\n\nZiel der Aufgabe\n\n\n\n\nIdentifizieren Sie die für die Untersuchung relevanten Artikel auf Basis deres Typen (type).\nHintergrundinformation zur Variable type finden Sie in der API-Dokumentation von OpenAlex.\n\n\n\n\nNutzen Sie die Funktion sjmisc::frq() und schauen Sie sich im Datensatz review_works_correct die Variablen type an. Verwenden Sie das Argument sort.frq = \"desc\", um die Typen in Abhängigkeit Ihrer Häufigkeit absteigend zu sortieren.\nNotieren Sie sich die Ausprägungen der Variable type, die aus Ihrer Sicht später bei 📋 Exercise 4: Erstellung Subsample als Filter genutzt werden soll.\n\n\n\nLösung anzeigen\nreview_works_correct %&gt;% \n    sjmisc::frq(\"type\", sort.frq = \"desc\")\n\n\ntype &lt;character&gt; \n# total N=93655 valid N=93655 mean=3.09 sd=4.64\n\nValue                   |     N | Raw % | Valid % | Cum. %\n----------------------------------------------------------\narticle                 | 73716 | 78.71 |   78.71 |  78.71\nreview                  |  6437 |  6.87 |    6.87 |  85.58\npreprint                |  3570 |  3.81 |    3.81 |  89.40\nbook-chapter            |  2860 |  3.05 |    3.05 |  92.45\nlibguides               |  2414 |  2.58 |    2.58 |  95.03\ndataset                 |  1471 |  1.57 |    1.57 |  96.60\npeer-review             |  1400 |  1.49 |    1.49 |  98.09\ndissertation            |   975 |  1.04 |    1.04 |  99.13\nreport                  |   422 |  0.45 |    0.45 |  99.58\nbook                    |   167 |  0.18 |    0.18 |  99.76\nother                   |    64 |  0.07 |    0.07 |  99.83\nparatext                |    57 |  0.06 |    0.06 |  99.89\nerratum                 |    47 |  0.05 |    0.05 |  99.94\nletter                  |    25 |  0.03 |    0.03 |  99.97\neditorial               |    17 |  0.02 |    0.02 |  99.99\nsupplementary-materials |     7 |  0.01 |    0.01 |  99.99\nreference-entry         |     6 |  0.01 |    0.01 | 100.00\n&lt;NA&gt;                    |     0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\n\nLösung anzeigen\n# Notiz(en)\n# - Überwiegende Anzahl der heruntergeladenen Datenbankbeiträge sind `article.`\n# - Die folgenden Auswertungen beziehen sich auf die Publikationen des Typs article, da uns besonders die praktische Anwendung/Umsetzung der Methode in verschiedenen Kontexten interessiert. Läge der Fokus auf (die Entwicklung) der Methode selbst, wäre vermutlich eher die Typen book oder book-chapter relevant.\n\n\n\n\n📋 Exercise 3: Forschungsfeld der Publikationen\n\n\n\n\n\n\nZiel der Aufgabe\n\n\n\n\nIdentifizieren Sie die für die Untersuchung relevanten Artikel auf Basis des von OpenAlex dem Artikel zugeordnenten Foschungsfeldes (field))\nHintergrundinformation zur Variable field finden Sie in der API-Dokumentation von OpenAlex.\n\n\n\n\nBasierend auf dem Datensatz review_works_correct\n\nnutzen Sie die Funktion unnest() um die Variablen der topics-Liste zu extrahieren. Verwenden Sie dabei das Argument names_sep = \"_\". um doppelte Variablennamen durch Hinzufügen des Prefixes topics_ zu verhindern.\nfiltern Sie anschließen mit Hilfe der Funktion filter und der Variable bzw. dem Argument topics_name == \"field\" nur die Informationen zum Forschungsfeld, sowie mit der Variable bzw. dem Argument topics_i == \"1\" nur die erste Zuordnung.\nNutzen Sie die Funktion sjmisc::frq() und schauen Sie sich die Variablen topics_display_name an. Verwenden Sie das Argument sort.frq = \"desc\", um die Forschungsfelder in Abhängigkeit Ihrer Häufigkeit absteigend zu sortieren.\n\nNotieren Sie sich die Ausprägungen der Variable topics_display_name, die aus Ihrer Sicht später bei 📋 Exercise 4: Erstellung Subsample als Filter genutzt werden soll.\n\n\n\nLösung anzeigen\nreview_works_correct %&gt;% \n    unnest(topics, names_sep = \"_\") %&gt;%\n    filter(topics_name == \"field\") %&gt;% \n    filter(topics_i == \"1\") %&gt;% \n    sjmisc::frq(\"topics_display_name\", sort.frq = \"desc\")\n\n\ntopics_display_name &lt;character&gt; \n# total N=93655 valid N=93655 mean=4.41 sd=1.62\n\nValue                               |     N | Raw % | Valid % | Cum. %\n----------------------------------------------------------------------\nSocial Sciences                     | 30580 | 32.65 |   32.65 |  32.65\nPsychology                          | 29054 | 31.02 |   31.02 |  63.67\nBusiness, Management and Accounting | 15558 | 16.61 |   16.61 |  80.29\nDecision Sciences                   |  7261 |  7.75 |    7.75 |  88.04\nEconomics, Econometrics and Finance |  6796 |  7.26 |    7.26 |  95.30\nArts and Humanities                 |  4406 |  4.70 |    4.70 | 100.00\n&lt;NA&gt;                                |     0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\n\nLösung anzeigen\n# Notiz(en)\n# - Focus auf \"primäre\" Forschungsfelder `Social Sciences` & `Psychology`\n\n\n\n\n📋 Exercise 4: Erstellung Subsample\n\n\n\n\n\n\nZiel der Aufgabe\n\n\n\n\nErstellung sie den Datensatz review_subsample, in dem Sie mit Hilfe der Funktionen select() und/oder filter() das Datenmaterial weiter eingrenzen. Sie können sich sowohl auf die Variablen aus der Übung, als auch auf die aus der Sitzung (bzw. den Slides) beziehen.\nDer Code dieses Chunks wird in der nächsten Sitzung benötigt bzw. besprochen, halten Sie diesen deshalb bitte bereit.\n\n\n\n\n\nLösung anzeigen\nreview_subsample &lt;- review_works_correct %&gt;% \n  filter(language == \"en\") %&gt;% # nur englischsprachige Einträge\n  filter(type == \"article\") %&gt;% # nur Artikel\n  # Datentranformation\n  unnest(topics, names_sep = \"_\") %&gt;%\n  filter(topics_name == \"field\") %&gt;% \n  filter(topics_i == \"1\") %&gt;% \n  filter(\n    topics_display_name == \"Social Sciences\"|\n    topics_display_name == \"Psychology\"\n    ) %&gt;%\n  glimpse()\n\n\nRows: 45,221\nColumns: 45\n$ id                          &lt;chr&gt; \"https://openalex.org/W4293003987\", \"https…\n$ title                       &lt;chr&gt; \"The WHO-5 Well-Being Index: A Systematic …\n$ display_name                &lt;chr&gt; \"The WHO-5 Well-Being Index: A Systematic …\n$ author                      &lt;list&gt; [&lt;data.frame[4 x 12]&gt;], [&lt;data.frame[2 x …\n$ ab                          &lt;chr&gt; \"The 5-item World Health Organization Well…\n$ publication_date            &lt;chr&gt; \"2015-01-01\", \"2017-08-28\", \"2014-01-01\", …\n$ relevance_score             &lt;dbl&gt; 938.7603, 752.3500, 591.2553, 576.1210, 56…\n$ so                          &lt;chr&gt; \"Psychotherapy and psychosomatics\", \"Journ…\n$ so_id                       &lt;chr&gt; \"https://openalex.org/S184803288\", \"https:…\n$ host_organization           &lt;chr&gt; \"Karger Publishers\", \"SAGE Publishing\", NA…\n$ issn_l                      &lt;chr&gt; \"0033-3190\", \"0739-456X\", NA, \"2214-7829\",…\n$ url                         &lt;chr&gt; \"https://doi.org/10.1159/000376585\", \"http…\n$ pdf_url                     &lt;chr&gt; \"https://www.karger.com/Article/Pdf/376585…\n$ license                     &lt;chr&gt; \"cc-by-nc\", NA, NA, \"cc-by\", NA, NA, \"cc-b…\n$ version                     &lt;chr&gt; \"publishedVersion\", NA, \"publishedVersion\"…\n$ first_page                  &lt;chr&gt; \"167\", \"93\", NA, \"89\", \"55\", \"2150\", \"e356…\n$ last_page                   &lt;chr&gt; \"176\", \"112\", NA, \"106\", \"64\", \"2159\", \"e3…\n$ volume                      &lt;chr&gt; \"84\", \"39\", NA, \"6\", \"277\", \"32\", \"2\", \"24…\n$ issue                       &lt;chr&gt; \"3\", \"1\", NA, NA, NA, \"19\", \"8\", NA, \"9\", …\n$ is_oa                       &lt;lgl&gt; TRUE, FALSE, TRUE, TRUE, TRUE, FALSE, TRUE…\n$ is_oa_anywhere              &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE,…\n$ oa_status                   &lt;chr&gt; \"hybrid\", \"green\", \"bronze\", \"gold\", \"bron…\n$ oa_url                      &lt;chr&gt; \"https://www.karger.com/Article/Pdf/376585…\n$ any_repository_has_fulltext &lt;lgl&gt; FALSE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE…\n$ language                    &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", …\n$ grants                      &lt;list&gt; NA, NA, NA, &lt;\"https://openalex.org/F43203…\n$ cited_by_count              &lt;int&gt; 2657, 1375, 2568, 803, 3664, 1553, 2895, 9…\n$ counts_by_year              &lt;list&gt; [&lt;data.frame[11 x 2]&gt;], [&lt;data.frame[7 x …\n$ publication_year            &lt;int&gt; 2015, 2017, 2014, 2016, 2020, 2014, 2017, …\n$ cited_by_api_url            &lt;chr&gt; \"https://api.openalex.org/works?filter=cit…\n$ ids                         &lt;list&gt; &lt;\"https://openalex.org/W4293003987\", \"htt…\n$ doi                         &lt;chr&gt; \"https://doi.org/10.1159/000376585\", \"http…\n$ type                        &lt;chr&gt; \"article\", \"article\", \"article\", \"article\"…\n$ referenced_works            &lt;list&gt; &lt;\"https://openalex.org/W1492518593\", \"htt…\n$ related_works               &lt;list&gt; &lt;\"https://openalex.org/W3020194755\", \"htt…\n$ is_paratext                 &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ is_retracted                &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ concepts                    &lt;list&gt; [&lt;data.frame[7 x 5]&gt;], [&lt;data.frame[18 x …\n$ topics_i                    &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ topics_score                &lt;dbl&gt; 0.9926, 0.9050, 0.9995, 0.9987, 0.9999, 1.…\n$ topics_name                 &lt;chr&gt; \"field\", \"field\", \"field\", \"field\", \"field…\n$ topics_id                   &lt;chr&gt; \"https://openalex.org/fields/32\", \"https:/…\n$ topics_display_name         &lt;chr&gt; \"Psychology\", \"Social Sciences\", \"Psycholo…\n$ publication_year_fct        &lt;fct&gt; 2015, 2017, 2014, 2016, 2020, 2014, 2017, …\n$ type_fct                    &lt;fct&gt; article, article, article, article, articl…"
  },
  {
    "objectID": "computing/computing-cheatsheets.html",
    "href": "computing/computing-cheatsheets.html",
    "title": "R cheatsheets",
    "section": "",
    "text": "The following cheatsheets come from https://posit.co/resources/cheatsheets. We haven’t covered every function and functionality listed on them, but you might still find them useful as references.",
    "crumbs": [
      "Computing",
      "R Cheatsheets"
    ]
  }
]