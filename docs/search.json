[
  {
    "objectID": "ps_ws-schedule.html",
    "href": "ps_ws-schedule.html",
    "title": "Semesterplan",
    "section": "",
    "text": "Note\n\n\n\nDiese Seite erh√§lt eine √úbersicht √ºber die Sitzung bzw. Themen des Methodenseminars im Sommersemester 2024. Bitte beachten Sie, dass die Inhalte des Kurses ( Folien,  Exercises,  Showcases und  Hintergrundinformationen) im Laufe des Semesters stetig aktualisiert werden, wobei alle √Ñnderungen hier dokumentiert werden.\n\n\n\n\n\n\n\n\n\n\n\nSitzung\nDatum\nThema\nUnterlagen\n\n\n\n\n1\n30.10.2024\nKick-Off\n\n\n\n2\n06.11.2024\nWorkflow & Analysestrategie I\n\n\n\n3\n13.11.2024\nWorkflow & Analysestrategie II\n\n\n\n4\n20.11.2024\nGruppenarbeit\n\n\n\n5\n27.11.2024\nAll things R: RefreshR & Vorstellung Datensatz\n\n\n\n6\n04.12.2024\nUpdate zum Workflow I\n\n\n\n7\n11.12.2024\nGruppenarbeit\n\n\n\n8\n18.12.2024\nAll things R\n\n\n\n9\n08.01.2025\nUpdate zum Workflow II\n\n\n\n10\n15.01.2025\nGruppenarbeit\n\n\n\n11\n22.01.2025\nAll things R\n\n\n\n\n27.01.2025\nSondertermin: Vorstellung Projektseminar\n\n\n\n12\n29.01.2025\nAbschlusspr√§sentation (inklusive Feedback)\n\n\n\n13\n05.02.2025\nSemesterabschluss: Projektbericht & Evaluation",
    "crumbs": [
      "Kursunterlagen",
      "Projektseminar"
    ]
  },
  {
    "objectID": "computing/computing-cheatsheets.html",
    "href": "computing/computing-cheatsheets.html",
    "title": "R cheatsheets",
    "section": "",
    "text": "The following cheatsheets come from https://posit.co/resources/cheatsheets. We haven‚Äôt covered every function and functionality listed on them, but you might still find them useful as references.",
    "crumbs": [
      "Computing",
      "R Cheatsheets"
    ]
  },
  {
    "objectID": "exercises/ms-exercise-07_solution.html",
    "href": "exercises/ms-exercise-07_solution.html",
    "title": "API mining and data wrangling with R",
    "section": "",
    "text": "Link to source file"
  },
  {
    "objectID": "exercises/ms-exercise-07_solution.html#background",
    "href": "exercises/ms-exercise-07_solution.html#background",
    "title": "API mining and data wrangling with R",
    "section": "Background",
    "text": "Background\n\n\n\n\n\n\nTodays‚Äôs data basis: OpenAlex\n\n\n\nOpenAlex is a free and open catalog of the global research system. It‚Äôs named after the ancient Library of Alexandria and made by the nonprofit OurResearch.\n\n\n\n\n\n\n\n\nAt the heart of OpenAlex is our dataset‚Äîa catalog of works. A work is any sort of scholarly output. A research article is one kind of work, but there are others such as datasets, books, and dissertations. We keep track of these works‚Äîtheir titles (and abstracts and full text in many cases), when they were created, etc. But that‚Äôs not all we do. We also keep track of the connections between these works, finding associations through things like journals, authors, institutional affiliations, citations, topics, and funders. There are hundreds of millions of works out there, and tens of thousands more being created every day, so it‚Äôs important that we have these relationships to help us make sense of research at a large scale."
  },
  {
    "objectID": "exercises/ms-exercise-07_solution.html#preparation",
    "href": "exercises/ms-exercise-07_solution.html#preparation",
    "title": "API mining and data wrangling with R",
    "section": "Preparation",
    "text": "Preparation\n\n\n\n\n\n\nWichtige Information\n\n\n\n\nBitte stellen Sie sicher, dass Sie das jeweilige R-Studio Projekt zur √úbung ge√∂ffnet haben. Nur so funktionieren alle Dependencies korrekt.\nUm den einwandfreien Ablauf der √úbung zu gew√§hrleisten, wird f√ºr die Aufgaben auf eine eigenst√§ndige Datenerhebung verzichtet und ein √úbungsdatensatz zu verf√ºgung gestelt.\n\n\n\n\nPackages\n\nZum Laden der Pakete wird das Paket pacman::pload() genutzt, dass gegen√ºber der herk√∂mmlichen Methode mit library() eine Reihe an Vorteile hat:\n\nPr√§gnante Syntax\nAutomatische Installation (wenn Paket noch nicht vorhanden)\nLaden mehrerer Pakete auf einmal\nAutomatische Suche nach dependencies\n\n\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  here, qs, # file management\n  magrittr, janitor, # data wrangling\n  easystats, sjmisc, # data analysis\n  ggpubr, # visualization\n  openalexR, \n  tidyverse # load last to avoid masking issues\n  )\n\n\n\nImport und Vorverarbeitung der Daten\n\n# Import from local\nreview_works &lt;- qs::qread(here(\"data/session-07/openalex-review_works-2013_2023.qs\"))\n\n# Create correct data\nreview_works_correct &lt;- review_works %&gt;% \n    mutate(\n        # Create additional factor variables\n        publication_year_fct = as.factor(publication_year), \n        type_fct = as.factor(type)\n        )"
  },
  {
    "objectID": "exercises/ms-exercise-07_solution.html#praktische-anwendung",
    "href": "exercises/ms-exercise-07_solution.html#praktische-anwendung",
    "title": "API mining and data wrangling with R",
    "section": "üõ†Ô∏è Praktische Anwendung",
    "text": "üõ†Ô∏è Praktische Anwendung\n\n\n\n\n\n\nAchtung, bitte lesen!\n\n\n\n\nBevor Sie mit der Arbeit an den folgenden üìã Exercises beginnen, stellen Sie bitte sicher, dass Sie alle Chunks des Abschnitts Preparation gerendert haben. Das k√∂nnen Sie tun, indem Sie den ‚ÄúRun all chunks above‚Äù-Knopf  des n√§chsten Chunks benutzen.\nBei Fragen zum Code lohnt sich ein Blick in den Showcase (.qmd oder .html). Beim Showcase handelt es sich um eine kompakte Darstellung des in der Pr√§sentation verwenden R-Codes. Sie k√∂nnen das Showcase also nutzen, um sich die Code-Bausteine anzusehen, die f√ºr die R-Outputs auf den Slides benutzt wurden.\n\n\n\n\nüìã Exercise 1: Sprache der Publikationen\n\n\n\n\n\n\nZiel der Aufgabe\n\n\n\n\nIdentifizieren Sie die f√ºr die Untersuchung relevanten Artikel auf Basis von deren Sprache (language)\nHintergrundinformation zur Variable language finden Sie in der API-Dokumentation von OpenAlex.\n\n\n\n\nNutzen Sie die Funktion sjmisc::frq() und schauen Sie sich im Datensatz review_works_correct die Variablen language an. Verwenden Sie das Argument sort.frq = \"desc\", um die H√§ufigkeit der Sprachen absteigend zu sortieren.\nNotieren Sie sich den jeweilgen ISO 639-1 language code, um Ihn sp√§ter bei üìã Exercise 4: Erstellung Subsample als Filter zu nutzen.\n\n\n\nL√∂sung anzeigen\nreview_works_correct %&gt;% \n    sjmisc::frq(\"language\", sort.frq = \"desc\")\n\n\nlanguage &lt;character&gt; \n# total N=93655 valid N=93476 mean=10.33 sd=2.27\n\nValue |     N | Raw % | Valid % | Cum. %\n----------------------------------------\nen    | 90653 | 96.79 |   96.98 |  96.98\nid    |  1118 |  1.19 |    1.20 |  98.18\npt    |   521 |  0.56 |    0.56 |  98.73\nes    |   309 |  0.33 |    0.33 |  99.06\ntr    |   182 |  0.19 |    0.19 |  99.26\nko    |   130 |  0.14 |    0.14 |  99.40\nfr    |   127 |  0.14 |    0.14 |  99.53\nru    |    72 |  0.08 |    0.08 |  99.61\nde    |    59 |  0.06 |    0.06 |  99.67\nit    |    43 |  0.05 |    0.05 |  99.72\npl    |    36 |  0.04 |    0.04 |  99.76\nfa    |    20 |  0.02 |    0.02 |  99.78\nro    |    20 |  0.02 |    0.02 |  99.80\nja    |    19 |  0.02 |    0.02 |  99.82\nca    |    16 |  0.02 |    0.02 |  99.84\nuk    |    16 |  0.02 |    0.02 |  99.86\nar    |    15 |  0.02 |    0.02 |  99.87\nth    |    14 |  0.01 |    0.01 |  99.89\nnl    |    13 |  0.01 |    0.01 |  99.90\nsv    |    13 |  0.01 |    0.01 |  99.91\nhr    |    11 |  0.01 |    0.01 |  99.93\nhu    |    11 |  0.01 |    0.01 |  99.94\nsl    |    10 |  0.01 |    0.01 |  99.95\naf    |     8 |  0.01 |    0.01 |  99.96\ncs    |     6 |  0.01 |    0.01 |  99.96\nda    |     5 |  0.01 |    0.01 |  99.97\nel    |     5 |  0.01 |    0.01 |  99.97\nbg    |     4 |  0.00 |    0.00 |  99.98\nzh-cn |     4 |  0.00 |    0.00 |  99.98\net    |     3 |  0.00 |    0.00 |  99.99\nlt    |     3 |  0.00 |    0.00 |  99.99\nfi    |     2 |  0.00 |    0.00 |  99.99\nmk    |     2 |  0.00 |    0.00 |  99.99\nno    |     2 |  0.00 |    0.00 | 100.00\nta    |     2 |  0.00 |    0.00 | 100.00\ncy    |     1 |  0.00 |    0.00 | 100.00\nhi    |     1 |  0.00 |    0.00 | 100.00\n&lt;NA&gt;  |   179 |  0.19 |    &lt;NA&gt; |   &lt;NA&gt;\n\n\nL√∂sung anzeigen\n# Notiz(en)\n# - √úberwiegende Anzahl der heruntergeladenen Datenbankbeitr√§ge ist in englischer Sprache (en) verfasst.\n# -Die Sprache en wird als Filterkriterium f√ºr die Erstellung des Subsamples genutzt, auch um sp√§tere Probleme bei der Analyse durch multi-linguale Texte zu vermeiden.\n\n\n\n\nüìã Exercise 2: Typ der Publikationen\n\n\n\n\n\n\nZiel der Aufgabe\n\n\n\n\nIdentifizieren Sie die f√ºr die Untersuchung relevanten Artikel auf Basis deres Typen (type).\nHintergrundinformation zur Variable type finden Sie in der API-Dokumentation von OpenAlex.\n\n\n\n\nNutzen Sie die Funktion sjmisc::frq() und schauen Sie sich im Datensatz review_works_correct die Variablen type an. Verwenden Sie das Argument sort.frq = \"desc\", um die Typen in Abh√§ngigkeit Ihrer H√§ufigkeit absteigend zu sortieren.\nNotieren Sie sich die Auspr√§gungen der Variable type, die aus Ihrer Sicht sp√§ter bei üìã Exercise 4: Erstellung Subsample als Filter genutzt werden soll.\n\n\n\nL√∂sung anzeigen\nreview_works_correct %&gt;% \n    sjmisc::frq(\"type\", sort.frq = \"desc\")\n\n\ntype &lt;character&gt; \n# total N=93655 valid N=93655 mean=3.09 sd=4.64\n\nValue                   |     N | Raw % | Valid % | Cum. %\n----------------------------------------------------------\narticle                 | 73716 | 78.71 |   78.71 |  78.71\nreview                  |  6437 |  6.87 |    6.87 |  85.58\npreprint                |  3570 |  3.81 |    3.81 |  89.40\nbook-chapter            |  2860 |  3.05 |    3.05 |  92.45\nlibguides               |  2414 |  2.58 |    2.58 |  95.03\ndataset                 |  1471 |  1.57 |    1.57 |  96.60\npeer-review             |  1400 |  1.49 |    1.49 |  98.09\ndissertation            |   975 |  1.04 |    1.04 |  99.13\nreport                  |   422 |  0.45 |    0.45 |  99.58\nbook                    |   167 |  0.18 |    0.18 |  99.76\nother                   |    64 |  0.07 |    0.07 |  99.83\nparatext                |    57 |  0.06 |    0.06 |  99.89\nerratum                 |    47 |  0.05 |    0.05 |  99.94\nletter                  |    25 |  0.03 |    0.03 |  99.97\neditorial               |    17 |  0.02 |    0.02 |  99.99\nsupplementary-materials |     7 |  0.01 |    0.01 |  99.99\nreference-entry         |     6 |  0.01 |    0.01 | 100.00\n&lt;NA&gt;                    |     0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\n\nL√∂sung anzeigen\n# Notiz(en)\n# - √úberwiegende Anzahl der heruntergeladenen Datenbankbeitr√§ge sind `article.`\n# - Die folgenden Auswertungen beziehen sich auf die Publikationen des Typs article, da uns besonders die praktische Anwendung/Umsetzung der Methode in verschiedenen Kontexten interessiert. L√§ge der Fokus auf (die Entwicklung) der Methode selbst, w√§re vermutlich eher die Typen book oder book-chapter relevant.\n\n\n\n\nüìã Exercise 3: Forschungsfeld der Publikationen\n\n\n\n\n\n\nZiel der Aufgabe\n\n\n\n\nIdentifizieren Sie die f√ºr die Untersuchung relevanten Artikel auf Basis des von OpenAlex dem Artikel zugeordnenten Foschungsfeldes (field))\nHintergrundinformation zur Variable field finden Sie in der API-Dokumentation von OpenAlex.\n\n\n\n\nBasierend auf dem Datensatz review_works_correct\n\nnutzen Sie die Funktion unnest() um die Variablen der topics-Liste zu extrahieren. Verwenden Sie dabei das Argument names_sep = \"_\". um doppelte Variablennamen durch Hinzuf√ºgen des Prefixes topics_ zu verhindern.\nfiltern Sie anschlie√üen mit Hilfe der Funktion filter und der Variable bzw. dem Argument topics_name == \"field\" nur die Informationen zum Forschungsfeld, sowie mit der Variable bzw. dem Argument topics_i == \"1\" nur die erste Zuordnung.\nNutzen Sie die Funktion sjmisc::frq() und schauen Sie sich die Variablen topics_display_name an. Verwenden Sie das Argument sort.frq = \"desc\", um die Forschungsfelder in Abh√§ngigkeit Ihrer H√§ufigkeit absteigend zu sortieren.\n\nNotieren Sie sich die Auspr√§gungen der Variable topics_display_name, die aus Ihrer Sicht sp√§ter bei üìã Exercise 4: Erstellung Subsample als Filter genutzt werden soll.\n\n\n\nL√∂sung anzeigen\nreview_works_correct %&gt;% \n    unnest(topics, names_sep = \"_\") %&gt;%\n    filter(topics_name == \"field\") %&gt;% \n    filter(topics_i == \"1\") %&gt;% \n    sjmisc::frq(\"topics_display_name\", sort.frq = \"desc\")\n\n\ntopics_display_name &lt;character&gt; \n# total N=93655 valid N=93655 mean=4.41 sd=1.62\n\nValue                               |     N | Raw % | Valid % | Cum. %\n----------------------------------------------------------------------\nSocial Sciences                     | 30580 | 32.65 |   32.65 |  32.65\nPsychology                          | 29054 | 31.02 |   31.02 |  63.67\nBusiness, Management and Accounting | 15558 | 16.61 |   16.61 |  80.29\nDecision Sciences                   |  7261 |  7.75 |    7.75 |  88.04\nEconomics, Econometrics and Finance |  6796 |  7.26 |    7.26 |  95.30\nArts and Humanities                 |  4406 |  4.70 |    4.70 | 100.00\n&lt;NA&gt;                                |     0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\n\nL√∂sung anzeigen\n# Notiz(en)\n# - Focus auf \"prim√§re\" Forschungsfelder `Social Sciences` & `Psychology`\n\n\n\n\nüìã Exercise 4: Erstellung Subsample\n\n\n\n\n\n\nZiel der Aufgabe\n\n\n\n\nErstellung sie den Datensatz review_subsample, in dem Sie mit Hilfe der Funktionen select() und/oder filter() das Datenmaterial weiter eingrenzen. Sie k√∂nnen sich sowohl auf die Variablen aus der √úbung, als auch auf die aus der Sitzung (bzw. den Slides) beziehen.\nDer Code dieses Chunks wird in der n√§chsten Sitzung ben√∂tigt bzw. besprochen, halten Sie diesen deshalb bitte bereit.\n\n\n\n\n\nL√∂sung anzeigen\nreview_subsample &lt;- review_works_correct %&gt;% \n  filter(language == \"en\") %&gt;% # nur englischsprachige Eintr√§ge\n  filter(type == \"article\") %&gt;% # nur Artikel\n  # Datentranformation\n  unnest(topics, names_sep = \"_\") %&gt;%\n  filter(topics_name == \"field\") %&gt;% \n  filter(topics_i == \"1\") %&gt;% \n  filter(\n    topics_display_name == \"Social Sciences\"|\n    topics_display_name == \"Psychology\"\n    ) %&gt;%\n  glimpse()\n\n\nRows: 45,221\nColumns: 45\n$ id                          &lt;chr&gt; \"https://openalex.org/W4293003987\", \"https‚Ä¶\n$ title                       &lt;chr&gt; \"The WHO-5 Well-Being Index: A Systematic ‚Ä¶\n$ display_name                &lt;chr&gt; \"The WHO-5 Well-Being Index: A Systematic ‚Ä¶\n$ author                      &lt;list&gt; [&lt;data.frame[4 x 12]&gt;], [&lt;data.frame[2 x ‚Ä¶\n$ ab                          &lt;chr&gt; \"The 5-item World Health Organization Well‚Ä¶\n$ publication_date            &lt;chr&gt; \"2015-01-01\", \"2017-08-28\", \"2014-01-01\", ‚Ä¶\n$ relevance_score             &lt;dbl&gt; 938.7603, 752.3500, 591.2553, 576.1210, 56‚Ä¶\n$ so                          &lt;chr&gt; \"Psychotherapy and psychosomatics\", \"Journ‚Ä¶\n$ so_id                       &lt;chr&gt; \"https://openalex.org/S184803288\", \"https:‚Ä¶\n$ host_organization           &lt;chr&gt; \"Karger Publishers\", \"SAGE Publishing\", NA‚Ä¶\n$ issn_l                      &lt;chr&gt; \"0033-3190\", \"0739-456X\", NA, \"2214-7829\",‚Ä¶\n$ url                         &lt;chr&gt; \"https://doi.org/10.1159/000376585\", \"http‚Ä¶\n$ pdf_url                     &lt;chr&gt; \"https://www.karger.com/Article/Pdf/376585‚Ä¶\n$ license                     &lt;chr&gt; \"cc-by-nc\", NA, NA, \"cc-by\", NA, NA, \"cc-b‚Ä¶\n$ version                     &lt;chr&gt; \"publishedVersion\", NA, \"publishedVersion\"‚Ä¶\n$ first_page                  &lt;chr&gt; \"167\", \"93\", NA, \"89\", \"55\", \"2150\", \"e356‚Ä¶\n$ last_page                   &lt;chr&gt; \"176\", \"112\", NA, \"106\", \"64\", \"2159\", \"e3‚Ä¶\n$ volume                      &lt;chr&gt; \"84\", \"39\", NA, \"6\", \"277\", \"32\", \"2\", \"24‚Ä¶\n$ issue                       &lt;chr&gt; \"3\", \"1\", NA, NA, NA, \"19\", \"8\", NA, \"9\", ‚Ä¶\n$ is_oa                       &lt;lgl&gt; TRUE, FALSE, TRUE, TRUE, TRUE, FALSE, TRUE‚Ä¶\n$ is_oa_anywhere              &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE,‚Ä¶\n$ oa_status                   &lt;chr&gt; \"hybrid\", \"green\", \"bronze\", \"gold\", \"bron‚Ä¶\n$ oa_url                      &lt;chr&gt; \"https://www.karger.com/Article/Pdf/376585‚Ä¶\n$ any_repository_has_fulltext &lt;lgl&gt; FALSE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE‚Ä¶\n$ language                    &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", ‚Ä¶\n$ grants                      &lt;list&gt; NA, NA, NA, &lt;\"https://openalex.org/F43203‚Ä¶\n$ cited_by_count              &lt;int&gt; 2657, 1375, 2568, 803, 3664, 1553, 2895, 9‚Ä¶\n$ counts_by_year              &lt;list&gt; [&lt;data.frame[11 x 2]&gt;], [&lt;data.frame[7 x ‚Ä¶\n$ publication_year            &lt;int&gt; 2015, 2017, 2014, 2016, 2020, 2014, 2017, ‚Ä¶\n$ cited_by_api_url            &lt;chr&gt; \"https://api.openalex.org/works?filter=cit‚Ä¶\n$ ids                         &lt;list&gt; &lt;\"https://openalex.org/W4293003987\", \"htt‚Ä¶\n$ doi                         &lt;chr&gt; \"https://doi.org/10.1159/000376585\", \"http‚Ä¶\n$ type                        &lt;chr&gt; \"article\", \"article\", \"article\", \"article\"‚Ä¶\n$ referenced_works            &lt;list&gt; &lt;\"https://openalex.org/W1492518593\", \"htt‚Ä¶\n$ related_works               &lt;list&gt; &lt;\"https://openalex.org/W3020194755\", \"htt‚Ä¶\n$ is_paratext                 &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, ‚Ä¶\n$ is_retracted                &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, ‚Ä¶\n$ concepts                    &lt;list&gt; [&lt;data.frame[7 x 5]&gt;], [&lt;data.frame[18 x ‚Ä¶\n$ topics_i                    &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ‚Ä¶\n$ topics_score                &lt;dbl&gt; 0.9926, 0.9050, 0.9995, 0.9987, 0.9999, 1.‚Ä¶\n$ topics_name                 &lt;chr&gt; \"field\", \"field\", \"field\", \"field\", \"field‚Ä¶\n$ topics_id                   &lt;chr&gt; \"https://openalex.org/fields/32\", \"https:/‚Ä¶\n$ topics_display_name         &lt;chr&gt; \"Psychology\", \"Social Sciences\", \"Psycholo‚Ä¶\n$ publication_year_fct        &lt;fct&gt; 2015, 2017, 2014, 2016, 2020, 2014, 2017, ‚Ä¶\n$ type_fct                    &lt;fct&gt; article, article, article, article, articl‚Ä¶"
  },
  {
    "objectID": "exercises/ms-showcase-10.html",
    "href": "exercises/ms-showcase-10.html",
    "title": "Unsupervised Machine Learning II",
    "section": "",
    "text": "Link to slides"
  },
  {
    "objectID": "exercises/ms-showcase-10.html#preparation",
    "href": "exercises/ms-showcase-10.html#preparation",
    "title": "Unsupervised Machine Learning II",
    "section": "Preparation",
    "text": "Preparation\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n    here, qs, # file management\n    magrittr, janitor, # data wrangling\n    easystats, sjmisc, # data analysis\n    gt, gtExtras, # table visualization\n    ggpubr, ggwordcloud, # visualization\n    # text analysis    \n    tidytext, widyr, # based on tidytext\n    quanteda, # based on quanteda\n    quanteda.textmodels, quanteda.textplots, quanteda.textstats, \n    stm, # structural topic modeling\n    openalexR, pushoverr, tictoc, \n    tidyverse # load last to avoid masking issues\n  )\n\n\nreview_works &lt;- qs::qread(here(\"data/session-07/openalex-review_works-2013_2023.qs\"))\n\n# Create correct data\nreview_subsample &lt;- review_works %&gt;% \n    # Create additional factor variables\n    mutate(\n        publication_year_fct = as.factor(publication_year), \n        type_fct = as.factor(type)\n        ) %&gt;%\n    # Eingrenzung: Sprache und Typ\n    filter(language == \"en\") %&gt;% \n    filter(type == \"article\") %&gt;%\n    # Datentranformation\n    unnest(topics, names_sep = \"_\") %&gt;%\n    filter(topics_name == \"field\") %&gt;% \n    filter(topics_i == \"1\") %&gt;% \n    # Eingrenzung: Forschungsfeldes\n    filter(\n    topics_display_name == \"Social Sciences\"|\n    topics_display_name == \"Psychology\"\n    ) %&gt;% \n    mutate(\n        field = as.factor(topics_display_name)\n    ) %&gt;% \n    # Eingrenzung: Keine Eintr√§ge ohne Abstract\n    filter(!is.na(ab))\n\n\n# Create corpus\nquanteda_corpus &lt;- review_subsample %&gt;% \n  quanteda::corpus(\n    docid_field = \"id\", \n    text_field = \"ab\"\n  )\n\n# Tokenize\nquanteda_token &lt;- quanteda_corpus %&gt;% \n  quanteda::tokens(\n    remove_punct = TRUE,\n    remove_symbols = TRUE, \n    remove_numbers = TRUE, \n    remove_url = TRUE, \n    split_tags = FALSE # keep hashtags and mentions\n  ) %&gt;% \n  quanteda::tokens_tolower() %&gt;% \n  quanteda::tokens_remove(\n    pattern = stopwords(\"en\")\n    )\n\n# Convert to Document-Feature-Matrix (DFM)\nquanteda_dfm &lt;- quanteda_token %&gt;% \n  quanteda::dfm()\n\n# Pruning\nquanteda_dfm_trim &lt;- quanteda_dfm %&gt;% \n  dfm_trim( \n    min_docfreq = 10/nrow(review_subsample),\n    max_docfreq = 0.99, \n    docfreq_type = \"prop\")\n\n# Convert for stm topic modeling\nquanteda_stm &lt;- quanteda_dfm_trim %&gt;% \n   convert(to = \"stm\")"
  },
  {
    "objectID": "exercises/ms-showcase-10.html#codechunks-aus-der-sitzung",
    "href": "exercises/ms-showcase-10.html#codechunks-aus-der-sitzung",
    "title": "Unsupervised Machine Learning II",
    "section": "Codechunks aus der Sitzung",
    "text": "Codechunks aus der Sitzung\n\n(Re-)Estimation of k = 20 with metadata\n\n# Estimate model\nstm_mdl_k20 &lt;- stm::stm(\n    documents = quanteda_stm$documents,\n    vocab = quanteda_stm$vocab, \n    prevalence =~ publication_year_fct + field, \n    K = 20, \n    seed = 42,\n    max.em.its = 1000,\n    data = quanteda_stm$meta,\n    init.type = \"Spectral\",\n    verbose = TRUE)\n\n\n# Overview\nstm_mdl_k20\n\nA topic model with 20 topics, 36650 documents and a 14322 word dictionary.\n\n\n\n\nErweiterte Auswertungen\n\nBeta-Matrix\n\n# Create tidy beta matrix\ntd_beta &lt;- tidy(stm_mdl_k20)\n\n# Output \ntd_beta\n\n# A tibble: 286,440 √ó 3\n   topic term      beta\n   &lt;int&gt; &lt;chr&gt;    &lt;dbl&gt;\n 1     1 #x0d  6.83e-44\n 2     2 #x0d  2.37e-30\n 3     3 #x0d  3.35e-45\n 4     4 #x0d  7.24e- 3\n 5     5 #x0d  1.45e-23\n 6     6 #x0d  5.23e-20\n 7     7 #x0d  2.70e-12\n 8     8 #x0d  3.32e-39\n 9     9 #x0d  8.78e- 4\n10    10 #x0d  2.58e-12\n# ‚Ñπ 286,430 more rows\n\n\n\n# Create top terms\ntop_terms &lt;- td_beta %&gt;%\n  arrange(beta) %&gt;%\n  group_by(topic) %&gt;%\n  top_n(7, beta) %&gt;%\n  arrange(-beta) %&gt;%\n  select(topic, term) %&gt;%\n  summarise(terms = list(term)) %&gt;%\n  mutate(terms = map(terms, paste, collapse = \", \")) %&gt;% \n  unnest(cols = c(terms))\n\n# Output\ntop_terms\n\n# A tibble: 20 √ó 2\n   topic terms                                                                  \n   &lt;int&gt; &lt;chr&gt;                                                                  \n 1     1 sleep, studies, eating, cognitive, weight, associated, duration        \n 2     2 health, women, gender, cultural, barriers, men, countries              \n 3     3 treatment, disorders, symptoms, disorder, depression, anxiety, therapy \n 4     4 patients, cancer, nurses, patient, music, nursing, pain                \n 5     5 literature, university, author, gt, lt, et, al                         \n 6     6 prevalence, covid-19, suicide, pandemic, studies, among, risk          \n 7     7 articles, review, study, systematic, results, search, science          \n 8     8 physical, activity, disabilities, exercise, cognitive, adults, body    \n 9     9 learning, education, students, educational, skills, teachers, teaching \n10    10 de, la, y, en, los, ÁöÑ, el                                             \n11    11 work, study, development, can, public, management, factors             \n12    12 violence, sexual, use, abuse, risk, youth, h3                          \n13    13 health, mental, care, support, people, social, family                  \n14    14 social, use, media, digital, information, technology, communication    \n15    15 ci, effect, meta-analysis, p, studies, effects, trials                 \n16    16 research, literature, review, future, findings, systematic, paper      \n17    17 children, studies, factors, relationship, adolescents, review, associa‚Ä¶\n18    18 measures, assessment, used, studies, tools, measurement, quality       \n19    19 studies, interventions, included, evidence, review, systematic, outcom‚Ä¶\n20    20 programs, school, training, interventions, outcomes, review, intervent‚Ä¶\n\n\n\ntd_beta %&gt;%\n    group_by(topic) %&gt;%\n    slice_max(beta, n = 10) %&gt;%\n    ungroup() %&gt;%\n    ggplot(aes(beta, term)) +\n    geom_col() +\n    facet_wrap(~ topic, scales = \"free\")\n\n\n\n\n\n\n\nFigure¬†1\n\n\n\n\n\n\n\nGamma-Matrix\n\n# Create tidy gamma matrix\ntd_gamma &lt;- tidy(\n  stm_mdl_k20, \n  matrix = \"gamma\", \n  document_names = names(quanteda_stm$documents)\n  )\n\n# Output \ntd_gamma\n\n# A tibble: 733,000 √ó 3\n   document                         topic    gamma\n   &lt;chr&gt;                            &lt;int&gt;    &lt;dbl&gt;\n 1 https://openalex.org/W4293003987     1 0.00473 \n 2 https://openalex.org/W2750168540     1 0.000270\n 3 https://openalex.org/W1998933811     1 0.00368 \n 4 https://openalex.org/W2547134104     1 0.00340 \n 5 https://openalex.org/W3047898105     1 0.00263 \n 6 https://openalex.org/W2149640470     1 0.00230 \n 7 https://openalex.org/W2740726397     1 0.0374  \n 8 https://openalex.org/W2974087526     1 0.00339 \n 9 https://openalex.org/W2195703978     1 0.00287 \n10 https://openalex.org/W2093916237     1 0.170   \n# ‚Ñπ 732,990 more rows\n\n\n\n\nH√§ufigkeit und Top Begriffe der Themen\n\nprevalence &lt;- td_gamma %&gt;%\n  group_by(topic) %&gt;%\n  summarise(gamma = mean(gamma)) %&gt;%\n  arrange(desc(gamma)) %&gt;%\n  left_join(top_terms, by = \"topic\") %&gt;%\n  mutate(topic = paste0(\"Topic \",sprintf(\"%02d\", topic)),\n         topic = reorder(topic, gamma))\n\n\nTable\n\nprevalence %&gt;% \n  gt() %&gt;% \n  fmt_number(\n    columns = c(gamma), \n    decimals = 2) %&gt;% \n  gtExtras::gt_theme_538()\n\n\n\n\n\n\n\ntopic\ngamma\nterms\n\n\n\n\nTopic 16\n0.12\nresearch, literature, review, future, findings, systematic, paper\n\n\nTopic 19\n0.09\nstudies, interventions, included, evidence, review, systematic, outcomes\n\n\nTopic 07\n0.08\narticles, review, study, systematic, results, search, science\n\n\nTopic 11\n0.07\nwork, study, development, can, public, management, factors\n\n\nTopic 09\n0.06\nlearning, education, students, educational, skills, teachers, teaching\n\n\nTopic 17\n0.06\nchildren, studies, factors, relationship, adolescents, review, associated\n\n\nTopic 13\n0.06\nhealth, mental, care, support, people, social, family\n\n\nTopic 14\n0.05\nsocial, use, media, digital, information, technology, communication\n\n\nTopic 15\n0.05\nci, effect, meta-analysis, p, studies, effects, trials\n\n\nTopic 03\n0.04\ntreatment, disorders, symptoms, disorder, depression, anxiety, therapy\n\n\nTopic 06\n0.04\nprevalence, covid-19, suicide, pandemic, studies, among, risk\n\n\nTopic 18\n0.04\nmeasures, assessment, used, studies, tools, measurement, quality\n\n\nTopic 20\n0.04\nprograms, school, training, interventions, outcomes, review, intervention\n\n\nTopic 02\n0.04\nhealth, women, gender, cultural, barriers, men, countries\n\n\nTopic 01\n0.03\nsleep, studies, eating, cognitive, weight, associated, duration\n\n\nTopic 05\n0.03\nliterature, university, author, gt, lt, et, al\n\n\nTopic 12\n0.03\nviolence, sexual, use, abuse, risk, youth, h3\n\n\nTopic 04\n0.03\npatients, cancer, nurses, patient, music, nursing, pain\n\n\nTopic 08\n0.03\nphysical, activity, disabilities, exercise, cognitive, adults, body\n\n\nTopic 10\n0.00\nde, la, y, en, los, ÁöÑ, el\n\n\n\n\n\n\n\n\n\nVisualization\n\nprevalence %&gt;%\n  ggplot(aes(topic, gamma, label = terms, fill = topic)) +\n  geom_col(show.legend = FALSE) +\n  geom_text(hjust = 0, nudge_y = 0.0005, size = 3) +\n  coord_flip() +\n  scale_y_continuous(\n    expand = c(0,0),\n    limits = c(0, 0.18)) +\n  theme_pubr() +\n  theme(\n    plot.title = element_text(size = 16),\n    plot.subtitle = element_text(size = 13)) +\n  labs(\n    x = NULL, y = expression(gamma),\n    title = \"Topic Prevalence in the OpenAlex Corpus\",\n    subtitle = \"With the top seven words that contribute to each topic\")\n\n\n\n\n\n\n\nFigure¬†2\n\n\n\n\n\n\n\n\nEffect metadata\n\neffects &lt;- estimateEffect(\n  1:20 ~ publication_year_fct + field, \n  stm_mdl_k20, \n  meta = quanteda_stm$meta)\n\n\n# Comparison\n# Effects of covariates on Topic 6\neffects %&gt;% summary(topics = 6)\n\n\nCall:\nestimateEffect(formula = 1:20 ~ publication_year_fct + field, \n    stmobj = stm_mdl_k20, metadata = quanteda_stm$meta)\n\n\nTopic 6:\n\nCoefficients:\n                           Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               0.0397265  0.0041205   9.641  &lt; 2e-16 ***\npublication_year_fct2014 -0.0016197  0.0056966  -0.284    0.776    \npublication_year_fct2015 -0.0024181  0.0054444  -0.444    0.657    \npublication_year_fct2016 -0.0032024  0.0052295  -0.612    0.540    \npublication_year_fct2017 -0.0033798  0.0047017  -0.719    0.472    \npublication_year_fct2018 -0.0048699  0.0048575  -1.003    0.316    \npublication_year_fct2019 -0.0009479  0.0046170  -0.205    0.837    \npublication_year_fct2020  0.0059633  0.0045198   1.319    0.187    \npublication_year_fct2021  0.0219055  0.0045459   4.819 1.45e-06 ***\npublication_year_fct2022  0.0254267  0.0044530   5.710 1.14e-08 ***\npublication_year_fct2023  0.0125154  0.0044587   2.807    0.005 ** \nfieldSocial Sciences     -0.0103125  0.0014550  -7.088 1.39e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Effects of covariates on Topic 16\neffects %&gt;% summary(topics = 16)\n\n\nCall:\nestimateEffect(formula = 1:20 ~ publication_year_fct + field, \n    stmobj = stm_mdl_k20, metadata = quanteda_stm$meta)\n\n\nTopic 16:\n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               0.064203   0.005203  12.339   &lt;2e-16 ***\npublication_year_fct2014  0.005880   0.007346   0.800   0.4235    \npublication_year_fct2015  0.013201   0.006726   1.963   0.0497 *  \npublication_year_fct2016  0.004145   0.006331   0.655   0.5127    \npublication_year_fct2017  0.006875   0.006313   1.089   0.2762    \npublication_year_fct2018  0.001558   0.006391   0.244   0.8074    \npublication_year_fct2019 -0.001864   0.005705  -0.327   0.7439    \npublication_year_fct2020  0.002190   0.005882   0.372   0.7097    \npublication_year_fct2021  0.002234   0.005559   0.402   0.6877    \npublication_year_fct2022 -0.001616   0.005693  -0.284   0.7766    \npublication_year_fct2023  0.007731   0.005593   1.382   0.1669    \nfieldSocial Sciences      0.085855   0.001704  50.375   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\neffects %&gt;%\n  tidy() %&gt;% \n  filter(\n    term != \"(Intercept)\",\n    term == \"fieldSocial Sciences\") %&gt;% \n    select(-term) %&gt;% \n  gt() %&gt;% \n    fmt_number(\n      columns = -c(topic),\n      decimals = 3\n    ) %&gt;% \n  # Color social science topics \"blue\"\n  data_color(\n    columns = topic,\n    rows = estimate &gt; 0,\n    method = \"numeric\",\n    palette = c(\"#04316A\"),\n    alpha = 0.4\n  ) %&gt;% \n  # Color psychology topics \"yellow\"\n  data_color(\n    columns = topic,\n    rows = estimate &lt; 0,\n    method = \"numeric\",\n    palette = c(\"#D3A518\"),\n    alpha = 0.4\n  ) %&gt;% \n  # # Color effect size for estimation\n  # data_color(\n  #   columns = estimate,\n  #   method = \"numeric\",\n  #   palette = \"viridis\"\n  # ) %&gt;% \n  # # Color insignificant p-values\n  # data_color(\n  #   columns = p.value,\n  #   rows = p.value &gt; 0.05,\n  #   method = \"numeric\",\n  #   palette = c(\"#C50F3C\", \"#C50F3C\")\n  # ) %&gt;% \n  gtExtras::gt_theme_538()    \n\n\n\n\n\n\n\ntopic\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n1\n‚àí0.056\n0.001\n‚àí39.497\n0.000\n\n\n2\n0.014\n0.001\n10.569\n0.000\n\n\n3\n‚àí0.069\n0.001\n‚àí51.306\n0.000\n\n\n4\n‚àí0.014\n0.001\n‚àí11.760\n0.000\n\n\n5\n0.027\n0.001\n19.712\n0.000\n\n\n6\n‚àí0.010\n0.001\n‚àí7.024\n0.000\n\n\n7\n0.014\n0.001\n11.299\n0.000\n\n\n8\n‚àí0.015\n0.001\n‚àí12.789\n0.000\n\n\n9\n0.052\n0.002\n28.032\n0.000\n\n\n10\n‚àí0.001\n0.001\n‚àí2.432\n0.015\n\n\n11\n0.091\n0.002\n55.019\n0.000\n\n\n12\n0.003\n0.001\n2.320\n0.020\n\n\n13\n‚àí0.025\n0.001\n‚àí18.543\n0.000\n\n\n14\n0.044\n0.002\n29.024\n0.000\n\n\n15\n‚àí0.060\n0.002\n‚àí36.809\n0.000\n\n\n16\n0.086\n0.002\n50.103\n0.000\n\n\n17\n‚àí0.033\n0.001\n‚àí23.177\n0.000\n\n\n18\n‚àí0.012\n0.001\n‚àí10.842\n0.000\n\n\n19\n‚àí0.044\n0.001\n‚àí32.745\n0.000\n\n\n20\n0.008\n0.001\n7.706\n0.000\n\n\n\n\n\n\n\n\n\n\nZusammenf√ºhrung der Daten\n\nMerge mit Stammdaten\n\ngamma_export &lt;- stm_mdl_k20 %&gt;% \n  tidytext::tidy(\n    matrix = \"gamma\", \n    document_names = names(quanteda_stm$documents)) %&gt;%\n  dplyr::group_by(document) %&gt;% \n  dplyr::slice_max(gamma) %&gt;% \n  dplyr::mutate(main_topic = ifelse(gamma &gt; 0.5, topic, NA)) %&gt;% \n      rename(\n        top_topic = topic,\n        top_gamma = gamma) %&gt;% \n  dplyr::ungroup() %&gt;% \n  dplyr::left_join(review_subsample, by = c(\"document\" = \"id\")) %&gt;% \n  dplyr::rename(id = document) %&gt;% \n  dplyr::mutate(\n    stm_topic = as.factor(paste(\"Topic\", sprintf(\"%02d\", top_topic)))\n  )\n\n\n\nAnzahl der Abstracts nach Thema\n\ngamma_export %&gt;% \n  ggplot(aes(x = fct_rev(fct_infreq(stm_topic)))) +\n  geom_bar() +\n  coord_flip() +\n  theme_pubr()\n\n\n\n\n\n\n\nFigure¬†3\n\n\n\n\n\n\n\nAnzahl der Abstracts nach Thema und Feld\n\ngamma_export %&gt;% \n  gtsummary::tbl_cross(\n    row = stm_topic, \n    col = field,\n    percent = \"row\",\n    )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfield\nTotal\n\n\nPsychology\nSocial Sciences\n\n\n\n\nstm_topic\n\n\n\n\n\n\n\n\n¬†¬†¬†¬†Topic 01\n1,470 (94%)\n87 (5.6%)\n1,557 (100%)\n\n\n¬†¬†¬†¬†Topic 02\n649 (50%)\n652 (50%)\n1,301 (100%)\n\n\n¬†¬†¬†¬†Topic 03\n1,481 (95%)\n71 (4.6%)\n1,552 (100%)\n\n\n¬†¬†¬†¬†Topic 04\n459 (60%)\n306 (40%)\n765 (100%)\n\n\n¬†¬†¬†¬†Topic 05\n301 (29%)\n734 (71%)\n1,035 (100%)\n\n\n¬†¬†¬†¬†Topic 06\n1,116 (60%)\n745 (40%)\n1,861 (100%)\n\n\n¬†¬†¬†¬†Topic 07\n825 (44%)\n1,051 (56%)\n1,876 (100%)\n\n\n¬†¬†¬†¬†Topic 08\n540 (70%)\n227 (30%)\n767 (100%)\n\n\n¬†¬†¬†¬†Topic 09\n821 (27%)\n2,260 (73%)\n3,081 (100%)\n\n\n¬†¬†¬†¬†Topic 10\n114 (67%)\n57 (33%)\n171 (100%)\n\n\n¬†¬†¬†¬†Topic 11\n303 (12%)\n2,244 (88%)\n2,547 (100%)\n\n\n¬†¬†¬†¬†Topic 12\n439 (41%)\n630 (59%)\n1,069 (100%)\n\n\n¬†¬†¬†¬†Topic 13\n1,477 (67%)\n722 (33%)\n2,199 (100%)\n\n\n¬†¬†¬†¬†Topic 14\n509 (28%)\n1,277 (72%)\n1,786 (100%)\n\n\n¬†¬†¬†¬†Topic 15\n1,815 (86%)\n299 (14%)\n2,114 (100%)\n\n\n¬†¬†¬†¬†Topic 16\n1,369 (27%)\n3,742 (73%)\n5,111 (100%)\n\n\n¬†¬†¬†¬†Topic 17\n1,672 (75%)\n566 (25%)\n2,238 (100%)\n\n\n¬†¬†¬†¬†Topic 18\n737 (65%)\n396 (35%)\n1,133 (100%)\n\n\n¬†¬†¬†¬†Topic 19\n2,552 (70%)\n1,118 (30%)\n3,670 (100%)\n\n\n¬†¬†¬†¬†Topic 20\n342 (42%)\n475 (58%)\n817 (100%)\n\n\nTotal\n18,991 (52%)\n17,659 (48%)\n36,650 (100%)\n\n\n\n\n\n\n\n\n\nFokus: Thema 16\n\ngamma_export %&gt;% \n  filter(stm_topic == \"Topic 16\") %&gt;%\n  arrange(-top_gamma) %&gt;%\n  select(title, so, top_gamma, type, ab) %&gt;%\n  slice_head(n = 3) %&gt;% \n  gt() %&gt;% \n  fmt_number(\n    columns = c(top_gamma), \n    decimals = 2) %&gt;% \n  gtExtras::gt_theme_538()\n\n\n\n\n\n\n\ntitle\nso\ntop_gamma\ntype\nab\n\n\n\n\nTheory of Knowledge for Literature Reviews: An Epistemological Model, Taxonomy and Empirical Analysis of IS Literature\nNA\n0.93\narticle\nLiterature reviews play an important role in the development of knowledge. Yet, we observe a lack of theoretical underpinning of and epistemological insights into how literature reviews can contribute to knowledge creation and have actually contributed in the IS discipline. To address these theoretical and empirical research gaps, we suggest a novel epistemological model of literature reviews. This model allows us to align different contributions of literature reviews with their underlying knowledge conversions - thereby building a bridge between the previously largely unconnected fields of literature reviews and epistemology. We evaluate the appropriateness of the model by conducting an empirical analysis of 173 IS literature reviews which were published in 39 pertinent IS journals between 2000 and 2014. Based on this analysis, we derive an epistemological taxonomy of IS literature reviews, which complements previously suggested typologies.\n\n\nTheory of Knowledge for Literature Reviews: An Epistemological Model, Taxonomy and Empirical Analysis of IS Literature Completed Research Paper\nNA\n0.93\narticle\nLiterature reviews play an important role in the development of knowledge. Yet, we observe a lack of theoretical underpinning of and epistemological insights into how literature reviews can contribute to knowledge creation and have actually contributed in the IS discipline. To address these theoretical and empirical research gaps, we suggest a novel epistemological model of literature reviews. This model allows us to align different contributions of literature reviews with their underlying knowledge conversions - thereby building a bridge between the previously largely unconnected fields of literature reviews and epistemology. We evaluate the appropriateness of the model by conducting an empirical analysis of 173 IS literature reviews which were published in 39 pertinent IS journals between 2000 and 2014. Based on this analysis, we derive an epistemological taxonomy of IS literature reviews, which complements previously suggested typologies.\n\n\nRelationality in negotiations: a systematic review and propositions for future research\nÀúThe ≈ìinternational journal of conflict management/International journal of conflict management\n0.93\narticle\nPurpose The purpose of this paper is to systematically review and analyze the important, yet under-researched, topic of relationality in negotiations and propose new directions for future negotiation research. Design/methodology/approach This paper conducts a systematic review of negotiation literature related to relationality from multiple disciplines. Thirty-nine leading and topical academic journals are selected and 574 papers on negotiation are reviewed from 1990 to 2014. Based on the systematic review, propositions regarding the rationales for relationality in negotiations are developed and future research avenues in this area are discussed. Findings Of 574 papers on negotiations published in 39 peer-reviewed journals between 1990 and 2014, only 18 papers have studied and discussed relationality in negotiations. This suggests that relationality as a theoretical theme has long been under-researched in negotiation research. For future research, this paper proposes to incorporate the dynamic, cultural and mechanism perspectives, and to use a qualitative approach to study relationality in negotiations. Originality/value This paper presents the first systematic review of the negotiation literature on relationality, and identifies new research topics on relationality in negotiations. In so doing, this research opens new avenues for future negotiation research on relationality.\n\n\n\n\n\n\n\n\n\nFokus: Thema 6\n\ngamma_export %&gt;% \n  filter(stm_topic == \"Topic 06\") %&gt;%\n  arrange(-top_gamma) %&gt;%\n  select(title, so, top_gamma, type, ab) %&gt;%\n  slice_head(n = 3) %&gt;% \n  gt() %&gt;% \n  fmt_number(\n    columns = c(top_gamma), \n    decimals = 2) %&gt;% \n  gtExtras::gt_theme_538()\n\n\n\n\n\n\n\ntitle\nso\ntop_gamma\ntype\nab\n\n\n\n\nThe Acceptance of COVID-19 Vaccine: A Global Rapid Systematic Review and Meta-Analysis\nSocial Science Research Network\n0.89\narticle\nBackground: Vaccination seems to be the most effective way to prevent and control the spread of COVID-19, a disease that has been playing havoc with the lives of over 7 billion people across the globe. Vaccine hesitancy is probably the most common problem worldwide. This study aims to inspect the COVID-19 vaccine acceptance rates worldwide among the general population and healthcare workers. In addition, it compares the vaccine acceptance rates between the pre-and post-vaccine approval periods.Method: A systematic search was conducted on April 25, 2021, through PubMed, MEDLINE, Web of Science, and GOOGLE SCHOLAR databases using PRISMA and MOOSE statements. Q-test, and statistics were used to search for heterogeneity, and Eggers's test and funnel plot were applied to assess the publication bias. The random-effects model was used to estimate the pooled acceptance rates of the COVID-19 vaccines.Results: The combined COVID-19 vaccine acceptance rate among the general population and healthcare workers (n=1,581,562) was estimated at 61.74%. The vaccine acceptance rate among the general population was 62.66% and the rate among healthcare workers was 57.89%. The acceptance rate decreased from 67.21% to 53.44% among the general population and remained constant among healthcare workers during the pre and post-vaccine approval periods. The acceptance rates also vary in different regions of the world. The highest acceptance rate was found in Western Pacific Region (67.85%) and the lowest was found in African Region (39.51%).Conclusion: Low COVID-19 vaccine acceptance rate might be a massive barrier to getting rid of the pandemic. More researches are needed to address the responsible factors influencing the global rate of COVID-19 vaccine acceptance. Integrated global efforts are required to remove the barriers.\n\n\nPrevalence of Suicidal Behavior Among Students in South-East Asia: A Systematic Review and Meta-Analysis\nArchives of Suicide Research\n0.88\narticle\nEstimation of rates of suicidal behaviors (ideation, plan, and attempt) would help to understand the burden and prioritize prevention strategies. However, no attempt to assess suicidal behavior among students was identified in South-East Asia (SEA). We aimed to assess the prevalence of suicidal behavior (ideation, plan, and attempt) among students in SEA.We followed PRISMA 2020 guidelines and registered the protocol in PROSPERO (CRD42022353438). We searched in Medline, Embase, and PsycINFO and performed meta-analyses to pool the lifetime, 1-year, and point prevalence rates for suicidal ideation, plans, and attempts. We considered the duration of a month for point prevalence.The search identified 40 separate populations from which 46 were included in the analyses, as some studies included samples from multiple countries. The pooled prevalence of suicidal ideation was 17.4% (confidence interval [95% CI], 12.4%-23.9%) for lifetime, 9.33% (95% CI, 7.2%-12%) for the past year, and 4.8% (95% CI, 3.6%-6.4%) for the present time. The pooled prevalence of suicide plans was 9% (95% CI, 6.2%-12.9%) for lifetime, 7.3% (95% CI, 5.1%-10.3%) for the past year, and 2.3% (95% CI, 0.8%-6.7%) for the present time. The pooled prevalence of suicide attempts was 5.2% (95% CI, 3.5%-7.8%) for lifetime and 4.5% (95% CI, 3.4%-5.8%) for the past year. Higher rates of suicide attempts in the lifetime were noted in Nepal (10%) and Bangladesh (9%), while lower rates were reported in India (4%) and Indonesia (5%).Suicidal behaviors are a common phenomenon among students in the SEA region. These findings call for integrated, multisectoral efforts to prevent suicidal behaviors in this group.\n\n\nFirst COVID-19 Booster Dose in the General Population: A Systematic Review and Meta-Analysis of Willingness and Its Predictors\nVaccines\n0.88\narticle\nThe emergence of breakthrough infections and new highly contagious variants of SARS-CoV-2 threaten the immunization in individuals who had completed the primary COVID-19 vaccination. This systematic review and meta-analysis investigated, for the first time, acceptance of the first COVID-19 booster dose and its associated factors among fully vaccinated individuals. We followed the PRISMA guidelines. We searched Scopus, Web of Science, Medline, PubMed, ProQuest, CINAHL and medrxiv from inception to 21 May 2022. We found 14 studies including 104,047 fully vaccinated individuals. The prevalence of individuals who intend to accept a booster was 79.0%, while the prevalence of unsure individuals was 12.6%, and the prevalence of individuals that intend to refuse a booster was 14.3%. The main predictors of willingness were older age, flu vaccination in the previous season, and confidence in COVID-19 vaccination. The most important reasons for decline were adverse reactions and discomfort experienced after previous COVID-19 vaccine doses and concerns for serious adverse reactions to COVID-19 booster doses. Considering the burden of COVID-19, a high acceptance rate of booster doses could be critical in controlling the pandemic. Our findings are innovative and could help policymakers to design and implement specific COVID-19 vaccination programs in order to decrease booster vaccine hesitancy.\n\n\n\n\n\n\n\n\n\n\nDie Suche nach dem optimalen k\n\n# Define parameters\nfuture::plan(future::multisession()) # use multiple sessions\ntopic_range &lt;- seq(from = 10, to = 100, by = 10) \n\n# Initiate notifications & time tracking\ntic(\"STM extended search\")\n\n# Estimate models\nstm_search  &lt;- tibble(k = topic_range) %&gt;%\n    mutate(\n        mdl = furrr::future_map(\n            k, \n            ~stm::stm(\n                documents = quanteda_stm$documents,\n                vocab = quanteda_stm$vocab, \n                prevalence =~ publication_year_fct + field,\n                K = ., \n                seed = 42,\n                max.em.its = 1000,\n                data = quanteda_stm$meta,\n                init.type = \"Spectral\",\n                verbose = FALSE),\n            .options = furrr::furrr_options(seed = 42)\n            )\n    )\n\n# Sent status update and finish time tracking\ntoc(log = TRUE)\n\n\n\nErstellung des ‚ÄúHeldouts‚Äù\n\nheldout &lt;- make.heldout(\n  documents = quanteda_stm$documents,\n  vocab = quanteda_stm$vocab,\n  seed = 42)\n\n\n\nEvaluation der Modelle\n\nstm_search$results &lt;- stm_search %&gt;%\n  mutate(\n    exclusivity = map(mdl, exclusivity),\n    semantic_coherence = map(mdl, semanticCoherence, quanteda_stm$documents),\n    eval_heldout = map(mdl, eval.heldout, heldout$missing),\n    residual = map(mdl, checkResiduals, quanteda_stm$documents),\n    bound =  map_dbl(mdl, function(x) max(x$convergence$bound)),\n    lfact = map_dbl(mdl, function(x) lfactorial(x$settings$dim$K)),\n    lbound = bound + lfact,\n    iterations = map_dbl(mdl, function(x) length(x$convergence$bound)))\n\n\nVergleich verschiedener Statistiken\n\nstm_search$results %&gt;% \n  # Create data for graph\n  transmute(\n    k, \n    `Lower bound` = lbound,\n    Residuals = map_dbl(residual, \"dispersion\"),\n    `Semantic coherence` = map_dbl(semantic_coherence, mean),\n    `Held-out likelihood` = map_dbl(eval_heldout, \"expected.heldout\")\n    ) %&gt;%   \n  gather(Metric, Value, -k) %&gt;%\n  # Create graph\n  ggplot(aes(k, Value, color = Metric)) +\n    geom_line(linewidth = 1.5, alpha = 0.7, show.legend = FALSE) +\n    geom_point(size = 3) +\n    # Add marker\n    geom_vline(aes(xintercept = 20), color = \"#C77CFF\", alpha = .5) +\n    geom_vline(aes(xintercept = 40), color = \"#00BFC4\", alpha = .5) +\n    geom_vline(aes(xintercept = 60), color = \"#C77CFF\", alpha = .5) +\n    geom_vline(aes(xintercept = 70), color = \"#00BFC4\", alpha = .5) +  \n    scale_x_continuous(breaks = seq(from = 10, to = 100, by = 10)) +\n    facet_wrap(~Metric, scales = \"free_y\") +\n    labs(x = \"K (number of topics)\",\n        y = NULL,\n        title = \"Model diagnostics by number of topics\"\n        ) +\n    theme_pubr()\n\n\n\n\n\n\n\nFigure¬†4\n\n\n\n\n\n\nstm_search$results %&gt;% \n  select(k, exclusivity, semantic_coherence) %&gt;% \n  filter(k %in% c(20, 40, 70)) %&gt;%\n  unnest(cols = c(exclusivity, semantic_coherence)) %&gt;%\n  mutate(k = as.factor(k)) %&gt;%\n  ggplot(aes(semantic_coherence, exclusivity, color = k)) +\n  geom_point(size = 2, alpha = 0.7) +\n  labs(x = \"Semantic coherence\",\n       y = \"Exclusivity\",\n       title = \"Comparing exclusivity and semantic coherence\",\n       subtitle = \"Models with fewer topics have higher semantic coherence for more topics, but lower exclusivity\"\n       ) +\n  theme_minimal() \n\n\n\n\n\n\n\nFigure¬†5"
  },
  {
    "objectID": "exercises/ms-exercise-07.html",
    "href": "exercises/ms-exercise-07.html",
    "title": "API mining and data wrangling with R",
    "section": "",
    "text": "Link to source file"
  },
  {
    "objectID": "exercises/ms-exercise-07.html#background",
    "href": "exercises/ms-exercise-07.html#background",
    "title": "API mining and data wrangling with R",
    "section": "Background",
    "text": "Background\n\n\n\n\n\n\nTodays‚Äôs data basis: OpenAlex\n\n\n\nOpenAlex is a free and open catalog of the global research system. It‚Äôs named after the ancient Library of Alexandria and made by the nonprofit OurResearch.\n\n\n\n\n\n\n\n\nAt the heart of OpenAlex is our dataset‚Äîa catalog of works. A work is any sort of scholarly output. A research article is one kind of work, but there are others such as datasets, books, and dissertations. We keep track of these works‚Äîtheir titles (and abstracts and full text in many cases), when they were created, etc. But that‚Äôs not all we do. We also keep track of the connections between these works, finding associations through things like journals, authors, institutional affiliations, citations, topics, and funders. There are hundreds of millions of works out there, and tens of thousands more being created every day, so it‚Äôs important that we have these relationships to help us make sense of research at a large scale."
  },
  {
    "objectID": "exercises/ms-exercise-07.html#preparation",
    "href": "exercises/ms-exercise-07.html#preparation",
    "title": "API mining and data wrangling with R",
    "section": "Preparation",
    "text": "Preparation\n\n\n\n\n\n\nWichtige Information\n\n\n\n\nBitte stellen Sie sicher, dass Sie das jeweilige R-Studio Projekt zur √úbung ge√∂ffnet haben. Nur so funktionieren alle Dependencies korrekt.\nUm den einwandfreien Ablauf der √úbung zu gew√§hrleisten, wird f√ºr die Aufgaben auf eine eigenst√§ndige Datenerhebung verzichtet und ein √úbungsdatensatz zu verf√ºgung gestelt.\n\n\n\n\nPackages\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  here, qs, # file management\n  magrittr, janitor, # data wrangling\n  easystats, sjmisc, # data analysis\n  ggpubr, # visualization\n  openalexR, \n  tidyverse # load last to avoid masking issues\n  )\n\n\n\nImport und Vorverarbeitung der Daten\n\n# Import from local\nreview_works &lt;- qs::qread(here(\"data/session-07/openalex-review_works-2013_2023.qs\"))\n\n# Create correct data\nreview_works_correct &lt;- review_works %&gt;% \n    mutate(\n        # Create additional factor variables\n        publication_year_fct = as.factor(publication_year), \n        type_fct = as.factor(type)\n        )"
  },
  {
    "objectID": "exercises/ms-exercise-07.html#praktische-anwendung",
    "href": "exercises/ms-exercise-07.html#praktische-anwendung",
    "title": "API mining and data wrangling with R",
    "section": "üõ†Ô∏è Praktische Anwendung",
    "text": "üõ†Ô∏è Praktische Anwendung\n\n\n\n\n\n\nAchtung, bitte lesen!\n\n\n\n\nBevor Sie mit der Arbeit an den folgenden üìã Exercises beginnen, stellen Sie bitte sicher, dass Sie alle Chunks des Abschnitts Preparation gerendert haben. Das k√∂nnen Sie tun, indem Sie den ‚ÄúRun all chunks above‚Äù-Knopf des n√§chsten Chunks benutzen.\nBei Fragen zum Code lohnt sich ein Blick in den Showcase (.qmd oder .html). Beim Showcase handelt es sich um eine kompakte Darstellung des in der Pr√§sentation verwenden R-Codes. Sie k√∂nnen das Showcase also nutzen, um sich die Code-Bausteine anzusehen, die f√ºr die R-Outputs auf den Slides benutzt wurden.\n\n\n\n\nüìã Exercise 1: Sprache der Publikationen\n\n\n\n\n\n\nZiel der Aufgabe\n\n\n\n\nIdentifizieren Sie die f√ºr die Untersuchung relevanten Artikel auf Basis von deren Sprache (language)\nHintergrundinformation zur Variable language finden Sie in der API-Dokumentation von OpenAlex.\n\n\n\n\nNutzen Sie die Funktion sjmisc::frq() und schauen Sie sich im Datensatz review_works_correct die Variablen language an. Verwenden Sie das Argument sort.frq = \"desc\", um die H√§ufigkeit der Sprachen absteigend zu sortieren.\nNotieren Sie sich den jeweilgen ISO 639-1 language code, um Ihn sp√§ter bei üìã Exercise 4: Erstellung Subsample als Filter zu nutzen.\n\n\n# Create frequency table for the variable language\n...\n\n\n\nüìã Exercise 2: Typ der Publikationen\n\n\n\n\n\n\nZiel der Aufgabe\n\n\n\n\nIdentifizieren Sie die f√ºr die Untersuchung relevanten Artikel auf Basis deres Typen (type).\nHintergrundinformation zur Variable type finden Sie in der API-Dokumentation von OpenAlex.\n\n\n\n\nNutzen Sie die Funktion sjmisc::frq() und schauen Sie sich im Datensatz review_works_correct die Variablen type an. Verwenden Sie das Argument sort.frq = \"desc\", um die Typen in Abh√§ngigkeit Ihrer H√§ufigkeit absteigend zu sortieren.\nNotieren Sie sich die Auspr√§gungen der Variable type, die aus Ihrer Sicht sp√§ter bei üìã Exercise 4: Erstellung Subsample als Filter genutzt werden soll.\n\n\n# Create frequency table for the variable type\n...\n\n\n\nüìã Exercise 3: Forschungsfeld der Publikationen\n\n\n\n\n\n\nZiel der Aufgabe\n\n\n\n\nIdentifizieren Sie die f√ºr die Untersuchung relevanten Artikel auf Basis des von OpenAlex dem Artikel zugeordnenten Foschungsfeldes (field)\nHintergrundinformation zur Variable field finden Sie in der API-Dokumentation von OpenAlex.\n\n\n\n\nBasierend auf dem Datensatz review_works_correct\n\nnutzen Sie die Funktion unnest() um die Variablen der topics-Liste zu extrahieren. Verwenden Sie dabei das Argument names_sep = \"_\". um doppelte Variablennamen durch Hinzuf√ºgen des Prefixes topics_ zu verhindern.\nfiltern Sie anschlie√üen mit Hilfe der Funktion filter und der Variable bzw. dem Argument topics_name == \"field\" nur die Informationen zum Forschungsfeld, sowie mit der Variable bzw. dem Argument topics_i == \"1\" nur die erste Zuordnung.\nNutzen Sie die Funktion sjmisc::frq() und schauen Sie sich die Variablen topics_display_name an. Verwenden Sie das Argument sort.frq = \"desc\", um die Forschungsfelder in Abh√§ngigkeit Ihrer H√§ufigkeit absteigend zu sortieren.\n\nNotieren Sie sich die Auspr√§gungen der Variable topics_display_name, die aus Ihrer Sicht sp√§ter bei üìã Exercise 4: Erstellung Subsample als Filter genutzt werden soll.\n\n\n# Unnest topis variable and create frequency table for the variable topics_display_name\n...\n\n\n\nüìã Exercise 4: Erstellung Subsample\n\n\n\n\n\n\nZiel der Aufgabe\n\n\n\n\nErstellung sie den Datensatz review_subsample, in dem Sie mit Hilfe der Funktionen select() und/oder filter() das Datenmaterial weiter eingrenzen. Sie k√∂nnen sich sowohl auf die Variablen aus der √úbung, als auch auf die aus der Sitzung (bzw. den Slides) beziehen.\nDer Code dieses Chunks wird in der n√§chsten Sitzung ben√∂tigt bzw. besprochen, halten Sie diesen deshalb bitte bereit.\n\n\n\n\nreview_subsample &lt;- review_works_correct %&gt;% \n    filter(...)"
  },
  {
    "objectID": "exercises/ms-exercise-08.html",
    "href": "exercises/ms-exercise-08.html",
    "title": "Text processing with R",
    "section": "",
    "text": "Link to source file"
  },
  {
    "objectID": "exercises/ms-exercise-08.html#background",
    "href": "exercises/ms-exercise-08.html#background",
    "title": "Text processing with R",
    "section": "Background",
    "text": "Background\n\n\n\n\n\n\nTodays‚Äôs data basis: OpenAlex\n\n\n\n\nVia API bzw. openalexR (Aria et al. 2024) gesammelte ‚Äúworks‚Äù der Datenbank OpenAlex mit Bezug zu Literaturriews in den Sozialwissenschaften zwischen 2013 und 2023\nDetaillierte Informationen und Ergebnisse zur Suchquery finden Sie hier."
  },
  {
    "objectID": "exercises/ms-exercise-08.html#preparation",
    "href": "exercises/ms-exercise-08.html#preparation",
    "title": "Text processing with R",
    "section": "Preparation",
    "text": "Preparation\n\n\n\n\n\n\nWichtige Information\n\n\n\n\nBitte stellen Sie sicher, dass Sie das jeweilige R-Studio Projekt zur √úbung ge√∂ffnet haben. Nur so funktionieren alle Dependencies korrekt.\nUm den einwandfreien Ablauf der √úbung zu gew√§hrleisten, wird f√ºr die Aufgaben auf eine eigenst√§ndige Datenerhebung verzichtet und ein √úbungsdatensatz zu verf√ºgung gestelt.\n\n\n\n\nPackages\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n    here, qs, # file management\n    magrittr, janitor, # data wrangling\n    easystats, sjmisc, # data analysis\n    gt, gtExtras, # table visualization\n    ggpubr, ggwordcloud, # visualization\n    tidytext, widyr, # text analysis    \n    openalexR, \n    tidyverse # load last to avoid masking issues\n  )\n\n\n\nImport und Vorverarbeitung der Daten\n\n# Import from local\nreview_works &lt;- qs::qread(here(\"data/session-07/openalex-review_works-2013_2023.qs\"))\n\n# Create correct data\nreview_works_correct &lt;- review_works %&gt;% \n    mutate(\n        # Create additional factor variables\n        publication_year_fct = as.factor(publication_year), \n        type_fct = as.factor(type)\n        )"
  },
  {
    "objectID": "exercises/ms-exercise-08.html#praktische-anwendung",
    "href": "exercises/ms-exercise-08.html#praktische-anwendung",
    "title": "Text processing with R",
    "section": "üõ†Ô∏è Praktische Anwendung",
    "text": "üõ†Ô∏è Praktische Anwendung\n\n\n\n\n\n\nAchtung, bitte lesen!\n\n\n\n\nBevor Sie mit der Arbeit an den folgenden üìã Exercises beginnen, stellen Sie bitte sicher, dass Sie alle Chunks des Abschnitts Preparation gerendert haben. Das k√∂nnen Sie tun, indem Sie den ‚ÄúRun all chunks above‚Äù-Knopf des n√§chsten Chunks benutzen.\nBei Fragen zum Code lohnt sich ein Blick in den Showcase (.qmd oder .html). Beim Showcase handelt es sich um eine kompakte Darstellung des in der Pr√§sentation verwenden R-Codes. Sie k√∂nnen das Showcase also nutzen, um sich die Code-Bausteine anzusehen, die f√ºr die R-Outputs auf den Slides benutzt wurden.\n\n\n\n\nüìã Exercise 1: Neues Subsample\n\n\n\n\n\n\nZiel der Aufgabe\n\n\n\n\nErstellung eines neuen Datensatzes review_subsample_new, der sich auf englischsprachig B√ºcher bzw. Buchrartikel beschr√§nkt.\n\n\n\n\nErstellen Sie einen neuen Datensatz review_subsample_new\n\nBasierend auf dem Datensatzes review_works_correct:\n\nNutzen Sie die filter()-Funktion, um\n\nnur englischsprachige (language),\nB√ºcher und Buchkapitel (type) herauszufiltern.\n\nSpeichern Sie diese Umwandlung in einem neuen Datensatz mit dem Namen review_subsample_new\n\n\n√úberpr√ºfen Sie die Transformation mit Hilfe der glimpse()-Funktion.\n‚úçÔ∏è Notieren Sie, wie viele Artikel im neuen Subsample enthalten sind.\n\n\n# Erstellung Subsample\n\n# √úberpr√ºfung\n\n# Notiz:\n# Subsample enth√§lt ... Eintr√§ge\n\n\n\nüìã Exercise 2: Umwandlung zu ‚Äòtidy text‚Äô\n\nErstellen Sie einen neuen Datensatz subsample_new_tidy,\n\nBasierend auf dem Datensatz review_subsample_new, mit folgenden Schritten:\n\nTokenisierung der Abstracts (ab) mit der Funktion unnest_tokens.\nAusschluss von Stoppw√∂rter mit filter und stopwords$words heraus.\nSpeichern Sie diese Umwandlung, indem Sie einen neuen Datensatz mit dem Namen subsample_new_tidy erstellen.\n\n\nPr√ºfen Sie, ob die Umwandlung erfolgreich war (z.B. mit der Funktion glimpse())\n‚úçÔ∏è Notieren Sie, wie viele Token im neuen Datensatz subsample_new_tidy enthalten sind.\n\n\n# Erstellung des neuen Datensatzes `subsample_new_tidy`\n\n# √úberpr√ºfung\n\n# Notiz:\n# Der neue Datensatz enth√§lt ... Token. \n\n\n\nüìã Exercise 3: Auswertung der Token\n\nErstellen Sie einen neuen Datensatz subsample_new_summarized,\n\nFassen Sie auf der Grundlage des Datensatzes subsample_new_tidy die H√§ufigkeit der einzelnen Token zusammen, indem Sie die Funktion count() auf die Variable text anwenden. Verwenden Sie das Argument sort = TRUE, um den Datensatz nach absteigender H√§ufigkeit der Token zu sortieren.\nSpeichern Sie diese Transformation, indem Sie einen neuen Datensatz mit dem Namen subsample_new_summarized erstellen.\n\nPr√ºfen Sie, ob die Umwandlung erfolgreich war, indem Sie die Funktion print() verwenden.\n\nVerwenden Sie das Argument n = 50, um die 50 wichtigsten Token anzuzeigen (nur m√∂glich, wenn das Argument sort = TRUE bei der Ausf√ºhrung der Funktion count() verwendet wurde)\n\nVerteilung der Token pr√ºfen\n\nVerwenden Sie die Funktion datawizard::describe_distribution(), um verschiedene Verteilungsparameter des neuen Datensatzes zu √ºberpr√ºfen\n‚úçÔ∏è Notieren Sie, wie viele Token ein Abstract durchschnittlich enth√§lt.\n\n\n\nOptional: Ergebnisse mit einer Wortwolke √ºberpr√ºfen\n\nBasierend auf dem sortierten Datensatz subsample_new_summarized\n\nAuswahl der 50 h√§ufigsten Token mit Hilfe der Funktion top_n()\nErstellen Sie eine ggplot()-Basis mit label = text und size = n als aes() und\nBenutze ggwordcloud::geom_text_wordclout() um die Wortwolke zu erstellen.\nVerwenden Sie scale_size_are(), um die Skalierung der Wortwolke zu √ºbernehmen.\nVerwenden Sie theme_minimal() f√ºr eine saubere Visualisierung.\n\n\n\n\n# Erstellung des neuen Datensatzes `subsample_new_summmarized`\n\n# Preview Top 50 token\n\n# Check distribution parameters \n\n# Notiz:\n# Ein Absatz enth√§lt durchschnittlich ... Token. \n\n\n# Optional: Check results with a wordcloud\n\n\n\nüìã Exercise 4: Wortbeziehungen im Fokus\n\n4.1 Couting word pairs\n\nZ√§hlen von h√§ufigen Wortpaaren\n\nZ√§hlen Sie auf der Grundlage des Datensatzes subsample_new_tidy Wortpaare mit widyr::pairwise_count(), mit den Argumenten item = text, feature = id und sort = TRUE.\nSpeichern Sie diese Transformation, indem Sie einen neuen Datensatz mit dem Namen subsample_new_word_pairs erstellen.\n\nPr√ºfen Sie, ob die Umwandlung erfolgreich war, indem Sie die Funktion print() verwenden.\n\nVerwenden Sie das Argument n = 50, um die 50 wichtigsten Token anzuzeigen (nur m√∂glich, wenn bei der Ausf√ºhrung der Funktion count() das Argument sort = TRUE verwendet wurde)\n\n\n\n# Couting word pairs among sections\n\n# √úberpr√ºfung \n\n\n\n4.2 Pairwise correlation\n\nErmittlung der paarweisen Korrelation\n\nBasierend auf dem Datensatz subsample_new_tidy,\ngruppieren Sie die Daten mit der Funktion group_by() nach der Variable text und\nverwenden Sie filter(n() &gt;= X), um nur Token zu verwenden, die mindestens in einer bestimmte Anzahl (X) vorkommen; Sie k√∂nnen f√ºr X einen Wert Ihrer Wahl w√§hlen, ich w√ºrde jedoch dringend empfehlen, ein X &gt; 100 zu w√§hlen, da die folgende Funktion sonst m√∂glicherweise nicht in der Lage ist, die Berechnung durchzuf√ºhren.\nErstellen Sie Wortkorrelationen mit widyr::pairwise_cor(), mit den Argumenten item = text,feature = id und sort = TRUE.\nSpeichern Sie diese Transformation, indem Sie einen neuen Datensatz mit dem Namen subsample_new_corr erstellen.\n\nPr√ºfen Sie die Paare mit der h√∂chsten Korrelation mit der Funktion print()..\n\n\n# Getting pairwise correlation \n\n# Check pairs with highest correlation\n\n\n\n\nüìã Exercise 5: Inhaltlicher Vergleich\n\nVergleichen Sie die Ergebnisse der √úbung mit den Auswertungen der Folien:\n\nWie unterscheiden sich die Ergebnisse?\nW√ºrden Sie die B√ºcher bzw. Buchabschnitte mit in die Untersuchung integrieren?"
  },
  {
    "objectID": "exercises/ms-exercise-08_solution.html",
    "href": "exercises/ms-exercise-08_solution.html",
    "title": "Text processing with R",
    "section": "",
    "text": "Link to source file"
  },
  {
    "objectID": "exercises/ms-exercise-08_solution.html#background",
    "href": "exercises/ms-exercise-08_solution.html#background",
    "title": "Text processing with R",
    "section": "Background",
    "text": "Background\n\n\n\n\n\n\nTodays‚Äôs data basis: OpenAlex\n\n\n\n\nVia API bzw. openalexR (Aria et al. 2024) gesammelte ‚Äúworks‚Äù der Datenbank OpenAlex mit Bezug zu Literaturriews in den Sozialwissenschaften zwischen 2013 und 2023\nDetaillierte Informationen und Ergebnisse zur Suchquery finden Sie hier."
  },
  {
    "objectID": "exercises/ms-exercise-08_solution.html#preparation",
    "href": "exercises/ms-exercise-08_solution.html#preparation",
    "title": "Text processing with R",
    "section": "Preparation",
    "text": "Preparation\n\n\n\n\n\n\nWichtige Information\n\n\n\n\nBitte stellen Sie sicher, dass Sie das jeweilige R-Studio Projekt zur √úbung ge√∂ffnet haben. Nur so funktionieren alle Dependencies korrekt.\nUm den einwandfreien Ablauf der √úbung zu gew√§hrleisten, wird f√ºr die Aufgaben auf eine eigenst√§ndige Datenerhebung verzichtet und ein √úbungsdatensatz zu verf√ºgung gestelt.\n\n\n\n\nPackages\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n    here, qs, # file management\n    magrittr, janitor, # data wrangling\n    easystats, sjmisc, # data analysis\n    gt, gtExtras, # table visualization\n    ggpubr, ggwordcloud, # visualization\n    tidytext, widyr, # text analysis    \n    openalexR, \n    tidyverse # load last to avoid masking issues\n  )\n\n\n\nImport und Vorverarbeitung der Daten\n\n# Import from local\nreview_works &lt;- qs::qread(here(\"data/session-07/openalex-review_works-2013_2023.qs\"))\n\n# Create correct data\nreview_works_correct &lt;- review_works %&gt;% \n    mutate(\n        # Create additional factor variables\n        publication_year_fct = as.factor(publication_year), \n        type_fct = as.factor(type)\n        )"
  },
  {
    "objectID": "exercises/ms-exercise-08_solution.html#praktische-anwendung",
    "href": "exercises/ms-exercise-08_solution.html#praktische-anwendung",
    "title": "Text processing with R",
    "section": "üõ†Ô∏è Praktische Anwendung",
    "text": "üõ†Ô∏è Praktische Anwendung\n\n\n\n\n\n\nAchtung, bitte lesen!\n\n\n\n\nBevor Sie mit der Arbeit an den folgenden üìã Exercises beginnen, stellen Sie bitte sicher, dass Sie alle Chunks des Abschnitts Preparation gerendert haben. Das k√∂nnen Sie tun, indem Sie den ‚ÄúRun all chunks above‚Äù-Knopf des n√§chsten Chunks benutzen.\nBei Fragen zum Code lohnt sich ein Blick in den Showcase (.qmd oder .html). Beim Showcase handelt es sich um eine kompakte Darstellung des in der Pr√§sentation verwenden R-Codes. Sie k√∂nnen das Showcase also nutzen, um sich die Code-Bausteine anzusehen, die f√ºr die R-Outputs auf den Slides benutzt wurden.\n\n\n\n\nüìã Exercise 1: Neues Subsample\n\n\n\n\n\n\nZiel der Aufgabe\n\n\n\n\nErstellung eines neuen Datensatzes review_subsample_new, der sich auf englischsprachig B√ºcher bzw. Buchrartikel beschr√§nkt.\n\n\n\n\nErstellen Sie einen neuen Datensatz review_subsample_new\n\nBasierend auf dem Datensatzes review_works_correct:\n\nNutzen Sie die filter()-Funktion, um\n\nnur englischsprachige (language),\nB√ºcher und Buchkapitel (type) herauszufiltern.\n\nSpeichern Sie diese Umwandlung in einem neuen Datensatz mit dem Namen review_subsample_new\n\n\n√úberpr√ºfen Sie die Transformation mit Hilfe der glimpse()-Funktion.\n‚úçÔ∏è Notieren Sie, wie viele Artikel im neuen Subsample enthalten sind.\n\n\n\nL√∂sung anzeigen\n# Erstellung Subsample\nreview_subsample_new &lt;- review_works_correct %&gt;% \n  # Eingrenzung: Sprache und Typ\n  filter(language == \"en\") %&gt;% \n  filter(type == \"book\" | type == \"book-chapter\")\n\n# √úberpr√ºfung\nreview_subsample_new %&gt;% glimpse\n\n\nRows: 2,994\nColumns: 41\n$ id                          &lt;chr&gt; \"https://openalex.org/W2899962821\", \"https‚Ä¶\n$ title                       &lt;chr&gt; \"Introduction to systematic reviews\", \"Cha‚Ä¶\n$ display_name                &lt;chr&gt; \"Introduction to systematic reviews\", \"Cha‚Ä¶\n$ author                      &lt;list&gt; [&lt;data.frame[2 x 12]&gt;], NA, [&lt;data.frame[‚Ä¶\n$ ab                          &lt;chr&gt; \"A systematic review is a vital part of th‚Ä¶\n$ publication_date            &lt;chr&gt; \"2018-10-01\", \"2020-01-01\", \"2019-01-01\", ‚Ä¶\n$ relevance_score             &lt;dbl&gt; 300.1405, 255.6323, 248.6059, 231.0026, 22‚Ä¶\n$ so                          &lt;chr&gt; \"Manchester University Press eBooks\", \"JBI‚Ä¶\n$ so_id                       &lt;chr&gt; \"https://openalex.org/S4306463591\", \"https‚Ä¶\n$ host_organization           &lt;chr&gt; \"Winchester University Press\", NA, NA, NA,‚Ä¶\n$ issn_l                      &lt;chr&gt; NA, NA, NA, NA, \"0302-9743\", NA, NA, NA, N‚Ä¶\n$ url                         &lt;chr&gt; \"https://doi.org/10.7765/9781526136527.000‚Ä¶\n$ pdf_url                     &lt;chr&gt; \"https://www.manchesteropenhive.com/downlo‚Ä¶\n$ license                     &lt;chr&gt; \"cc-by-nc-nd\", NA, NA, NA, NA, NA, NA, \"cc‚Ä¶\n$ version                     &lt;chr&gt; \"publishedVersion\", \"publishedVersion\", \"p‚Ä¶\n$ first_page                  &lt;chr&gt; NA, NA, NA, NA, \"214\", \"48\", NA, \"3\", \"85\"‚Ä¶\n$ last_page                   &lt;chr&gt; NA, NA, NA, NA, \"227\", \"78\", NA, \"22\", \"94‚Ä¶\n$ volume                      &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA‚Ä¶\n$ issue                       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA‚Ä¶\n$ is_oa                       &lt;lgl&gt; TRUE, TRUE, TRUE, FALSE, FALSE, FALSE, TRU‚Ä¶\n$ is_oa_anywhere              &lt;lgl&gt; TRUE, TRUE, TRUE, FALSE, FALSE, FALSE, TRU‚Ä¶\n$ oa_status                   &lt;chr&gt; \"hybrid\", \"bronze\", \"bronze\", \"closed\", \"c‚Ä¶\n$ oa_url                      &lt;chr&gt; \"https://www.manchesteropenhive.com/downlo‚Ä¶\n$ any_repository_has_fulltext &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, ‚Ä¶\n$ language                    &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", ‚Ä¶\n$ grants                      &lt;list&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n$ cited_by_count              &lt;int&gt; 423, 496, 421, 547, 274, 64, 247, 189, 136‚Ä¶\n$ counts_by_year              &lt;list&gt; [&lt;data.frame[8 x 2]&gt;], [&lt;data.frame[6 x 2‚Ä¶\n$ publication_year            &lt;int&gt; 2018, 2020, 2019, 2013, 2014, 2015, 2020, ‚Ä¶\n$ cited_by_api_url            &lt;chr&gt; \"https://api.openalex.org/works?filter=cit‚Ä¶\n$ ids                         &lt;list&gt; &lt;\"https://openalex.org/W2899962821\", \"htt‚Ä¶\n$ doi                         &lt;chr&gt; \"https://doi.org/10.7765/9781526136527.000‚Ä¶\n$ type                        &lt;chr&gt; \"book-chapter\", \"book-chapter\", \"book-chap‚Ä¶\n$ referenced_works            &lt;list&gt; NA, NA, NA, &lt;\"https://openalex.org/W11488‚Ä¶\n$ related_works               &lt;list&gt; &lt;\"https://openalex.org/W4385987771\", \"htt‚Ä¶\n$ is_paratext                 &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, ‚Ä¶\n$ is_retracted                &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, ‚Ä¶\n$ concepts                    &lt;list&gt; [&lt;data.frame[16 x 5]&gt;], [&lt;data.frame[7 x ‚Ä¶\n$ topics                      &lt;list&gt; [&lt;tbl_df[4 x 5]&gt;], [&lt;tbl_df[4 x 5]&gt;], [&lt;t‚Ä¶\n$ publication_year_fct        &lt;fct&gt; 2018, 2020, 2019, 2013, 2014, 2015, 2020, ‚Ä¶\n$ type_fct                    &lt;fct&gt; book-chapter, book-chapter, book-chapter, ‚Ä¶\n\n\nL√∂sung anzeigen\n# Notiz:\n# Subsample enth√§lt 2994 Eintr√§ge\n\n\n\n\nüìã Exercise 2: Umwandlung zu ‚Äòtidy text‚Äô\n\nErstellen Sie einen neuen Datensatz subsample_new_tidy,\n\nBasierend auf dem Datensatz review_subsample_new, mit folgenden Schritten:\n\nTokenisierung der Abstracts (ab) mit der Funktion unnest_tokens.\nAusschluss von Stoppw√∂rter mit filter und stopwords$words heraus.\nSpeichern Sie diese Umwandlung, indem Sie einen neuen Datensatz mit dem Namen subsample_new_tidy erstellen.\n\n\nPr√ºfen Sie, ob die Umwandlung erfolgreich war (z.B. mit der Funktion glimpse())\n‚úçÔ∏è Notieren Sie, wie viele Token im neuen Datensatz subsample_new_tidy enthalten sind.\n\n\n\nL√∂sung anzeigen\n# Erstellung des neuen Datensatzes `subsample_new_tidy`\nsubsample_new_tidy &lt;- review_subsample_new %&gt;% \n  tidytext::unnest_tokens(\"text\", ab) %&gt;% \n   filter(!text %in% tidytext::stop_words$word)\n\n# √úberpr√ºfung\nsubsample_new_tidy %&gt;% print()\n\n\n# A tibble: 182,776 √ó 41\n   id     title display_name author publication_date relevance_score so    so_id\n   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;        &lt;list&gt; &lt;chr&gt;                      &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n 1 https‚Ä¶ Intr‚Ä¶ Introductio‚Ä¶ &lt;df&gt;   2018-10-01                  300. Manc‚Ä¶ http‚Ä¶\n 2 https‚Ä¶ Intr‚Ä¶ Introductio‚Ä¶ &lt;df&gt;   2018-10-01                  300. Manc‚Ä¶ http‚Ä¶\n 3 https‚Ä¶ Intr‚Ä¶ Introductio‚Ä¶ &lt;df&gt;   2018-10-01                  300. Manc‚Ä¶ http‚Ä¶\n 4 https‚Ä¶ Intr‚Ä¶ Introductio‚Ä¶ &lt;df&gt;   2018-10-01                  300. Manc‚Ä¶ http‚Ä¶\n 5 https‚Ä¶ Intr‚Ä¶ Introductio‚Ä¶ &lt;df&gt;   2018-10-01                  300. Manc‚Ä¶ http‚Ä¶\n 6 https‚Ä¶ Intr‚Ä¶ Introductio‚Ä¶ &lt;df&gt;   2018-10-01                  300. Manc‚Ä¶ http‚Ä¶\n 7 https‚Ä¶ Intr‚Ä¶ Introductio‚Ä¶ &lt;df&gt;   2018-10-01                  300. Manc‚Ä¶ http‚Ä¶\n 8 https‚Ä¶ Intr‚Ä¶ Introductio‚Ä¶ &lt;df&gt;   2018-10-01                  300. Manc‚Ä¶ http‚Ä¶\n 9 https‚Ä¶ Intr‚Ä¶ Introductio‚Ä¶ &lt;df&gt;   2018-10-01                  300. Manc‚Ä¶ http‚Ä¶\n10 https‚Ä¶ Intr‚Ä¶ Introductio‚Ä¶ &lt;df&gt;   2018-10-01                  300. Manc‚Ä¶ http‚Ä¶\n# ‚Ñπ 182,766 more rows\n# ‚Ñπ 33 more variables: host_organization &lt;chr&gt;, issn_l &lt;chr&gt;, url &lt;chr&gt;,\n#   pdf_url &lt;chr&gt;, license &lt;chr&gt;, version &lt;chr&gt;, first_page &lt;chr&gt;,\n#   last_page &lt;chr&gt;, volume &lt;chr&gt;, issue &lt;chr&gt;, is_oa &lt;lgl&gt;,\n#   is_oa_anywhere &lt;lgl&gt;, oa_status &lt;chr&gt;, oa_url &lt;chr&gt;,\n#   any_repository_has_fulltext &lt;lgl&gt;, language &lt;chr&gt;, grants &lt;list&gt;,\n#   cited_by_count &lt;int&gt;, counts_by_year &lt;list&gt;, publication_year &lt;int&gt;, ‚Ä¶\n\n\nL√∂sung anzeigen\n# Notiz:\n# Der neue Datensatz enth√§lt 182776 Token. \n\n\n\n\nüìã Exercise 3: Auswertung der Token\n\nErstellen Sie einen neuen Datensatz subsample_new_summarized,\n\nFassen Sie auf der Grundlage des Datensatzes subsample_new_tidy die H√§ufigkeit der einzelnen Token zusammen, indem Sie die Funktion count() auf die Variable text anwenden. Verwenden Sie das Argument sort = TRUE, um den Datensatz nach absteigender H√§ufigkeit der Token zu sortieren.\nSpeichern Sie diese Transformation, indem Sie einen neuen Datensatz mit dem Namen subsample_new_summarized erstellen.\n\nPr√ºfen Sie, ob die Umwandlung erfolgreich war, indem Sie die Funktion print() verwenden.\n\nVerwenden Sie das Argument n = 50, um die 50 wichtigsten Token anzuzeigen (nur m√∂glich, wenn das Argument sort = TRUE bei der Ausf√ºhrung der Funktion count() verwendet wurde)\n\nVerteilung der Token pr√ºfen\n\nVerwenden Sie die Funktion datawizard::describe_distribution(), um verschiedene Verteilungsparameter des neuen Datensatzes zu √ºberpr√ºfen\n‚úçÔ∏è Notieren Sie, wie viele Token ein Abstract durchschnittlich enth√§lt.\n\n\n\nOptional: Ergebnisse mit einer Wortwolke √ºberpr√ºfen\n\nBasierend auf dem sortierten Datensatz subsample_new_summarized\n\nAuswahl der 50 h√§ufigsten Token mit Hilfe der Funktion top_n()\nErstellen Sie eine ggplot()-Basis mit label = text und size = n als aes() und\nBenutze ggwordcloud::geom_text_wordclout() um die Wortwolke zu erstellen.\nVerwenden Sie scale_size_are(), um die Skalierung der Wortwolke zu √ºbernehmen.\nVerwenden Sie theme_minimal() f√ºr eine saubere Visualisierung.\n\n\n\n\n\nL√∂sung anzeigen\n# Erstellung des neuen Datensatzes `subsample_new_summmarized`\nsubsample_new_summmarized &lt;- subsample_new_tidy %&gt;% \n  count(text, sort = TRUE) \n\n# Preview Top 50 token\nsubsample_new_summmarized %&gt;% \n    print(n = 50)\n\n\n# A tibble: 14,959 √ó 2\n   text            n\n   &lt;chr&gt;       &lt;int&gt;\n 1 research     2761\n 2 literature   2539\n 3 review       2466\n 4 studies      1478\n 5 systematic   1231\n 6 study        1206\n 7 &lt;NA&gt;          936\n 8 social        903\n 9 analysis      881\n10 chapter       851\n11 data          824\n12 management    788\n13 based         786\n14 paper         695\n15 articles      686\n16 learning      666\n17 development   627\n18 future        622\n19 results       596\n20 information   581\n21 knowledge     572\n22 reviews       545\n23 education     518\n24 factors       516\n25 process       498\n26 related       491\n27 business      483\n28 health        454\n29 technology    452\n30 findings      449\n31 digital       437\n32 methods       424\n33 main          421\n34 design        417\n35 provide       410\n36 field         400\n37 quality       396\n38 context       384\n39 identify      382\n40 current       380\n41 performance   369\n42 published     367\n43 approach      356\n44 identified    356\n45 impact        347\n46 relevant      344\n47 systems       334\n48 role          331\n49 model         328\n50 conducted     327\n# ‚Ñπ 14,909 more rows\n\n\nL√∂sung anzeigen\n# Check distribution parameters \nsubsample_new_summmarized %&gt;%\n  datawizard::describe_distribution()\n\n\nVariable |  Mean |    SD | IQR |           Range | Skewness | Kurtosis |     n | n_Missing\n------------------------------------------------------------------------------------------\nn        | 12.22 | 57.24 |   5 | [1.00, 2761.00] |    24.32 |   925.44 | 14959 |         0\n\n\nL√∂sung anzeigen\n# Notiz:\n# Ein Absatz enth√§lt durchschnittlich 22 Token. \n\n# Optional: Check results with a wordcloud\nsubsample_new_summmarized %&gt;% \n    top_n(50) %&gt;% \n    ggplot(aes(label = text, size = n)) +\n    ggwordcloud::geom_text_wordcloud() +\n    scale_size_area(max_size = 15) +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nüìã Exercise 4: Wortbeziehungen im Fokus\n\n4.1 Couting word pairs\n\nZ√§hlen von h√§ufigen Wortpaaren\n\nZ√§hlen Sie auf der Grundlage des Datensatzes subsample_new_tidy Wortpaare mit widyr::pairwise_count(), mit den Argumenten item = text, feature = id und sort = TRUE.\nSpeichern Sie diese Transformation, indem Sie einen neuen Datensatz mit dem Namen subsample_new_word_pairs erstellen.\n\nPr√ºfen Sie, ob die Umwandlung erfolgreich war, indem Sie die Funktion print() verwenden.\n\nVerwenden Sie das Argument n = 50, um die 50 wichtigsten Token anzuzeigen (nur m√∂glich, wenn bei der Ausf√ºhrung der Funktion count() das Argument sort = TRUE verwendet wurde)\n\n\n\n\nL√∂sung anzeigen\n# Couting word pairs among sections\nsubsample_new_word_pairs &lt;- subsample_new_tidy %&gt;% \n  widyr::pairwise_count(\n    item = text,\n    feature = id,\n    sort = TRUE)\n\n# Check \nsubsample_new_word_pairs %&gt;% print(n = 50)\n\n\n# A tibble: 7,286,586 √ó 3\n   item1      item2          n\n   &lt;chr&gt;      &lt;chr&gt;      &lt;dbl&gt;\n 1 literature review      1160\n 2 review     literature  1160\n 3 research   review       947\n 4 review     research     947\n 5 literature research     915\n 6 research   literature   915\n 7 review     systematic   795\n 8 systematic review       795\n 9 literature systematic   642\n10 systematic literature   642\n11 studies    review       588\n12 review     studies      588\n13 research   systematic   561\n14 systematic research     561\n15 study      literature   555\n16 literature study        555\n17 study      review       549\n18 review     study        549\n19 literature studies      510\n20 studies    literature   510\n21 studies    research     508\n22 research   studies      508\n23 study      research     505\n24 research   study        505\n25 based      review       420\n26 review     based        420\n27 future     research     417\n28 research   future       417\n29 analysis   review       408\n30 review     analysis     408\n31 literature chapter      404\n32 chapter    literature   404\n33 paper      review       394\n34 review     paper        394\n35 future     review       391\n36 review     future       391\n37 results    review       385\n38 review     results      385\n39 literature paper        382\n40 paper      literature   382\n41 future     literature   381\n42 literature future       381\n43 literature analysis     379\n44 analysis   literature   379\n45 based      literature   379\n46 literature based        379\n47 chapter    review       377\n48 review     chapter      377\n49 studies    systematic   371\n50 systematic studies      371\n# ‚Ñπ 7,286,536 more rows\n\n\n\n\n4.2 Pairwise correlation\n\nErmittlung der paarweisen Korrelation\n\nBasierend auf dem Datensatz subsample_new_tidy,\ngruppieren Sie die Daten mit der Funktion group_by() nach der Variable text und\nverwenden Sie filter(n() &gt;= X), um nur Token zu verwenden, die mindestens in einer bestimmte Anzahl (X) vorkommen; Sie k√∂nnen f√ºr X einen Wert Ihrer Wahl w√§hlen, ich w√ºrde jedoch dringend empfehlen, ein X &gt; 100 zu w√§hlen, da die folgende Funktion sonst m√∂glicherweise nicht in der Lage ist, die Berechnung durchzuf√ºhren.\nErstellen Sie Wortkorrelationen mit widyr::pairwise_cor(), mit den Argumenten item = text,feature = id und sort = TRUE.\nSpeichern Sie diese Transformation, indem Sie einen neuen Datensatz mit dem Namen subsample_new_corr erstellen.\n\nPr√ºfen Sie die Paare mit der h√∂chsten Korrelation mit der Funktion print()..\n\n\n\nL√∂sung anzeigen\n# Getting pairwise correlation \nsubsample_new_corr &lt;- subsample_new_tidy %&gt;% \n  group_by(text) %&gt;% \n  filter(n() &gt;= 250) %&gt;% \n  pairwise_cor(text, id, sort = TRUE)\n\n# Check pairs with highest correlation\nsubsample_new_corr %&gt;% print(n = 50)\n\n\n# A tibble: 8,372 √ó 3\n   item1          item2          correlation\n   &lt;chr&gt;          &lt;chr&gt;                &lt;dbl&gt;\n 1 chain          supply               0.886\n 2 supply         chain                0.886\n 3 literature     review               0.614\n 4 review         literature           0.614\n 5 review         systematic           0.561\n 6 systematic     review               0.561\n 7 sustainability sustainable          0.467\n 8 sustainable    sustainability       0.467\n 9 research       review               0.462\n10 review         research             0.462\n11 literature     research             0.446\n12 research       literature           0.446\n13 future         research             0.373\n14 research       future               0.373\n15 media          social               0.368\n16 social         media                0.368\n17 literature     systematic           0.354\n18 systematic     literature           0.354\n19 learning       education            0.330\n20 education      learning             0.330\n21 studies        review               0.330\n22 review         studies              0.330\n23 research       systematic           0.319\n24 systematic     research             0.319\n25 study          research             0.309\n26 research       study                0.309\n27 study          literature           0.307\n28 literature     study                0.307\n29 studies        research             0.306\n30 research       studies              0.306\n31 articles       published            0.297\n32 published      articles             0.297\n33 articles       systematic           0.280\n34 systematic     articles             0.280\n35 models         model                0.280\n36 model          models               0.280\n37 study          review               0.279\n38 review         study                0.279\n39 results        review               0.278\n40 review         results              0.278\n41 results        systematic           0.272\n42 systematic     results              0.272\n43 based          review               0.269\n44 review         based                0.269\n45 articles       review               0.268\n46 review         articles             0.268\n47 studies        systematic           0.266\n48 systematic     studies              0.266\n49 theory         theoretical          0.266\n50 theoretical    theory               0.266\n# ‚Ñπ 8,322 more rows\n\n\n\n\n\nüìã Exercise 5: Inhaltlicher Vergleich\n\nVergleichen Sie die Ergebnisse der √úbung mit den Auswertungen der Folien:\n\nWie unterscheiden sich die Ergebnisse?\nW√ºrden Sie die B√ºcher bzw. Buchabschnitte mit in die Untersuchung integrieren?"
  },
  {
    "objectID": "exercises/ms-showcase-08.html",
    "href": "exercises/ms-showcase-08.html",
    "title": "Text processing in R",
    "section": "",
    "text": "Link to slides"
  },
  {
    "objectID": "exercises/ms-showcase-08.html#preparation",
    "href": "exercises/ms-showcase-08.html#preparation",
    "title": "Text processing in R",
    "section": "Preparation",
    "text": "Preparation\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n    here, qs, # file management\n    magrittr, janitor, # data wrangling\n    easystats, sjmisc, # data analysis\n    gt, gtExtras, # table visualization\n    ggpubr, ggwordcloud, # visualization\n    tidytext, widyr, # text analysis    \n    openalexR, \n    tidyverse # load last to avoid masking issues\n  )"
  },
  {
    "objectID": "exercises/ms-showcase-08.html#codechunks-aus-der-sitzung",
    "href": "exercises/ms-showcase-08.html#codechunks-aus-der-sitzung",
    "title": "Text processing in R",
    "section": "Codechunks aus der Sitzung",
    "text": "Codechunks aus der Sitzung\n\nErstelle Subsample\n\nreview_subsample &lt;- review_works_correct %&gt;% \n  # Eingrenzung: Sprache und Typ\n  filter(language == \"en\") %&gt;% \n  filter(type == \"article\") %&gt;%\n  # Datentranformation\n  unnest(topics, names_sep = \"_\") %&gt;%\n  filter(topics_name == \"field\") %&gt;% \n  filter(topics_i == \"1\") %&gt;% \n  # Eingrenzung: Forschungsfeldes\n  filter(\n   topics_display_name == \"Social Sciences\"|\n   topics_display_name == \"Psychology\"\n    )\n\n\n\nSubsample im Zeitverlauf\n\nreview_works_correct %&gt;% \n  mutate(\n    included = ifelse(id %in% review_subsample$id, \"Ja\", \"Nein\"),\n    included = factor(included, levels = c(\"Nein\", \"Ja\"))\n    ) %&gt;%\n  ggplot(aes(x = publication_year_fct, fill = included)) +\n    geom_bar() +\n    labs(\n      x = \"\",\n      y = \"Anzahl der Eintr√§ge\", \n      fill = \"In Subsample enthalten?\"\n     ) +\n    scale_fill_manual(values = c(\"#A0ACBD50\", \"#FF707F\")) +\n    theme_pubr() \n\n\n\n\n\n\n\n\n\n\nTokenization der Abstracts\n\n# Create tidy data\nreview_tidy &lt;- review_subsample %&gt;% \n    # Tokenization\n    tidytext::unnest_tokens(\"text\", ab) %&gt;% \n    # Remove stopwords\n    filter(!text %in% tidytext::stop_words$word)\n\n# Preview\nreview_tidy %&gt;% \n  select(id, text) %&gt;% \n  print(n = 10)\n\n# A tibble: 4,880,965 √ó 2\n   id                               text          \n   &lt;chr&gt;                            &lt;chr&gt;         \n 1 https://openalex.org/W4293003987 5             \n 2 https://openalex.org/W4293003987 item          \n 3 https://openalex.org/W4293003987 world         \n 4 https://openalex.org/W4293003987 health        \n 5 https://openalex.org/W4293003987 organization  \n 6 https://openalex.org/W4293003987 index         \n 7 https://openalex.org/W4293003987 5             \n 8 https://openalex.org/W4293003987 widely        \n 9 https://openalex.org/W4293003987 questionnaires\n10 https://openalex.org/W4293003987 assessing     \n# ‚Ñπ 4,880,955 more rows\n\n\n\nVergleich eines Abstraktes in Rohform und nach Tokenisierung\n\nreview_subsample$ab[[1]]\n\n[1] \"The 5-item World Health Organization Well-Being Index (WHO-5) is among the most widely used questionnaires assessing subjective psychological well-being. Since its first publication in 1998, the WHO-5 has been translated into more than 30 languages and has been used in research studies all over the world. We now provide a systematic review of the literature on the WHO-5.We conducted a systematic search for literature on the WHO-5 in PubMed and PsycINFO in accordance with the PRISMA guidelines. In our review of the identified articles, we focused particularly on the following aspects: (1) the clinimetric validity of the WHO-5; (2) the responsiveness/sensitivity of the WHO-5 in controlled clinical trials; (3) the potential of the WHO-5 as a screening tool for depression, and (4) the applicability of the WHO-5 across study fields.A total of 213 articles met the predefined criteria for inclusion in the review. The review demonstrated that the WHO-5 has high clinimetric validity, can be used as an outcome measure balancing the wanted and unwanted effects of treatments, is a sensitive and specific screening tool for depression and its applicability across study fields is very high.The WHO-5 is a short questionnaire consisting of 5 simple and non-invasive questions, which tap into the subjective well-being of the respondents. The scale has adequate validity both as a screening tool for depression and as an outcome measure in clinical trials and has been applied successfully across a wide range of study fields.\"\n\n\n\nreview_tidy %&gt;% \n  filter(id == \"https://openalex.org/W4293003987\") %&gt;% \n  pull(text) %&gt;% \n  paste(collapse = \" \")\n\n[1] \"5 item world health organization index 5 widely questionnaires assessing subjective psychological publication 1998 5 translated 30 languages research studies world provide systematic review literature 5 conducted systematic search literature 5 pubmed psycinfo accordance prisma guidelines review identified articles focused aspects 1 clinimetric validity 5 2 responsiveness sensitivity 5 controlled clinical trials 3 potential 5 screening tool depression 4 applicability 5 study fields.a total 213 articles met predefined criteria inclusion review review demonstrated 5 clinimetric validity outcome measure balancing unwanted effects treatments sensitive specific screening tool depression applicability study fields high.the 5 short questionnaire consisting 5 simple invasive questions tap subjective respondents scale adequate validity screening tool depression outcome measure clinical trials applied successfully wide range study fields\"\n\n\n\n\n\nCount token frequency\n\n# Create summarized data\nreview_summarized &lt;- review_tidy %&gt;% \n  count(text, sort = TRUE) \n\n# Preview Top 15 token\nreview_summarized %&gt;% \n    print(n = 15)\n\n# A tibble: 122,148 √ó 2\n   text              n\n   &lt;chr&gt;         &lt;int&gt;\n 1 studies       73398\n 2 review        57878\n 3 research      42689\n 4 health        35108\n 5 systematic    32431\n 6 literature    31374\n 7 study         29012\n 8 interventions 22731\n 9 included      21987\n10 social        21528\n11 articles      20631\n12 results       20166\n13 analysis      19624\n14 based         18929\n15 evidence      18545\n# ‚Ñπ 122,133 more rows"
  },
  {
    "objectID": "exercises/ms-showcase-08.html#the-unavoidable-word-cloud",
    "href": "exercises/ms-showcase-08.html#the-unavoidable-word-cloud",
    "title": "Text processing in R",
    "section": "The (Unavoidable) Word Cloud",
    "text": "The (Unavoidable) Word Cloud\n\nreview_summarized %&gt;% \n    top_n(50) %&gt;% \n    ggplot(aes(label = text, size = n)) +\n    ggwordcloud::geom_text_wordcloud() +\n    scale_size_area(max_size = 20) +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\nWortkombinationen (n-grams)\n\n# Create word paris\nreview_word_pairs &lt;- review_tidy %&gt;% \n    widyr::pairwise_count(\n        text,\n        id,\n        sort = TRUE)\n\n# Preview\nreview_word_pairs %&gt;% \n    print(n = 14)\n\n# A tibble: 114,446,724 √ó 3\n   item1      item2          n\n   &lt;chr&gt;      &lt;chr&gt;      &lt;dbl&gt;\n 1 review     studies    20494\n 2 studies    review     20494\n 3 review     systematic 20266\n 4 systematic review     20266\n 5 review     research   16902\n 6 research   review     16902\n 7 literature review     16754\n 8 review     literature 16754\n 9 systematic studies    16097\n10 studies    systematic 16097\n11 study      review     13391\n12 review     study      13391\n13 studies    research   13173\n14 research   studies    13173\n# ‚Ñπ 114,446,710 more rows\n\n\n\n\nWortkorrelationen\n\n# Create word correlation\nreview_pairs_corr &lt;- review_tidy %&gt;% \n    group_by(text) %&gt;% \n    filter(n() &gt;= 300) %&gt;% \n    pairwise_cor(\n        text, \n        id, \n        sort = TRUE)\n\n# Preview\nreview_pairs_corr %&gt;% \n    print(n = 15)\n\n# A tibble: 5,529,552 √ó 3\n   item1      item2      correlation\n   &lt;chr&gt;      &lt;chr&gt;            &lt;dbl&gt;\n 1 ottawa     newcastle        0.977\n 2 newcastle  ottawa           0.977\n 3 briggs     joanna           0.967\n 4 joanna     briggs           0.967\n 5 scholar    google           0.938\n 6 google     scholar          0.938\n 7 obsessive  compulsive       0.929\n 8 compulsive obsessive        0.929\n 9 nervosa    anorexia         0.893\n10 anorexia   nervosa          0.893\n11 ci         95               0.887\n12 95         ci               0.887\n13 las        los              0.886\n14 los        las              0.886\n15 gay        bisexual         0.861\n# ‚Ñπ 5,529,537 more rows\n\n\n\n\nSpezifische ‚ÄúPartner‚Äù in spezifischen Umgebungen\n\nreview_pairs_corr %&gt;% #| \n  filter(\n    item1 %in% c(\n      \"review\",\n      \"literature\",\n      \"systematic\")\n    ) %&gt;% \n  group_by(item1) %&gt;% \n  slice_max(correlation, n = 5) %&gt;% \n  ungroup() %&gt;% \n  mutate(\n    item2 = reorder(item2, correlation)\n    ) %&gt;% \n  ggplot(\n    aes(item2, correlation, fill = item1)\n    ) +\n  geom_bar(stat = \"identity\") +\n  facet_wrap(~ item1, scales = \"free_y\") +\n  coord_flip() +\n  scale_fill_manual(\n    values = c(\n      \"#04316A\",\n      \"#C50F3C\",\n      \"#00B2D1\")) +\n  theme_pubr()\n\n\n\n\n\n\n\n\n\n\nDie h√§ufigsten ‚Äúpositiven‚Äù und ‚Äúnegativen‚Äù W√∂rter in den Abstracts\n\nreview_sentiment_count &lt;- review_tidy %&gt;% \n  inner_join(\n     get_sentiments(\"bing\"),\n     by = c(\"text\" = \"word\"),\n     relationship = \"many-to-many\") %&gt;% \n  count(text, sentiment)\n  \n# Preview\nreview_sentiment_count %&gt;% \n  group_by(sentiment) %&gt;%\n  slice_max(n, n = 10) %&gt;% \n  ungroup() %&gt;% \n  mutate(text = reorder(text, n)) %&gt;%\n  ggplot(aes(n, text, fill = sentiment)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(\n    ~sentiment, scales = \"free_y\") +\n  labs(x = \"Contribution to sentiment\",\n       y = NULL) +\n  scale_fill_manual(\n    values = c(\"#C50F3C\", \"#007900\")) +\n  theme_pubr()\n\n\n\n\n\n\n\n\n\n\nVerkn√ºpfung des Sentiemnt (‚ÄúScores‚Äù) mit den Abstracts\n\nreview_sentiment &lt;- review_tidy %&gt;% \n  inner_join(\n     get_sentiments(\"bing\"),\n     by = c(\"text\" = \"word\"),\n     relationship = \"many-to-many\") %&gt;% \n  count(id, sentiment) %&gt;% \n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %&gt;% \n  mutate(sentiment = positive - negative)\n  \n# Check\nreview_sentiment \n\n# A tibble: 35,710 √ó 4\n   id                               negative positive sentiment\n   &lt;chr&gt;                               &lt;int&gt;    &lt;int&gt;     &lt;int&gt;\n 1 https://openalex.org/W1000529773        2        2         0\n 2 https://openalex.org/W1006561082        0        1         1\n 3 https://openalex.org/W100685805         4       15        11\n 4 https://openalex.org/W1007410967        0        7         7\n 5 https://openalex.org/W1008209175        8        1        -7\n 6 https://openalex.org/W1009104829        2        4         2\n 7 https://openalex.org/W1009607471       15        8        -7\n 8 https://openalex.org/W1031503832       13        6        -7\n 9 https://openalex.org/W1035654938       10        5        -5\n10 https://openalex.org/W1044055445        5        0        -5\n# ‚Ñπ 35,700 more rows\n\n\n\n\nVerteilung des Sentiment (Scores) in den Abstracts\n\n\n[1] 0.4858737\n\n\n\nreview_sentiment %&gt;% \n  ggplot(aes(sentiment)) +\n  geom_histogram(binwidth = 0.5, fill = \"#FF707F\") +\n  labs(\n    x = \"Sentiment (Score) des Abstracts\", \n    y = \"Anzahl der Eintr√§ge\"\n  ) +\n  theme_pubr() +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\n\n\n\n\n\n\n\n\nEntwicklung des Sentiment (Scores) der Abstracts im Zeitverlauf\n\n# Create first graph\ng1 &lt;- review_works_correct %&gt;% \n  filter(id %in% review_sentiment$id) %&gt;% \n  left_join(review_sentiment, by = join_by(id)) %&gt;% \n  sjmisc::rec(\n    sentiment,\n    rec = \"min:-2=negative; -1:1=neutral; 2:max=positive\") %&gt;% \n  ggplot(aes(x = publication_year_fct, fill = as.factor(sentiment_r))) +\n    geom_bar() +\n    labs(\n      x = \"\",\n      y = \"Anzahl der Eintr√§ge\", \n      fill = \"Sentiment (Score)\") +\n    scale_fill_manual(values = c(\"#C50F3C\", \"#90A0AF\", \"#007900\")) +\n    theme_pubr() \n    #theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n# Create second graph\ng2 &lt;- review_works_correct %&gt;% \n  filter(id %in% review_sentiment$id) %&gt;% \n  left_join(review_sentiment, by = join_by(id)) %&gt;% \n  sjmisc::rec(\n    sentiment,\n    rec = \"min:-2=negative; -1:1=neutral; 2:max=positive\") %&gt;% \n  ggplot(aes(x = publication_year_fct, fill = as.factor(sentiment_r))) +\n    geom_bar(position = \"fill\") +\n    labs(\n      x = \"\",\n      y = \"Anteil der Eintr√§ge\", \n      fill = \"Sentiment (Score)\") +\n    scale_fill_manual(values = c(\"#C50F3C\", \"#90A0AF\", \"#007D29\")) +\n    theme_pubr() \n\n# COMBINE GRPAHS\nggarrange(g1, g2,\n          nrow = 1, ncol = 2, \n          align = \"hv\",\n          common.legend = TRUE)"
  },
  {
    "objectID": "sessions/ms-session-07.html",
    "href": "sessions/ms-session-07.html",
    "title": "Introduction to Text as Data",
    "section": "",
    "text": "üñ•Ô∏è Slides\nüìã Showcase"
  },
  {
    "objectID": "sessions/ms-session-07.html#participate",
    "href": "sessions/ms-session-07.html#participate",
    "title": "Introduction to Text as Data",
    "section": "",
    "text": "üñ•Ô∏è Slides\nüìã Showcase"
  },
  {
    "objectID": "sessions/ms-session-07.html#practice",
    "href": "sessions/ms-session-07.html#practice",
    "title": "Introduction to Text as Data",
    "section": "Practice",
    "text": "Practice\n‚úçÔ∏è Exercise"
  },
  {
    "objectID": "sessions/ms-session-07.html#suggested-readings",
    "href": "sessions/ms-session-07.html#suggested-readings",
    "title": "Introduction to Text as Data",
    "section": "Suggested readings",
    "text": "Suggested readings\n\nBenoit, K. (2020). Text as Data: An Overview (L. Curini & R. Franzese, Eds.). SAGE Publications Ltd. https://methods.sagepub.com/book/research-methods-in-political-science-and-international-relations\nGrimmer, J., Roberts, M. E., & Stewart, B. M. (2022). Text as data: A new framework for machine learning and the social sciences. Princeton University Press.\nGrimmer, J., & Stewart, B. M. (2013). Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts. Political Analysis, 21(3), 267‚Äì297. https://doi.org/10/f458q9\n\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "sessions/ms-session-08.html",
    "href": "sessions/ms-session-08.html",
    "title": "Text processing in R",
    "section": "",
    "text": "üñ•Ô∏è Slides\nüìã Showcase"
  },
  {
    "objectID": "sessions/ms-session-08.html#participate",
    "href": "sessions/ms-session-08.html#participate",
    "title": "Text processing in R",
    "section": "",
    "text": "üñ•Ô∏è Slides\nüìã Showcase"
  },
  {
    "objectID": "sessions/ms-session-08.html#practice",
    "href": "sessions/ms-session-08.html#practice",
    "title": "Text processing in R",
    "section": "Practice",
    "text": "Practice\n‚úçÔ∏è Exercise"
  },
  {
    "objectID": "sessions/ms-session-08.html#suggested-readings",
    "href": "sessions/ms-session-08.html#suggested-readings",
    "title": "Text processing in R",
    "section": "Suggested readings",
    "text": "Suggested readings\n\nAtteveldt, W. van, Trilling, D., & Arc√≠la, C. (2021). Computational analysis of communication: A practical introduction to the analysis of texts, networks, and images with code examples in python and r. John Wiley & Sons.\nSilge, J., & Hvitfeldt, E. (n.d.). Supervised machine learning for text analysis in r. https://smltar.com/\nSilge, J., & Robinson, D. (2017). Text mining with r: A tidy approach (First edition). O‚ÄôReilly.\nWelbers, K., Van Atteveldt, W., & Benoit, K. (2017). Text Analysis in R. Communication Methods and Measures, 11(4), 245‚Äì265. https://doi.org/10.1080/19312458.2017.1387238"
  },
  {
    "objectID": "sessions/ms-session-08.html#useful-resources",
    "href": "sessions/ms-session-08.html#useful-resources",
    "title": "Text processing in R",
    "section": "Useful resources",
    "text": "Useful resources\n\nTutorials des CCS-Amsterdam zu ‚Äútidytext-based‚Äù Textanalyse:\n\nüìñ TidyText basics\nüìñ Dictionary analysis with TidyText\n\n\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "slides/ps-slides-05.html#seminarplan",
    "href": "slides/ps-slides-05.html#seminarplan",
    "title": "Datensatzvorstellung & RefreshR",
    "section": "Seminarplan",
    "text": "Seminarplan\n\n\n\n\n\n\n\n\n\n\n\n\n\nSitzung\nDatum\nThema (synchron)\n\n\n\n\n1\n30.10.2024\nKick-Off\n\n\n2\n06.11.2024\nWorkflow & Analysestrategie I\n\n\n3\n13.11.2024\nWorkflow & Analysestrategie II\n\n\n4\n20.11.2024\nGruppenarbeit\n\n\n5\n27.11.2024\nAll things R: Datensatzvorstellung & Refresher\n\n\n6\n04.12.2024\nUpdate zum Workflow I\n\n\n7\n11.12.2024\nGruppenarbeit\n\n\n8\n18.12.2024\nAll things R\n\n\n9\n08.01.2025\nUpdate zum Workflow II\n\n\n10\n15.01.2025\nGruppenarbeit\n\n\n11\n22.01.2025\nAll things R\n\n\n12\n27.01.2025\nSondertermin: Vorstellung Projektseminar\n\n\n13\n29.01.2025\nAbschlusspr√§sentation (inkl. Feedback)\n\n\n1\n05.02.2025\nüèÅ Semesterabschluss: Projektbericht & Evaluation"
  },
  {
    "objectID": "slides/ps-slides-05.html#wir-starten-mit-dem-resultat",
    "href": "slides/ps-slides-05.html#wir-starten-mit-dem-resultat",
    "title": "Datensatzvorstellung & RefreshR",
    "section": "Wir starten mit dem Resultat",
    "text": "Wir starten mit dem Resultat\nBeschreibung des finalen Datensatzes\n\nResultat der Erhebung (26.11.2024) sind drei Datens√§tze:\n\nreference.qs ‚ûú vorgefilterter, finaler Datensatz mit 32518 Eintr√§gen\nreferences_full.qs ‚ûú Datensatz (bzw. ‚ÄúListe‚Äù) mit verschiedenen Datens√§tzen (OpenAlex, Scopus & Kombination) in verschiedenen Verarbeitungstufen\nreferences_import_bibliometrix.RDA ‚ûú Umstrukturierte Version von references.qs f√ºr den einfachen Import in Bibliometrix\n\n\n\n\nAlle Erhebungsschritte sind auf der Kurspage unter Datenerhebung dokumentiert\nIm Folgenden kurze Darstellung von ausgew√§hlten Details"
  },
  {
    "objectID": "slides/ps-slides-05.html#mining-√ºberpr√ºfen-kombinieren",
    "href": "slides/ps-slides-05.html#mining-√ºberpr√ºfen-kombinieren",
    "title": "Datensatzvorstellung & RefreshR",
    "section": "Mining ‚ûú √úberpr√ºfen ‚ûú Kombinieren",
    "text": "Mining ‚ûú √úberpr√ºfen ‚ûú Kombinieren\n√úberblick √ºber den Erhebungsprozess\n\nInitiale OpenAlex-API Abfrage\n\nVerschiedene Qualit√§tskontrollen (Duplikate & Missings von zentralen Variablen, wie ID, Abstract & DOI)\n\nScopus-API Abfrage\n\nVerschiedene Qualit√§tskontrollen (Duplikate & Missings von zentralen Variablen, wie ID, Abstract & DOI)\n\nAbgleich & Kombination der Referenzen beider API-Abfragen\n\nIdentifikation von Scopus-Quellen, die in OpenAlex-Datensatz fehlen\nErg√§nzung (eines Teils) der fehlenden Scopus-Quellen durch erneute OpenAlex-API Abfrage (DOI als Grundlage)\nSubstition fehlender OpenAlex-Abstracts aus Scopus-Daten"
  },
  {
    "objectID": "slides/ps-slides-05.html#first-step",
    "href": "slides/ps-slides-05.html#first-step",
    "title": "Datensatzvorstellung & RefreshR",
    "section": "First Step",
    "text": "First Step\nInitiale OpenAlex-API Abfrage\n\nreferences$openalex$api &lt;- openalexR::oa_fetch(\n  entity = \"works\",\n  title_and_abstract.search = \n  '(\"artificial intelligence\" OR AI OR \"chatbot\" OR \"AI-based chatbot\" OR\n   \"artificial intelligence-based chatbot\" OR \"chat agent\" OR \"voice bot\" OR\n    \"voice assistant\" OR \"voice-based assistant\" OR\n    \"conversational agent\" OR \"conversational assistant\" OR \"conversational AI\" OR\n    \"AI-based assistant\" OR \"artificial intelligence-based assistant\" OR \n    \"virtual assistant\" OR \"intelligent assistant\" OR \"digital assistant\" OR\n    \"smart speaker\" OR\n    chatgpt OR \"google gemini\" OR \"google bard\" OR \"bing chat\" OR\n    \"microsoft copilot\" OR \"claude ai\" OR \"perplexity ai\") \n    AND\n    (anthropomorphism OR humanlike OR humanness OR humanized OR \n     \"user experience\" OR UX OR usability OR trust* OR\n     \"conversational experience\" OR CUX OR \"conversation design\" OR\n     safety OR privacy)',\n  publication_year = \"2016-2025\",\n  primary_topic.field.id = c(\n    \"fields/33\", # Social Science\n    \"fields/32\" # Psychology\n  ),\n  language = \"en\",\n  type = c(\"article\", \"conference-paper\", \"preprint\"),\n  verbose = TRUE\n)"
  },
  {
    "objectID": "slides/ps-slides-05.html#fehlen-informationen-f√ºr-zentrale-variablen",
    "href": "slides/ps-slides-05.html#fehlen-informationen-f√ºr-zentrale-variablen",
    "title": "Datensatzvorstellung & RefreshR",
    "section": "Fehlen Informationen f√ºr zentrale Variablen?",
    "text": "Fehlen Informationen f√ºr zentrale Variablen?\n√úberpr√ºfung der Datenqualit√§t: Missing Values\n\nreferences$openalex$api %&gt;% \n naniar::vis_miss(warn_large_data = FALSE)"
  },
  {
    "objectID": "slides/ps-slides-05.html#gibt-es-doppelte-eintr√§ge",
    "href": "slides/ps-slides-05.html#gibt-es-doppelte-eintr√§ge",
    "title": "Datensatzvorstellung & RefreshR",
    "section": "Gibt es doppelte Eintr√§ge?",
    "text": "Gibt es doppelte Eintr√§ge?\n√úberpr√ºfung der Datenqualit√§t: DOI\n\nreferences$openalex$api %&gt;% \n    # exclude ID duplicates\n    distinct(id, .keep_all = TRUE) %&gt;% \n    # exclude cases without DOI\n    filter(!is.na(doi)) %&gt;% \n    group_by(doi) %&gt;% \n    summarise(n = n()) %&gt;% \n    frq(n, sort.frq = \"desc\")\n\nn &lt;integer&gt; \n# total N=26496 valid N=26496 mean=1.00 sd=0.02\n\nValue |     N | Raw % | Valid % | Cum. %\n----------------------------------------\n    1 | 26488 | 99.97 |   99.97 |  99.97\n    2 |     8 |  0.03 |    0.03 | 100.00\n &lt;NA&gt; |     0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;"
  },
  {
    "objectID": "slides/ps-slides-05.html#besonderheiten-und-herausforderungen",
    "href": "slides/ps-slides-05.html#besonderheiten-und-herausforderungen",
    "title": "Datensatzvorstellung & RefreshR",
    "section": "Besonderheiten und Herausforderungen",
    "text": "Besonderheiten und Herausforderungen\nBesonderheiten der Scopus-API-Abfrage:\n\nCustom functions statt R-Paket ‚ûú Output weniger strukturiert\nStrikteres Charakterlimit f√ºr Scopus-API-Abfrage als bei OpenAlex ‚ûú Erh√∂hter Aufwand bei Qualit√§tskontrolle\n\nHerausforderungen bei der Kombination der Daten:\n\nUnterschiedliche Struktur der Datens√§tze (z.B. Variablennamen, Autorenangaben etc.)\nUnterschiedliche Kategorisierung der Referenzen (z.B. in Bezug auf Forschungsfeld oder Publikationstyp)\nFehlende Abstracts in OpenAlex-Datensatz ‚ûú Substitution durch Scopus-Daten"
  },
  {
    "objectID": "slides/ps-slides-05.html#finalisierung-der-daten",
    "href": "slides/ps-slides-05.html#finalisierung-der-daten",
    "title": "Datensatzvorstellung & RefreshR",
    "section": "Finalisierung der Daten",
    "text": "Finalisierung der Daten\nCode zur Erstellung des finalen Datensatzes\n\nreferences$openalex$correct &lt;- references$openalex$combined$raw_updated %&gt;% \n  filter(type %in% c(\"article\", \"conference-paper\", \"preprint\")) %&gt;% \n  filter(language == \"en\") %&gt;% \n  mutate(\n  # Create additional factor variables\n    publication_year_fct = as.factor(publication_year), \n    type_fct = as.factor(type), \n  # Clean abstracts\n    ab = ab %&gt;%\n      str_replace_all(\"\\ufffe\", \"\") %&gt;%    # Remove invalid U+FFFE characters\n      str_replace_all(\"[^\\x20-\\x7E\\n]\", \"\") %&gt;% # Optional: Remove other non-ASCII chars\n      iconv(from = \"UTF-8\", to = \"UTF-8\", sub = \"\"), # Ensure UTF-8 encoding\n  )\n\n\n# Export data for bibliometrix\nreferences_bibliometrix &lt;- oa2bibliometrix(references$openalex$correct)\nsaveRDS(references_bibliometrix, file = here(\"local_data/references_import_bibliometrix.RDS\"))"
  },
  {
    "objectID": "slides/ps-slides-05.html#√ºberblick-finaler-datensatz",
    "href": "slides/ps-slides-05.html#√ºberblick-finaler-datensatz",
    "title": "Datensatzvorstellung & RefreshR",
    "section": "√úberblick finaler Datensatz",
    "text": "√úberblick finaler Datensatz\nStruktur des finalen Datensatzes\n\nrefs %&gt;% glimpse\n\nRows: 32,518\nColumns: 42\n$ id                          &lt;chr&gt; \"https://openalex.org/W3013998503\", \"https‚Ä¶\n$ title                       &lt;chr&gt; \"Human Trust in Artificial Intelligence: R‚Ä¶\n$ display_name                &lt;chr&gt; \"Human Trust in Artificial Intelligence: R‚Ä¶\n$ author                      &lt;list&gt; [&lt;data.frame[2 x 12]&gt;], [&lt;data.frame[1 x ‚Ä¶\n$ ab                          &lt;chr&gt; \"Artificial intelligence (AI) characterize‚Ä¶\n$ publication_date            &lt;chr&gt; \"2020-03-26\", \"2018-04-12\", \"2019-01-29\", ‚Ä¶\n$ relevance_score             &lt;dbl&gt; 986.2187, 888.2551, 623.6296, 593.6381, 58‚Ä¶\n$ so                          &lt;chr&gt; \"Academy of Management Annals\", \"Business ‚Ä¶\n$ so_id                       &lt;chr&gt; \"https://openalex.org/S27614628\", \"https:/‚Ä¶\n$ host_organization           &lt;chr&gt; \"Routledge\", \"Elsevier BV\", \"Elsevier BV\",‚Ä¶\n$ issn_l                      &lt;chr&gt; \"1941-6520\", \"0007-6813\", \"0747-5632\", \"09‚Ä¶\n$ url                         &lt;chr&gt; \"https://doi.org/10.5465/annals.2018.0057\"‚Ä¶\n$ pdf_url                     &lt;chr&gt; NA, NA, NA, NA, \"https://link.springer.com‚Ä¶\n$ license                     &lt;chr&gt; NA, NA, NA, NA, \"cc-by\", \"cc-by\", NA, NA, ‚Ä¶\n$ version                     &lt;chr&gt; NA, NA, NA, NA, \"publishedVersion\", \"publi‚Ä¶\n$ first_page                  &lt;chr&gt; \"627\", \"577\", \"304\", \"611\", \"2749\", \"e19\",‚Ä¶\n$ last_page                   &lt;chr&gt; \"660\", \"586\", \"316\", \"623\", \"2767\", \"e19\",‚Ä¶\n$ volume                      &lt;chr&gt; \"14\", \"61\", \"97\", \"35\", \"26\", \"4\", \"61\", \"‚Ä¶\n$ issue                       &lt;chr&gt; \"2\", \"4\", NA, \"3\", \"5\", \"2\", \"4\", \"6\", \"21‚Ä¶\n$ is_oa                       &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, FA‚Ä¶\n$ is_oa_anywhere              &lt;lgl&gt; FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, FAL‚Ä¶\n$ oa_status                   &lt;chr&gt; \"closed\", \"closed\", \"closed\", \"green\", \"hy‚Ä¶\n$ oa_url                      &lt;chr&gt; NA, NA, NA, \"https://pure.uva.nl/ws/files/‚Ä¶\n$ any_repository_has_fulltext &lt;lgl&gt; FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, FAL‚Ä¶\n$ language                    &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", ‚Ä¶\n$ grants                      &lt;list&gt; NA, NA, NA, NA, NA, NA, NA, NA, &lt;\"https:/‚Ä¶\n$ cited_by_count              &lt;int&gt; 948, 1179, 614, 487, 281, 1568, 688, 511, ‚Ä¶\n$ counts_by_year              &lt;list&gt; [&lt;data.frame[5 x 2]&gt;], [&lt;data.frame[7 x 2‚Ä¶\n$ publication_year            &lt;int&gt; 2020, 2018, 2019, 2020, 2020, 2017, 2019, ‚Ä¶\n$ cited_by_api_url            &lt;chr&gt; \"https://api.openalex.org/works?filter=cit‚Ä¶\n$ ids                         &lt;list&gt; &lt;\"https://openalex.org/W3013998503\", \"htt‚Ä¶\n$ doi                         &lt;chr&gt; \"https://doi.org/10.5465/annals.2018.0057\"‚Ä¶\n$ type                        &lt;chr&gt; \"article\", \"article\", \"article\", \"article\"‚Ä¶\n$ referenced_works            &lt;list&gt; &lt;\"https://openalex.org/W1155971952\", \"htt‚Ä¶\n$ related_works               &lt;list&gt; &lt;\"https://openalex.org/W4289755905\", \"htt‚Ä¶\n$ is_paratext                 &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, ‚Ä¶\n$ is_retracted                &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, ‚Ä¶\n$ concepts                    &lt;list&gt; [&lt;data.frame[19 x 5]&gt;], [&lt;data.frame[17 x‚Ä¶\n$ topics                      &lt;list&gt; [&lt;tbl_df[12 x 5]&gt;], [&lt;tbl_df[12 x 5]&gt;], [‚Ä¶\n$ mining_source               &lt;chr&gt; \"openalex_initial\", \"openalex_initial\", \"o‚Ä¶\n$ publication_year_fct        &lt;fct&gt; 2020, 2018, 2019, 2020, 2020, 2017, 2019, ‚Ä¶\n$ type_fct                    &lt;fct&gt; article, article, article, article, articl‚Ä¶"
  },
  {
    "objectID": "slides/ps-slides-05.html#lern-die-daten-kennen",
    "href": "slides/ps-slides-05.html#lern-die-daten-kennen",
    "title": "Datensatzvorstellung & RefreshR",
    "section": "Lern die Daten kennen!",
    "text": "Lern die Daten kennen!\n√úberblick √ºber die n√§chten Schritte\n\nErneute Filterung der Daten bzw. Auswahl der relevanten Referenzen ‚ûú Eigene Query-Abfrage auf Basis der Abstracts\n√úberpr√ºfung der f√ºr die Analyse zentralen Variablen ‚ûú Sind weitere Bereinigung notwendig?\nExplorative Datenanalyse (EDA) zur Erkundung des Datensatzes ‚ûú Identifiaktion von Mustern und Auff√§lligkeiten"
  },
  {
    "objectID": "slides/ps-slides-05.html#custom-query-mit-r",
    "href": "slides/ps-slides-05.html#custom-query-mit-r",
    "title": "Datensatzvorstellung & RefreshR",
    "section": "Custom query mit R",
    "text": "Custom query mit R\nAuswahl der relevanten Referenzen\n\n# Define patterns for each part of the search string\npart1 &lt;- paste(\n  \"artificial intelligence|AI|chatbot|AI-based chatbot|artificial intelligence-based chatbot|chat agent\",\n  \"voice bot|voice assistant|voice-based assistant|conversational agent|conversational assistant\",\n  \"conversational AI|AI-based assistant|artificial intelligence-based assistant|virtual assistant\",\n  \"intelligent assistant|digital assistant|smart speaker|chatgpt|google gemini|google bard\",\n  \"bing chat|microsoft copilot|claude ai|perplexity ai\", sep = \"|\"\n)\npart2 &lt;- \"misinformation\"\n\n\n# Apply the patterns to the column `ab` in the tibble `works`\nmisinfo &lt;- refs %&gt;%\n    mutate(\n        matches = str_detect(ab, regex(part1, ignore_case = TRUE)) &\n            str_detect(ab, regex(part2, ignore_case = TRUE)) \n    ) %&gt;%\n    filter(matches)"
  },
  {
    "objectID": "slides/ps-slides-05.html#missing-abstracts",
    "href": "slides/ps-slides-05.html#missing-abstracts",
    "title": "Datensatzvorstellung & RefreshR",
    "section": "Missing Abstracts?",
    "text": "Missing Abstracts?\n√úberpr√ºfung der zentralen Variablen\n\nmisinfo %&gt;% naniar::vis_miss(warn_large_data = FALSE)"
  },
  {
    "objectID": "slides/ps-slides-05.html#viele-wege-der-exploration",
    "href": "slides/ps-slides-05.html#viele-wege-der-exploration",
    "title": "Datensatzvorstellung & RefreshR",
    "section": "Viele Wege der Exploration",
    "text": "Viele Wege der Exploration\nN√ºtzliche Funktionen zur Datenexploration\n\nGrunds√§tzlich viel Funktionen & Pakete zur √úberpr√ºfung der Daten in R verf√ºgbar\nEmpfehlungen:\n\nskimr::skim() f√ºr schnellen & umfassenden √úberblick der Daten in R\neasystats-verse f√ºr vertiefende Analysen in R\nbibliometrix f√ºr bibliometrische Analysen in R ‚ûú biblioshiny() f√ºr Analyse mit GUI"
  },
  {
    "objectID": "slides/ps-slides-05.html#fragen-zu--code",
    "href": "slides/ps-slides-05.html#fragen-zu--code",
    "title": "Datensatzvorstellung & RefreshR",
    "section": "Fragen zu -Code",
    "text": "Fragen zu -Code\n\nWie identifiziert & entfernt man Paper-Duplikate mit R genau?\n\n\nAbh√§ngig von der Definition von ‚ÄúDuplikat‚Äù (z.B. ein Kriterium oder mehrere Kriterien)\nsiehe Beispiel auf der Folie Gibt es doppelte Eintr√§ge?\n\n\nWie f√ºhrt man 2 Datens√§tze (z.B. den ‚Äûnormalen‚Äú & den aus der forward- & backward-search) am einfachsten/unkompliziertesten zusammen?\n\n\nAbh√§ngig von Datengrundlage (z.B. importierte .csv vs.¬†Erhebung via OpenAlex-API) & Datenstruktur (z.B. mit bind_rows(), merge(), left_join() etc)"
  },
  {
    "objectID": "slides/ps-slides-05.html#fragen-zum-topic-modeling",
    "href": "slides/ps-slides-05.html#fragen-zum-topic-modeling",
    "title": "Datensatzvorstellung & RefreshR",
    "section": "Fragen zum Topic Modeling",
    "text": "Fragen zum Topic Modeling\n\nIn Bezug auf das Thema Topic Modeling: W√ºrdest du uns empfehlen, Structural Topic Modeling (STM) anzuwenden oder denkst du, dass eines der anderen Verfahren der Themenmodellierung besser zu unserer Forschungsfrage passen k√∂nnte\n\n\nSchwierig zu beantworten, da STM auch ‚Äúnur‚Äù ein Teil der Analyse sein kann\n\n\nWie sieht Topic Modelling mit Seed Words aus?¬†\n\n\nz.B. seededlda Paket (siehe Beispiele)"
  },
  {
    "objectID": "slides/ps-slides-05.html#fragen-zur-praktischen-arbeit-mit",
    "href": "slides/ps-slides-05.html#fragen-zur-praktischen-arbeit-mit",
    "title": "Datensatzvorstellung & RefreshR",
    "section": "Fragen zur praktischen Arbeit mit ",
    "text": "Fragen zur praktischen Arbeit mit \n\nGibt es in R eine M√∂glichkeit, dass mehrere Personen parallel an¬†demselben Skript arbeiten k√∂nnen? Wenn nein, wie k√∂nnte man die¬†√Ñnderungen am besten zusammenf√ºhren?\n\n\nKeine gleichzeitge Arbeit an einem Dokument wie bei Google Docs, aber\n\nVerwendung des RStudio-Servers (nicht zeitgleich, aber zumindest im selben Dokument)\ngit (sehr kompliziert)\n\nEmpfehlung: Aufteilung anhand verschiedener Bereiche (Daten, Analyse, Interpretation) ‚ûú Code‚Äùger√ºst‚Äù schreiben, an ‚Äúneue‚Äù Daten anpassen"
  },
  {
    "objectID": "slides/ms-slides-09.html#seminarplan",
    "href": "slides/ms-slides-09.html#seminarplan",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Seminarplan",
    "text": "Seminarplan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSitzung\nDatum\nThema (synchron)\n√úbung (asynchron)\nDozent:in\n\n\n\n\n1\n18.04.2024\nEinf√ºhrung & √úberblick\n\nAM & CA\n\n\n\nüìö\nTeil 1: Systematic Review\n\n\n\n\n2\n25.04.2024\nEinf√ºhrung in Systematic Reviews I\nR-Einf√ºhrung\nAM\n\n\n3\n02.05.2024\nEinf√ºhrung in Systematic Reviews II\nR-Einf√ºhrung\nAM\n\n\n\n09.05.2024\nüèñÔ∏è Feiertag\nR-Einf√ºhrung\n\n\n\n4\n16.05.2024\nAutomatisierung von SRs & KI-Tools\nR-Einf√ºhrung\nAM\n\n\n\n23.05.2024\nüçª WiSo-Projekt-Woche\nR-Einf√ºhrung\n\n\n\n5\n04.06.2024\nüçï Gastvortrag: Prof.¬†Dr.¬†Emese Domahidi\nR-Einf√ºhrung\nED\n\n\n6\n06.06.2024\nAutomatisierung von SRs & KI-Tools\nR-Einf√ºhrung\nAM\n\n\n\nüíª\nTeil 2: Text as Data & Unsupervised Machine Learning\n\n\n\n\n7\n13.06.2024\nIntroduction to Text as Data\nzur Sitzung\nCA\n\n\n8\n20.06.2024\nText processing\nzur Sitzung\nCA\n\n\n9\n27.06.2024\nUnsupervised Machine Learning I\nzur Sitzung\nCA\n\n\n10\n04.07.2024\nUnsupervised Machine Learning II\nzur Sitzung\nCA & AM\n\n\n11\n11.07.2024\nRecap & Ausblick\nzur Sitzung\nCA & AM\n\n\n12\n18.07.2024\nüèÅ Semesterabschluss\nzur Sitzung\nCA & AM"
  },
  {
    "objectID": "slides/ms-slides-09.html#besprechung-der-r-√ºbung",
    "href": "slides/ms-slides-09.html#besprechung-der-r-√ºbung",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Besprechung der R-√úbung",
    "text": "Besprechung der R-√úbung\nSollten wir die Daten weiter eingrenzen?\n\n\n\nBitte scannt den QR-Code oder nutzt den folgenden Link f√ºr die Teilnahme an einer kurzen Umfrage:\n\nhttps://www.menti.com/als6ys2e2y69\nTemporary Access Code: 2250 1954\n\n\n\n\n \n\n    \n\n\n\n\n‚àí+\n02:00"
  },
  {
    "objectID": "slides/ms-slides-09.html#ergebnis",
    "href": "slides/ms-slides-09.html#ergebnis",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Ergebnis",
    "text": "Ergebnis"
  },
  {
    "objectID": "slides/ms-slides-09.html#quick-reminder-die-tidytext-pipeline",
    "href": "slides/ms-slides-09.html#quick-reminder-die-tidytext-pipeline",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Quick reminder: Die tidytext Pipeline",
    "text": "Quick reminder: Die tidytext Pipeline\nFokus auf einzelne W√∂rter, deren Beziehungen zueinander und Sentiments\n\n\n\n\n\n\n\n\n\n\n\n\n\nSilge & Robinson (2017)"
  },
  {
    "objectID": "slides/ms-slides-09.html#expansion-der-pipeline",
    "href": "slides/ms-slides-09.html#expansion-der-pipeline",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Expansion der Pipeline",
    "text": "Expansion der Pipeline\nFokus auf die Modelierung der Beziehung zwischen W√∂rtern & Dokumenten\n\n\n\n\n\n\n\n\n\n\n\n\n\nSilge & Robinson (2017)"
  },
  {
    "objectID": "slides/ms-slides-09.html#possibilities-over-possibilities",
    "href": "slides/ms-slides-09.html#possibilities-over-possibilities",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Possibilities over possibilities",
    "text": "Possibilities over possibilities\n√úberblick √ºber verschiedene Methoden der Textanalyse (Grimmer & Stewart, 2013)"
  },
  {
    "objectID": "slides/ms-slides-09.html#promises-pitfalls",
    "href": "slides/ms-slides-09.html#promises-pitfalls",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Promises & pitfalls",
    "text": "Promises & pitfalls\nVier Grunds√§tze der quantitativen Textanalyse (Grimmer & Stewart, 2013)\n1Ô∏è‚É£ All quantitative models of language are wrong ‚Äî but some are useful.\n2Ô∏è‚É£ Quantitative methods for text amplify resources and augment humans.\n3Ô∏è‚É£ There is no globally best method for automated text analysis.\n4Ô∏è‚É£ Validate, Validate, Validate!"
  },
  {
    "objectID": "slides/ms-slides-09.html#verteilung-von-w√∂rtern-auf-themen-auf-dokumente",
    "href": "slides/ms-slides-09.html#verteilung-von-w√∂rtern-auf-themen-auf-dokumente",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Verteilung von W√∂rtern auf Themen auf Dokumente",
    "text": "Verteilung von W√∂rtern auf Themen auf Dokumente\nDie Grundidee des (LDA) Topic Modeling\n\n(Blei, 2012)\nEach topic is a distribution of words\nEach document is a mixture of corpus-wide topics\nEach words is drawn from one of those topics"
  },
  {
    "objectID": "slides/ms-slides-09.html#in-a-nutshell",
    "href": "slides/ms-slides-09.html#in-a-nutshell",
    "title": "Unsupervised Machine Learning (I)",
    "section": "In a nutshell",
    "text": "In a nutshell\nGrundlagen des Topic Modeling kurz zusammengefasst\n\nVerfahren des un√ºberwachten maschinellen Lernens, das sich daher insbesondere zur Exploration und Deskription gro√üer Textmengen eignet\nThemen werden strikt auf Basis von Worth√§ufigkeiten in den einzelnen Dokumenten vermeintlich objektiv berechnet, ganz ohne subjektive Einsch√§tzungen und damit einhergehenden etwaigen Verzerrungen\nBekanntesten dieser Verfahren sind LDA (Latent Dirichlet Allocation) sowie die darauf aufbauenden CTM (Correlated Topic Models) und STM (Structural Topic Models)"
  },
  {
    "objectID": "slides/ms-slides-09.html#wenn-missing-values-zum-problem-werden",
    "href": "slides/ms-slides-09.html#wenn-missing-values-zum-problem-werden",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Wenn Missing Values zum Problem werden",
    "text": "Wenn Missing Values zum Problem werden\nExkurs zu √úberpr√ºfung der Daten auf fehlende Werte\n\nvisdat::vis_miss(review_subsample, warn_large_data = FALSE)"
  },
  {
    "objectID": "slides/ms-slides-09.html#bereinigung-der-subsample",
    "href": "slides/ms-slides-09.html#bereinigung-der-subsample",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Bereinigung der Subsample",
    "text": "Bereinigung der Subsample\nAusschluss von Referenzen mit fehlendem Abstract\n\n\n\n# Create subsample\nreview_subsample &lt;- review_works_correct %&gt;% \n  # Eingrenzung: Sprache und Typ\n  filter(language == \"en\") %&gt;% \n  filter(type == \"article\") %&gt;%\n  # Datentranformation\n  unnest(topics, names_sep = \"_\") %&gt;%\n  filter(topics_name == \"field\") %&gt;% \n  filter(topics_i == \"1\") %&gt;% \n  # Eingrenzung: Forschungsfeldes\n  filter(\n   topics_display_name == \"Social Sciences\"|\n   topics_display_name == \"Psychology\"\n    )\n\n# Overview\nreview_subsample %&gt;% glimpse  \n\n\nRows: 45,221\nColumns: 45\n$ id                          &lt;chr&gt; \"https://openalex.org/W4293003987\", \"https‚Ä¶\n$ title                       &lt;chr&gt; \"The WHO-5 Well-Being Index: A Systematic ‚Ä¶\n$ display_name                &lt;chr&gt; \"The WHO-5 Well-Being Index: A Systematic ‚Ä¶\n$ author                      &lt;list&gt; [&lt;data.frame[4 x 12]&gt;], [&lt;data.frame[2 x ‚Ä¶\n$ ab                          &lt;chr&gt; \"The 5-item World Health Organization Well‚Ä¶\n$ publication_date            &lt;chr&gt; \"2015-01-01\", \"2017-08-28\", \"2014-01-01\", ‚Ä¶\n$ relevance_score             &lt;dbl&gt; 938.7603, 752.3500, 591.2553, 576.1210, 56‚Ä¶\n$ so                          &lt;chr&gt; \"Psychotherapy and psychosomatics\", \"Journ‚Ä¶\n$ so_id                       &lt;chr&gt; \"https://openalex.org/S184803288\", \"https:‚Ä¶\n$ host_organization           &lt;chr&gt; \"Karger Publishers\", \"SAGE Publishing\", NA‚Ä¶\n$ issn_l                      &lt;chr&gt; \"0033-3190\", \"0739-456X\", NA, \"2214-7829\",‚Ä¶\n$ url                         &lt;chr&gt; \"https://doi.org/10.1159/000376585\", \"http‚Ä¶\n$ pdf_url                     &lt;chr&gt; \"https://www.karger.com/Article/Pdf/376585‚Ä¶\n$ license                     &lt;chr&gt; \"cc-by-nc\", NA, NA, \"cc-by\", NA, NA, \"cc-b‚Ä¶\n$ version                     &lt;chr&gt; \"publishedVersion\", NA, \"publishedVersion\"‚Ä¶\n$ first_page                  &lt;chr&gt; \"167\", \"93\", NA, \"89\", \"55\", \"2150\", \"e356‚Ä¶\n$ last_page                   &lt;chr&gt; \"176\", \"112\", NA, \"106\", \"64\", \"2159\", \"e3‚Ä¶\n$ volume                      &lt;chr&gt; \"84\", \"39\", NA, \"6\", \"277\", \"32\", \"2\", \"24‚Ä¶\n$ issue                       &lt;chr&gt; \"3\", \"1\", NA, NA, NA, \"19\", \"8\", NA, \"9\", ‚Ä¶\n$ is_oa                       &lt;lgl&gt; TRUE, FALSE, TRUE, TRUE, TRUE, FALSE, TRUE‚Ä¶\n$ is_oa_anywhere              &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE,‚Ä¶\n$ oa_status                   &lt;chr&gt; \"hybrid\", \"green\", \"bronze\", \"gold\", \"bron‚Ä¶\n$ oa_url                      &lt;chr&gt; \"https://www.karger.com/Article/Pdf/376585‚Ä¶\n$ any_repository_has_fulltext &lt;lgl&gt; FALSE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE‚Ä¶\n$ language                    &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", ‚Ä¶\n$ grants                      &lt;list&gt; NA, NA, NA, &lt;\"https://openalex.org/F43203‚Ä¶\n$ cited_by_count              &lt;int&gt; 2657, 1375, 2568, 803, 3664, 1553, 2895, 9‚Ä¶\n$ counts_by_year              &lt;list&gt; [&lt;data.frame[11 x 2]&gt;], [&lt;data.frame[7 x ‚Ä¶\n$ publication_year            &lt;int&gt; 2015, 2017, 2014, 2016, 2020, 2014, 2017, ‚Ä¶\n$ cited_by_api_url            &lt;chr&gt; \"https://api.openalex.org/works?filter=cit‚Ä¶\n$ ids                         &lt;list&gt; &lt;\"https://openalex.org/W4293003987\", \"htt‚Ä¶\n$ doi                         &lt;chr&gt; \"https://doi.org/10.1159/000376585\", \"http‚Ä¶\n$ type                        &lt;chr&gt; \"article\", \"article\", \"article\", \"article\"‚Ä¶\n$ referenced_works            &lt;list&gt; &lt;\"https://openalex.org/W1492518593\", \"htt‚Ä¶\n$ related_works               &lt;list&gt; &lt;\"https://openalex.org/W3020194755\", \"htt‚Ä¶\n$ is_paratext                 &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, ‚Ä¶\n$ is_retracted                &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, ‚Ä¶\n$ concepts                    &lt;list&gt; [&lt;data.frame[7 x 5]&gt;], [&lt;data.frame[18 x ‚Ä¶\n$ topics_i                    &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ‚Ä¶\n$ topics_score                &lt;dbl&gt; 0.9926, 0.9050, 0.9995, 0.9987, 0.9999, 1.‚Ä¶\n$ topics_name                 &lt;chr&gt; \"field\", \"field\", \"field\", \"field\", \"field‚Ä¶\n$ topics_id                   &lt;chr&gt; \"https://openalex.org/fields/32\", \"https:/‚Ä¶\n$ topics_display_name         &lt;chr&gt; \"Psychology\", \"Social Sciences\", \"Psycholo‚Ä¶\n$ publication_year_fct        &lt;fct&gt; 2015, 2017, 2014, 2016, 2020, 2014, 2017, ‚Ä¶\n$ type_fct                    &lt;fct&gt; article, article, article, article, articl‚Ä¶"
  },
  {
    "objectID": "slides/ms-slides-09.html#bereinigung-der-subsample-1",
    "href": "slides/ms-slides-09.html#bereinigung-der-subsample-1",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Bereinigung der Subsample",
    "text": "Bereinigung der Subsample\nAusschluss von Referenzen mit fehlendem Abstract\n\n\n\n# Create subsample\nreview_subsample &lt;- review_works_correct %&gt;% \n  # Eingrenzung: Sprache und Typ\n  filter(language == \"en\") %&gt;% \n  filter(type == \"article\") %&gt;%\n  # Datentranformation\n  unnest(topics, names_sep = \"_\") %&gt;%\n  filter(topics_name == \"field\") %&gt;% \n  filter(topics_i == \"1\") %&gt;% \n  # Eingrenzung: Forschungsfeldes\n  filter(\n   topics_display_name == \"Social Sciences\"|\n   topics_display_name == \"Psychology\"\n    ) %&gt;% \n  # Eingrenzung: Keine Eintr√§ge ohne Abstract\n  filter(!is.na(ab))\n\n# Overview\nreview_subsample %&gt;% glimpse  \n\n\nRows: 36,680\nColumns: 45\n$ id                          &lt;chr&gt; \"https://openalex.org/W4293003987\", \"https‚Ä¶\n$ title                       &lt;chr&gt; \"The WHO-5 Well-Being Index: A Systematic ‚Ä¶\n$ display_name                &lt;chr&gt; \"The WHO-5 Well-Being Index: A Systematic ‚Ä¶\n$ author                      &lt;list&gt; [&lt;data.frame[4 x 12]&gt;], [&lt;data.frame[2 x ‚Ä¶\n$ ab                          &lt;chr&gt; \"The 5-item World Health Organization Well‚Ä¶\n$ publication_date            &lt;chr&gt; \"2015-01-01\", \"2017-08-28\", \"2014-01-01\", ‚Ä¶\n$ relevance_score             &lt;dbl&gt; 938.7603, 752.3500, 591.2553, 576.1210, 56‚Ä¶\n$ so                          &lt;chr&gt; \"Psychotherapy and psychosomatics\", \"Journ‚Ä¶\n$ so_id                       &lt;chr&gt; \"https://openalex.org/S184803288\", \"https:‚Ä¶\n$ host_organization           &lt;chr&gt; \"Karger Publishers\", \"SAGE Publishing\", NA‚Ä¶\n$ issn_l                      &lt;chr&gt; \"0033-3190\", \"0739-456X\", NA, \"2214-7829\",‚Ä¶\n$ url                         &lt;chr&gt; \"https://doi.org/10.1159/000376585\", \"http‚Ä¶\n$ pdf_url                     &lt;chr&gt; \"https://www.karger.com/Article/Pdf/376585‚Ä¶\n$ license                     &lt;chr&gt; \"cc-by-nc\", NA, NA, \"cc-by\", NA, NA, \"cc-b‚Ä¶\n$ version                     &lt;chr&gt; \"publishedVersion\", NA, \"publishedVersion\"‚Ä¶\n$ first_page                  &lt;chr&gt; \"167\", \"93\", NA, \"89\", \"55\", \"2150\", \"e356‚Ä¶\n$ last_page                   &lt;chr&gt; \"176\", \"112\", NA, \"106\", \"64\", \"2159\", \"e3‚Ä¶\n$ volume                      &lt;chr&gt; \"84\", \"39\", NA, \"6\", \"277\", \"32\", \"2\", \"24‚Ä¶\n$ issue                       &lt;chr&gt; \"3\", \"1\", NA, NA, NA, \"19\", \"8\", NA, \"9\", ‚Ä¶\n$ is_oa                       &lt;lgl&gt; TRUE, FALSE, TRUE, TRUE, TRUE, FALSE, TRUE‚Ä¶\n$ is_oa_anywhere              &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE,‚Ä¶\n$ oa_status                   &lt;chr&gt; \"hybrid\", \"green\", \"bronze\", \"gold\", \"bron‚Ä¶\n$ oa_url                      &lt;chr&gt; \"https://www.karger.com/Article/Pdf/376585‚Ä¶\n$ any_repository_has_fulltext &lt;lgl&gt; FALSE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE‚Ä¶\n$ language                    &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", ‚Ä¶\n$ grants                      &lt;list&gt; NA, NA, NA, &lt;\"https://openalex.org/F43203‚Ä¶\n$ cited_by_count              &lt;int&gt; 2657, 1375, 2568, 803, 3664, 1553, 2895, 9‚Ä¶\n$ counts_by_year              &lt;list&gt; [&lt;data.frame[11 x 2]&gt;], [&lt;data.frame[7 x ‚Ä¶\n$ publication_year            &lt;int&gt; 2015, 2017, 2014, 2016, 2020, 2014, 2017, ‚Ä¶\n$ cited_by_api_url            &lt;chr&gt; \"https://api.openalex.org/works?filter=cit‚Ä¶\n$ ids                         &lt;list&gt; &lt;\"https://openalex.org/W4293003987\", \"htt‚Ä¶\n$ doi                         &lt;chr&gt; \"https://doi.org/10.1159/000376585\", \"http‚Ä¶\n$ type                        &lt;chr&gt; \"article\", \"article\", \"article\", \"article\"‚Ä¶\n$ referenced_works            &lt;list&gt; &lt;\"https://openalex.org/W1492518593\", \"htt‚Ä¶\n$ related_works               &lt;list&gt; &lt;\"https://openalex.org/W3020194755\", \"htt‚Ä¶\n$ is_paratext                 &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, ‚Ä¶\n$ is_retracted                &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, ‚Ä¶\n$ concepts                    &lt;list&gt; [&lt;data.frame[7 x 5]&gt;], [&lt;data.frame[18 x ‚Ä¶\n$ topics_i                    &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ‚Ä¶\n$ topics_score                &lt;dbl&gt; 0.9926, 0.9050, 0.9995, 0.9987, 0.9999, 1.‚Ä¶\n$ topics_name                 &lt;chr&gt; \"field\", \"field\", \"field\", \"field\", \"field‚Ä¶\n$ topics_id                   &lt;chr&gt; \"https://openalex.org/fields/32\", \"https:/‚Ä¶\n$ topics_display_name         &lt;chr&gt; \"Psychology\", \"Social Sciences\", \"Psycholo‚Ä¶\n$ publication_year_fct        &lt;fct&gt; 2015, 2017, 2014, 2016, 2020, 2014, 2017, ‚Ä¶\n$ type_fct                    &lt;fct&gt; article, article, article, article, articl‚Ä¶"
  },
  {
    "objectID": "slides/ms-slides-09.html#aus-text-werden-zahlen",
    "href": "slides/ms-slides-09.html#aus-text-werden-zahlen",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Aus Text werden Zahlen",
    "text": "Aus Text werden Zahlen\nDocument-Term-Matrix [DTM] im Fokus\n\n\nSilge & Robinson (2017)"
  },
  {
    "objectID": "slides/ms-slides-09.html#kurzer-r√ºckblick-auf-die-document-term-matrix-dtm",
    "href": "slides/ms-slides-09.html#kurzer-r√ºckblick-auf-die-document-term-matrix-dtm",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Kurzer R√ºckblick auf die Document-Term Matrix [DTM]",
    "text": "Kurzer R√ºckblick auf die Document-Term Matrix [DTM]\nH√§ufig verwendete Datenstruktur f√ºr (klassische) Textanalyse\n\n\nEine Matrix, bei der:\n\njede Zeile steht f√ºr ein Dokument (z.B. ein Abstract),\njede Spalte einen Begriff darstellt, und\njeder Wert (in der Regel) die H√§ufigkeit des Begriffs in einem Dokument enth√§lt.\n\n\n\n\n\n\n\n\n(Zheng & Casari, 2018)"
  },
  {
    "objectID": "slides/ms-slides-09.html#schritt-f√ºr-schritt-zur-dtm",
    "href": "slides/ms-slides-09.html#schritt-f√ºr-schritt-zur-dtm",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Schritt f√ºr Schritt zur DTM",
    "text": "Schritt f√ºr Schritt zur DTM\nTextverarbeitung entlang der tidytext Pipeline: Tokenize\n\n\n\n# Create tidy data\nsubsample_tidy &lt;- review_subsample %&gt;% \n    tidytext::unnest_tokens(\"text\", ab) %&gt;% \n    filter(!text %in% tidytext::stop_words$word)\n\n# Preview\nsubsample_tidy %&gt;% \n  select(id, text) %&gt;% \n  print(n = 15)\n\n\n# A tibble: 4,872,424 √ó 2\n   id                               text          \n   &lt;chr&gt;                            &lt;chr&gt;         \n 1 https://openalex.org/W4293003987 5             \n 2 https://openalex.org/W4293003987 item          \n 3 https://openalex.org/W4293003987 world         \n 4 https://openalex.org/W4293003987 health        \n 5 https://openalex.org/W4293003987 organization  \n 6 https://openalex.org/W4293003987 index         \n 7 https://openalex.org/W4293003987 5             \n 8 https://openalex.org/W4293003987 widely        \n 9 https://openalex.org/W4293003987 questionnaires\n10 https://openalex.org/W4293003987 assessing     \n11 https://openalex.org/W4293003987 subjective    \n12 https://openalex.org/W4293003987 psychological \n13 https://openalex.org/W4293003987 publication   \n14 https://openalex.org/W4293003987 1998          \n15 https://openalex.org/W4293003987 5             \n# ‚Ñπ 4,872,409 more rows"
  },
  {
    "objectID": "slides/ms-slides-09.html#schritt-f√ºr-schritt-zur-dtm-1",
    "href": "slides/ms-slides-09.html#schritt-f√ºr-schritt-zur-dtm-1",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Schritt f√ºr Schritt zur DTM",
    "text": "Schritt f√ºr Schritt zur DTM\nTextverarbeitung entlang der tidytext Pipeline: Tokenize ‚ñ∂Ô∏è Summarize\n\n\n\n# Create tidy data\nsubsample_tidy &lt;- review_subsample %&gt;% \n    tidytext::unnest_tokens(\"text\", ab) %&gt;% \n    filter(!text %in% tidytext::stop_words$word)\n\n# Create summarized data\nsubsample_summarized &lt;- subsample_tidy %&gt;% \n  count(id, text) \n\n# Preview \nsubsample_summarized %&gt;% \n  print(n = 15)\n\n\n# A tibble: 3,280,664 √ó 3\n   id                               text           n\n   &lt;chr&gt;                            &lt;chr&gt;      &lt;int&gt;\n 1 https://openalex.org/W1000529773 aim            1\n 2 https://openalex.org/W1000529773 anne           1\n 3 https://openalex.org/W1000529773 approaches     1\n 4 https://openalex.org/W1000529773 critical       1\n 5 https://openalex.org/W1000529773 current        1\n 6 https://openalex.org/W1000529773 dick           1\n 7 https://openalex.org/W1000529773 effective      1\n 8 https://openalex.org/W1000529773 employed       1\n 9 https://openalex.org/W1000529773 enhancing      1\n10 https://openalex.org/W1000529773 evaluation     1\n11 https://openalex.org/W1000529773 examined       1\n12 https://openalex.org/W1000529773 explored       1\n13 https://openalex.org/W1000529773 extended       2\n14 https://openalex.org/W1000529773 girls          1\n15 https://openalex.org/W1000529773 government     1\n# ‚Ñπ 3,280,649 more rows"
  },
  {
    "objectID": "slides/ms-slides-09.html#schritt-f√ºr-schritt-zur-dtm-2",
    "href": "slides/ms-slides-09.html#schritt-f√ºr-schritt-zur-dtm-2",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Schritt f√ºr Schritt zur DTM",
    "text": "Schritt f√ºr Schritt zur DTM\nTextverarbeitung entlang der tidytext Pipeline: Tokenize ‚ñ∂Ô∏è Summarize ‚ñ∂Ô∏è DTM\n\n\n\n# Create tidy data\nsubsample_tidy &lt;- review_subsample %&gt;% \n    tidytext::unnest_tokens(\"text\", ab) %&gt;% \n    filter(!text %in% tidytext::stop_words$word)\n\n# Create summarized data\nsubsample_summarized &lt;- subsample_tidy %&gt;% \n  count(id, text) \n\n# Create DTM\nsubsample_dtm &lt;- subsample_summarized %&gt;% \n  cast_dtm(id, text, n)\n\n# Preview\nsubsample_dtm\n\n\n&lt;&lt;DocumentTermMatrix (documents: 36654, terms: 122147)&gt;&gt;\nNon-/sparse entries: 3280664/4473895474\nSparsity           : 100%\nMaximal term length: 188\nWeighting          : term frequency (tf)"
  },
  {
    "objectID": "slides/ms-slides-09.html#einfach-mit-tidytext-pr√§zise-mit-quanteda",
    "href": "slides/ms-slides-09.html#einfach-mit-tidytext-pr√§zise-mit-quanteda",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Einfach mit tidytext, pr√§zise mit quanteda",
    "text": "Einfach mit tidytext, pr√§zise mit quanteda\nVergleich von Texttransformation mit verschiedenen Paketen\n\n\n\n\n# Create tidy data\nsubsample_tidy &lt;- review_subsample %&gt;% \n    tidytext::unnest_tokens(\"text\", ab) %&gt;% \n    filter(!text %in% tidytext::stop_words$word)\n\n# Create summarized data\nsubsample_summarized &lt;- subsample_tidy %&gt;% \n  count(id, text) \n\n# Create DTM\nsubsample_dtm &lt;- subsample_summarized %&gt;% \n  cast_dtm(id, text, n)\n\n# Preview\nsubsample_dtm\n\n\n\n# Create corpus\nquanteda_corpus &lt;- review_subsample %&gt;% \n  quanteda::corpus(\n    docid_field = \"id\", \n    text_field = \"ab\"\n  )\n\n# Tokenize\nquanteda_token &lt;- quanteda_corpus %&gt;% \n  quanteda::tokens(\n    remove_punct = TRUE,\n    remove_symbols = TRUE, \n    remove_numbers = TRUE, \n    remove_url = TRUE, \n    split_tags = FALSE # keep hashtags and mentions\n  ) %&gt;% \n  quanteda::tokens_tolower() %&gt;% \n  quanteda::tokens_remove(\n    pattern = stopwords(\"en\")\n    )\n\n# Convert to Document-Feature-Matrix (DFM)\nquanteda_dfm &lt;- quanteda_token %&gt;% \n  quanteda::dfm()"
  },
  {
    "objectID": "slides/ms-slides-09.html#netzwerk-der-top-begriffe",
    "href": "slides/ms-slides-09.html#netzwerk-der-top-begriffe",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Netzwerk der Top-Begriffe",
    "text": "Netzwerk der Top-Begriffe\nVergleich zwischen tidytext & quanteda\n\n\n\n# Extract most common hashtags\ntop_features_tidy &lt;- subsample_tidy %&gt;% \n  count(text, sort = TRUE) %&gt;%\n  slice_head(n = 20) %&gt;% \n  pull(text)\n\n# Visualize\nsubsample_tidy %&gt;% \n  count(id, text, sort = TRUE) %&gt;% \n  filter(!is.na(text)) %&gt;% \n  cast_dfm(id, text, n) %&gt;% \n  quanteda::fcm() %&gt;% \n  quanteda::fcm_select(\n    pattern = top_features_tidy,\n    case_insensitive = FALSE\n  ) %&gt;%  \n  quanteda.textplots::textplot_network(\n    edge_color = \"#04316A\"\n  )"
  },
  {
    "objectID": "slides/ms-slides-09.html#netzwerk-der-top-begriffe-1",
    "href": "slides/ms-slides-09.html#netzwerk-der-top-begriffe-1",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Netzwerk der Top-Begriffe",
    "text": "Netzwerk der Top-Begriffe\nVergleich zwischen tidytext & quanteda\n\n\n\n\n# Extract most common features \ntop_features_quanteda &lt;- quanteda_dfm %&gt;% \n  topfeatures(20) %&gt;% \n  names()\n\n# Construct feature-occurrence matrix of features\nquanteda_dfm %&gt;% \n  fcm() %&gt;% \n  fcm_select(pattern = top_features_quanteda) %&gt;% \n  textplot_network(\n    edge_color = \"#C50F3C\"\n  )"
  },
  {
    "objectID": "slides/ms-slides-09.html#netzwerk-der-top-begriffe-2",
    "href": "slides/ms-slides-09.html#netzwerk-der-top-begriffe-2",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Netzwerk der Top-Begriffe",
    "text": "Netzwerk der Top-Begriffe\nVergleich zwischen tidytext & quanteda\n\n\n\n\nExpand for full code\nsubsample_tidy %&gt;% \n  count(id, text, sort = TRUE) %&gt;% \n  filter(!is.na(text)) %&gt;% \n  cast_dfm(id, text, n) %&gt;% \n  quanteda::fcm() %&gt;% \n  quanteda::fcm_select(\n    pattern = top_features_tidy,\n    case_insensitive = FALSE\n  ) %&gt;%  \n  quanteda.textplots::textplot_network(\n    edge_color = \"#04316A\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nExpand for full code\nquanteda_dfm %&gt;% \n  fcm() %&gt;% \n  fcm_select(pattern = top_features_quanteda) %&gt;% \n  textplot_network(\n    edge_color = \"#C50F3C\"\n  )"
  },
  {
    "objectID": "slides/ms-slides-09.html#neuer-input-in-die-pipeline",
    "href": "slides/ms-slides-09.html#neuer-input-in-die-pipeline",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Neuer Input in die Pipeline",
    "text": "Neuer Input in die Pipeline\nUnsupervised learning example: Topic modeling\n\n\nSilge & Robinson (2017)"
  },
  {
    "objectID": "slides/ms-slides-09.html#building-a-shared-vocabulary-again",
    "href": "slides/ms-slides-09.html#building-a-shared-vocabulary-again",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Building a shared vocabulary ‚Ä¶ again",
    "text": "Building a shared vocabulary ‚Ä¶ again\nGrundbegriffe und Definitionen im Kontext des Topic Modelings\n\nK: Anzahl der Themen, die f√ºr ein bestimmtes Themenmodell berechnen werden.\nWord-Topic-Matrix: Matrix, die die bedingte Wahrscheinlichkeit (beta) beschreibt, mit der ein Wort in einem bestimmten Thema vorkommt.\nDocument-Topic-Matrix: Matrix, die die bedingte Wahrscheinlichkeit (gamma) beschreibt, mit der ein Thema in einem bestimmten Dokument vorkommt."
  },
  {
    "objectID": "slides/ms-slides-09.html#beyond-lda",
    "href": "slides/ms-slides-09.html#beyond-lda",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Beyond LDA",
    "text": "Beyond LDA\nVerschiedene Ans√§tze der Themenmodellierung\n\n\nLatent Dirichlet Allocation [LDA] (Blei et al., 2003) ist ein probabilistisches generatives Modell, das davon ausgeht, dass jedes Dokumentin einem KorpuseineMischung von Themen ist und jedes Wort im Dokument einem der Themen des Dokuments zuzuordnenist.\nStructural Topic Modeling [STM] (Roberts et al., 2016; Roberts et al., 2019) erweitert LDA durch die Einbeziehung von Kovariaten auf Dokumentenebene und erm√∂glicht die Modellierung des Einflusses externer Faktoren auf die Themenpr√§valenz.\nWord embeddings (Word2Vec (Mikolov et al., 2013) , Glove (Pennington et al., 2014)) stellen W√∂rter als kontinuierliche Vektoren in einem hochdimensionalen Raum dar und erfassen semantische Beziehungen zwischen W√∂rtern basierend auf ihrem Kontext in den Daten.\nTopic Modeling mit Neural Networks (BERTopic(Devlin et al., 2019), Doc2Vec(Le & Mikolov, 2014)) nutzt Deep Learning-Architekturen, um automatisch latente Themen aus Textdaten zu lernen"
  },
  {
    "objectID": "slides/ms-slides-09.html#preparation-is-everything",
    "href": "slides/ms-slides-09.html#preparation-is-everything",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Preparation is everything",
    "text": "Preparation is everything\nEmpfohlene Vorverarbeitungsschritte f√ºr das Topic Modeling nach Maier et al. (2018)\n\n\n\n‚ö†Ô∏è Deduplication;\n‚úÖ tokenization;\n‚úÖ transforming all characters to lowercase;\n‚úÖ removing punctuation and special characters;\n‚úÖ Removing stop-words;\n‚ö†Ô∏è term unification (lemmatizing or stemming);\nüèóÔ∏è relative pruning (attributed to Zipf‚Äôs law);\n\n\n\n# Pruning\nquanteda_dfm_trim &lt;- quanteda_dfm %&gt;% \n  dfm_trim( \n    min_docfreq = 10/nrow(review_subsample),\n    max_docfreq = 0.99, \n    docfreq_type = \"prop\")\n\n# Convert for stm topic modeling\nquanteda_stm &lt;- quanteda_dfm_trim %&gt;% \n   convert(to = \"stm\")\n\n\n\nZipf‚Äôs law states that the frequency that a word appears is inversely proportional to its rank."
  },
  {
    "objectID": "slides/ms-slides-09.html#ein-erstes-modell",
    "href": "slides/ms-slides-09.html#ein-erstes-modell",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Ein erstes Modell",
    "text": "Ein erstes Modell\nSch√§tzung und Sichtung eines Structural Topic Models mit 20 Themen\n\n\n\nstm_mdl &lt;- stm::stm(\n  documents = quanteda_stm$documents,\n  vocab = quanteda_stm$vocab, \n  K = 20, \n  seed = 42,\n  max.em.its = 10,\n  init.type = \"Spectral\",\n  verbose = TRUE)\n\n\n\nstm_mdl\n\nA topic model with 20 topics, 36650 documents and a 14322 word dictionary."
  },
  {
    "objectID": "slides/ms-slides-09.html#ein-erster-√ºberblick",
    "href": "slides/ms-slides-09.html#ein-erster-√ºberblick",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Ein erster √úberblick",
    "text": "Ein erster √úberblick\nVerteilung und Beschreibung der Themen\n\nstm_mdl %&gt;% plot(type = \"summary\")"
  },
  {
    "objectID": "slides/ms-slides-09.html#selber-√ºberblick-anderes-format",
    "href": "slides/ms-slides-09.html#selber-√ºberblick-anderes-format",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Selber √úberblick, anderes Format",
    "text": "Selber √úberblick, anderes Format\nVerteilung und Beschreibung der Themen\n\n\nExpand for full code\ntop_gamma &lt;- stm_mdl %&gt;%\n  tidy(matrix = \"gamma\") %&gt;% \n  dplyr::group_by(topic) %&gt;%\n  dplyr::summarise(gamma = mean(gamma), .groups = \"drop\") %&gt;%\n  dplyr::arrange(desc(gamma))\n\ntop_beta &lt;- stm_mdl %&gt;%\n  tidytext::tidy(.) %&gt;% \n  dplyr::group_by(topic) %&gt;%\n  dplyr::arrange(-beta) %&gt;%\n  dplyr::top_n(10, wt = beta) %&gt;% \n  dplyr::select(topic, term) %&gt;%\n  dplyr::summarise(terms_beta = toString(term), .groups = \"drop\")\n\ntop_topics_terms &lt;- top_beta %&gt;% \n  dplyr::left_join(top_gamma, by = \"topic\") %&gt;%\n  dplyr::mutate(\n          topic = reorder(topic, gamma)\n      )\n\n# Preview\ntop_topics_terms %&gt;%\n  mutate(across(gamma, ~round(.,3))) %&gt;% \n  dplyr::arrange(-gamma) %&gt;% \n  gt() %&gt;% \n  gt::tab_options(\n    table.font.size = \"14px\") %&gt;% \n  cols_label(\n    topic = \"Topic\", \n    terms_beta = \"Top Terms (based on beta)\",\n    gamma = \"Gamma\"\n  ) %&gt;% \n  gtExtras::gt_theme_538()\n\n\n\n\n\n\n\n\nTopic\nTop Terms (based on beta)\nGamma\n\n\n\n\n16\nresearch, literature, review, paper, systematic, future, study, analysis, findings, knowledge\n0.142\n\n\n19\nstudies, interventions, review, systematic, evidence, outcomes, included, quality, intervention, health\n0.096\n\n\n13\nhealth, mental, care, review, support, family, children, social, factors, studies\n0.079\n\n\n9\nlearning, students, education, school, review, skills, educational, teachers, teaching, study\n0.072\n\n\n11\nstudy, literature, research, can, work, development, review, also, human, economic\n0.068\n\n\n15\nstudies, ci, effect, meta-analysis, depression, p, trials, anxiety, effects, interventions\n0.061\n\n\n14\nsocial, media, information, use, digital, data, technology, communication, review, research\n0.060\n\n\n17\nstudies, children, relationship, review, language, variables, research, factors, results, effects\n0.052\n\n\n6\nprevalence, studies, covid-19, suicide, among, risk, pandemic, countries, ci, vaccine\n0.050\n\n\n1\nsleep, studies, eating, cognitive, review, associated, weight, body, may, association\n0.044\n\n\n2\nhealth, studies, women, gender, review, care, social, services, cultural, access\n0.043\n\n\n18\nstudies, health, used, measures, review, tools, instruments, assessment, training, n\n0.039\n\n\n3\ntreatment, disorder, disorders, patients, ptsd, symptoms, clinical, studies, therapy, anxiety\n0.036\n\n\n7\narticles, review, adolescents, studies, literature, search, use, results, systematic, databases\n0.036\n\n\n4\npatients, articles, review, music, therapy, cancer, can, study, pain, life\n0.032\n\n\n12\nviolence, studies, use, sexual, risk, h3, ipv, substance, alcohol, review\n0.031\n\n\n5\net, al, university, review, gt, literature, lt, author, p, search\n0.030\n\n\n8\nphysical, training, studies, disability, exercise, disabilities, employment, ed, review, strength\n0.015\n\n\n20\nattachment, studies, scholar, google, science, review, social, styles, welfare, research\n0.009\n\n\n10\nde, la, y, en, los, e, ÁöÑ, el, se, que\n0.004"
  },
  {
    "objectID": "slides/ms-slides-09.html#verbindung-der-themen-untereinander",
    "href": "slides/ms-slides-09.html#verbindung-der-themen-untereinander",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Verbindung der Themen untereinander",
    "text": "Verbindung der Themen untereinander\nKorrelation der Themen\n\nstm_corr &lt;- stm::topicCorr(stm_mdl)\nplot(stm_corr)"
  },
  {
    "objectID": "slides/ms-slides-09.html#prominente-w√∂rter-einzelner-themen",
    "href": "slides/ms-slides-09.html#prominente-w√∂rter-einzelner-themen",
    "title": "Unsupervised Machine Learning (I)",
    "section": "Prominente W√∂rter einzelner Themen",
    "text": "Prominente W√∂rter einzelner Themen\n√úberblick √ºber Top-Begriffe verschiedener Themen\n\n# Fokus auf Themas 16 (h√∂chtes Gamma)\nstm::labelTopics(stm_mdl, topic=16)\n\nTopic 16 Top Words:\n     Highest Prob: research, literature, review, paper, systematic, future, study \n     FREX: tourism, sustainable, sustainability, originality, innovation, conceptual, agenda \n     Lift: paradigmatic, hrd, edlm, positivist, internationalisation, tccm, wom \n     Score: tourism, research, literature, paper, leadership, themes, sustainable \n\n# Fokus auf Thema 10 (isoliertes Thema)\nstm::labelTopics(stm_mdl, topic=10)\n\nTopic 10 Top Words:\n     Highest Prob: de, la, y, en, los, e, ÁöÑ \n     FREX: resultados, foram, que, sobre, intervenciones, riesgo, uma \n     Lift: criterios, efecto, as√≠, comparaci√≥n, cumplieron, debido, depresi√≥n \n     Score: de, la, ÁöÑ, en, los, y, que"
  },
  {
    "objectID": "slides/ms-slides-09.html#and-now-you-textanalyse-mit-r",
    "href": "slides/ms-slides-09.html#and-now-you-textanalyse-mit-r",
    "title": "Unsupervised Machine Learning (I)",
    "section": "üß™ And now ‚Ä¶ you: Textanalyse mit R",
    "text": "üß™ And now ‚Ä¶ you: Textanalyse mit R\nNext Steps: Wiederholung der Inhalte\n\nLaden Sie die auf StudOn bereitgestellten Dateien f√ºr die Sitzungen herunter\nLaden Sie die .zip-Datei in Ihren RStudio Workspace\nNavigieren Sie zu dem Ordner, in dem die Datei ps_24_binder.Rproj liegt. √ñffnen Sie diese Datei mit einem Doppelklick. Nur dadurch ist gew√§hrleistet, dass alle Dependencies korrekt funktionieren.\n√ñffnen Sie die Datei exercise-09.qmd im Ordner exercises und lesen Sie sich gr√ºndlich die Anweisungen durch.\nTipp: Sie finden alle in den Folien verwendeten Code-Bausteine in der Datei showcase.qmd (f√ºr den ‚Äúrohen‚Äù Code) oder showcase.html (mit gerenderten Ausgaben)."
  },
  {
    "objectID": "slides/ms-slides-09.html#references",
    "href": "slides/ms-slides-09.html#references",
    "title": "Unsupervised Machine Learning (I)",
    "section": "References",
    "text": "References\n\n\nBlei, D. M. (2012). Probabilistic topic models. Communications of the ACM, 55(4), 77‚Äì84. https://doi.org/10.1145/2133806.2133826\n\n\nBlei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichlet allocation. The Journal of Machine Learning Research, 3, 9931022.\n\n\nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of deep bidirectional transformers for language understanding (J. Burstein, C. Doran, & T. Solorio, Eds.; p. 41714186). Association for Computational Linguistics. https://doi.org/10.18653/v1/N19-1423\n\n\nGrimmer, J., & Stewart, B. M. (2013). Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts. Political Analysis, 21(3), 267‚Äì297. https://doi.org/10/f458q9\n\n\nLe, Q., & Mikolov, T. (2014). Distributed representations of sentences and documents (E. P. Xing & T. Jebara, Eds.; Vol. 32, p. 11881196). PMLR. https://proceedings.mlr.press/v32/le14.html\n\n\nMaier, D., Waldherr, A., Miltner, P., Wiedemann, G., Niekler, A., Keinert, A., Pfetsch, B., Heyer, G., Reber, U., H√§ussler, T., Schmid-Petri, H., & Adam, S. (2018). Applying LDA Topic Modeling in Communication Research: Toward a Valid and Reliable Methodology. Communication Methods and Measures, 12(2-3), 93‚Äì118. https://doi.org/10.1080/19312458.2018.1430754\n\n\nMikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed representations of words and phrases and their compositionality (C. J. Burges, L. Bottou, M. Welling, Z. Ghahramani, & K. Q. Weinberger, Eds.; Vol. 26). Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf\n\n\nPennington, J., Socher, R., & Manning, C. D. (2014). Glove: Global vectors for word representation. 15321543. https://doi.org/10.3115/v1/D14-1162\n\n\nRoberts, M. E., Stewart, B. M., & Airoldi, E. M. (2016). A model of text for experimentation in the social sciences. Journal of the American Statistical Association, 111(515), 988‚Äì1003. https://doi.org/10/f88tzh\n\n\nRoberts, M. E., Stewart, B. M., & Tingley, D. (2019). stm: An R Package for Structural Topic Models. Journal of Statistical Software, 91(1), 1‚Äì40. https://doi.org/10.18637/jss.v091.i02\n\n\nSilge, J., & Robinson, D. (2017). Text mining with r: A tidy approach (First edition). O‚ÄôReilly.\n\n\nZheng, A., & Casari, A. (2018). Feature engineering for machine learning: Principles and techniques for data scientists (First edition). O‚ÄôReilly."
  },
  {
    "objectID": "slides/ms-slides-11.html#seminarplan",
    "href": "slides/ms-slides-11.html#seminarplan",
    "title": "Wiederholung & Ausblick",
    "section": "Seminarplan",
    "text": "Seminarplan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSitzung\nDatum\nThema (synchron)\n√úbung (asynchron)\nDozent:in\n\n\n\n\n1\n18.04.2024\nEinf√ºhrung & √úberblick\n\nAM & CA\n\n\n\nüìö\nTeil 1: Systematic Review\n\n\n\n\n2\n25.04.2024\nEinf√ºhrung in Systematic Reviews I\nR-Einf√ºhrung\nAM\n\n\n3\n02.05.2024\nEinf√ºhrung in Systematic Reviews II\nR-Einf√ºhrung\nAM\n\n\n\n09.05.2024\nüèñÔ∏è Feiertag\nR-Einf√ºhrung\n\n\n\n4\n16.05.2024\nAutomatisierung von SRs & KI-Tools\nR-Einf√ºhrung\nAM\n\n\n\n23.05.2024\nüçª WiSo-Projekt-Woche\nR-Einf√ºhrung\n\n\n\n5\n04.06.2024\nüçï Gastvortrag: Prof.¬†Dr.¬†Emese Domahidi\nR-Einf√ºhrung\nED\n\n\n6\n06.06.2024\nAutomatisierung von SRs & KI-Tools\nR-Einf√ºhrung\nAM\n\n\n\nüíª\nTeil 2: Text as Data & Unsupervised Machine Learning\n\n\n\n\n7\n13.06.2024\nIntroduction to Text as Data\nzur Sitzung\nCA\n\n\n8\n20.06.2024\nText processing\nzur Sitzung\nCA\n\n\n9\n27.06.2024\nUnsupervised Machine Learning I\nzur Sitzung\nCA\n\n\n10\n04.07.2024\nUnsupervised Machine Learning II\nzur Sitzung\nCA & AM\n\n\n11\n11.07.2024\nRecap & Ausblick\nzur Sitzung\nCA & AM\n\n\n12\n18.07.2024\nüèÅ Semesterabschluss\nzur Sitzung\nCA & AM"
  },
  {
    "objectID": "slides/ms-slides-11.html#auswahl-des-passenden-models",
    "href": "slides/ms-slides-11.html#auswahl-des-passenden-models",
    "title": "Wiederholung & Ausblick",
    "section": "Auswahl des passenden Models",
    "text": "Auswahl des passenden Models\nüìã Exercise 1.1: Visualisierung der Themenpr√§valenz\n\n\n# Pull tpm with 40 topics\nstm_mdl_k40 &lt;- stm_search %&gt;% \n  filter(k == 40) %&gt;% \n  pull(mdl) %&gt;% \n  .[[1]]\n\n# Check\nstm_mdl_k40\n\n\nA topic model with 40 topics, 36650 documents and a 14322 word dictionary."
  },
  {
    "objectID": "slides/ms-slides-11.html#identifikation-der-top-terms-f√ºr-jedes-thema",
    "href": "slides/ms-slides-11.html#identifikation-der-top-terms-f√ºr-jedes-thema",
    "title": "Wiederholung & Ausblick",
    "section": "Identifikation der Top-Terms f√ºr jedes Thema",
    "text": "Identifikation der Top-Terms f√ºr jedes Thema\nüìã Exercise 1.2: Visualisierung der Themenpr√§valenz\n\n\n# Create tidy beta matrix\ntd_beta &lt;- tidy(\n  stm_mdl_k40, \n  method = \"frex\")\n\n# Create top terms\ntop_terms &lt;- td_beta %&gt;%\n  arrange(beta) %&gt;%\n  group_by(topic) %&gt;%\n  top_n(7, beta) %&gt;%\n  arrange(-beta) %&gt;%\n  select(topic, term) %&gt;%\n  summarise(terms = list(term)) %&gt;%\n  mutate(terms = map(\n    terms,\n    paste,\n    collapse = \", \")) %&gt;% \n  unnest(cols = c(terms))\n\n# Output\ntop_terms\n\n\n# A tibble: 40 √ó 2\n   topic terms                                                                  \n   &lt;int&gt; &lt;chr&gt;                                                                  \n 1     1 care, nursing, healthcare, nurses, professionals, patients, patient    \n 2     2 students, school, academic, education, educational, schools, literacy  \n 3     3 ÁöÑ, Á†îÁ©∂, Âíå, rs, Âú®, ‰∫Ü, ÊÄß                                           \n 4     4 elderly, #x0d, can, review, literature, google, keywords               \n 5     5 article, journal, decision, describes, aids, pressure, section         \n 6     6 prevalence, countries, among, studies, rates, population, higher       \n 7     7 depression, anxiety, psychological, stress, life, symptoms, cancer     \n 8     8 people, services, community, service, barriers, participation, support \n 9     9 factors, relationship, positive, studies, associated, behavior, negati‚Ä¶\n10    10 b, et, al, r, s, c, d                                                  \n# ‚Ñπ 30 more rows"
  },
  {
    "objectID": "slides/ms-slides-11.html#erstellung-der-pr√§valenz-tabelle-f√ºr-die-themen",
    "href": "slides/ms-slides-11.html#erstellung-der-pr√§valenz-tabelle-f√ºr-die-themen",
    "title": "Wiederholung & Ausblick",
    "section": "Erstellung der Pr√§valenz-Tabelle f√ºr die Themen",
    "text": "Erstellung der Pr√§valenz-Tabelle f√ºr die Themen\nüìã Exercise 1.3: Visualisierung der Themenpr√§valenz\n\n\nExpand for full code\n# Create tidy gamma matrix\ntd_gamma &lt;- tidy(\n  stm_mdl_k40, \n  matrix = \"gamma\", \n  document_names = names(quanteda_stm$documents)\n  )\n\n# Create prevalence\nprevalence &lt;- td_gamma %&gt;%\n  group_by(topic) %&gt;%\n  summarise(gamma = mean(gamma)) %&gt;%\n  arrange(desc(gamma)) %&gt;%\n  left_join(top_terms, by = \"topic\") %&gt;%\n  mutate(topic = paste0(\"Topic \",sprintf(\"%02d\", topic)),\n         topic = reorder(topic, gamma))\n\n# Output\nprevalence %&gt;% \n  gt() %&gt;% \n  fmt_number(\n    columns = vars(gamma), \n    decimals = 2) %&gt;% \n  gtExtras::gt_theme_538()\n\n\n\n\n\n\n\n\ntopic\ngamma\nterms\n\n\n\n\nTopic 16\n0.09\nresearch, review, literature, future, findings, systematic, studies\n\n\nTopic 19\n0.07\nstudies, included, review, quality, evidence, data, systematic\n\n\nTopic 38\n0.05\narticles, search, science, databases, review, systematic, criteria\n\n\nTopic 39\n0.04\nstudy, research, literature, analysis, used, results, method\n\n\nTopic 09\n0.04\nfactors, relationship, positive, studies, associated, behavior, negative\n\n\nTopic 25\n0.04\nlearning, education, students, teaching, teachers, skills, higher\n\n\nTopic 29\n0.04\ncultural, change, policy, political, human, identity, different\n\n\nTopic 35\n0.04\ninterventions, intervention, effectiveness, n, outcomes, effective, studies\n\n\nTopic 20\n0.03\nmanagement, tourism, development, public, paper, economic, marketing\n\n\nTopic 14\n0.03\ndigital, use, information, technology, online, communication, technologies\n\n\nTopic 34\n0.03\neffect, effects, meta-analysis, p, ptsd, ci, significant\n\n\nTopic 30\n0.03\ndisorders, disorder, suicide, eating, risk, suicidal, psychiatric\n\n\nTopic 33\n0.03\nphysical, cognitive, activity, studies, body, exercise, weight\n\n\nTopic 28\n0.03\nsleep, ci, meta-analysis, risk, studies, pooled, p\n\n\nTopic 21\n0.03\ntreatment, therapy, trials, patients, music, pain, controlled\n\n\nTopic 18\n0.03\nmeasures, assessment, used, tools, measurement, instruments, measure\n\n\nTopic 32\n0.03\nhealth, mental, stigma, problems, wellbeing, outcomes, review\n\n\nTopic 06\n0.02\nprevalence, countries, among, studies, rates, population, higher\n\n\nTopic 08\n0.02\npeople, services, community, service, barriers, participation, support\n\n\nTopic 15\n0.02\nreviews, outcomes, reporting, systematic, outcome, items, preferred\n\n\nTopic 13\n0.02\nfamily, support, resilience, experiences, caregivers, parents, parental\n\n\nTopic 07\n0.02\ndepression, anxiety, psychological, stress, life, symptoms, cancer\n\n\nTopic 17\n0.02\nchildren, adolescents, language, development, early, child, skills\n\n\nTopic 01\n0.02\ncare, nursing, healthcare, nurses, professionals, patients, patient\n\n\nTopic 23\n0.02\nsocial, media, older, adults, use, loneliness, people\n\n\nTopic 37\n0.02\ntraining, programs, work, program, professional, skills, workplace\n\n\nTopic 36\n0.02\nliterature, history, american, black, book, literary, historical\n\n\nTopic 24\n0.02\nviolence, women, abuse, sexual, ipv, child, trauma\n\n\nTopic 26\n0.02\ncovid-19, pandemic, vaccine, vaccination, health, acceptance, disease\n\n\nTopic 27\n0.02\nuse, gender, sexual, substance, alcohol, sex, men\n\n\nTopic 02\n0.02\nstudents, school, academic, education, educational, schools, literacy\n\n\nTopic 31\n0.02\nenvironment, environmental, urban, travel, physical, transport, safety\n\n\nTopic 40\n0.01\nauthors, interest, information, group, studies, case, term\n\n\nTopic 12\n0.01\ncrime, review, police, et, al, studies, may\n\n\nTopic 11\n0.01\nuniversity, author, papers, college, search, review, share\n\n\nTopic 04\n0.01\nelderly, #x0d, can, review, literature, google, keywords\n\n\nTopic 10\n0.01\nb, et, al, r, s, c, d\n\n\nTopic 05\n0.00\narticle, journal, decision, describes, aids, pressure, section\n\n\nTopic 22\n0.00\nde, la, y, en, los, el, se\n\n\nTopic 03\n0.00\nÁöÑ, Á†îÁ©∂, Âíå, rs, Âú®, ‰∫Ü, ÊÄß"
  },
  {
    "objectID": "slides/ms-slides-11.html#sch√§tzung-der-meta-effekte",
    "href": "slides/ms-slides-11.html#sch√§tzung-der-meta-effekte",
    "title": "Wiederholung & Ausblick",
    "section": "Sch√§tzung der Meta-Effekte",
    "text": "Sch√§tzung der Meta-Effekte\nüìã Exercise 2.1: Einfluss der Metadaten\n\n# Create data\neffects_k40 &lt;- estimateEffect(\n    1:40 ~ publication_year_fct + field,\n    stm_mdl_k40,\n    meta = quanteda_stm$meta)\n\n\n# Filter effect data\neffects_tidy &lt;- effects_k40 %&gt;% \n    tidy() %&gt;% \n    filter(\n        term != \"(Intercept)\",\n        term == \"fieldSocial Sciences\") %&gt;% \n        select(-term)\n\n# Check transformation\neffects_tidy %&gt;% head()\n\n# A tibble: 6 √ó 5\n  topic  estimate std.error statistic  p.value\n  &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1     1 -0.00788   0.000839     -9.40 5.70e-21\n2     2  0.0108    0.000737     14.7  1.27e-48\n3     3 -0.000487  0.000305     -1.59 1.11e- 1\n4     4  0.00495   0.000863      5.73 9.86e- 9\n5     5  0.00334   0.000262     12.7  3.84e-37\n6     6  0.00516   0.000806      6.40 1.54e-10"
  },
  {
    "objectID": "slides/ms-slides-11.html#untersuchung-der-effekte",
    "href": "slides/ms-slides-11.html#untersuchung-der-effekte",
    "title": "Wiederholung & Ausblick",
    "section": "Untersuchung der Effekte",
    "text": "Untersuchung der Effekte\nüìã Exercise 2.2: Einfluss der Metadaten\n\n\nExpand for full code\n# Explore effects (table outpu)\neffects_tidy %&gt;% \n    arrange(-estimate) %&gt;% \n    slice_head(n = 10) %&gt;%\n    gt() %&gt;% \n    tab_header(\n      title = \"Top 10 Social Science Topics\"\n    ) %&gt;% \n    fmt_number(\n      columns = -c(topic),\n      decimals = 3\n    ) %&gt;% \n    data_color(\n       columns = estimate,\n    method = \"numeric\",\n    palette = \"viridis\"\n    ) %&gt;% \n    gtExtras::gt_theme_538()\n\n\n\n\n\n\n\n\nTop 10 Social Science Topics\n\n\ntopic\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n20\n0.053\n0.001\n41.599\n0.000\n\n\n39\n0.042\n0.001\n44.842\n0.000\n\n\n29\n0.038\n0.001\n35.070\n0.000\n\n\n16\n0.037\n0.001\n28.809\n0.000\n\n\n25\n0.036\n0.001\n25.311\n0.000\n\n\n36\n0.022\n0.001\n22.780\n0.000\n\n\n14\n0.021\n0.001\n17.601\n0.000\n\n\n23\n0.013\n0.001\n19.832\n0.000\n\n\n8\n0.012\n0.001\n14.805\n0.000\n\n\n37\n0.012\n0.001\n15.010\n0.000"
  },
  {
    "objectID": "slides/ms-slides-11.html#benennung-des-themas-k-20",
    "href": "slides/ms-slides-11.html#benennung-des-themas-k-20",
    "title": "Wiederholung & Ausblick",
    "section": "Benennung des Themas k = 20",
    "text": "Benennung des Themas k = 20\nüìã Exercise 3.1: Einzelthema im Fokus\n\n# Create topic label\nstm_mdl_k40 %&gt;% labelTopics(topic = 20)\n\nTopic 20 Top Words:\n     Highest Prob: management, tourism, development, public, paper, economic, marketing \n     FREX: tourism, marketing, consumer, halal, sustainable, disaster, economy \n     Lift: hotel, mega-events, tourism, post-disaster, smes, tourist, b2b \n     Score: tourism, marketing, halal, business, governance, sustainable, disaster \n\n\n\n‚úçÔ∏è Wie w√ºrdet Ihr das Thema benennen und warum?"
  },
  {
    "objectID": "slides/ms-slides-11.html#verteilungsparameter-von-thema-20",
    "href": "slides/ms-slides-11.html#verteilungsparameter-von-thema-20",
    "title": "Wiederholung & Ausblick",
    "section": "Verteilungsparameter von Thema 20",
    "text": "Verteilungsparameter von Thema 20\nüìã Exercise 3.3: Einzelthema im Fokus\n\n# Create distribution parameters\ngamma_export %&gt;% \n  filter(topic == 20) %&gt;%\n  select(gamma, relevance_score, cited_by_count) %&gt;% \n  datawizard::describe_distribution()\n\nVariable        |  Mean |    SD |   IQR |          Range | Skewness | Kurtosis |    n | n_Missing\n-------------------------------------------------------------------------------------------------\ngamma           |  0.37 |  0.13 |  0.17 |   [0.12, 0.87] |     0.74 |     0.43 | 1477 |         0\nrelevance_score | 32.55 | 40.35 | 36.74 | [2.01, 402.59] |     3.07 |    14.76 | 1477 |         0\ncited_by_count  | 13.54 | 50.51 |  7.00 | [0.00, 948.00] |    10.41 |   143.55 | 1477 |         0\n\n\n\nFragen:\n\nAnzahl der Abstracts von Thema 20?\nDurchschnittlicher Relevace Score?\nDurchschnittliche Zitationen?\nAnzahl der Zitationen des am meisten zitierten Dokuments?"
  },
  {
    "objectID": "slides/ms-slides-11.html#top-dokumente-des-themas",
    "href": "slides/ms-slides-11.html#top-dokumente-des-themas",
    "title": "Wiederholung & Ausblick",
    "section": "Top-Dokumente des Themas",
    "text": "Top-Dokumente des Themas\nüìã Exercise 3.4: Einzelthema im Fokus\n\n\nExpand for full code\n# Identify top documents for topic 20\ntop_docs_k20 &lt;- gamma_export %&gt;% \n  filter(stm_topic == \"Topic 20\") %&gt;%\n  arrange(-gamma) %&gt;%\n  select(title, so, gamma, type, ab) %&gt;%\n  slice_head(n = 5) \n\n# Creae output\ntop_docs_k20 %&gt;% \n  gt() %&gt;% \n  fmt_number(\n    columns = c(gamma), \n    decimals = 2) %&gt;% \n  gtExtras::gt_theme_538()\n\n\n\n\n\n\n\n\ntitle\nso\ngamma\ntype\nab\n\n\n\n\nLiterature Review of Overseas Tourism Destination Brand Research\nJournal of Chongqing Technology and Business University\n0.87\narticle\nApplying brand theory to the study on tourism destination has been always hot issues for overseas scholars since 1990s.Tourism destination branding management is a significant marketing tool which can bring about effective identification internally,achieving differentiation with external competitors.Systematically reviewing and analyzing recent overseas tourism destination brand literatures,this paper makes conclusion and evaluation of the tourism destination brand construction,branding,brand stakeholders,brand operation,branding performance evaluation to provide reference for domestic tourism destination brand research and management.\n\n\nNatural Disasters in Colombia and Their Impact on the Food Security of the Affected Population. a Quick Review of the Literature.\nSocial Science Research Network\n0.85\narticle\nNatural disasters in Colombia significantly impact multiple domains of the affected population, including food security. Those events can cause food production and distribution interruptions, leading to scarcity and increased prices. Additionally, they can damage infrastructure and limit the communities' ability to access food. Food assistance during disasters is crucial to ensuring food security. The former is crucial for human survival and development, and entities responsible for risk management and food assistance play a fundamental role in protecting populations affected by natural disasters. Entities responsible for risk management in Colombia, such as the National Unit for Disaster Risk Management (UNGRD) and the Departmental and Municipal Risk Management Councils, coordinate efforts to provide food assistance and other basic needs. The Colombia Food Bank also plays an essential role in responding to food emergencies as a first responder. In this article, we investigate some of the leading natural disasters that Colombia has suffered in recent years and how these events have affected different communities. Likewise, we explore how the response has been made from the risk management framework, highlighting food assistance.\n\n\nGovernment Responsibility as the Main Stakeholder in Tourism Development With Collaboration Approach: Literature Review on Heritage Tourism\nNA\n0.84\narticle\ncurrently, the economic growth of some countries is a contribution by the vast development of the tourism sector, and one of the potential destinations is heritage motivation.More than fifty-three of previous study founds five elements that associated with Heritage Tourism Development, which are four elements influencing directly and one element is impact after development as an outcome.The study is focusing on stakeholders responsibility which led by government to do the development of heritage tourism.Barriers of policies and low attention to strategic plants and policies are an influencer to the obstacles because of less attention from the leader.Collaboration approach helped government to control the system in heritage tourism process.\n\n\nBusiness Strategy in Management Perspective: A Literature Review\nIndonesian Journal of Economic & Management Sciences\n0.83\narticle\nBusiness development in the world has entered the era of free markets and broad competition, not only in small areas but also in large areas. Efforts made by a company to win the market are by providing competitive advantages, analyzing competitors, and implementing effective and efficient marketing strategies\n\n\nA Literature Review on Structural Reform of Agricultural Supply Side\nNA\n0.82\narticle\nThe structural reform of the agricultural supply side is the major deployment of the \"No. 1 document\" on agriculture, not only for the direction of agricultural industry development, but also for the agricultural industry structure optimization adjustment to play a needle \"tonic\", also engaged in agricultural economic research experts and scholars Correct future and long-term research direction to play a \"heading\" role.This paper summarizes the policy of \"structural reform of agricultural supply side\", which is related to the optimization and upgrading of agricultural industry structure, the cultivation of agricultural enterprises, the integration of agriculture, the development strategy of agricultural brand, the innovation of agricultural technology, \"Rural land management system reform\", \"agricultural development policy\" and other research results."
  },
  {
    "objectID": "slides/ms-slides-11.html#get-up-and-running-with-llms",
    "href": "slides/ms-slides-11.html#get-up-and-running-with-llms",
    "title": "Wiederholung & Ausblick",
    "section": "Get up and running with LLMs",
    "text": "Get up and running with LLMs\nRun LLMs locally with Ollama\n\n\n\n\n\nopen-source project that serves as a powerful and user-friendly platform for running LLMs on your local machine.\nbridge between the complexities of LLM technology and the desire for an accessible and customizable AI experience.\nprovides access to a diverse and continuously expanding library of pre-trained LLM models (e.g.Llama 3, Phi 3, Mistral, Gemma 2)"
  },
  {
    "objectID": "slides/ms-slides-11.html#r-wrapper-for-ollama-api",
    "href": "slides/ms-slides-11.html#r-wrapper-for-ollama-api",
    "title": "Wiederholung & Ausblick",
    "section": "R-Wrapper for Ollama API",
    "text": "R-Wrapper for Ollama API\nRun local LLMs in R with rollama (Gruber & Weber, 2024)\n\n\n\n\n\nthe goal of rollama is to wrap the Ollama API, which allows you to run different LLMs locally and create an experience similar to ChatGPT/OpenAI‚Äôs API."
  },
  {
    "objectID": "slides/ms-slides-11.html#chat-with-a-llm-via-r",
    "href": "slides/ms-slides-11.html#chat-with-a-llm-via-r",
    "title": "Wiederholung & Ausblick",
    "section": "Chat with a LLM via R",
    "text": "Chat with a LLM via R\nDemonstration on how to use a local LLM with rollama in R\n\n\n\n\ndemo_1_llama3 &lt;- rollama::query(\n    \"Why is the sky blue?\",\n    model = \"llama3\"\n)\n\nglue::glue(demo_1_llama3$message$content)\n\nThe sky appears blue because of a phenomenon called scattering, which occurs when sunlight interacts with tiny molecules of gases in the Earth's atmosphere.\n\nHere's what happens:\n\n1. Sunlight enters the Earth's atmosphere and encounters tiny molecules of gases such as nitrogen (N2) and oxygen (O2). These molecules are much smaller than the wavelength of light.\n2. When sunlight hits these molecules, it scatters in all directions. This is known as Rayleigh scattering, named after the British physicist Lord Rayleigh who first described the phenomenon in the late 19th century.\n3. The shorter, blue wavelengths of light are scattered more than the longer, red wavelengths. This is because the smaller molecules are better at scattering the shorter wavelengths.\n4. As a result, our eyes see the scattered blue light as the dominant color, giving the sky its blue appearance.\n\nThis scattering effect is more pronounced during the daytime when the sun is overhead and the amount of sunlight entering the atmosphere is greater. The sky can appear more intense blue near the horizon because the light has to travel longer distances through the atmosphere, scattering off more molecules and increasing the amount of blue light that reaches our eyes.\n\nIt's worth noting that the color of the sky can be affected by atmospheric conditions such as pollution, dust, and water vapor, which can scatter light in different ways. Additionally, during sunrise and sunset, the sky can take on hues of red, orange, and pink due to the scattering of longer wavelengths of light.\n\nSo, to summarize: the sky appears blue because of the scattering of sunlight by tiny molecules in the Earth's atmosphere, with shorter wavelengths (like blue) being scattered more than longer wavelengths (like red).\n\n\n\n\ndemo_1_mistral &lt;- rollama::query(\n    \"Why is the sky blue?\",\n    model = \"mistral\"\n)\n\nglue::glue(demo_1_mistral$message$content)\n\n The sky appears blue due to a process called Rayleigh scattering. As sunlight reaches Earth, it's made up of different wavelengths or colors, but blue and violet light are scattered more easily because they have shorter wavelengths. However, our eyes are more sensitive to blue light than violet light, which is why the sky generally looks blue rather than violet. At sunrise and sunset, when sunlight has to pass through a thicker atmosphere, red and orange light, which are not scattered as easily, are able to reach us, making the sky appear reddish or orange."
  },
  {
    "objectID": "slides/ms-slides-11.html#choose-your-model-wisely",
    "href": "slides/ms-slides-11.html#choose-your-model-wisely",
    "title": "Wiederholung & Ausblick",
    "section": "Choose your model wisely!",
    "text": "Choose your model wisely!\nCompare outputs of different versions of Llama model\n\n\n\n\ndemo_2_llama2 &lt;- rollama::query(\n    \"What is the longest five letter word in english?\",\n    model = \"llama2\"\n)\n\nglue::glue(demo_2_llama2$message$content)\n\nThe longest five-letter word in English is \"pneumonoultramicroscopicsilicovolcanoconiosis,\" which refers to a type of lung disease caused by inhaling very fine silica particles. It has 14 letters.\n\n\n\n\ndemo_2_llama3 &lt;- rollama::query(\n     \"What is the longest five letter word in english?\",\n    model = \"llama3\"\n)\n\nglue::glue(demo_2_llama3$message$content)\n\nAccording to various sources, including dictionaries and linguistic authorities, the longest five-letter word in English is \"horse\"."
  },
  {
    "objectID": "slides/ms-slides-11.html#choose-your-model-wisely-1",
    "href": "slides/ms-slides-11.html#choose-your-model-wisely-1",
    "title": "Wiederholung & Ausblick",
    "section": "Choose your model wisely!",
    "text": "Choose your model wisely!\nModels differ in their suffistication and performance\n\n\n\n\ndemo_3_llama3 &lt;- rollama::query(\n    \"Is 9677 a prime number?\",\n    model = \"llama3\"\n)\n\nglue::glue(demo_3_llama3$message$content)\n\nThe answer is: no, 9677 is not a prime number.\n\nA prime number is a positive integer that has exactly two distinct positive divisors: 1 and itself. In other words, the only factors of a prime number are 1 and the number itself.\n\n9677 can be divided by 83 and 117, so it has more than two distinct positive divisors. Therefore, it is not a prime number.\n\n\n\n\ndemo_3_mistral &lt;- rollama::query(\n    \"Is 9677 a prime number?\",\n    model = \"mistral\"\n)\n\nglue::glue(demo_3_mistral$message$content)\n\n No, 9677 is not a prime number. A prime number is a natural number greater than 1 that has no positive divisors other than 1 and itself. The factors of 9677 are 1, 7, 1383, and 9677, so it does not meet the criteria to be a prime number."
  },
  {
    "objectID": "slides/ms-slides-11.html#eigene-funktion-zur-themennamensgebung",
    "href": "slides/ms-slides-11.html#eigene-funktion-zur-themennamensgebung",
    "title": "Wiederholung & Ausblick",
    "section": "Eigene Funktion zur Themennamensgebung",
    "text": "Eigene Funktion zur Themennamensgebung\nEinsatz von LLM im Kontext von Topic Modeling\n\n\ncreate_ollama_labels &lt;- function(\n  data, topic = \"topic\", terms = \"terms\", docs, \n  ollama_model = \"llama3\", \n  output_seed = 42, output_temperature = 0.8, output_top_k = 40, output_top_p = 0.9) {\n  \n  # Initialize a list to store labels for each document column\n  labels &lt;- setNames(vector(\"list\", length(docs)), docs)\n  \n  # Loop over each row in the data\n  for (i in seq_along(data[[topic]])) {\n    \n    # Loop over each document column\n    for (doc in docs) {\n      # Define parameters\n      docs_text &lt;- data[[doc]][[i]]\n      terms_text &lt;- data[[terms]][[i]]\n\n      # Create query\n      q &lt;- tibble::tribble(\n        ~role, ~content,\n        \"user\", \n        paste(\"text: I have a topic that contains the following documents: \\n\",\n        docs_text,\n        \"\\n The topic is described by the following keywords:\",\n        terms_text,\n        \"\\n Based on the above information, can you please give one short label (no longer than 5 words) for the topic?\")\n      )\n      \n      # Generate output\n      output &lt;- query(\n        q,\n        model = ollama_model,\n        model_params = list(\n            seed = output_seed, \n            temperature = output_temperature,\n            top_k = output_top_k, \n            top_p = output_top_p \n      ))\n      \n      # Initialize the label list for the current doc if it does not exist\n      if (is.null(labels[[doc]])) {\n        labels[[doc]] &lt;- vector(\"character\", nrow(data))\n      }\n      \n      # Store answer\n      labels[[doc]][i] &lt;- pluck(output, \"message\", \"content\")\n    }\n  }\n  \n  # Combine the labels with the original data\n  for (doc in docs) {\n    data[[paste0(\"label_\", doc)]] &lt;- labels[[doc]]\n  }\n  \n  return(data)\n}"
  },
  {
    "objectID": "slides/ms-slides-11.html#erstellung-der-themennamen",
    "href": "slides/ms-slides-11.html#erstellung-der-themennamen",
    "title": "Wiederholung & Ausblick",
    "section": "Erstellung der Themennamen",
    "text": "Erstellung der Themennamen\nAnwendung der Funktion auf die Daten\n\ntopiclabels_llama3 &lt;- create_ollama_labels(\n    data = tpm_label_base,\n    ollama_model = \"llama3\",\n    docs = c(\"docs_all\")) \n\ntopiclabels_mistral &lt;- create_ollama_labels(\n    data = tpm_label_base,\n    ollama_model = \"mistral\",\n    docs = c(\"docs_all\"))\n\n\ntopiclabels &lt;- topiclabels_llama3 %&gt;% \n    left_join(topiclabels_mistral %&gt;% select(topic, label_docs_all), by = join_by(\"topic\")) %&gt;% \n    janitor::clean_names() %&gt;% \n    select(topic, starts_with(\"label_docs_all_\")) %&gt;% \n    rename(\n        label_llama3 = label_docs_all_x, \n        label_mistral = label_docs_all_y\n    )"
  },
  {
    "objectID": "slides/ms-slides-11.html#validieren-validieren-validieren",
    "href": "slides/ms-slides-11.html#validieren-validieren-validieren",
    "title": "Wiederholung & Ausblick",
    "section": "Validieren, validieren, validieren!",
    "text": "Validieren, validieren, validieren!\n√úberpr√ºfung und Vergleich der mit LLMs generierten Themennamen\n\ntopiclabels %&gt;% \n    gt() %&gt;% \n    gtExtras::gt_theme_538()\n\n\n\n\n\n\n\ntopic\nlabel_llama3\nlabel_mistral\n\n\n\n\n1\nNursing Professional Practice Issues\nNursing Safety & Incivility\n\n\n2\n\"Education Policy and School Performance\"\n\"School Decision-making Impact\"\n\n\n3\n\"Factors of Hope in Cancer\"\n\"Empirical Studies on Hope in Cancer Patients\"\n\n\n4\n\"Psychological Interventions in Tuberculosis\"\n\"Psychological Interventions for TB Patients\" For the cancer-related topic: \"Acupressure for Chemo Nausea Reduction\"\n\n\n5\n\"Corrected Research Article References\"\nCorrected Research Articles with Errors\n\n\n6\n\"Global Health Prevalence Studies\"\n\"Global Suicidality Prevalence\" For Brazil data: \"Brazil HIV Prevalence Trends\"\n\n\n7\nHere are a few options: * \"Mental Health and Cancer\" * \"Stress and Anxiety Management\" * \"Cancer Patient Mental Wellbeing\" * \"Quality of Life Improvement\" Based on the text, I would suggest: **\"Cancer-Related Stress Management\"**\nMindfulness & Mental Health in Cancer Patients\n\n\n8\n\"Disability and Inclusive Service Provision\"\n\"Barriers to Transgender Participation in Disability Services\"\n\n\n9\n\"Emotions and Personality Traits in Sport\"\n\"Emotion Regulation & Dominance\"\n\n\n10\n\"Karate Attack Velocity Study\"\n\"Karate Attack Velocity\"\n\n\n11\nBased on the provided text, I would suggest a short label for the topic: \"Higher Education Peer Education\" Let me know if this fits your requirements or if you need further assistance!\nPeer Education Reviews in Higher Ed\n\n\n12\nCriminal Justice and Community Policing\n\"Community Policing & Gang Crime\"\n\n\n13\nFamily Caregiver Experiences\n\"Dyadic Caregiving Impact\"\n\n\n14\n\"Digital Technologies and Privacy Concerns\"\nDeep Fake & Info Detection Technologies\n\n\n15\n\"Core Outcome Set Development\"\n\"Core Outcomes in Women's Health Reviews\"\n\n\n16\n\"Research Frameworks and Methodologies\"\n\"Relational Negotiation Research\"\n\n\n17\nChild Language Development and Disorders\n\"Pragmatic Language Development in ASD and DLD\"\n\n\n18\n\"Instrument Validity and Reliability\"\n\"Psychometric Validation of Thriving Scales\"\n\n\n19\nSystematic Review Protocols\n\"Yoga Barriers & Facilitators Review\" for the first text, \"Ethnic Mortality Systematic Review\" for the second, and \"RI & Cognition Systematic Review\" for the third.\n\n\n20\nHere are a few options: * Tourism and Destination Management * Marketing in Tourism Industry * Sustainable Tourism Development * Disaster Risk Management Based on the keywords provided, I would suggest the following short label: **Tourism Destination Management** This label captures the essence of the topics discussed, which include tourism destination branding, marketing, consumer behavior, sustainable development, disaster risk management, and more.\n\"Tourism Development Strategies\"\n\n\n21\n\"Alternative Therapies for Mental Health\"\n\"Alternative Therapies for Insomnia and Tics\"\n\n\n22\nMental Health Interventions\nSystematic Review of ED Self-Help Interventions\n\n\n23\nBased on the abstracts and keywords provided, I would suggest the following short label: \"Social Media and Aging Adults\" This label captures the main themes of the topic, including social media, aging, and adults, which are present in all three abstracts.\nSocial Media and Loneliness in Older Adults\n\n\n24\n\"Violence Against Women and Children\"\nReproductive Coercion & Sexual Exploitation in Violence Contexts\n\n\n25\n\"Teaching and Learning Strategies\"\nFlipped Learning & Active Pedagogy\n\n\n26\n\"Covid-19 Vaccine Acceptance and Hesitancy\"\n\"COVID-19 Vaccine Hesitancy\"\n\n\n27\n\"Substance Abuse and Risk Behaviors\"\n\"Transgender HIV Risk Factors\"\n\n\n28\nSleep and Disease Associations\n\"Sleep Duration & Dementia/Heart Disease Risk\"\n\n\n29\n\"Global Political and Social Dynamics\"\n\"Transnational Migration & Remittances\"\n\n\n30\n\"Suicide and Mental Health Disorders\"\n\"Nightmares & Psychiatric Disorders\" and \"Eating Disorders & Neurodevelopmental Conditions\" for the respective topics. For the third topic, a possible label could be \"APSD/SSD & DDC Polymorphism\".\n\n\n31\n\"Driver Vigilance and Safety Research\"\nDancing Injuries and Travel Trauma\n\n\n32\nCultural and Social Determinants\n\"Immigrant Mental Health Barriers\"\n\n\n33\n\"Cognitive Neuroscience of Obesity\"\nCognitive Control and Weight Regulation\n\n\n34\n\"Mental Health Treatment Efficacy\"\n\"Distance-delivered PTSD treatment efficacy\"\n\n\n35\nBehavioral Interventions Effectiveness\n\"Interventions for Dementia Physical Activity\"\n\n\n36\n\"Early Medieval Chinese Studies\"\n\"Early Medieval Chinese Literature & Philosophy\"\n\n\n37\n\"Employability and Career Development\"\n\"Internship Duration Impact\" \"Career Women Challenges\" \"Mentoring in Academia\" \"Women Faculty Inequities\"\n\n\n38\nSystematic Literature Reviews in Health\n\"Socioeconomic Status Indicators in Iran Health Studies\" \"Radiology Gamification Impact Analysis\" \"Chronic Exercise Effect on Anxiety Symptoms\"\n\n\n39\n\"Research in Islamic Banking\"\n\"Islamic Banking Performance Analysis\"\n\n\n40\n\"Gender Bias in Science Funding\"\n\"Gender Bias in Astronomy Proposal Reviews\""
  },
  {
    "objectID": "slides/ms-slides-11.html#tools-for-bibliometrics-scientometrics",
    "href": "slides/ms-slides-11.html#tools-for-bibliometrics-scientometrics",
    "title": "Wiederholung & Ausblick",
    "section": "Tools for bibliometrics & scientometrics",
    "text": "Tools for bibliometrics & scientometrics\nbibliometrix: R package for scinece mapping workflow (Aria & Cuccurullo, 2017)\n\n\n\n\n\nbibliometrix is an open-source tool for quantitative research in scientometrics and bibliometrics that includes all the main bibliometric methods of analysis.\nWith biblioshiny, a shiny web app, bibliometrix has become very easy to use even for those who have no coding skills."
  },
  {
    "objectID": "slides/ms-slides-11.html#tools-for-bibliometrics-scientometrics-1",
    "href": "slides/ms-slides-11.html#tools-for-bibliometrics-scientometrics-1",
    "title": "Wiederholung & Ausblick",
    "section": "Tools for bibliometrics & scientometrics",
    "text": "Tools for bibliometrics & scientometrics\nbibliometrix: R package for scinece mapping workflow (Aria & Cuccurullo, 2017)"
  },
  {
    "objectID": "slides/ms-slides-11.html#openalex-openalexr-bibliometrix",
    "href": "slides/ms-slides-11.html#openalex-openalexr-bibliometrix",
    "title": "Wiederholung & Ausblick",
    "section": "OpenAlex & openalexR ü§ù bibliometrix",
    "text": "OpenAlex & openalexR ü§ù bibliometrix\nPipeline f√ºr die Integration von OpenAlex-Daten in bibliometrix\n\nbibliometrix_data &lt;- review_subsample %&gt;% oa2bibliometrix()\nbibliometrix_df &lt;- biblioAnalysis(bibliometrix_data, sep = \";\")\n\n\n\n\n\nbibliometrix_data %&gt;% glimpse\n\nRows: 36,680\nColumns: 60\n$ AU                          &lt;chr&gt; \"CHRISTIAN WINTHER TOPP;S√òREN DINESEN √òSTE‚Ä¶\n$ RP                          &lt;chr&gt; \"PSYCHIATRIC RESEARCH UNIT, PSYCHIATRIC CE‚Ä¶\n$ C1                          &lt;chr&gt; \"PSYCHIATRIC RESEARCH UNIT, PSYCHIATRIC CE‚Ä¶\n$ AU_UN                       &lt;chr&gt; \"REGION ZEALAND;AARHUS UNIVERSITY HOSPITAL‚Ä¶\n$ AU_CO                       &lt;chr&gt; \"DENMARK;DENMARK;DENMARK;DENMARK\", \"USA;US‚Ä¶\n$ ID                          &lt;chr&gt; \"PSYCINFO;SYSTEMATIC REVIEW;PSYCHOLOGY;CLI‚Ä¶\n$ id_url                      &lt;chr&gt; \"https://openalex.org/W4293003987\", \"https‚Ä¶\n$ title                       &lt;chr&gt; \"The WHO-5 Well-Being Index: A Systematic ‚Ä¶\n$ author                      &lt;list&gt; [&lt;data.frame[4 x 12]&gt;], [&lt;data.frame[2 x ‚Ä¶\n$ publication_date            &lt;chr&gt; \"2015-01-01\", \"2017-08-28\", \"2014-01-01\", ‚Ä¶\n$ relevance_score             &lt;dbl&gt; 938.7603, 752.3500, 591.2553, 576.1210, 56‚Ä¶\n$ so_id                       &lt;chr&gt; \"https://openalex.org/S184803288\", \"https:‚Ä¶\n$ host_organization           &lt;chr&gt; \"Karger Publishers\", \"SAGE Publishing\", NA‚Ä¶\n$ issn_l                      &lt;chr&gt; \"0033-3190\", \"0739-456X\", NA, \"2214-7829\",‚Ä¶\n$ url                         &lt;chr&gt; \"https://doi.org/10.1159/000376585\", \"http‚Ä¶\n$ pdf_url                     &lt;chr&gt; \"https://www.karger.com/Article/Pdf/376585‚Ä¶\n$ license                     &lt;chr&gt; \"cc-by-nc\", NA, NA, \"cc-by\", NA, NA, \"cc-b‚Ä¶\n$ version                     &lt;chr&gt; \"publishedVersion\", NA, \"publishedVersion\"‚Ä¶\n$ first_page                  &lt;chr&gt; \"167\", \"93\", NA, \"89\", \"55\", \"2150\", \"e356‚Ä¶\n$ last_page                   &lt;chr&gt; \"176\", \"112\", NA, \"106\", \"64\", \"2159\", \"e3‚Ä¶\n$ volume                      &lt;chr&gt; \"84\", \"39\", NA, \"6\", \"277\", \"32\", \"2\", \"24‚Ä¶\n$ issue                       &lt;chr&gt; \"3\", \"1\", NA, NA, NA, \"19\", \"8\", NA, \"9\", ‚Ä¶\n$ is_oa                       &lt;lgl&gt; TRUE, FALSE, TRUE, TRUE, TRUE, FALSE, TRUE‚Ä¶\n$ is_oa_anywhere              &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE,‚Ä¶\n$ oa_status                   &lt;chr&gt; \"hybrid\", \"green\", \"bronze\", \"gold\", \"bron‚Ä¶\n$ oa_url                      &lt;chr&gt; \"https://www.karger.com/Article/Pdf/376585‚Ä¶\n$ any_repository_has_fulltext &lt;lgl&gt; FALSE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE‚Ä¶\n$ language                    &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", ‚Ä¶\n$ grants                      &lt;list&gt; NA, NA, NA, &lt;\"https://openalex.org/F43203‚Ä¶\n$ counts_by_year              &lt;list&gt; [&lt;data.frame[11 x 2]&gt;], [&lt;data.frame[7 x ‚Ä¶\n$ cited_by_api_url            &lt;chr&gt; \"https://api.openalex.org/works?filter=cit‚Ä¶\n$ ids                         &lt;list&gt; &lt;\"https://openalex.org/W4293003987\", \"htt‚Ä¶\n$ doi                         &lt;chr&gt; \"https://doi.org/10.1159/000376585\", \"http‚Ä¶\n$ referenced_works            &lt;list&gt; &lt;\"https://openalex.org/W1492518593\", \"htt‚Ä¶\n$ related_works               &lt;list&gt; &lt;\"https://openalex.org/W3020194755\", \"htt‚Ä¶\n$ is_paratext                 &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, ‚Ä¶\n$ is_retracted                &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, ‚Ä¶\n$ concepts                    &lt;list&gt; [&lt;data.frame[7 x 5]&gt;], [&lt;data.frame[18 x ‚Ä¶\n$ topics_i                    &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ‚Ä¶\n$ topics_score                &lt;dbl&gt; 0.9926, 0.9050, 0.9995, 0.9987, 0.9999, 1.‚Ä¶\n$ topics_name                 &lt;chr&gt; \"field\", \"field\", \"field\", \"field\", \"field‚Ä¶\n$ topics_id                   &lt;chr&gt; \"https://openalex.org/fields/32\", \"https:/‚Ä¶\n$ topics_display_name         &lt;chr&gt; \"Psychology\", \"Social Sciences\", \"Psycholo‚Ä¶\n$ publication_year_fct        &lt;fct&gt; 2015, 2017, 2014, 2016, 2020, 2014, 2017, ‚Ä¶\n$ type_fct                    &lt;fct&gt; article, article, article, article, articl‚Ä¶\n$ field                       &lt;fct&gt; Psychology, Social Sciences, Psychology, P‚Ä¶\n$ id_oa                       &lt;chr&gt; \"W4293003987\", \"W2750168540\", \"W1998933811‚Ä¶\n$ CR                          &lt;chr&gt; \"W1492518593;W1544712724;W1802168796;W1821‚Ä¶\n$ TI                          &lt;chr&gt; \"THE WHO-5 WELL-BEING INDEX: A SYSTEMATIC ‚Ä¶\n$ AB                          &lt;chr&gt; \"THE 5-ITEM WORLD HEALTH ORGANIZATION WELL‚Ä¶\n$ SO                          &lt;chr&gt; \"PSYCHOTHERAPY AND PSYCHOSOMATICS\", \"JOURN‚Ä¶\n$ DT                          &lt;chr&gt; \"ARTICLE\", \"ARTICLE\", \"ARTICLE\", \"ARTICLE\"‚Ä¶\n$ DB                          &lt;chr&gt; \"OPENALEX\", \"OPENALEX\", \"OPENALEX\", \"OPENA‚Ä¶\n$ JI                          &lt;chr&gt; \"S184803288\", \"S200299299\", NA, \"S42102325‚Ä¶\n$ J9                          &lt;chr&gt; \"S184803288\", \"S200299299\", NA, \"S42102325‚Ä¶\n$ PY                          &lt;int&gt; 2015, 2017, 2014, 2016, 2020, 2014, 2017, ‚Ä¶\n$ TC                          &lt;int&gt; 2657, 1375, 2568, 803, 3664, 1553, 2895, 9‚Ä¶\n$ DI                          &lt;chr&gt; \"10.1159/000376585\", \"10.1177/0739456x1772‚Ä¶\n$ SR_FULL                     &lt;chr&gt; \"CHRISTIAN WINTHER TOPP, 2015, PSYCHOTHERA‚Ä¶\n$ SR                          &lt;chr&gt; \"CHRISTIAN WINTHER TOPP, 2015, PSYCHOTHERA‚Ä¶\n\n\n\n\nbibliometrix_df %&gt;% glimpse\n\nList of 26\n $ Articles            : int 36680\n $ Authors             : 'table' int [1:106265(1d)] 71 51 38 36 33 28 28 28 28 28 ...\n  ..- attr(*, \"dimnames\")=List of 1\n  .. ..$ AU: chr [1:106265] \"PIM CUIJPERS\" \"MARK D. GRIFFITHS\" \"G. J. MEL√âNDEZ‚ÄêTORRES\" \"SEENA FAZEL\" ...\n $ AuthorsFrac         :'data.frame':   106265 obs. of  2 variables:\n  ..$ Author   : chr [1:106265] \"MARK D. GRIFFITHS\" \"PIM CUIJPERS\" \"MELOR MD YUNUS\" \"DAVID P. FARRINGTON\" ...\n  ..$ Frequency: num [1:106265] 13.47 9.21 8.83 8.67 8.58 ...\n $ FirstAuthors        : chr [1:36680] \"CHRISTIAN WINTHER TOPP\" \"XIAO YU\" \"JUHO HAMARI\" \"DANIEL JOHNSON\" ...\n $ nAUperPaper         : int [1:36680] 4 2 3 6 11 5 8 4 9 2 ...\n $ Appearances         : int 146925\n $ nAuthors            : int 106265\n $ AuMultiAuthoredArt  : int 102093\n $ AuSingleAuthoredArt : int 4172\n $ MostCitedPapers     :'data.frame':   36680 obs. of  5 variables:\n  ..$ Paper         : chr [1:36680] \"JIAQI XIONG, 2020, JOURNAL OF AFFECTIVE DISORDERS\" \"KAREN HUGHES, 2017, \\u0098THE \\u009cLANCET. PUBLIC HEALTH\" \"SOFIA PAPPA, 2020, BRAIN, BEHAVIOR, AND IMMUNITY\" \"CHRISTIAN WINTHER TOPP, 2015, PSYCHOTHERAPY AND PSYCHOSOMATICS\" ...\n  ..$ DOI           : chr [1:36680] \"10.1016/j.jad.2020.08.001\" \"10.1016/s2468-2667(17)30118-4\" \"10.1016/j.bbi.2020.05.026\" \"10.1159/000376585\" ...\n  ..$ TC            : num [1:36680] 3664 2895 2675 2657 2568 ...\n  ..$ TCperYear     : num [1:36680] 733 362 535 266 233 ...\n  ..$ NTC           : num [1:36680] 132 56 96.4 47.1 37.5 ...\n $ Years               : num [1:36680] 2015 2017 2014 2016 2020 ...\n $ FirstAffiliation    : chr [1:36680] \"REGION ZEALAND\" \"TEXAS A&M UNIVERSITY\" \"UNIVERSITY UCINF\" \"QUEENSLAND UNIVERSITY OF TECHNOLOGY\" ...\n $ Affiliations        : 'table' int [1:9702(1d)] 20528 1396 1237 796 732 653 636 538 533 529 ...\n  ..- attr(*, \"dimnames\")=List of 1\n  .. ..$ AFF: chr [1:9702] \"NA\" \"KING'S COLLEGE LONDON\" \"UNIVERSITY COLLEGE LONDON\" \"UNIVERSITY OF SYDNEY\" ...\n $ Aff_frac            :'data.frame':   9702 obs. of  2 variables:\n  ..$ Affiliation: chr [1:9702] \"NA\" \"KING'S COLLEGE LONDON\" \"UNIVERSITY COLLEGE LONDON\" \"UNIVERSITY OF SYDNEY\" ...\n  ..$ Frequency  : num [1:9702] 6174 279 245 169 160 ...\n $ CO                  : chr [1:36680] NA \"USA\" NA \"AUSTRALIA\" ...\n $ Countries           : 'table' int [1:136(1d)] 3311 2432 1611 1308 1017 962 939 894 854 819 ...\n  ..- attr(*, \"dimnames\")=List of 1\n  .. ..$ Tab: chr [1:136] \"USA\" \"AUSTRALIA\" \"CHINA\" \"CANADA\" ...\n $ CountryCollaboration:'data.frame':   136 obs. of  3 variables:\n  ..$ Country: chr [1:136] \"USA\" \"AUSTRALIA\" \"CHINA\" \"CANADA\" ...\n  ..$ SCP    : num [1:136] 2681 1828 1088 937 710 ...\n  ..$ MCP    : num [1:136] 630 604 523 371 307 196 278 275 309 205 ...\n $ TotalCitation       : num [1:36680] 2657 1375 2568 803 3664 ...\n $ TCperYear           : num [1:36680] 265.7 171.9 233.5 89.2 732.8 ...\n $ Sources             : 'table' int [1:8443(1d)] 519 438 396 368 366 316 308 243 229 228 ...\n  ..- attr(*, \"dimnames\")=List of 1\n  .. ..$ SO: chr [1:8443] \"INTERNATIONAL JOURNAL OF ENVIRONMENTAL  RESEARCH AND PUBLIC HEALTH/INTERNATIONAL JOURNAL OF ENVIRONMENTAL RESEA\"| __truncated__ \"PLOS ONE\" \"FRONTIERS IN PSYCHOLOGY\" \"BMJ OPEN\" ...\n $ DE                  : NULL\n $ ID                  : 'table' int [1:10069(1d)] 25495 22956 13848 13747 11887 10037 9975 9294 8577 7854 ...\n  ..- attr(*, \"dimnames\")=List of 1\n  .. ..$ Tab: chr [1:10069] \"PSYCHOLOGY\" \"MEDICINE\" \"PSYCHIATRY\" \"MEDLINE\" ...\n $ Documents           : 'table' int [1(1d)] 36680\n  ..- attr(*, \"dimnames\")=List of 1\n  .. ..$ : chr \"ARTICLE     \"\n $ IntColl             : num 25\n $ nReferences         : int 934546\n $ DB                  : chr \"OPENALEX\"\n - attr(*, \"class\")= chr \"bibliometrix\""
  },
  {
    "objectID": "slides/ms-slides-11.html#zusammenfassung-mit-summary",
    "href": "slides/ms-slides-11.html#zusammenfassung-mit-summary",
    "title": "Wiederholung & Ausblick",
    "section": "Zusammenfassung mit summary()",
    "text": "Zusammenfassung mit summary()\nBrief introduction to bibliometrix\n\nbibliometrix_df %&gt;% \n  summary(k = 10, pause = FALSE)\n\n\n\nMAIN INFORMATION ABOUT DATA\n\n Timespan                              2013 : 2023 \n Sources (Journals, Books, etc)        8443 \n Documents                             36680 \n Annual Growth Rate %                  19.49 \n Document Average Age                  4.2 \n Average citations per doc             26.24 \n Average citations per year per doc    4.195 \n References                            934546 \n \nDOCUMENT TYPES                     \n article      36680 \n \nDOCUMENT CONTENTS\n Keywords Plus (ID)                    10069 \n Author's Keywords (DE)                0 \n \nAUTHORS\n Authors                               106265 \n Author Appearances                    146925 \n Authors of single-authored docs       4172 \n \nAUTHORS COLLABORATION\n Single-authored docs                  4670 \n Documents per Author                  0.345 \n Co-Authors per Doc                    4.01 \n International co-authorships %        24.98 \n \n\nAnnual Scientific Production\n\n Year    Articles\n    2013     1165\n    2014     1279\n    2015     1549\n    2016     1867\n    2017     2260\n    2018     2545\n    2019     3147\n    2020     4200\n    2021     5626\n    2022     6128\n    2023     6914\n\nAnnual Percentage Growth Rate 19.49 \n\n\nMost Productive Authors\n\n          Authors        Articles      Authors        Articles Fractionalized\n1  PIM CUIJPERS                71 MARK D. GRIFFITHS                     13.47\n2  MARK D. GRIFFITHS           51 PIM CUIJPERS                           9.21\n3  G. J. MEL√âNDEZ‚ÄêTORRES       38 MELOR MD YUNUS                         8.83\n4  SEENA FAZEL                 36 DAVID P. FARRINGTON                    8.67\n5  PAULA WILLIAMSON            33 SHEFALY SHOREY                         8.58\n6  DAVID P. FARRINGTON         28 SOHRAB AMIRI                           8.17\n7  HELEN SKOUTERIS             28 PHILLIPA HAY                           8.13\n8  JAMES THOMAS                28 MASLAWATI MOHAMAD                      8.06\n9  JANET TREASURE              28 BART LENART                            8.00\n10 PHILLIPA HAY                28 SEENA FAZEL                            7.92\n\n\nTop manuscripts per citations\n\n                                                                           Paper                                    DOI\n1  JIAQI XIONG, 2020, JOURNAL OF AFFECTIVE DISORDERS                                      10.1016/j.jad.2020.08.001    \n2  KAREN HUGHES, 2017, \\u0098THE \\u009cLANCET. PUBLIC HEALTH                              10.1016/s2468-2667(17)30118-4\n3  SOFIA PAPPA, 2020, BRAIN, BEHAVIOR, AND IMMUNITY                                       10.1016/j.bbi.2020.05.026    \n4  CHRISTIAN WINTHER TOPP, 2015, PSYCHOTHERAPY AND PSYCHOSOMATICS                         10.1159/000376585            \n5  JUHO HAMARI, 2014, NA                                                                  10.1109/hicss.2014.377       \n6  NADER SALARI, 2020, GLOBALIZATION AND HEALTH                                           10.1186/s12992-020-00589-w   \n7  NINA VINDEGAARD, 2020, BRAIN, BEHAVIOR, AND IMMUNITY                                   10.1016/j.bbi.2020.05.048    \n8  SARAH CL√âMENT, 2014, PSYCHOLOGICAL MEDICINE                                            10.1017/s0033291714000129    \n9  ZACHARY STEEL, 2014, INTERNATIONAL JOURNAL OF EPIDEMIOLOGY                             10.1093/ije/dyu038           \n10 MARIA LOADES, 2020, JOURNAL OF THE AMERICAN ACADEMY OF CHILD AND ADOLESCENT PSYCHIATRY 10.1016/j.jaac.2020.05.009   \n     TC TCperYear   NTC\n1  3664       733 132.0\n2  2895       362  56.0\n3  2675       535  96.4\n4  2657       266  47.1\n5  2568       233  37.5\n6  2550       510  91.9\n7  2396       479  86.3\n8  2080       189  30.4\n9  1981       180  28.9\n10 1896       379  68.3\n\n\nCorresponding Author's Countries\n\n          Country Articles   Freq  SCP MCP MCP_Ratio\n1  USA                3311 0.1428 2681 630     0.190\n2  AUSTRALIA          2432 0.1049 1828 604     0.248\n3  CHINA              1611 0.0695 1088 523     0.325\n4  CANADA             1308 0.0564  937 371     0.284\n5  UNITED KINGDOM     1017 0.0439  710 307     0.302\n6  INDONESIA           962 0.0415  766 196     0.204\n7  SPAIN               939 0.0405  661 278     0.296\n8  GERMANY             894 0.0386  619 275     0.308\n9  ITALY               854 0.0368  545 309     0.362\n10 MALAYSIA            819 0.0353  614 205     0.250\n\n\nSCP: Single Country Publications\n\nMCP: Multiple Country Publications\n\n\nTotal Citations per Country\n\n           Country      Total Citations Average Article Citations\n1  USA                           112947                    34.113\n2  AUSTRALIA                     107366                    44.147\n3  UNITED KINGDOM                 59199                    58.209\n4  CANADA                         48426                    37.023\n5  NETHERLANDS                    38246                    51.131\n6  GERMANY                        37808                    42.291\n7  CHINA                          36807                    22.847\n8  SPAIN                          26761                    28.499\n9  ITALY                          26174                    30.649\n10 BRAZIL                         15686                    21.576\n\n\nMost Relevant Sources\n\n                                                                                                                         Sources       \n1  INTERNATIONAL JOURNAL OF ENVIRONMENTAL  RESEARCH AND PUBLIC HEALTH/INTERNATIONAL JOURNAL OF ENVIRONMENTAL RESEARCH AND PUBLIC HEALTH\n2  PLOS ONE                                                                                                                            \n3  FRONTIERS IN PSYCHOLOGY                                                                                                             \n4  BMJ OPEN                                                                                                                            \n5  TRAUMA VIOLENCE & ABUSE                                                                                                             \n6  SOCIAL SCIENCE RESEARCH NETWORK                                                                                                     \n7  JOURNAL OF AFFECTIVE DISORDERS                                                                                                      \n8  FRONTIERS IN PSYCHIATRY                                                                                                             \n9  JMIR. JOURNAL OF MEDICAL INTERNET RESEARCH/JOURNAL OF MEDICAL INTERNET RESEARCH                                                     \n10 PUBMED                                                                                                                              \n   Articles\n1       519\n2       438\n3       396\n4       368\n5       366\n6       316\n7       308\n8       243\n9       229\n10      228"
  },
  {
    "objectID": "slides/ms-slides-11.html#top-authors-productivity-over-the-time",
    "href": "slides/ms-slides-11.html#top-authors-productivity-over-the-time",
    "title": "Wiederholung & Ausblick",
    "section": "Top-Authors‚Äô Productivity over the Time",
    "text": "Top-Authors‚Äô Productivity over the Time\nBrief introduction to bibliometrix\n\ntop_authors = bibliometrix_data %&gt;% \n  authorProdOverTime(k = 10, graph = TRUE)"
  },
  {
    "objectID": "slides/ms-slides-11.html#country-scientific-collaboration",
    "href": "slides/ms-slides-11.html#country-scientific-collaboration",
    "title": "Wiederholung & Ausblick",
    "section": "Country Scientific Collaboration",
    "text": "Country Scientific Collaboration\nBrief introduction to bibliometrix\n\n# Create a country collaboration network\nbibliometrix_author_meta &lt;- bibliometrix_data %&gt;% \n  metaTagExtraction(Field = \"AU_CO\", sep = \";\")\n\ncountry_collab_matrix &lt;- bibliometrix_author_meta %&gt;% \n  biblioNetwork(\n    analysis = \"collaboration\",\n    network = \"countries\", sep = \";\")\n\n# Plot the network\nnetworkPlot(\n  country_collab_matrix, \n  n = dim(collab_matrix)[1],\n  Title = \"Country Collaboration\",\n  type = \"circle\",\n  size = TRUE,\n  remove.multiple = FALSE,\n  labelsize=0.7,\n  cluster = \"none\")"
  },
  {
    "objectID": "slides/ms-slides-11.html#country-scientific-collaboration-1",
    "href": "slides/ms-slides-11.html#country-scientific-collaboration-1",
    "title": "Wiederholung & Ausblick",
    "section": "Country Scientific Collaboration",
    "text": "Country Scientific Collaboration\nBrief introduction to bibliometrix\n\n\nExpand for full code\nnet = networkPlot(\n  country_collab_matrix, \n  n = 20,\n  Title = \"Country Collaboration\",\n  type = \"circle\",\n  size = TRUE,\n  remove.multiple = FALSE,\n  labelsize=0.7,cluster = \"none\")"
  },
  {
    "objectID": "slides/ms-slides-11.html#keyword-co-occurrences",
    "href": "slides/ms-slides-11.html#keyword-co-occurrences",
    "title": "Wiederholung & Ausblick",
    "section": "Keyword co-occurrences",
    "text": "Keyword co-occurrences\nBrief introduction to bibliometrix\n\nkeyword_matrix &lt;- bibliometrix_data %&gt;% \n  biblioNetwork(\n    analysis = \"co-occurrences\",\n    network = \"keywords\",\n    sep = \";\")\n\n# Plot the network\nnet=keyword_matrix(\n  NetMatrix,\n  normalize=\"association\",\n  weighted=T, n = 30,\n  Title = \"Keyword Co-occurrences\",\n  type = \"fruchterman\",\n  size=T,\n  edgesize = 5,\n  labelsize=0.7)"
  },
  {
    "objectID": "slides/ms-slides-11.html#keyword-co-occurrences-1",
    "href": "slides/ms-slides-11.html#keyword-co-occurrences-1",
    "title": "Wiederholung & Ausblick",
    "section": "Keyword co-occurrences",
    "text": "Keyword co-occurrences\nBrief introduction to bibliometrix\n\n\nExpand for full code\nnet=networkPlot(\n  keyword_matrix,\n  normalize=\"association\",\n  weighted=T, n = 30,\n  Title = \"Keyword Co-occurrences\",\n  type = \"fruchterman\",\n  size=T,\n  edgesize = 5,\n  labelsize=0.7)"
  },
  {
    "objectID": "slides/ms-slides-11.html#references",
    "href": "slides/ms-slides-11.html#references",
    "title": "Wiederholung & Ausblick",
    "section": "References",
    "text": "References\n\n\nAria, M., & Cuccurullo, C. (2017). bibliometrix : An R-tool for comprehensive science mapping analysis. Journal of Informetrics, 11(4), 959‚Äì975. https://doi.org/10.1016/j.joi.2017.08.007\n\n\nGruber, J. B., & Weber, M. (2024). Rollama: An r package for using generative large language models through ollama. https://doi.org/10.48550/ARXIV.2404.07654"
  },
  {
    "objectID": "course-team.html",
    "href": "course-team.html",
    "title": "Teaching team",
    "section": "",
    "text": "is Tenure-Track Assistant Professor for Communication Science at Friedrich-Alexander-Universit√§t (FAU) Erlangen-N√ºrnberg.\nHis work investigates the positive and negative consequences of digital communication (e.g., via smartphones and social media) for users‚Äô well-being, health, and self-regulation. His current research focusses on users‚Äô digital well-being at the interface of work and leisure, for instance, digital stress, availability pressures, procrastinatory media use, and digital communication when working from home.",
    "crumbs": [
      "Hintergrundinformationen",
      "zum Teaching Team"
    ]
  },
  {
    "objectID": "course-team.html#prof.-dr.-adrian-meier",
    "href": "course-team.html#prof.-dr.-adrian-meier",
    "title": "Teaching team",
    "section": "",
    "text": "is Tenure-Track Assistant Professor for Communication Science at Friedrich-Alexander-Universit√§t (FAU) Erlangen-N√ºrnberg.\nHis work investigates the positive and negative consequences of digital communication (e.g., via smartphones and social media) for users‚Äô well-being, health, and self-regulation. His current research focusses on users‚Äô digital well-being at the interface of work and leisure, for instance, digital stress, availability pressures, procrastinatory media use, and digital communication when working from home.",
    "crumbs": [
      "Hintergrundinformationen",
      "zum Teaching Team"
    ]
  },
  {
    "objectID": "course-team.html#christoph-adrian-hehim",
    "href": "course-team.html#christoph-adrian-hehim",
    "title": "Teaching team",
    "section": "Christoph Adrian (he/him)",
    "text": "Christoph Adrian (he/him)\n\n   \nis a Research Assistant at the Chair of Communication Science at Friedrich-Alexander-Universit√§t (FAU) Erlangen-N√ºrnberg.\nHis work focuses on computational methods, especially Text as Data Approaches and workin with Digital behavioral data, with an emphasis on computing, reproducible research, student-centered learning, and open-source education.",
    "crumbs": [
      "Hintergrundinformationen",
      "zum Teaching Team"
    ]
  },
  {
    "objectID": "ms-schedule.html",
    "href": "ms-schedule.html",
    "title": "Semesterplan",
    "section": "",
    "text": "Note\n\n\n\nDiese Seite erh√§lt eine √úbersicht √ºber die Sitzung bzw. Themen des Methodenseminars im Sommersemester 2024. Bitte beachten Sie, dass die Inhalte des Kurses ( Folien,  Exercises,  Showcases und  Hintergrundinformationen) im Laufe des Semesters stetig aktualisiert werden, wobei alle √Ñnderungen hier dokumentiert werden.\n\n\n\n\n\n\n\n\n\n\n\nSitzung\nDatum\nThema\nUnterlagen\n\n\n\n\n\n\nIntroduction\n\n\n\n1\n18.04.2024\nEinf√ºhrung & √úberblick\n\n\n\n\nüìö\nBlock I: Systematic Review\n\n\n\n2\n25.04.2024\nEinf√ºhrung in Systematic Reviews I\n\n\n\n3\n02.05.2024\nEinf√ºhrung in Systematic Reviews II\n\n\n\n\n09.05.2024\nüèñÔ∏è Feiertag\n\n\n\n4\n16.05.2024\nAutomatisierung von SRs & KI-Tools I\n\n\n\n\n23.05.2024\nüçª WiSo-Projekt-Woche\n\n\n\n\n30.05.2024\nüèñÔ∏è Feiertag\n\n\n\n5\n04.06.2024\nüçï Gastvortrag: Prof.¬†Dr.¬†Emese Domahidi\n\n\n\n6\n06.06.2024\nAutomatisierung von SRs & KI-Tools\n\n\n\n\nüíª\nBlock II: Text as Data\n\n\n\n7\n13.06.2024\nIntroduction to Text as Data\n |¬† |  | \n\n\n8\n20.06.2024\nText processing in R\n |¬† |  | \n\n\n9\n27.06.2024\nUnsupervised Machine Learning I\n |¬† |  | \n\n\n10\n04.07.2024\nUnsupervised Machine Learning II\n |¬† |  | \n\n\n11\n11.07.2024\nRecap & Ausblick\n\n\n\n12\n18.07.2024\nüèÅ Semesterabschluss",
    "crumbs": [
      "Kursunterlagen",
      "Methodenseminar"
    ]
  },
  {
    "objectID": "data_mining/ps-01_01-data-mining_openalex.html",
    "href": "data_mining/ps-01_01-data-mining_openalex.html",
    "title": "API mining: OpenAlex",
    "section": "",
    "text": "# Load necessary packages\npacman::p_load(\n  here, qs, \n  magrittr, janitor,\n  naniar, visdat,\n  easystats, sjmisc,\n  ggpubr, \n  gt, gtExtras, gtsummary,\n  openalexR, bibliometrix, \n  tidyverse\n)",
    "crumbs": [
      "Datenerhebung",
      "API Mining: OpenAlex"
    ]
  },
  {
    "objectID": "data_mining/ps-01_01-data-mining_openalex.html#preparation",
    "href": "data_mining/ps-01_01-data-mining_openalex.html#preparation",
    "title": "API mining: OpenAlex",
    "section": "",
    "text": "# Load necessary packages\npacman::p_load(\n  here, qs, \n  magrittr, janitor,\n  naniar, visdat,\n  easystats, sjmisc,\n  ggpubr, \n  gt, gtExtras, gtsummary,\n  openalexR, bibliometrix, \n  tidyverse\n)",
    "crumbs": [
      "Datenerhebung",
      "API Mining: OpenAlex"
    ]
  },
  {
    "objectID": "data_mining/ps-01_01-data-mining_openalex.html#mining-openalex-api",
    "href": "data_mining/ps-01_01-data-mining_openalex.html#mining-openalex-api",
    "title": "API mining: OpenAlex",
    "section": "Mining OpenAlex API",
    "text": "Mining OpenAlex API\n\nSet credentials\n\n# Set openalexR.mailto option so that your requests go to the polite pool for faster response times\noptions(openalexR.mailto = \"christoph.adrian@fau.de\")\n\n\n\nInitial OpenAlex API query\n\nreferences &lt;- list()\n\n# Download data via API\nreferences$openalex$api &lt;- openalexR::oa_fetch(\n  entity = \"works\",\n  title_and_abstract.search = '(\"artificial intelligence\" OR AI OR \"chatbot\" OR \"AI-based chatbot\" OR \"artificial intelligence-based chatbot\" OR \"chat agent\" OR \"voice bot\" OR \"voice assistant\" OR \"voice-based assistant\" OR \"conversational agent\" OR \"conversational assistant\" OR \"conversational AI\" OR \"AI-based assistant\" OR \"artificial intelligence-based assistant\" OR \"virtual assistant\" OR \"intelligent assistant\" OR \"digital assistant\" OR \"smart speaker\" OR chatgpt OR \"google gemini\" OR \"google bard\" OR \"bing chat\" OR \"microsoft copilot\" OR \"claude ai\" OR \"perplexity ai\") AND (anthropomorphism OR humanlike OR humanness OR humanized OR \"user experience\" OR UX OR usability OR trust* OR \"conversational experience\" OR CUX OR \"conversation design\" OR safety OR privacy)',\n  publication_year = \"2016-2025\",\n  primary_topic.field.id = c(\n    \"fields/33\", # Social Science\n    \"fields/32\" # Psychology\n  ),\n  language = \"en\",\n  type = c(\"article\", \"conference-paper\", \"preprint\"),\n  verbose = TRUE\n)",
    "crumbs": [
      "Datenerhebung",
      "API Mining: OpenAlex"
    ]
  },
  {
    "objectID": "data_mining/ps-01_01-data-mining_openalex.html#quality-control",
    "href": "data_mining/ps-01_01-data-mining_openalex.html#quality-control",
    "title": "API mining: OpenAlex",
    "section": "Quality control",
    "text": "Quality control\n\nreferences$openalex$api %&gt;% \n  skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n30646\n\n\nNumber of columns\n39\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n23\n\n\nlist\n8\n\n\nlogical\n5\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nid\n0\n1.00\n30\n32\n0\n28528\n0\n\n\ntitle\n1\n1.00\n4\n500\n0\n27743\n0\n\n\ndisplay_name\n1\n1.00\n4\n500\n0\n27743\n0\n\n\nab\n1188\n0.96\n0\n51771\n10\n26719\n0\n\n\npublication_date\n0\n1.00\n10\n10\n0\n2672\n0\n\n\nso\n5735\n0.81\n1\n244\n0\n7969\n0\n\n\nso_id\n5735\n0.81\n27\n32\n0\n7994\n0\n\n\nhost_organization\n13032\n0.57\n3\n155\n0\n1274\n0\n\n\nissn_l\n10525\n0.66\n9\n9\n0\n7295\n0\n\n\nurl\n137\n1.00\n21\n273\n0\n28390\n0\n\n\npdf_url\n18826\n0.39\n29\n359\n0\n10860\n0\n\n\nlicense\n20216\n0.34\n3\n21\n0\n11\n0\n\n\nversion\n12190\n0.60\n15\n16\n0\n3\n0\n\n\nfirst_page\n13186\n0.57\n1\n15\n0\n4030\n0\n\n\nlast_page\n13299\n0.57\n1\n15\n0\n4038\n0\n\n\nvolume\n13362\n0.56\n1\n25\n0\n604\n0\n\n\nissue\n15657\n0.49\n1\n30\n0\n629\n0\n\n\noa_status\n0\n1.00\n4\n7\n0\n6\n0\n\n\noa_url\n11799\n0.61\n20\n359\n0\n17197\n0\n\n\nlanguage\n0\n1.00\n2\n2\n0\n1\n0\n\n\ncited_by_api_url\n0\n1.00\n53\n55\n0\n28528\n0\n\n\ndoi\n2133\n0.93\n26\n96\n0\n26496\n0\n\n\ntype\n0\n1.00\n7\n8\n0\n2\n0\n\n\n\nVariable type: list\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nn_unique\nmin_length\nmax_length\n\n\n\n\nauthor\n249\n0.99\n27647\n1\n12\n\n\ngrants\n28143\n0.08\n2046\n1\n33\n\n\ncounts_by_year\n15039\n0.51\n4415\n1\n2\n\n\nids\n0\n1.00\n28528\n1\n5\n\n\nreferenced_works\n10836\n0.65\n18263\n1\n442\n\n\nrelated_works\n423\n0.99\n20233\n1\n20\n\n\nconcepts\n0\n1.00\n28523\n5\n5\n\n\ntopics\n0\n1.00\n27999\n5\n5\n\n\n\nVariable type: logical\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\nis_oa\n134\n1\n0.56\nTRU: 17130, FAL: 13382\n\n\nis_oa_anywhere\n0\n1\n0.61\nTRU: 18797, FAL: 11849\n\n\nany_repository_has_fulltext\n0\n1\n0.30\nFAL: 21399, TRU: 9247\n\n\nis_paratext\n0\n1\n0.00\nFAL: 30646\n\n\nis_retracted\n0\n1\n0.00\nFAL: 30595, TRU: 51\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nrelevance_score\n0\n1\n9.48\n26.05\n0.04\n0.84\n3.09\n7.01\n986.22\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\ncited_by_count\n0\n1\n7.47\n32.62\n0.00\n0.00\n1.00\n4.00\n1878.00\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\npublication_year\n0\n1\n2021.95\n1.99\n2016.00\n2021.00\n2023.00\n2024.00\n2025.00\n‚ñÅ‚ñÇ‚ñÖ‚ñá‚ñÜ\n\n\n\n\n\n\n\n\n\n\n\nQuick overview\n\n\n\n\nNearly all references have an abstract (¬±¬†96 percent) and DOI (¬±¬†93 percent), which are the critical information for the analysis.\nThe difference in the number of cases and the number of unique IDs indicates that there are duplicates in the data.\n\n\n\n\nCheck duplicates\n\nbased on OpenAlex ID\n\n# Check for duplicates based on OpenAlex ID\nreferences$openalex$api %&gt;% \n  group_by(id) %&gt;% \n  summarise(n = n()) %&gt;% \n  frq(n, sort.frq = \"desc\")\n\nn &lt;integer&gt; \n# total N=28528 valid N=28528 mean=1.07 sd=0.26\n\nValue |     N | Raw % | Valid % | Cum. %\n----------------------------------------\n    1 | 26410 | 92.58 |   92.58 |  92.58\n    2 |  2118 |  7.42 |    7.42 | 100.00\n &lt;NA&gt; |     0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\n\n\nduplicates &lt;- list()\n\n# Extract duplicated IDs\nduplicates$openalex$api$id$string &lt;- references$openalex$api %&gt;% \n  group_by(id) %&gt;%\n  summarise(n = n()) %&gt;%\n  filter(n &gt; 1) %&gt;% \n  pull(id)\n\n# Extract cases with duplicated IDs\nduplicates$openalex$api$id$data &lt;- references$openalex$api %&gt;% \n  filter(id %in% duplicates$openalex$api$id$string) %&gt;% \n  arrange(id)\n\n\n# Extract uneven (odd) rows\ndf1 &lt;- duplicates$openalex$api$id$data[seq(1, nrow(duplicates$openalex$api$id$data), by = 2), ]\ndf2 &lt;- duplicates$openalex$api$id$data[seq(2, nrow(duplicates$openalex$api$id$data), by = 2), ]\n\n# Compare the two data frames\nsummary(arsenal::comparedf(df1, df2))\n\n\n\nTable: Summary of data.frames\n\nversion   arg    ncol   nrow\n--------  ----  -----  -----\nx         df1      39   2118\ny         df2      39   2118\n\n\n\nTable: Summary of overall comparison\n\nstatistic                                                      value\n------------------------------------------------------------  ------\nNumber of by-variables                                             0\nNumber of non-by variables in common                              39\nNumber of variables compared                                      39\nNumber of variables in x but not y                                 0\nNumber of variables in y but not x                                 0\nNumber of variables compared with some values unequal              1\nNumber of variables compared with all values equal                38\nNumber of observations in common                                2118\nNumber of observations in x but not y                              0\nNumber of observations in y but not x                              0\nNumber of observations with some compared variables unequal     2118\nNumber of observations with all compared variables equal           0\nNumber of values unequal                                        2118\n\n\n\nTable: Variables not shared\n\n                         \n ------------------------\n No variables not shared \n ------------------------\n\n\n\nTable: Other variables not compared\n\n                                 \n --------------------------------\n No other variables not compared \n --------------------------------\n\n\n\nTable: Observations not shared\n\n                            \n ---------------------------\n No observations not shared \n ---------------------------\n\n\n\nTable: Differences detected by variable\n\nvar.x                         var.y                             n   NAs\n----------------------------  ----------------------------  -----  ----\nid                            id                                0     0\ntitle                         title                             0     0\ndisplay_name                  display_name                      0     0\nauthor                        author                            0     0\nab                            ab                                0     0\npublication_date              publication_date                  0     0\nrelevance_score               relevance_score                2118     0\nso                            so                                0     0\nso_id                         so_id                             0     0\nhost_organization             host_organization                 0     0\nissn_l                        issn_l                            0     0\nurl                           url                               0     0\npdf_url                       pdf_url                           0     0\nlicense                       license                           0     0\nversion                       version                           0     0\nfirst_page                    first_page                        0     0\nlast_page                     last_page                         0     0\nvolume                        volume                            0     0\nissue                         issue                             0     0\nis_oa                         is_oa                             0     0\nis_oa_anywhere                is_oa_anywhere                    0     0\noa_status                     oa_status                         0     0\noa_url                        oa_url                            0     0\nany_repository_has_fulltext   any_repository_has_fulltext       0     0\nlanguage                      language                          0     0\ngrants                        grants                            0     0\ncited_by_count                cited_by_count                    0     0\ncounts_by_year                counts_by_year                    0     0\npublication_year              publication_year                  0     0\ncited_by_api_url              cited_by_api_url                  0     0\nids                           ids                               0     0\ndoi                           doi                               0     0\ntype                          type                              0     0\nreferenced_works              referenced_works                  0     0\nrelated_works                 related_works                     0     0\nis_paratext                   is_paratext                       0     0\nis_retracted                  is_retracted                      0     0\nconcepts                      concepts                          0     0\ntopics                        topics                            0     0\n\n\n\nTable: Differences detected (2108 not shown)\n\nvar.x             var.y              ..row.names..  values.x    values.y     row.x   row.y\n----------------  ----------------  --------------  ----------  ----------  ------  ------\nrelevance_score   relevance_score                1  5.394589    5.377582         1       1\nrelevance_score   relevance_score                2  2.179161    2.139825         2       2\nrelevance_score   relevance_score                3  0.8233411   0.8151723        3       3\nrelevance_score   relevance_score                4  1.192189    1.161013         4       4\nrelevance_score   relevance_score                5  2.006378    1.954127         5       5\nrelevance_score   relevance_score                6  1.195497    1.183922         6       6\nrelevance_score   relevance_score                7  0.6909628   0.6844353        7       7\nrelevance_score   relevance_score                8  0.8900583   0.879919         8       8\nrelevance_score   relevance_score                9  8.128157    8.087495         9       9\nrelevance_score   relevance_score               10  0.7075946   0.7026421       10      10\n\n\n\nTable: Non-identical attributes\n\n                             \n ----------------------------\n No non-identical attributes \n ----------------------------\n\n\n\n\nbased on DOI\n\n# Check for duplicates based on DOI\nreferences$openalex$api %&gt;% \n  distinct(id, .keep_all = TRUE) %&gt;% # exclude ID duplicates\n  filter(!is.na(doi)) %&gt;% # exclude cases without DOI\n  group_by(doi) %&gt;% \n  summarise(n = n()) %&gt;% \n  frq(n, sort.frq = \"desc\")\n\nn &lt;integer&gt; \n# total N=26496 valid N=26496 mean=1.00 sd=0.02\n\nValue |     N | Raw % | Valid % | Cum. %\n----------------------------------------\n    1 | 26488 | 99.97 |   99.97 |  99.97\n    2 |     8 |  0.03 |    0.03 | 100.00\n &lt;NA&gt; |     0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\n\n\n# Extract duplicated IDs\nduplicates$openalex$api$doi$string &lt;- references$openalex$api %&gt;%  \n  distinct(id, .keep_all = TRUE) %&gt;% \n  filter(!is.na(doi)) %&gt;%\n  group_by(doi) %&gt;%\n  summarise(n = n()) %&gt;%\n  filter(n &gt; 1) %&gt;% \n  pull(doi)\n\n# Extract cases with duplicated IDs\nduplicates$openalex$api$doi$data &lt;- references$openalex$api %&gt;% \n  filter(doi %in% duplicates$openalex$api$doi$string)\n\n# Extract cases to be deleted\nduplicates$openalex$api$doi$delete &lt;- duplicates$openalex$api$doi$data %&gt;%\n  mutate(id_number = as.numeric(sub(\".*W\", \"\", id))) %&gt;% \n  group_by(doi) %&gt;% # Group by `doi`\n  slice_min(id_number, n = 1, with_ties = FALSE) %&gt;% \n  select(-id_number) \n\n\n\n\n\n\n\nSummary\n\n\n\n\nThe duplicates based on the OpenAlex ID seem to only have differences in columns, that are less relevant for the analysis. The duplicates are therefore eliminated with the distinct() function.\nThe duplicates based on the DOI are a result of pre-prints being published. Therefore, only the most recent entry for each duplicated DOI will be kept.",
    "crumbs": [
      "Datenerhebung",
      "API Mining: OpenAlex"
    ]
  },
  {
    "objectID": "data_mining/ps-01_01-data-mining_openalex.html#transformation",
    "href": "data_mining/ps-01_01-data-mining_openalex.html#transformation",
    "title": "API mining: OpenAlex",
    "section": "Transformation",
    "text": "Transformation\n\nreferences$openalex$raw &lt;- references$openalex$api %&gt;% \n  distinct(id, .keep_all = TRUE) %&gt;% # delete duplicates based on ID\n  anti_join(duplicates$openalex$api$doi$delete, by = \"id\") # delete one case of each DOI duplicated()\n\n\nCheck transformation\n\nreferences$openalex$raw %&gt;% \n  group_by(id) %&gt;% \n  summarise(n = n()) %&gt;% \n  frq(n, sort.frq = \"desc\")\n\nn &lt;integer&gt; \n# total N=28520 valid N=28520 mean=1.00 sd=0.00\n\nValue |     N | Raw % | Valid % | Cum. %\n----------------------------------------\n    1 | 28520 |   100 |     100 |    100\n &lt;NA&gt; |     0 |     0 |    &lt;NA&gt; |   &lt;NA&gt;\n\nreferences$openalex$raw %&gt;% \n  filter(!is.na(doi)) %&gt;% # exclude cases without DOI\n  group_by(doi) %&gt;% \n  summarise(n = n()) %&gt;% \n  frq(n, sort.frq = \"desc\")\n\nn &lt;integer&gt; \n# total N=26496 valid N=26496 mean=1.00 sd=0.00\n\nValue |     N | Raw % | Valid % | Cum. %\n----------------------------------------\n    1 | 26496 |   100 |     100 |    100\n &lt;NA&gt; |     0 |     0 |    &lt;NA&gt; |   &lt;NA&gt;",
    "crumbs": [
      "Datenerhebung",
      "API Mining: OpenAlex"
    ]
  },
  {
    "objectID": "data_mining/ps-01_03-data-remining_openalex.html",
    "href": "data_mining/ps-01_03-data-remining_openalex.html",
    "title": "Combine OpenAlex & Scopus",
    "section": "",
    "text": "# Load necessary packages\npacman::p_load(\n  here, qs, \n  magrittr, janitor,\n  naniar, visdat,\n  easystats, sjmisc,\n  ggpubr, \n  gt, gtExtras, gtsummary,\n  openalexR, bibliometrix, \n  tidyverse\n)",
    "crumbs": [
      "Datenerhebung",
      "Combine OpenAlex & Scopus"
    ]
  },
  {
    "objectID": "data_mining/ps-01_03-data-remining_openalex.html#preparation",
    "href": "data_mining/ps-01_03-data-remining_openalex.html#preparation",
    "title": "Combine OpenAlex & Scopus",
    "section": "",
    "text": "# Load necessary packages\npacman::p_load(\n  here, qs, \n  magrittr, janitor,\n  naniar, visdat,\n  easystats, sjmisc,\n  ggpubr, \n  gt, gtExtras, gtsummary,\n  openalexR, bibliometrix, \n  tidyverse\n)",
    "crumbs": [
      "Datenerhebung",
      "Combine OpenAlex & Scopus"
    ]
  },
  {
    "objectID": "data_mining/ps-01_03-data-remining_openalex.html#comparison-with-scopus",
    "href": "data_mining/ps-01_03-data-remining_openalex.html#comparison-with-scopus",
    "title": "Combine OpenAlex & Scopus",
    "section": "Comparison with Scopus",
    "text": "Comparison with Scopus\n\n\nreferences &lt;- qs::qread(here(\"local_data/references_openalex.qs\"))\nreferences$scopus$raw &lt;- qs::qread(here(\"local_data/references_scopus.qs\"))$raw\n\n\nIdentification of ‚Äúmissing‚Äù references\n\n# Identify the number of scopus references missing in the openalex data\nreferences$scopus$raw %&gt;% \n  filter(!is.na(doi)) %&gt;% \n  mutate(doi_full = paste0(\"https://doi.org/\", doi)) %&gt;% \n  filter(!(doi_full %in% references$openalex$raw$doi)) %&gt;% \n  glimpse()\n\nRows: 4,819\nColumns: 17\n$ scopusID     &lt;chr&gt; \"2-s2.0-85208654899\", \"2-s2.0-85201379794\", \"2-s2.0-85209‚Ä¶\n$ doi          &lt;chr&gt; \"10.1016/j.csi.2024.103940\", \"10.1016/j.csi.2024.103903\",‚Ä¶\n$ pmid         &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, \"39389117\", NA, NA, NA, N‚Ä¶\n$ authors      &lt;chr&gt; \"Alier M.|Pereira J.|Garc√≠a-Pe√±alvo F.J.|Casa√± M.J.|Cabr√©‚Ä¶\n$ affiliations &lt;chr&gt; \"Universidad de Salamanca|Universitat Polit√©cnica de Cata‚Ä¶\n$ countries    &lt;chr&gt; \"Spain\", \"Russian Federation|Uzbekistan\", \"South Korea|Ho‚Ä¶\n$ year         &lt;chr&gt; \"2025\", \"2025\", \"2025\", \"2025\", \"2025\", \"2025\", \"2025\", \"‚Ä¶\n$ articletitle &lt;chr&gt; \"LAMB: An open-source software framework to create artifi‚Ä¶\n$ journal      &lt;chr&gt; \"Computer Standards and Interfaces\", \"Computer Standards ‚Ä¶\n$ volume       &lt;chr&gt; \"92\", \"92\", \"210\", \"194\", \"163\", \"149\", \"95\", \"106\", \"369‚Ä¶\n$ issue        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, \"1\", NA, \"1\", \"1\", NA‚Ä¶\n$ pages        &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"625-632\", \"\", \"\", \"\", \"2‚Ä¶\n$ keywords     &lt;chr&gt; \"Education domain|Generative artificial intelligence|IMS ‚Ä¶\n$ abstract     &lt;chr&gt; \"This paper presents LAMB (Learning Assistant Manager and‚Ä¶\n$ ptype        &lt;chr&gt; \"Article\", \"Article\", \"Article\", \"Article\", \"Article\", \"A‚Ä¶\n$ timescited   &lt;chr&gt; \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"2\", \"1\", \"0\", \"0\", \"0\", \"0‚Ä¶\n$ doi_full     &lt;chr&gt; \"https://doi.org/10.1016/j.csi.2024.103940\", \"https://doi‚Ä¶\n\n\n\n\nExtract DOIs for missing references\n\nmissing_references &lt;- list()\n\n# Format DOIs \nmissing_references$scopus_dois &lt;- references$scopus$raw %&gt;%\n  filter(!is.na(doi)) %&gt;% \n  mutate(doi = paste0(\"https://doi.org/\", doi)) %&gt;% \n  filter(!(doi %in% references$openalex$raw$doi)) %&gt;%\n  pull(doi)\n\n# Split DOIs into chunks of 25\nchunk_size &lt;- 10\nmissing_references$scopus_dois_chunks &lt;- split(missing_references$scopus_dois, ceiling(seq_along(missing_references$scopus_dois) / chunk_size))",
    "crumbs": [
      "Datenerhebung",
      "Combine OpenAlex & Scopus"
    ]
  },
  {
    "objectID": "data_mining/ps-01_03-data-remining_openalex.html#completion-of-openalex-data",
    "href": "data_mining/ps-01_03-data-remining_openalex.html#completion-of-openalex-data",
    "title": "Combine OpenAlex & Scopus",
    "section": "Completion of OpenAlex data",
    "text": "Completion of OpenAlex data\n\nMining missing references via OpenAlex API\n\n# Download missing references via API\nmissing_references$data$chunks &lt;- map(\n  missing_references$scopus_dois_chunks, function(chunk) {\n  Sys.sleep(2)  # Pause for 1 second\n  \n  tryCatch(\n    {\n      # Attempt the API call\n      openalexR::oa_fetch(\n        entity = \"works\",\n        doi = chunk,\n        verbose = TRUE\n      )\n    },\n    error = function(e) {\n      # Handle the error\n      message(\"Error with chunk: \", paste(chunk, collapse = \", \"))\n      message(\"Error message: \", e$message)\n      NULL  # Return NULL for failed chunks\n    }\n  )\n})\n\n\n# Combine rows\nmissing_references$data$combined &lt;- bind_rows(missing_references$data$chunks) %&gt;% \n  mutate(mining_source = \"openalex_rerun_doi\")\n\n\nqs::qsave(missing_references, file = here(\"local_data/missing_references.qs\"))\n\n\n\nQuality control\n\n# Check for duplicates based on OpenAlex ID\nmissing_references$data$combined %&gt;% \n  group_by(id) %&gt;% \n  summarise(n = n()) %&gt;% \n  frq(n, sort.frq = \"desc\")\n\nn &lt;integer&gt; \n# total N=4736 valid N=4736 mean=1.00 sd=0.00\n\nValue |    N | Raw % | Valid % | Cum. %\n---------------------------------------\n    1 | 4736 |   100 |     100 |    100\n &lt;NA&gt; |    0 |     0 |    &lt;NA&gt; |   &lt;NA&gt;\n\n\n\n# Check for duplicates based on DOI\nmissing_references$data$combined %&gt;% \n  distinct(id, .keep_all = TRUE) %&gt;% # exclude ID duplicates\n  filter(!is.na(doi)) %&gt;% # exclude cases without DOI\n  group_by(doi) %&gt;% \n  summarise(n = n()) %&gt;% \n  frq(n, sort.frq = \"desc\")\n\nn &lt;integer&gt; \n# total N=4734 valid N=4734 mean=1.00 sd=0.02\n\nValue |    N | Raw % | Valid % | Cum. %\n---------------------------------------\n    1 | 4732 | 99.96 |   99.96 |  99.96\n    2 |    2 |  0.04 |    0.04 | 100.00\n &lt;NA&gt; |    0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\n\n\nduplicates &lt;- list()\n\n# Extract duplicated IDs\nduplicates$missing_references$combined$doi$string  &lt;- missing_references$data$combined %&gt;%  \n  distinct(id, .keep_all = TRUE) %&gt;% \n  filter(!is.na(doi)) %&gt;%\n  group_by(doi) %&gt;%\n  summarise(n = n()) %&gt;%\n  filter(n &gt; 1) %&gt;% \n  pull(doi)\n\n# Extract cases with duplicated IDs\nduplicates$missing_references$combined$doi$data &lt;- missing_references$data$combined %&gt;% \n  filter(doi %in% duplicates$missing_references$combined$doi$string)\n\n# Extract cases to be deleted\nduplicates$missing_references$combined$doi$delete &lt;- duplicates$missing_references$combined$doi$data %&gt;%\n  mutate(id_number = as.numeric(sub(\".*W\", \"\", id))) %&gt;% \n  group_by(doi) %&gt;% # Group by `doi`\n  slice_min(id_number, n = 1, with_ties = FALSE) %&gt;% \n  select(-id_number) \n\n\nmissing_references$data$raw &lt;- missing_references$data$combined %&gt;%\n  distinct(id, .keep_all = TRUE) %&gt;%  # delete duplicates based on ID \n  anti_join(duplicates$missing_references$combined$doi$delete, by = \"id\")",
    "crumbs": [
      "Datenerhebung",
      "Combine OpenAlex & Scopus"
    ]
  },
  {
    "objectID": "data_mining/ps-01_03-data-remining_openalex.html#merging-openalex-data",
    "href": "data_mining/ps-01_03-data-remining_openalex.html#merging-openalex-data",
    "title": "Combine OpenAlex & Scopus",
    "section": "Merging OpenAlex data",
    "text": "Merging OpenAlex data\n\n# Combine the missing references with the existing data\nreferences$openalex$combined$api &lt;- references$openalex$raw %&gt;%\n  mutate(mining_source = \"openalex_initial\") %&gt;% \n  bind_rows(., missing_references$data$raw)\n\n\nQuality control\n\nreferences$openalex$combined$api %&gt;% \n  skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n33254\n\n\nNumber of columns\n40\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n24\n\n\nlist\n8\n\n\nlogical\n5\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nid\n0\n1.00\n30\n32\n0\n33079\n0\n\n\ntitle\n6\n1.00\n3\n500\n0\n32287\n0\n\n\ndisplay_name\n6\n1.00\n3\n500\n0\n32287\n0\n\n\nab\n2325\n0.93\n0\n51771\n11\n30110\n0\n\n\npublication_date\n0\n1.00\n10\n10\n0\n3052\n0\n\n\nso\n6257\n0.81\n1\n244\n0\n8410\n0\n\n\nso_id\n6257\n0.81\n27\n32\n0\n8436\n0\n\n\nhost_organization\n13463\n0.60\n3\n155\n0\n1308\n0\n\n\nissn_l\n11008\n0.67\n9\n9\n0\n7690\n0\n\n\nurl\n127\n1.00\n21\n273\n0\n32942\n0\n\n\npdf_url\n21198\n0.36\n29\n359\n0\n11940\n0\n\n\nlicense\n22241\n0.33\n3\n21\n0\n11\n0\n\n\nversion\n14375\n0.57\n15\n16\n0\n3\n0\n\n\nfirst_page\n13455\n0.60\n1\n15\n0\n5079\n0\n\n\nlast_page\n13563\n0.59\n1\n15\n0\n5092\n0\n\n\nvolume\n13865\n0.58\n1\n25\n0\n684\n0\n\n\nissue\n17210\n0.48\n1\n30\n0\n646\n0\n\n\noa_status\n0\n1.00\n4\n7\n0\n6\n0\n\n\noa_url\n13664\n0.59\n20\n359\n0\n19241\n0\n\n\nlanguage\n2\n1.00\n2\n2\n0\n8\n0\n\n\ncited_by_api_url\n0\n1.00\n53\n55\n0\n33079\n0\n\n\ndoi\n2024\n0.94\n26\n96\n0\n31055\n0\n\n\ntype\n0\n1.00\n6\n12\n0\n6\n0\n\n\nmining_source\n0\n1.00\n16\n18\n0\n2\n0\n\n\n\nVariable type: list\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nn_unique\nmin_length\nmax_length\n\n\n\n\nauthor\n253\n0.99\n32161\n1\n12\n\n\ngrants\n28882\n0.13\n2780\n0\n33\n\n\ncounts_by_year\n15199\n0.54\n5709\n0\n2\n\n\nids\n0\n1.00\n33079\n1\n5\n\n\nreferenced_works\n10700\n0.68\n22293\n0\n447\n\n\nrelated_works\n403\n0.99\n23309\n1\n20\n\n\nconcepts\n0\n1.00\n33074\n5\n5\n\n\ntopics\n0\n1.00\n32528\n0\n5\n\n\n\nVariable type: logical\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\nis_oa\n124\n1\n0.53\nTRU: 17652, FAL: 15478\n\n\nis_oa_anywhere\n0\n1\n0.59\nTRU: 19524, FAL: 13730\n\n\nany_repository_has_fulltext\n0\n1\n0.29\nFAL: 23747, TRU: 9507\n\n\nis_paratext\n0\n1\n0.00\nFAL: 33254\n\n\nis_retracted\n0\n1\n0.00\nFAL: 33209, TRU: 45\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nrelevance_score\n4734\n0.86\n9.91\n26.91\n0.04\n0.84\n3.13\n7.26\n986.22\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\ncited_by_count\n0\n1.00\n9.42\n38.52\n0.00\n0.00\n1.00\n5.00\n1878.00\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\npublication_year\n0\n1.00\n2021.81\n2.57\n1983.00\n2021.00\n2023.00\n2024.00\n2025.00\n‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñá\n\n\n\n\n\n\n\n\n\n\n\nQuick overview\n\n\n\n\nThe number of missing abstracts has risen. Therefore, the Scopus data will be checked for the possibility of filling in the missing abstracts.\nThe difference in the number of cases and the number of unique IDs indicates that there are duplicates in the data.\n\n\n\n\n# Check for duplicates based on OpenAlex ID\nreferences$openalex$combined$api %&gt;% \n  group_by(id) %&gt;% \n  summarise(n = n()) %&gt;% \n  frq(n, sort.frq = \"desc\")\n\nn &lt;integer&gt; \n# total N=33079 valid N=33079 mean=1.01 sd=0.07\n\nValue |     N | Raw % | Valid % | Cum. %\n----------------------------------------\n    1 | 32904 | 99.47 |   99.47 |  99.47\n    2 |   175 |  0.53 |    0.53 | 100.00\n &lt;NA&gt; |     0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\n\n\n# Extract duplicated IDs\nduplicates$openalex$combined$id$string &lt;- references$openalex$combined$api %&gt;% \n  group_by(id) %&gt;%\n  summarise(n = n()) %&gt;%\n  filter(n &gt; 1) %&gt;% \n  pull(id)\n\n# Extract cases with duplicated IDs\nduplicates$openalex$combined$id$data &lt;- references$openalex$combined$api %&gt;% \n  filter(id %in% duplicates$openalex$combined$id$string ) %&gt;% \n  arrange(id)\n\n\n# Extract uneven (odd) rows\ndf1 &lt;- duplicates$openalex$combined$id$data[seq(1, nrow(duplicates$openalex$combined$id$data), by = 2), ]\ndf2 &lt;- duplicates$openalex$combined$id$data[seq(2, nrow(duplicates$openalex$combined$id$data), by = 2), ]\n\n# Compare the two data frames\nsummary(arsenal::comparedf(df1, df2))\n\n\n\nTable: Summary of data.frames\n\nversion   arg    ncol   nrow\n--------  ----  -----  -----\nx         df1      40    175\ny         df2      40    175\n\n\n\nTable: Summary of overall comparison\n\nstatistic                                                      value\n------------------------------------------------------------  ------\nNumber of by-variables                                             0\nNumber of non-by variables in common                              40\nNumber of variables compared                                      40\nNumber of variables in x but not y                                 0\nNumber of variables in y but not x                                 0\nNumber of variables compared with some values unequal              4\nNumber of variables compared with all values equal                36\nNumber of observations in common                                 175\nNumber of observations in x but not y                              0\nNumber of observations in y but not x                              0\nNumber of observations with some compared variables unequal      175\nNumber of observations with all compared variables equal           0\nNumber of values unequal                                         404\n\n\n\nTable: Variables not shared\n\n                         \n ------------------------\n No variables not shared \n ------------------------\n\n\n\nTable: Other variables not compared\n\n                                 \n --------------------------------\n No other variables not compared \n --------------------------------\n\n\n\nTable: Observations not shared\n\n                            \n ---------------------------\n No observations not shared \n ---------------------------\n\n\n\nTable: Differences detected by variable\n\nvar.x                         var.y                            n   NAs\n----------------------------  ----------------------------  ----  ----\nid                            id                               0     0\ntitle                         title                            0     0\ndisplay_name                  display_name                     0     0\nauthor                        author                           0     0\nab                            ab                               0     0\npublication_date              publication_date                 0     0\nrelevance_score               relevance_score                175   175\nso                            so                               0     0\nso_id                         so_id                            0     0\nhost_organization             host_organization                0     0\nissn_l                        issn_l                           0     0\nurl                           url                              0     0\npdf_url                       pdf_url                          0     0\nlicense                       license                          0     0\nversion                       version                          0     0\nfirst_page                    first_page                       0     0\nlast_page                     last_page                        0     0\nvolume                        volume                           0     0\nissue                         issue                            0     0\nis_oa                         is_oa                            0     0\nis_oa_anywhere                is_oa_anywhere                   0     0\noa_status                     oa_status                        0     0\noa_url                        oa_url                           0     0\nany_repository_has_fulltext   any_repository_has_fulltext      0     0\nlanguage                      language                         0     0\ngrants                        grants                          50    50\ncited_by_count                cited_by_count                   0     0\ncounts_by_year                counts_by_year                   4     4\npublication_year              publication_year                 0     0\ncited_by_api_url              cited_by_api_url                 0     0\nids                           ids                              0     0\ndoi                           doi                              0     0\ntype                          type                             0     0\nreferenced_works              referenced_works                 0     0\nrelated_works                 related_works                    0     0\nis_paratext                   is_paratext                      0     0\nis_retracted                  is_retracted                     0     0\nconcepts                      concepts                         0     0\ntopics                        topics                           0     0\nmining_source                 mining_source                  175     0\n\n\n\nTable: Differences detected (370 not shown)\n\nvar.x             var.y              ..row.names..  values.x           values.y              row.x   row.y\n----------------  ----------------  --------------  -----------------  -------------------  ------  ------\nrelevance_score   relevance_score                1  8.167233           NA                        1       1\nrelevance_score   relevance_score                2  8.486296           NA                        2       2\nrelevance_score   relevance_score                3  280.2531           NA                        3       3\nrelevance_score   relevance_score                4  7.800102           NA                        4       4\nrelevance_score   relevance_score                5  7.819922           NA                        5       5\nrelevance_score   relevance_score                6  17.53957           NA                        6       6\nrelevance_score   relevance_score                7  1.024666           NA                        7       7\nrelevance_score   relevance_score                8  28.30165           NA                        8       8\nrelevance_score   relevance_score                9  30.97463           NA                        9       9\nrelevance_score   relevance_score               10  15.04404           NA                       10      10\ngrants            grants                         7  NA                 NULL                      7       7\ngrants            grants                         8  NA                 NULL                      8       8\ngrants            grants                        10  NA                 NULL                     10      10\ngrants            grants                        11  NA                 NULL                     11      11\ngrants            grants                        14  NA                 NULL                     14      14\ngrants            grants                        15  NA                 NULL                     15      15\ngrants            grants                        18  NA                 NULL                     18      18\ngrants            grants                        20  NA                 NULL                     20      20\ngrants            grants                        21  NA                 NULL                     21      21\ngrants            grants                        24  NA                 NULL                     24      24\ncounts_by_year    counts_by_year               166  NA                 NULL                    166     166\ncounts_by_year    counts_by_year               171  NA                 NULL                    171     171\ncounts_by_year    counts_by_year               172  NA                 NULL                    172     172\ncounts_by_year    counts_by_year               174  NA                 NULL                    174     174\nmining_source     mining_source                  1  openalex_initial   openalex_rerun_doi        1       1\nmining_source     mining_source                  2  openalex_initial   openalex_rerun_doi        2       2\nmining_source     mining_source                  3  openalex_initial   openalex_rerun_doi        3       3\nmining_source     mining_source                  4  openalex_initial   openalex_rerun_doi        4       4\nmining_source     mining_source                  5  openalex_initial   openalex_rerun_doi        5       5\nmining_source     mining_source                  6  openalex_initial   openalex_rerun_doi        6       6\nmining_source     mining_source                  7  openalex_initial   openalex_rerun_doi        7       7\nmining_source     mining_source                  8  openalex_initial   openalex_rerun_doi        8       8\nmining_source     mining_source                  9  openalex_initial   openalex_rerun_doi        9       9\nmining_source     mining_source                 10  openalex_initial   openalex_rerun_doi       10      10\n\n\n\nTable: Non-identical attributes\n\n                             \n ----------------------------\n No non-identical attributes \n ----------------------------\n\n\n\n# Check for duplicates based on DOI\nreferences$openalex$combined$api %&gt;% \n  distinct(id, .keep_all = TRUE) %&gt;% # exclude ID duplicates\n  filter(!is.na(doi)) %&gt;% # exclude cases without DOI\n  group_by(doi) %&gt;% \n  summarise(n = n()) %&gt;% \n  frq(n, sort.frq = \"desc\")\n\nn &lt;integer&gt; \n# total N=31055 valid N=31055 mean=1.00 sd=0.00\n\nValue |     N | Raw % | Valid % | Cum. %\n----------------------------------------\n    1 | 31055 |   100 |     100 |    100\n &lt;NA&gt; |     0 |     0 |    &lt;NA&gt; |   &lt;NA&gt;\n\n\n\n\nreferences$openalex$combined$raw &lt;- references$openalex$combined$api %&gt;%\n  distinct(id, .keep_all = TRUE) \n\n\n\nMissing abstracts\n\n# Identify cases with NA values in the variable ab\nna_abstracts &lt;- references$openalex$combined$raw %&gt;%\n  filter(is.na(ab)) \n\n\n# Check if Scopus data provides an abstract for those references\nna_abstracts_with_scopus &lt;- na_abstracts %&gt;%\n  mutate(doi_short = str_remove(doi, \"https://doi.org/\")) %&gt;% \n  left_join(scopus$raw %&gt;% \n              select(doi, abstract),\n              by = join_by(doi_short == doi)) %&gt;%\n  mutate(ab = ifelse(is.na(ab), abstract, ab)) %&gt;%\n  select(-abstract)\n\n# Update the combined references with the new abstracts from Scopus\nreferences$openalex$combined$raw_updated &lt;- references$openalex$combined$raw %&gt;%\n  left_join(na_abstracts_with_scopus %&gt;% select(id, ab), by = \"id\", suffix = c(\"\", \"_updated\")) %&gt;%\n  mutate(ab = ifelse(is.na(ab), ab_updated, ab)) %&gt;%\n  select(-ab_updated)",
    "crumbs": [
      "Datenerhebung",
      "Combine OpenAlex & Scopus"
    ]
  },
  {
    "objectID": "data_mining/ps-01_03-data-remining_openalex.html#create-correct-data",
    "href": "data_mining/ps-01_03-data-remining_openalex.html#create-correct-data",
    "title": "Combine OpenAlex & Scopus",
    "section": "Create correct data",
    "text": "Create correct data\n\n# Overview\nreferences$openalex$combined$raw_updated %&gt;% \n  skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n33079\n\n\nNumber of columns\n40\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n24\n\n\nlist\n8\n\n\nlogical\n5\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nid\n0\n1.00\n30\n32\n0\n33079\n0\n\n\ntitle\n6\n1.00\n3\n500\n0\n32287\n0\n\n\ndisplay_name\n6\n1.00\n3\n500\n0\n32287\n0\n\n\nab\n1429\n0.96\n0\n51771\n11\n30999\n0\n\n\npublication_date\n0\n1.00\n10\n10\n0\n3052\n0\n\n\nso\n6193\n0.81\n1\n244\n0\n8410\n0\n\n\nso_id\n6193\n0.81\n27\n32\n0\n8436\n0\n\n\nhost_organization\n13382\n0.60\n3\n155\n0\n1308\n0\n\n\nissn_l\n10936\n0.67\n9\n9\n0\n7690\n0\n\n\nurl\n127\n1.00\n21\n273\n0\n32942\n0\n\n\npdf_url\n21052\n0.36\n29\n359\n0\n11940\n0\n\n\nlicense\n22097\n0.33\n3\n21\n0\n11\n0\n\n\nversion\n14238\n0.57\n15\n16\n0\n3\n0\n\n\nfirst_page\n13387\n0.60\n1\n15\n0\n5079\n0\n\n\nlast_page\n13495\n0.59\n1\n15\n0\n5092\n0\n\n\nvolume\n13788\n0.58\n1\n25\n0\n684\n0\n\n\nissue\n17112\n0.48\n1\n30\n0\n646\n0\n\n\noa_status\n0\n1.00\n4\n7\n0\n6\n0\n\n\noa_url\n13547\n0.59\n20\n359\n0\n19241\n0\n\n\nlanguage\n2\n1.00\n2\n2\n0\n8\n0\n\n\ncited_by_api_url\n0\n1.00\n53\n55\n0\n33079\n0\n\n\ndoi\n2024\n0.94\n26\n96\n0\n31055\n0\n\n\ntype\n0\n1.00\n6\n12\n0\n6\n0\n\n\nmining_source\n0\n1.00\n16\n18\n0\n2\n0\n\n\n\nVariable type: list\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nn_unique\nmin_length\nmax_length\n\n\n\n\nauthor\n253\n0.99\n32161\n1\n12\n\n\ngrants\n28778\n0.13\n2780\n0\n33\n\n\ncounts_by_year\n15151\n0.54\n5709\n0\n2\n\n\nids\n0\n1.00\n33079\n1\n5\n\n\nreferenced_works\n10682\n0.68\n22293\n0\n447\n\n\nrelated_works\n403\n0.99\n23309\n1\n20\n\n\nconcepts\n0\n1.00\n33074\n5\n5\n\n\ntopics\n0\n1.00\n32528\n0\n5\n\n\n\nVariable type: logical\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\nis_oa\n124\n1\n0.53\nTRU: 17614, FAL: 15341\n\n\nis_oa_anywhere\n0\n1\n0.59\nTRU: 19465, FAL: 13614\n\n\nany_repository_has_fulltext\n0\n1\n0.29\nFAL: 23606, TRU: 9473\n\n\nis_paratext\n0\n1\n0.00\nFAL: 33079\n\n\nis_retracted\n0\n1\n0.00\nFAL: 33034, TRU: 45\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nrelevance_score\n4559\n0.86\n9.91\n26.91\n0.04\n0.84\n3.13\n7.26\n986.22\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\ncited_by_count\n0\n1.00\n9.41\n38.59\n0.00\n0.00\n1.00\n5.00\n1878.00\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\npublication_year\n0\n1.00\n2021.81\n2.58\n1983.00\n2021.00\n2023.00\n2024.00\n2025.00\n‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñá\n\n\n\n\n\n\nreferences$openalex$combined$raw_updated %&gt;% \n  frq(type, language)\n\ntype &lt;character&gt; \n# total N=33079 valid N=33079 mean=1.55 sd=1.38\n\nValue        |     N | Raw % | Valid % | Cum. %\n-----------------------------------------------\narticle      | 28539 | 86.28 |   86.28 |  86.28\nbook-chapter |    40 |  0.12 |    0.12 |  86.40\neditorial    |     2 |  0.01 |    0.01 |  86.40\nletter       |     1 |  0.00 |    0.00 |  86.41\npreprint     |  4369 | 13.21 |   13.21 |  99.61\nreview       |   128 |  0.39 |    0.39 | 100.00\n&lt;NA&gt;         |     0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\nlanguage &lt;character&gt; \n# total N=33079 valid N=33077 mean=2.00 sd=0.06\n\nValue |     N | Raw % | Valid % | Cum. %\n----------------------------------------\nde    |     1 |  0.00 |    0.00 |   0.00\nen    | 33052 | 99.92 |   99.92 |  99.93\nes    |    14 |  0.04 |    0.04 |  99.97\nfr    |     4 |  0.01 |    0.01 |  99.98\nit    |     3 |  0.01 |    0.01 |  99.99\nnl    |     1 |  0.00 |    0.00 |  99.99\npt    |     1 |  0.00 |    0.00 | 100.00\nsv    |     1 |  0.00 |    0.00 | 100.00\n&lt;NA&gt;  |     2 |  0.01 |    &lt;NA&gt; |   &lt;NA&gt;\n\n\n\nreferences$openalex$correct &lt;- references$openalex$combined$raw_updated %&gt;% \n  filter(type %in% c(\"article\", \"conference-paper\", \"preprint\")) %&gt;% \n  filter(language == \"en\") %&gt;% \n  filter(publication_year &gt;= 2016) %&gt;%\n  mutate(\n  # Create additional factor variables\n    publication_year_fct = as.factor(publication_year), \n    type_fct = as.factor(type), \n  # Clean abstracts\n    ab = ab %&gt;%\n      str_replace_all(\"\\ufffe\", \"\") %&gt;%    # Remove invalid U+FFFE characters\n      str_replace_all(\"[^\\x20-\\x7E\\n]\", \"\") %&gt;% # Optional: Remove other non-ASCII chars\n      iconv(from = \"UTF-8\", to = \"UTF-8\", sub = \"\"), # Ensure UTF-8 encoding\n  )",
    "crumbs": [
      "Datenerhebung",
      "Combine OpenAlex & Scopus"
    ]
  },
  {
    "objectID": "data_mining/ps-01_03-data-remining_openalex.html#export-data",
    "href": "data_mining/ps-01_03-data-remining_openalex.html#export-data",
    "title": "Combine OpenAlex & Scopus",
    "section": "Export data",
    "text": "Export data\n\nqs::qsave(references$openalex$correct, file = here(\"local_data/references.qs\"))\nqs::qsave(references, file = here(\"local_data/references_full.qs\"))\n\n\nreferences_bibliometrix &lt;- oa2bibliometrix(references$openalex$correct)\nsaveRDS(references_bibliometrix, file = here(\"local_data/references_import_bibliometrix.RDS\"))",
    "crumbs": [
      "Datenerhebung",
      "Combine OpenAlex & Scopus"
    ]
  },
  {
    "objectID": "data_mining/ps-01_02-data-mining_scopus.html",
    "href": "data_mining/ps-01_02-data-mining_scopus.html",
    "title": "API mining: Scopus",
    "section": "",
    "text": "# Load necessary packages\npacman::p_load(\n  here, qs, \n  magrittr, janitor,\n  easystats, sjmisc,\n  ggpubr, \n  openalexR, bibliometrix, \n  tidyverse\n)\n\n# Load custom functions\n# See https://github.com/christopherBelter/scopusAPI) for details\nsource(here(\"R/scopusAPI.R\"))",
    "crumbs": [
      "Datenerhebung",
      "API Mining: Scopus"
    ]
  },
  {
    "objectID": "data_mining/ps-01_02-data-mining_scopus.html#preparation",
    "href": "data_mining/ps-01_02-data-mining_scopus.html#preparation",
    "title": "API mining: Scopus",
    "section": "",
    "text": "# Load necessary packages\npacman::p_load(\n  here, qs, \n  magrittr, janitor,\n  easystats, sjmisc,\n  ggpubr, \n  openalexR, bibliometrix, \n  tidyverse\n)\n\n# Load custom functions\n# See https://github.com/christopherBelter/scopusAPI) for details\nsource(here(\"R/scopusAPI.R\"))",
    "crumbs": [
      "Datenerhebung",
      "API Mining: Scopus"
    ]
  },
  {
    "objectID": "data_mining/ps-01_02-data-mining_scopus.html#mining-scopus-api",
    "href": "data_mining/ps-01_02-data-mining_scopus.html#mining-scopus-api",
    "title": "API mining: Scopus",
    "section": "Mining Scopus API",
    "text": "Mining Scopus API\nSee scopusAPI for information about the custom functions.\n\n# Load API credentials\nscopus_api_key &lt;- Sys.getenv(\"Elsevier_API\")\n\n\nCreate search query\n\nscopus_query &lt;- 'TITLE-ABS-KEY ( ( \"artificial intelligence\" OR ai OR \"chatbot\" OR \"AI-based chatbot\" OR \"artificial intelligence-based chatbot\" OR \"chat agent\" OR \"voice bot\" OR \"voice assistant\" OR \"voice-based assistant\" OR \"conversational agent\" OR \"conversational assistant\" OR \"conversational AI\" OR \"AI-based assistant\" OR \"artificial intelligence-based assistant\" OR \"virtual assistant\" OR \"intelligent assistant\" OR \"digital assistant\" OR \"smart speaker\" OR chatgpt OR \"google gemini\" OR \"google bard\" OR \"bing chat\" OR \"microsoft copilot\" OR \"claude ai\" OR \"perplexity ai\" ) AND ( anthropomorphism OR humanlike OR humanness OR humanized OR \"user experience\" OR ux OR usability OR trust* OR \"conversational experience\" OR cux OR \"conversation design\" OR safety OR privacy ) ) AND (SUBJAREA(SOCI) OR SUBJAREA(PSYC)) AND LANGUAGE(\"English\")'\n\n\n\nSearch and fetch data\n\nscopus_xml &lt;- searchByString(\n  string = scopus_query,\n  outfile = here(\"local_data/scopus_API_export.xml\"))\n\n\n\nConvert .xml to data frame\n\nscopus &lt;- list(\n  xml = scopus_xml, \n  api = extractXML(scopus_xml)\n  )",
    "crumbs": [
      "Datenerhebung",
      "API Mining: Scopus"
    ]
  },
  {
    "objectID": "data_mining/ps-01_02-data-mining_scopus.html#quality-control",
    "href": "data_mining/ps-01_02-data-mining_scopus.html#quality-control",
    "title": "API mining: Scopus",
    "section": "Quality control",
    "text": "Quality control\n\nscopus$api %&gt;% \n  skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n7721\n\n\nNumber of columns\n16\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n16\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nscopusID\n0\n1.00\n17\n18\n0\n7721\n0\n\n\ndoi\n701\n0.91\n13\n69\n0\n7015\n0\n\n\npmid\n7256\n0.06\n7\n10\n0\n465\n0\n\n\nauthors\n0\n1.00\n5\n812\n0\n7436\n0\n\n\naffiliations\n0\n1.00\n2\n2364\n0\n5882\n0\n\n\ncountries\n0\n1.00\n0\n180\n5\n1256\n0\n\n\nyear\n0\n1.00\n4\n4\n0\n40\n0\n\n\narticletitle\n0\n1.00\n9\n614\n0\n7700\n0\n\n\njournal\n0\n1.00\n4\n432\n0\n2497\n0\n\n\nvolume\n2624\n0.66\n1\n15\n0\n381\n0\n\n\nissue\n4268\n0.45\n1\n20\n0\n130\n0\n\n\npages\n0\n1.00\n0\n11\n2967\n4176\n0\n\n\nkeywords\n1449\n0.81\n11\n939\n0\n6259\n0\n\n\nabstract\n86\n0.99\n161\n13294\n0\n7616\n0\n\n\nptype\n0\n1.00\n4\n17\n0\n13\n0\n\n\ntimescited\n0\n1.00\n1\n4\n0\n247\n0\n\n\n\n\n\n\n\n\n\n\n\nQuick overview\n\n\n\n\nNearly all references have an abstract (¬±¬†98 percent) and DOI (¬±¬†90 percent), which are the critical information for the analysis.\n\n\n\n\nCheck duplicates\n\n# Check for duplicates based on DOI\nscopus$api %&gt;% \n  filter(!is.na(doi)) %&gt;% # exclude cases without DOI\n  group_by(doi) %&gt;% \n  summarise(n = n()) %&gt;% \n  frq(n, sort.frq = \"desc\")\n\nn &lt;integer&gt; \n# total N=7015 valid N=7015 mean=1.00 sd=0.03\n\nValue |    N | Raw % | Valid % | Cum. %\n---------------------------------------\n    1 | 7010 | 99.93 |   99.93 |  99.93\n    2 |    5 |  0.07 |    0.07 | 100.00\n &lt;NA&gt; |    0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\n\n\nduplicates &lt;- list()\n\n# Extract duplicated IDs\nduplicates$api$doi$string &lt;- scopus$api %&gt;%  \n  filter(!is.na(doi)) %&gt;%\n  group_by(doi) %&gt;%\n  summarise(n = n()) %&gt;%\n  filter(n &gt; 1) %&gt;% \n  pull(doi)\n\n# Extract cases with duplicated IDs\nduplicates$api$doi$data &lt;- scopus$api %&gt;% \n  filter(doi %in% duplicates$api$doi$string)\n\n\n\n\n\n\n\nSummary\n\n\n\n\nThe duplicates based on the DOI do not seem to follow a systematic pattern. Therefore, DOI duplicates are eliminated with distinct().",
    "crumbs": [
      "Datenerhebung",
      "API Mining: Scopus"
    ]
  },
  {
    "objectID": "data_mining/ps-01_02-data-mining_scopus.html#transformation",
    "href": "data_mining/ps-01_02-data-mining_scopus.html#transformation",
    "title": "API mining: Scopus",
    "section": "Transformation",
    "text": "Transformation\n\nscopus$raw &lt;- scopus$api %&gt;% \n  distinct(doi, .keep_all = TRUE) %&gt;% \n  filter(ptype %in% c(\"Article\", \"Conference Paper\"))\n\n\nCheck transformation\n\nscopus$raw %&gt;% \n  filter(!is.na(doi)) %&gt;% # exclude cases without DOI\n  group_by(doi) %&gt;% \n  summarise(n = n()) %&gt;% \n  frq(n, sort.frq = \"desc\")\n\nn &lt;integer&gt; \n# total N=5667 valid N=5667 mean=1.00 sd=0.00\n\nValue |    N | Raw % | Valid % | Cum. %\n---------------------------------------\n    1 | 5667 |   100 |     100 |    100\n &lt;NA&gt; |    0 |     0 |    &lt;NA&gt; |   &lt;NA&gt;",
    "crumbs": [
      "Datenerhebung",
      "API Mining: Scopus"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "KI nutzen, um KI-Folgen zu verstehen",
    "section": "",
    "text": "Generative und kommunikative KI (bspw. ChatGPT, Bing, Gemini) ist derzeit in aller Munde. Den KI-Innovationen der letzten Jahre wird als ‚Äûgeneral purpose technology‚Äú eine enorme transformative Kraft zugeschrieben, mit potenziell weitreichenden Folgen f√ºr Individuen, Wirtschaft und Gesellschaft. Die sozialwissenschaftliche Forschung auf diesem Gebiet entwickelt sich ‚Äì ebenso wie ihr soziotechnologischer Gegenstand ‚Äì rasant. Binnen k√ºrzester Zeit entstehen weltweit hunderte Studien, die die individuellen und gesellschaftlichen Implikationen von generativer KI aus psychologischer, kommunikationswissenschaftlicher oder soziologischer Perspektive untersuchen. Gleichzeitig ist der Bedarf an evidenzbasierten Prognosen √ºber m√∂gliche Folgen von generativer KI gro√ü, etwa im Kontext von Regulierungsvorhaben wie dem AI Act der EU. Wie jedoch lassen sich die rasant anwachsende Forschung kosten- und ressourceneffizient √ºberblicken und Handlungsempfehlungen aus ihr ableiten?\nEine m√∂gliche Antwort liegt in der KI selbst. In diesem Projektseminar wollen wir daher den Versuch wagen, die aktuelle Forschung zu gesellschaftlichen Folgen von generativer und kommunikativer KI mit Hilfe von KI-gest√ºtzten Systematic Review Methoden schnell und zuverl√§ssig zu synthetisieren (siehe begleitendes Seminar Angewandte Methoden)."
  },
  {
    "objectID": "slides/ms-slides-07.html#seminarplan",
    "href": "slides/ms-slides-07.html#seminarplan",
    "title": "Introduction to Text as Data",
    "section": "Seminarplan",
    "text": "Seminarplan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSitzung\nDatum\nThema (synchron)\n√úbung (asynchron)\nDozent:in\n\n\n\n\n1\n18.04.2024\nEinf√ºhrung & √úberblick\n\nAM & CA\n\n\n\nüìö\nTeil 1: Systematic Review\n\n\n\n\n2\n25.04.2024\nEinf√ºhrung in Systematic Reviews I\nR-Einf√ºhrung\nAM\n\n\n3\n02.05.2024\nEinf√ºhrung in Systematic Reviews II\nR-Einf√ºhrung\nAM\n\n\n\n09.05.2024\nüèñÔ∏è Feiertag\nR-Einf√ºhrung\n\n\n\n4\n16.05.2024\nAutomatisierung von SRs & KI-Tools\nR-Einf√ºhrung\nAM\n\n\n\n23.05.2024\nüçª WiSo-Projekt-Woche\nR-Einf√ºhrung\n\n\n\n5\n04.06.2024\nüçï Gastvortrag: Prof.¬†Dr.¬†Emese Domahidi\nR-Einf√ºhrung\nED\n\n\n6\n06.06.2024\nAutomatisierung von SRs & KI-Tools\nR-Einf√ºhrung\nAM\n\n\n\nüíª\nTeil 2: Text as Data & Unsupervised Machine Learning\n\n\n\n\n7\n13.06.2024\nIntroduction to Text as Data\nzur Sitzung\nCA\n\n\n8\n20.06.2024\nText processing\nzur Sitzung\nCA\n\n\n9\n27.06.2024\nUnsupervised Machine Learning I\nzur Sitzung\nCA\n\n\n10\n04.07.2024\nUnsupervised Machine Learning II\nzur Sitzung\nCA & AM\n\n\n11\n11.07.2024\nRecap & Ausblick\nzur Sitzung\nCA & AM\n\n\n12\n18.07.2024\nüèÅ Semesterabschluss\nzur Sitzung\nCA & AM"
  },
  {
    "objectID": "slides/ms-slides-07.html#erst-input-dann-vertiefung",
    "href": "slides/ms-slides-07.html#erst-input-dann-vertiefung",
    "title": "Introduction to Text as Data",
    "section": "Erst Input, dann Vertiefung",
    "text": "Erst Input, dann Vertiefung\nTypischer Aufbau der n√§chsten vier Sitzungen\nTeil 1Ô∏è‚É£: Input (ca. 30-45 Minuten)\n\nVorstellung der ‚Äútheoretischen Grundlagen‚Äù inklusive zentraler Begriffe und Konzepte\nVorstellung der Methode(n) sowie des Kontext der praktischen Anwendung\n\nTeil 2Ô∏è‚É£: Praktische Anwendung (ca. 45-60 Minuten)\n\nVertiefung der Inhalte durch Bearbeitung kleiner Aufgaben, entweder in Einzel- oder Gruppenarbeit\nAufgaben zur Arbeit mit R, die im Kurs angefangen aber (vermutlich) au√üerhalb der Sitzung abgeschlossen werden"
  },
  {
    "objectID": "slides/ms-slides-07.html#bitte-rstudio-server-benutzen",
    "href": "slides/ms-slides-07.html#bitte-rstudio-server-benutzen",
    "title": "Introduction to Text as Data",
    "section": "Bitte RStudio Server benutzen!",
    "text": "Bitte RStudio Server benutzen!\nInformation zur Nutzung des RStudio Servers w√§hrend der Sitzung\n\n\n‚è∞ Zur Erinnerung:\n\nFunktion der RStudio-Projekte f√ºr die praktische Anwendung in Serverumgebung getestet\nNutzung des RStudio Servers vermeidet aufwendiges & zeitraubendes Trouble-Shooting\n\n\n\n\n‚ÑπÔ∏è Infos zum RStudio Server:\n\nNutzung nur √ºber W-LAN der FAU (ggf. mit aktivem VPN) m√∂glich\nVerf√ºgbar unter: http://10.204.20.178:8787\nZugangsdaten auf Teams\n\n\n\n\n\n\n\n\nBei Problemen: Fragen in den Teams-Kanal!"
  },
  {
    "objectID": "slides/ms-slides-07.html#was-versteht-ihr-unter-text-as-data",
    "href": "slides/ms-slides-07.html#was-versteht-ihr-unter-text-as-data",
    "title": "Introduction to Text as Data",
    "section": "Was versteht ihr unter Text as Data?",
    "text": "Was versteht ihr unter Text as Data?\nBitte nehmt an einer kurzen Umfrage teil\n\n\n\nBitte scannt den QR-Code oder nutzt den folgenden Link f√ºr die Teilnahme an einer kurzen Umfrage:\n\nhttps://www.menti.com/aly1gft3568e\nTemporary Access Code: 8113 5474\n\n\n\n\n \n\n    \n\n\n\n\n‚àí+\n01:00"
  },
  {
    "objectID": "slides/ms-slides-07.html#ergebnis",
    "href": "slides/ms-slides-07.html#ergebnis",
    "title": "Introduction to Text as Data",
    "section": "Ergebnis",
    "text": "Ergebnis"
  },
  {
    "objectID": "slides/ms-slides-07.html#altes-ph√§nomen-neue-dimension",
    "href": "slides/ms-slides-07.html#altes-ph√§nomen-neue-dimension",
    "title": "Introduction to Text as Data",
    "section": "Altes Ph√§nomen, neue Dimension",
    "text": "Altes Ph√§nomen, neue Dimension\nHintergrund zu dem Ph√§nomen Text as Data\n\n\nLange Tradition der Text- und Inhaltsanalyse\nNeue Chancen & Herausforderungen durch explosionsartige Vergr√∂√üerung des (Text-)Datenaufkommen und deren Verf√ºgbarkeit in den letzten Jahren (Websites, Plattformen & Digitalisierung) Verf√ºgbarkeit von (neuen) Datenquellen als Resultat der Digitalisierung"
  },
  {
    "objectID": "slides/ms-slides-07.html#from-text-to-data-to-data-analysis",
    "href": "slides/ms-slides-07.html#from-text-to-data-to-data-analysis",
    "title": "Introduction to Text as Data",
    "section": "From text to data to data analysis",
    "text": "From text to data to data analysis\nTransformation als essenzieller Bestandteil von Text as Data (Curini & Franzese, 2020)"
  },
  {
    "objectID": "slides/ms-slides-07.html#review-√ºber-literaturreviews",
    "href": "slides/ms-slides-07.html#review-√ºber-literaturreviews",
    "title": "Introduction to Text as Data",
    "section": "Review √ºber Literaturreviews",
    "text": "Review √ºber Literaturreviews\nGrundidee, Ziel und Schwerpunkte und der kommenden Sitzungen zu Text as data\n\nIdee: √úberblick √ºber Literatur zu Literatur√ºberblicken verschaffen\nZiel: Durchf√ºhrung einer Kombination aus (elaboriertem) Scoping Search & Scoping Review\nFokus: Computergest√ºtze Umsetzung m√∂glichst vieler Bestandteile des Review-Workflows, wie z.B.:\n\nEigene Datenerhebung via (OpenAlex-)API\nOberfl√§chliche bibliometrische Analyse (zur Datenexploration und -bereinigung)\nAnalye der Abstracts mit Hilfe von un√ºberwachtem Machine Learning (Topic Modeling)"
  },
  {
    "objectID": "slides/ms-slides-07.html#was-war-das-nochmal",
    "href": "slides/ms-slides-07.html#was-war-das-nochmal",
    "title": "Introduction to Text as Data",
    "section": "Was war das nochmal?",
    "text": "Was war das nochmal?\nZur Erinnerung: Definition von Scoping Search und Scoping Review\nüîé Scoping searches ‚Ä¶\n\n\nschnelle explorative Suche, die sich auf bestimmtes Konzept konzentriert\nsind oft Teil der Suchstringentwicklung\nsind nicht ausreichend f√ºr ein systematic review\n\n\n\n‚Ä¶ Scoping Review üìã\n\n\n\neine spezifische Form eines literature reviews\nmap a vast body of research literature in a field of interest in terms of the volume, nature, and characteristics of the primary research (Pham et al., 2014)\ndo not aim to produce a critically appraised and synthesised result/answer to a particular question, [they] rather aim to provide an overview or map of the evidence (Munn et al., 2018)"
  },
  {
    "objectID": "slides/ms-slides-07.html#what-we-do-not-do",
    "href": "slides/ms-slides-07.html#what-we-do-not-do",
    "title": "Introduction to Text as Data",
    "section": "What we (do not) do",
    "text": "What we (do not) do\nDisclaimer zum Inhalt und der Zielsetzung des Sitzungen zu Text as Data\n\n\n‚ùå Kein vollst√§ndig dokumentiertes Scoping Review, dass alle notwendigen (SALSA-)Schritte in vollem Umfang und nach wissenschaftlichen Standarts durchl√§uft\n\n‚ùå Keine umfassendes Einf√ºhrung in die Textanalyse mit R\n\n\n\n‚úÖ Exemplarische Darstellung einzelner Schritte des Workflows, mit Fokus auf die computergest√ºtzte Umsetzung\n \n‚úÖ √úberblick √ºber verschiedene Verfahren, mit Schwerpunkt auf Methoden, die im Kontext von Literaturreviews notwendig und n√ºtzlich sind"
  },
  {
    "objectID": "slides/ms-slides-07.html#wer-oder-was-ist-openalex",
    "href": "slides/ms-slides-07.html#wer-oder-was-ist-openalex",
    "title": "Introduction to Text as Data",
    "section": "Wer oder was ist OpenAlex?",
    "text": "Wer oder was ist OpenAlex?\nKurze Vorstellung und Hintergrundinformationen zur Datenquelle (OpenAlex)\n\n\nopen(-source) catalog of the world‚Äôs scholarly research system\ndata is free and reusable, available via bulk download or API\ngoverned by a sustainable and transparent nonprofit"
  },
  {
    "objectID": "slides/ms-slides-07.html#first-scoping-search",
    "href": "slides/ms-slides-07.html#first-scoping-search",
    "title": "Introduction to Text as Data",
    "section": "First scoping search",
    "text": "First scoping search\nSichtung der Daten- und Identfikation der Analysegrundlage\n\n\n\n\nEine simple Suchquery resultiert zwar in sehr vielen Treffern, bringt aber auch (praktische) Probleme mit sich:\n\nDeutliche √úberschreibung des t√§gliches API-Limit betr√§gt 100.000 Referenzen\n‚ÄúLokale‚Äù Datenbearbeitung und -analye ben√∂tigt bei der Menge an Daten ehrhebliche Rechenkapazit√§t\nL√∂sung: Optimierung der Suchquery durch Spezifizierung des Untersuchungsgegenstandes"
  },
  {
    "objectID": "slides/ms-slides-07.html#fine-tuning-der-search-query",
    "href": "slides/ms-slides-07.html#fine-tuning-der-search-query",
    "title": "Introduction to Text as Data",
    "section": "Fine-Tuning der Search Query",
    "text": "Fine-Tuning der Search Query\n√úberblick √ºber verwendete Search Query und ausgew√§hlte deskriptive Statistiken\n\n\n\n\n\n\n\n\n\n\n    \nLink zur Suche"
  },
  {
    "objectID": "slides/ms-slides-07.html#openalex-openalexr",
    "href": "slides/ms-slides-07.html#openalex-openalexr",
    "title": "Introduction to Text as Data",
    "section": "OpenAlex ü§ù openalexR",
    "text": "OpenAlex ü§ù openalexR\nZusammenspiel aus Datenbank und R-Package openalexR (Aria et al., 2024)\n\n\n\n\n\n\n\n\n\n\nopenalexR helps you interface with the OpenAlex API to retrieve bibliographic information about publications, authors, institutions, sources, funders, publishers, topics and concepts.\n\n\n\n\nManueller Export von Ergebnissen mit Hilfe des Web-Interface von OpenAlex m√∂glich, im Bulk aber umst√§ndlich\nSelbstst√§ndige Interaktion mit API ist aufwendig: Design der Query, Programmierung der Abfrage, Verarbeitung der Daten (nicht im Tabellenformat verf√ºgbar)\nR-Package bietet niedrigschwelligere Alternative f√ºr API-Abfragen"
  },
  {
    "objectID": "slides/ms-slides-07.html#reproduktion-der-webabfrage-mit-r",
    "href": "slides/ms-slides-07.html#reproduktion-der-webabfrage-mit-r",
    "title": "Introduction to Text as Data",
    "section": "Reproduktion der Webabfrage mit R",
    "text": "Reproduktion der Webabfrage mit R\nAbfrage, Download und Transformation der Daten mit einer Funktion\n\n# Download data via API\nreview_works &lt;- openalexR::oa_fetch(\n  entity = \"works\",\n  title.search = \"(literature OR systematic) AND review\",\n  primary_topic.domain.id = \"domains/2\", # Social Science\n  publication_year = \"2013 - 2023\",\n  verbose = TRUE\n)\n\n# Overview\nreview_works\n\n\n\n# A tibble: 93,655 √ó 39\n  id      title display_name author ab    publication_date relevance_score so   \n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;        &lt;list&gt; &lt;chr&gt; &lt;chr&gt;                      &lt;dbl&gt; &lt;chr&gt;\n1 https:‚Ä¶ The ‚Ä¶ The PRISMA ‚Ä¶ &lt;df&gt;   The ‚Ä¶ 2021-03-29                 1625. BMJ  \n2 https:‚Ä¶ Pref‚Ä¶ Preferred r‚Ä¶ &lt;df&gt;   Syst‚Ä¶ 2015-01-01                 1340. Syst‚Ä¶\n3 https:‚Ä¶ Rayy‚Ä¶ Rayyan‚Äîa we‚Ä¶ &lt;df&gt;   Synt‚Ä¶ 2016-12-01                 1314. Syst‚Ä¶\n4 https:‚Ä¶ Syst‚Ä¶ Systematic ‚Ä¶ &lt;df&gt;   Scop‚Ä¶ 2018-11-19                  990. BMC ‚Ä¶\n5 https:‚Ä¶ Upda‚Ä¶ Updated gui‚Ä¶ &lt;df&gt;   On a‚Ä¶ 2019-10-03                  963. Coch‚Ä¶\n# ‚Ñπ 93,650 more rows\n# ‚Ñπ 31 more variables: so_id &lt;chr&gt;, host_organization &lt;chr&gt;, issn_l &lt;chr&gt;,\n#   url &lt;chr&gt;, pdf_url &lt;chr&gt;, license &lt;chr&gt;, version &lt;chr&gt;, first_page &lt;chr&gt;,\n#   last_page &lt;chr&gt;, volume &lt;chr&gt;, issue &lt;chr&gt;, is_oa &lt;lgl&gt;,\n#   is_oa_anywhere &lt;lgl&gt;, oa_status &lt;chr&gt;, oa_url &lt;chr&gt;,\n#   any_repository_has_fulltext &lt;lgl&gt;, language &lt;chr&gt;, grants &lt;list&gt;,\n#   cited_by_count &lt;int&gt;, counts_by_year &lt;list&gt;, publication_year &lt;int&gt;, ‚Ä¶"
  },
  {
    "objectID": "slides/ms-slides-07.html#das-ergebnis-der-abfrage",
    "href": "slides/ms-slides-07.html#das-ergebnis-der-abfrage",
    "title": "Introduction to Text as Data",
    "section": "Das Ergebnis der Abfrage",
    "text": "Das Ergebnis der Abfrage\nFl√ºchtiger Blick auf den R-Datensatz inklusive erster Qualti√§tspr√ºfung\n\n\n\nTypische √úberpr√ºfungen\n\n\n\n\nWie viele F√§lle sind enthalten? Wie viele Variablen? Sind die Variablennamen aussagekr√§ftig?\nWelchen Typ hat jede Variable, z. B. numerisch, kategorisch, logisch?\n\n\n\n\n\n\nreview_works %&gt;% glimpse()\n\nRows: 93,655\nColumns: 39\n$ id                          &lt;chr&gt; \"https://openalex.org/W3118615836\", \"https‚Ä¶\n$ title                       &lt;chr&gt; \"The PRISMA 2020 statement: an updated gui‚Ä¶\n$ display_name                &lt;chr&gt; \"The PRISMA 2020 statement: an updated gui‚Ä¶\n$ author                      &lt;list&gt; [&lt;data.frame[26 x 12]&gt;], [&lt;data.frame[8 x‚Ä¶\n$ ab                          &lt;chr&gt; \"The Preferred Reporting Items for Systema‚Ä¶\n$ publication_date            &lt;chr&gt; \"2021-03-29\", \"2015-01-01\", \"2016-12-01\", ‚Ä¶\n$ relevance_score             &lt;dbl&gt; 1625.1708, 1340.1902, 1314.3904, 990.4521,‚Ä¶\n$ so                          &lt;chr&gt; \"BMJ\", \"Systematic reviews\", \"Systematic r‚Ä¶\n$ so_id                       &lt;chr&gt; \"https://openalex.org/S4393917726\", \"https‚Ä¶\n$ host_organization           &lt;chr&gt; NA, \"BioMed Central\", \"BioMed Central\", \"B‚Ä¶\n$ issn_l                      &lt;chr&gt; \"1756-1833\", \"2046-4053\", \"2046-4053\", \"14‚Ä¶\n$ url                         &lt;chr&gt; \"https://doi.org/10.1136/bmj.n71\", \"https:‚Ä¶\n$ pdf_url                     &lt;chr&gt; \"https://www.bmj.com/content/bmj/372/bmj.n‚Ä¶\n$ license                     &lt;chr&gt; \"cc-by\", \"cc-by\", \"cc-by\", \"cc-by\", NA, \"c‚Ä¶\n$ version                     &lt;chr&gt; \"publishedVersion\", \"publishedVersion\", \"p‚Ä¶\n$ first_page                  &lt;chr&gt; \"n71\", NA, NA, NA, NA, \"167\", \"g7647\", \"93‚Ä¶\n$ last_page                   &lt;chr&gt; \"n71\", NA, NA, NA, NA, \"176\", \"g7647\", \"11‚Ä¶\n$ volume                      &lt;chr&gt; NA, \"4\", \"5\", \"18\", NA, \"84\", \"349\", \"39\",‚Ä¶\n$ issue                       &lt;chr&gt; NA, \"1\", \"1\", \"1\", NA, \"3\", \"jan02 1\", \"1\"‚Ä¶\n$ is_oa                       &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, FALSE, TRUE, TRUE,‚Ä¶\n$ is_oa_anywhere              &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, ‚Ä¶\n$ oa_status                   &lt;chr&gt; \"hybrid\", \"gold\", \"gold\", \"gold\", \"green\",‚Ä¶\n$ oa_url                      &lt;chr&gt; \"https://www.bmj.com/content/bmj/372/bmj.n‚Ä¶\n$ any_repository_has_fulltext &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE,‚Ä¶\n$ language                    &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", ‚Ä¶\n$ grants                      &lt;list&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, &lt;\"htt‚Ä¶\n$ cited_by_count              &lt;int&gt; 30303, 17347, 10540, 5298, 5664, 2657, 909‚Ä¶\n$ counts_by_year              &lt;list&gt; [&lt;data.frame[5 x 2]&gt;], [&lt;data.frame[11 x ‚Ä¶\n$ publication_year            &lt;int&gt; 2021, 2015, 2016, 2018, 2019, 2015, 2015, ‚Ä¶\n$ cited_by_api_url            &lt;chr&gt; \"https://api.openalex.org/works?filter=cit‚Ä¶\n$ ids                         &lt;list&gt; &lt;\"https://openalex.org/W3118615836\", \"htt‚Ä¶\n$ doi                         &lt;chr&gt; \"https://doi.org/10.1136/bmj.n71\", \"https:‚Ä¶\n$ type                        &lt;chr&gt; \"article\", \"article\", \"article\", \"article\"‚Ä¶\n$ referenced_works            &lt;list&gt; &lt;\"https://openalex.org/W1528251861\", \"htt‚Ä¶\n$ related_works               &lt;list&gt; &lt;\"https://openalex.org/W4234875088\", \"htt‚Ä¶\n$ is_paratext                 &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, ‚Ä¶\n$ is_retracted                &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, ‚Ä¶\n$ concepts                    &lt;list&gt; [&lt;data.frame[20 x 5]&gt;], [&lt;data.frame[18 x‚Ä¶\n$ topics                      &lt;list&gt; [&lt;tbl_df[12 x 5]&gt;], [&lt;tbl_df[12 x 5]&gt;], [‚Ä¶"
  },
  {
    "objectID": "slides/ms-slides-07.html#wichtigkeit-von-gewissenhaftigkeit",
    "href": "slides/ms-slides-07.html#wichtigkeit-von-gewissenhaftigkeit",
    "title": "Introduction to Text as Data",
    "section": "Wichtigkeit von Gewissenhaftigkeit",
    "text": "Wichtigkeit von Gewissenhaftigkeit\nGute Gewohnheiten helfen bei Qualit√§tspr√ºfung und Datenverarbeitung\n\n\n\nPraktische Empfehlungen\n\n\n\n\nEinheitlicher Code-Style, Bearbeitungsschritte kommentieren\nVer√§nderungen in neuen Datensatz speichern\n\n\n\n\n\nStreben nach:\n\n# Corrections based on first glimpse \nreview_works_correct &lt;- review_works %&gt;% \n    mutate(\n        # Create additional factor variables\n        publication_year_fct = as.factor(publication_year), \n        type_fct = as.factor(type)\n        )\n\nBitte vermeiden:\n\nReWoCo&lt;-review_works %&gt;% mutate(pub_year_fct = as.factor(publication_year), type_fct = as.factor(type))"
  },
  {
    "objectID": "slides/ms-slides-07.html#ein-datensatz-im-datensatz",
    "href": "slides/ms-slides-07.html#ein-datensatz-im-datensatz",
    "title": "Introduction to Text as Data",
    "section": "Ein Datensatz im Datensatz",
    "text": "Ein Datensatz im Datensatz\nExkurs zu verschachtelten (nested) Daten und M√∂glichkeiten zur Verabeitung in R\n\nBesonderheit: Informationen zu (Themen-)Katalogisierung als Liste im Datensatz\n\n\nreview_works_correct$topics %&gt;% head()\n\n[[1]]\n# A tibble: 12 √ó 5\n       i score name     id                                  display_name        \n   &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;                               &lt;chr&gt;               \n 1     1 0.999 topic    https://openalex.org/T10206         Methods for Evidenc‚Ä¶\n 2     1 0.999 subfield https://openalex.org/subfields/1804 Statistics, Probabi‚Ä¶\n 3     1 0.999 field    https://openalex.org/fields/18      Decision Sciences   \n 4     1 0.999 domain   https://openalex.org/domains/2      Social Sciences     \n 5     2 0.983 topic    https://openalex.org/T10416         Epidemiology and Im‚Ä¶\n 6     2 0.983 subfield https://openalex.org/subfields/2713 Epidemiology        \n 7     2 0.983 field    https://openalex.org/fields/27      Medicine            \n 8     2 0.983 domain   https://openalex.org/domains/4      Health Sciences     \n 9     3 0.946 topic    https://openalex.org/T12443         The Delphi Method i‚Ä¶\n10     3 0.946 subfield https://openalex.org/subfields/3312 Sociology and Polit‚Ä¶\n11     3 0.946 field    https://openalex.org/fields/33      Social Sciences     \n12     3 0.946 domain   https://openalex.org/domains/2      Social Sciences     \n\n[[2]]\n# A tibble: 12 √ó 5\n       i score name     id                                  display_name        \n   &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;                               &lt;chr&gt;               \n 1     1 0.999 topic    https://openalex.org/T10206         Methods for Evidenc‚Ä¶\n 2     1 0.999 subfield https://openalex.org/subfields/1804 Statistics, Probabi‚Ä¶\n 3     1 0.999 field    https://openalex.org/fields/18      Decision Sciences   \n 4     1 0.999 domain   https://openalex.org/domains/2      Social Sciences     \n 5     2 0.982 topic    https://openalex.org/T12443         The Delphi Method i‚Ä¶\n 6     2 0.982 subfield https://openalex.org/subfields/3312 Sociology and Polit‚Ä¶\n 7     2 0.982 field    https://openalex.org/fields/33      Social Sciences     \n 8     2 0.982 domain   https://openalex.org/domains/2      Social Sciences     \n 9     3 0.957 topic    https://openalex.org/T10416         Epidemiology and Im‚Ä¶\n10     3 0.957 subfield https://openalex.org/subfields/2713 Epidemiology        \n11     3 0.957 field    https://openalex.org/fields/27      Medicine            \n12     3 0.957 domain   https://openalex.org/domains/4      Health Sciences     \n\n[[3]]\n# A tibble: 4 √ó 5\n      i score name     id                                  display_name         \n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;                               &lt;chr&gt;                \n1     1 0.937 topic    https://openalex.org/T10206         Methods for Evidence‚Ä¶\n2     1 0.937 subfield https://openalex.org/subfields/1804 Statistics, Probabil‚Ä¶\n3     1 0.937 field    https://openalex.org/fields/18      Decision Sciences    \n4     1 0.937 domain   https://openalex.org/domains/2      Social Sciences      \n\n[[4]]\n# A tibble: 12 √ó 5\n       i score name     id                                  display_name        \n   &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;                               &lt;chr&gt;               \n 1     1 0.997 topic    https://openalex.org/T10206         Methods for Evidenc‚Ä¶\n 2     1 0.997 subfield https://openalex.org/subfields/1804 Statistics, Probabi‚Ä¶\n 3     1 0.997 field    https://openalex.org/fields/18      Decision Sciences   \n 4     1 0.997 domain   https://openalex.org/domains/2      Social Sciences     \n 5     2 0.981 topic    https://openalex.org/T12443         The Delphi Method i‚Ä¶\n 6     2 0.981 subfield https://openalex.org/subfields/3312 Sociology and Polit‚Ä¶\n 7     2 0.981 field    https://openalex.org/fields/33      Social Sciences     \n 8     2 0.981 domain   https://openalex.org/domains/2      Social Sciences     \n 9     3 0.973 topic    https://openalex.org/T10416         Epidemiology and Im‚Ä¶\n10     3 0.973 subfield https://openalex.org/subfields/2713 Epidemiology        \n11     3 0.973 field    https://openalex.org/fields/27      Medicine            \n12     3 0.973 domain   https://openalex.org/domains/4      Health Sciences     \n\n[[5]]\n# A tibble: 12 √ó 5\n       i score name     id                                  display_name        \n   &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;                               &lt;chr&gt;               \n 1     1 0.993 topic    https://openalex.org/T10206         Methods for Evidenc‚Ä¶\n 2     1 0.993 subfield https://openalex.org/subfields/1804 Statistics, Probabi‚Ä¶\n 3     1 0.993 field    https://openalex.org/fields/18      Decision Sciences   \n 4     1 0.993 domain   https://openalex.org/domains/2      Social Sciences     \n 5     2 0.964 topic    https://openalex.org/T11744         Implementation of E‚Ä¶\n 6     2 0.964 subfield https://openalex.org/subfields/3600 General Health Prof‚Ä¶\n 7     2 0.964 field    https://openalex.org/fields/36      Health Professions  \n 8     2 0.964 domain   https://openalex.org/domains/4      Health Sciences     \n 9     3 0.954 topic    https://openalex.org/T12664         Development and Eva‚Ä¶\n10     3 0.954 subfield https://openalex.org/subfields/2739 Public Health, Envi‚Ä¶\n11     3 0.954 field    https://openalex.org/fields/27      Medicine            \n12     3 0.954 domain   https://openalex.org/domains/4      Health Sciences     \n\n[[6]]\n# A tibble: 12 √ó 5\n       i score name     id                                  display_name        \n   &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;                               &lt;chr&gt;               \n 1     1 0.993 topic    https://openalex.org/T10475         Role of Positive Em‚Ä¶\n 2     1 0.993 subfield https://openalex.org/subfields/3207 Social Psychology   \n 3     1 0.993 field    https://openalex.org/fields/32      Psychology          \n 4     1 0.993 domain   https://openalex.org/domains/2      Social Sciences     \n 5     2 0.992 topic    https://openalex.org/T10804         Health Economics an‚Ä¶\n 6     2 0.992 subfield https://openalex.org/subfields/2002 Economics and Econo‚Ä¶\n 7     2 0.992 field    https://openalex.org/fields/20      Economics, Economet‚Ä¶\n 8     2 0.992 domain   https://openalex.org/domains/2      Social Sciences     \n 9     3 0.984 topic    https://openalex.org/T12947         Salutogenesis and S‚Ä¶\n10     3 0.984 subfield https://openalex.org/subfields/3600 General Health Prof‚Ä¶\n11     3 0.984 field    https://openalex.org/fields/36      Health Professions  \n12     3 0.984 domain   https://openalex.org/domains/4      Health Sciences"
  },
  {
    "objectID": "slides/ms-slides-07.html#entpacken-der-schachteln-steigert-die-fallzahl",
    "href": "slides/ms-slides-07.html#entpacken-der-schachteln-steigert-die-fallzahl",
    "title": "Introduction to Text as Data",
    "section": "Entpacken der Schachteln steigert die Fallzahl",
    "text": "Entpacken der Schachteln steigert die Fallzahl\nExkurs zu verschachtelten (nested) Daten und M√∂glichkeiten zur Verabeitung in R\n\nreview_works_correct %&gt;% \n    unnest(topics, names_sep = \"_\") %&gt;%\n    glimpse()\n\nRows: 942,560\nColumns: 45\n$ id                          &lt;chr&gt; \"https://openalex.org/W3118615836\", \"https‚Ä¶\n$ title                       &lt;chr&gt; \"The PRISMA 2020 statement: an updated gui‚Ä¶\n$ display_name                &lt;chr&gt; \"The PRISMA 2020 statement: an updated gui‚Ä¶\n$ author                      &lt;list&gt; [&lt;data.frame[26 x 12]&gt;], [&lt;data.frame[26 ‚Ä¶\n$ ab                          &lt;chr&gt; \"The Preferred Reporting Items for Systema‚Ä¶\n$ publication_date            &lt;chr&gt; \"2021-03-29\", \"2021-03-29\", \"2021-03-29\", ‚Ä¶\n$ relevance_score             &lt;dbl&gt; 1625.171, 1625.171, 1625.171, 1625.171, 16‚Ä¶\n$ so                          &lt;chr&gt; \"BMJ\", \"BMJ\", \"BMJ\", \"BMJ\", \"BMJ\", \"BMJ\", ‚Ä¶\n$ so_id                       &lt;chr&gt; \"https://openalex.org/S4393917726\", \"https‚Ä¶\n$ host_organization           &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA‚Ä¶\n$ issn_l                      &lt;chr&gt; \"1756-1833\", \"1756-1833\", \"1756-1833\", \"17‚Ä¶\n$ url                         &lt;chr&gt; \"https://doi.org/10.1136/bmj.n71\", \"https:‚Ä¶\n$ pdf_url                     &lt;chr&gt; \"https://www.bmj.com/content/bmj/372/bmj.n‚Ä¶\n$ license                     &lt;chr&gt; \"cc-by\", \"cc-by\", \"cc-by\", \"cc-by\", \"cc-by‚Ä¶\n$ version                     &lt;chr&gt; \"publishedVersion\", \"publishedVersion\", \"p‚Ä¶\n$ first_page                  &lt;chr&gt; \"n71\", \"n71\", \"n71\", \"n71\", \"n71\", \"n71\", ‚Ä¶\n$ last_page                   &lt;chr&gt; \"n71\", \"n71\", \"n71\", \"n71\", \"n71\", \"n71\", ‚Ä¶\n$ volume                      &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA‚Ä¶\n$ issue                       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA‚Ä¶\n$ is_oa                       &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, ‚Ä¶\n$ is_oa_anywhere              &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, ‚Ä¶\n$ oa_status                   &lt;chr&gt; \"hybrid\", \"hybrid\", \"hybrid\", \"hybrid\", \"h‚Ä¶\n$ oa_url                      &lt;chr&gt; \"https://www.bmj.com/content/bmj/372/bmj.n‚Ä¶\n$ any_repository_has_fulltext &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, ‚Ä¶\n$ language                    &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", ‚Ä¶\n$ grants                      &lt;list&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n$ cited_by_count              &lt;int&gt; 30303, 30303, 30303, 30303, 30303, 30303, ‚Ä¶\n$ counts_by_year              &lt;list&gt; [&lt;data.frame[5 x 2]&gt;], [&lt;data.frame[5 x 2‚Ä¶\n$ publication_year            &lt;int&gt; 2021, 2021, 2021, 2021, 2021, 2021, 2021, ‚Ä¶\n$ cited_by_api_url            &lt;chr&gt; \"https://api.openalex.org/works?filter=cit‚Ä¶\n$ ids                         &lt;list&gt; &lt;\"https://openalex.org/W3118615836\", \"htt‚Ä¶\n$ doi                         &lt;chr&gt; \"https://doi.org/10.1136/bmj.n71\", \"https:‚Ä¶\n$ type                        &lt;chr&gt; \"article\", \"article\", \"article\", \"article\"‚Ä¶\n$ referenced_works            &lt;list&gt; &lt;\"https://openalex.org/W1528251861\", \"htt‚Ä¶\n$ related_works               &lt;list&gt; &lt;\"https://openalex.org/W4234875088\", \"htt‚Ä¶\n$ is_paratext                 &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, ‚Ä¶\n$ is_retracted                &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, ‚Ä¶\n$ concepts                    &lt;list&gt; [&lt;data.frame[20 x 5]&gt;], [&lt;data.frame[20 x‚Ä¶\n$ topics_i                    &lt;int&gt; 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 1, 1, ‚Ä¶\n$ topics_score                &lt;dbl&gt; 0.9993, 0.9993, 0.9993, 0.9993, 0.9832, 0.‚Ä¶\n$ topics_name                 &lt;chr&gt; \"topic\", \"subfield\", \"field\", \"domain\", \"t‚Ä¶\n$ topics_id                   &lt;chr&gt; \"https://openalex.org/T10206\", \"https://op‚Ä¶\n$ topics_display_name         &lt;chr&gt; \"Methods for Evidence Synthesis in Researc‚Ä¶\n$ publication_year_fct        &lt;fct&gt; 2021, 2021, 2021, 2021, 2021, 2021, 2021, ‚Ä¶\n$ type_fct                    &lt;fct&gt; article, article, article, article, articl‚Ä¶"
  },
  {
    "objectID": "slides/ms-slides-07.html#deskriptive-statistiken-zur-datenabfrage",
    "href": "slides/ms-slides-07.html#deskriptive-statistiken-zur-datenabfrage",
    "title": "Introduction to Text as Data",
    "section": "Deskriptive Statistiken zur Datenabfrage",
    "text": "Deskriptive Statistiken zur Datenabfrage\nRekonstruktion und Erweiterung des OpenAlex Web-Dashboards mit R\n\n\n\nIm Fokus:\n\nüîç Publikationen im Zeitverlauf\nFoschungsfelder\nRelevante Publikationen\nLageparameter\n\n\n\n\n\nreview_works_correct %&gt;% \n    ggplot(aes(publication_year_fct)) +\n    geom_bar() +\n    theme_pubr()"
  },
  {
    "objectID": "slides/ms-slides-07.html#deskriptive-statistiken-zur-datenabfrage-1",
    "href": "slides/ms-slides-07.html#deskriptive-statistiken-zur-datenabfrage-1",
    "title": "Introduction to Text as Data",
    "section": "Deskriptive Statistiken zur Datenabfrage",
    "text": "Deskriptive Statistiken zur Datenabfrage\nRekonstruktion und Erweiterung des OpenAlex Web-Dashboards mit R\n\n\n\nIm Fokus:\n\nPublikationen im Zeitverlauf\nüîç Foschungsfelder\nRelevante Publikationen\nLageparameter\n\n\n\n\n\nreview_works_correct %&gt;% \n    unnest(topics, names_sep = \"_\") %&gt;% \n    filter(topics_name == \"field\") %&gt;% \n    filter(topics_i == 1) %&gt;% \n    sjmisc::frq(topics_display_name, sort.frq = \"desc\")\n\ntopics_display_name &lt;character&gt; \n# total N=93655 valid N=93655 mean=4.41 sd=1.62\n\nValue                               |     N | Raw % | Valid % | Cum. %\n----------------------------------------------------------------------\nSocial Sciences                     | 30580 | 32.65 |   32.65 |  32.65\nPsychology                          | 29054 | 31.02 |   31.02 |  63.67\nBusiness, Management and Accounting | 15558 | 16.61 |   16.61 |  80.29\nDecision Sciences                   |  7261 |  7.75 |    7.75 |  88.04\nEconomics, Econometrics and Finance |  6796 |  7.26 |    7.26 |  95.30\nArts and Humanities                 |  4406 |  4.70 |    4.70 | 100.00\n&lt;NA&gt;                                |     0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;"
  },
  {
    "objectID": "slides/ms-slides-07.html#deskriptive-statistiken-zur-datenabfrage-2",
    "href": "slides/ms-slides-07.html#deskriptive-statistiken-zur-datenabfrage-2",
    "title": "Introduction to Text as Data",
    "section": "Deskriptive Statistiken zur Datenabfrage",
    "text": "Deskriptive Statistiken zur Datenabfrage\nRekonstruktion und Erweiterung des OpenAlex Web-Dashboards mit R\n\n\n\nIm Fokus:\n\nPublikationen im Zeitverlauf\nFoschungsfelder\nüîç Relevante Publikationen\nLageparameter\n\n\n\n\n\nreview_works_correct %&gt;% \n    arrange(desc(relevance_score)) %&gt;%\n    select(publication_year_fct, relevance_score, title) %&gt;% \n    head(5) %&gt;% \n    gt::gt()\n\n\n\n\n\n\n\npublication_year_fct\nrelevance_score\ntitle\n\n\n\n\n2021\n1625.1708\nThe PRISMA 2020 statement: an updated guideline for reporting systematic reviews\n\n\n2015\n1340.1902\nPreferred reporting items for systematic review and meta-analysis protocols (PRISMA-P) 2015 statement\n\n\n2016\n1314.3904\nRayyan‚Äîa web and mobile app for systematic reviews\n\n\n2018\n990.4521\nSystematic review or scoping review? Guidance for authors when choosing between a systematic or scoping review approach\n\n\n2019\n962.6738\nUpdated guidance for trusted systematic reviews: a new edition of the Cochrane Handbook for Systematic Reviews of Interventions"
  },
  {
    "objectID": "slides/ms-slides-07.html#deskriptive-statistiken-zur-datenabfrage-3",
    "href": "slides/ms-slides-07.html#deskriptive-statistiken-zur-datenabfrage-3",
    "title": "Introduction to Text as Data",
    "section": "Deskriptive Statistiken zur Datenabfrage",
    "text": "Deskriptive Statistiken zur Datenabfrage\nRekonstruktion und Erweiterung des OpenAlex Web-Dashboards mit R\n\n\n\nIm Fokus:\n\nPublikationen im Zeitverlauf\nFoschungsfelder\nRelevante Publikationen\nüîç Lageparameter\n\n\n\n\n\nreview_works_correct %&gt;% \n  select(where(is.numeric)) %&gt;% \n  datawizard::describe_distribution() %&gt;% \n  print_html()\n\n\n\n\n\n\n\nVariable\nMean\nSD\nIQR\nMin\nMax\nSkewness\nKurtosis\nn\nn_Missing\n\n\n\n\nrelevance_score\n31.73\n42.51\n36.48\n1.17\n1625.17\n4.75\n67.87\n93655\n0\n\n\ncited_by_count\n18.33\n146.36\n10.00\n0.00\n30303.00\n123.44\n22236.55\n93655\n0\n\n\npublication_year\n2019.40\n2.97\n5.00\n2013.00\n2023.00\n-0.58\n-0.77\n93655\n0"
  },
  {
    "objectID": "slides/ms-slides-07.html#and-now-you-√ºbung-eingrenzung",
    "href": "slides/ms-slides-07.html#and-now-you-√ºbung-eingrenzung",
    "title": "Introduction to Text as Data",
    "section": "üß™ And now ‚Ä¶ you: √úbung & Eingrenzung",
    "text": "üß™ And now ‚Ä¶ you: √úbung & Eingrenzung\nNext Steps: Wiederholung der R-Grundlagen an OpenAlex-Daten\n\nLaden Sie die auf StudOn bereitgestellten Dateien f√ºr die Sitzungen herunter\nLaden Sie die .zip-Datei in Ihren RStudio Workspace\nNavigieren Sie zu dem Ordner, in dem die Datei ps_24_binder.Rproj liegt. √ñffnen Sie diese Datei mit einem Doppelklick. Nur dadurch ist gew√§hrleistet, dass alle Dependencies korrekt funktionieren.\n√ñffnen Sie die Datei exercise-07.qmd im Ordner exercises und lesen Sie sich gr√ºndlich die Anweisungen durch.\nTipp: Sie finden alle in den Folien verwendeten Code-Bausteine in der Datei showcase.qmd (f√ºr den ‚Äúrohen‚Äù Code) oder showcase.html (mit gerenderten Ausgaben)."
  },
  {
    "objectID": "slides/ms-slides-07.html#literatur",
    "href": "slides/ms-slides-07.html#literatur",
    "title": "Introduction to Text as Data",
    "section": "Literatur",
    "text": "Literatur\n\n\nAria, M., Le, T., Cuccurullo, C., Belfiore, A., & Choe, J. (2024). openalexR: An R-Tool for Collecting Bibliometric Data from OpenAlex. The R Journal, 15(4), 167‚Äì180. https://doi.org/10.32614/rj-2023-089\n\n\nCurini, L., & Franzese, R. (2020). Text as Data: An Overview. SAGE Publications Ltd. https://methods.sagepub.com/book/research-methods-in-political-science-and-international-relations\n\n\nGrimmer, J., & Stewart, B. M. (2013). Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts. Political Analysis, 21(3), 267‚Äì297. https://doi.org/10/f458q9\n\n\nMunn, Z., Peters, M. D. J., Stern, C., Tufanaru, C., McArthur, A., & Aromataris, E. (2018). Systematic review or scoping review? Guidance for authors when choosing between a systematic or scoping review approach. BMC Medical Research Methodology, 18(1). https://doi.org/10.1186/s12874-018-0611-x\n\n\nPham, M. T., Rajiƒá, A., Greig, J. D., Sargeant, J. M., Papadopoulos, A., & McEwen, S. A. (2014). A scoping review of scoping reviews: advancing the approach and enhancing the consistency. Research Synthesis Methods, 5(4), 371‚Äì385. https://doi.org/10.1002/jrsm.1123"
  },
  {
    "objectID": "slides/ms-slides-10.html#seminarplan",
    "href": "slides/ms-slides-10.html#seminarplan",
    "title": "Unsupervised Machine Learning (II)",
    "section": "Seminarplan",
    "text": "Seminarplan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSitzung\nDatum\nThema (synchron)\n√úbung (asynchron)\nDozent:in\n\n\n\n\n1\n18.04.2024\nEinf√ºhrung & √úberblick\n\nAM & CA\n\n\n\nüìö\nTeil 1: Systematic Review\n\n\n\n\n2\n25.04.2024\nEinf√ºhrung in Systematic Reviews I\nR-Einf√ºhrung\nAM\n\n\n3\n02.05.2024\nEinf√ºhrung in Systematic Reviews II\nR-Einf√ºhrung\nAM\n\n\n\n09.05.2024\nüèñÔ∏è Feiertag\nR-Einf√ºhrung\n\n\n\n4\n16.05.2024\nAutomatisierung von SRs & KI-Tools\nR-Einf√ºhrung\nAM\n\n\n\n23.05.2024\nüçª WiSo-Projekt-Woche\nR-Einf√ºhrung\n\n\n\n5\n04.06.2024\nüçï Gastvortrag: Prof.¬†Dr.¬†Emese Domahidi\nR-Einf√ºhrung\nED\n\n\n6\n06.06.2024\nAutomatisierung von SRs & KI-Tools\nR-Einf√ºhrung\nAM\n\n\n\nüíª\nTeil 2: Text as Data & Unsupervised Machine Learning\n\n\n\n\n7\n13.06.2024\nIntroduction to Text as Data\nzur Sitzung\nCA\n\n\n8\n20.06.2024\nText processing\nzur Sitzung\nCA\n\n\n9\n27.06.2024\nUnsupervised Machine Learning I\nzur Sitzung\nCA\n\n\n10\n04.07.2024\nUnsupervised Machine Learning II\nzur Sitzung\nCA & AM\n\n\n11\n11.07.2024\nRecap & Ausblick\nzur Sitzung\nCA & AM\n\n\n12\n18.07.2024\nüèÅ Semesterabschluss\nzur Sitzung\nCA & AM"
  },
  {
    "objectID": "slides/ms-slides-10.html#was-bisher-geschah",
    "href": "slides/ms-slides-10.html#was-bisher-geschah",
    "title": "Unsupervised Machine Learning (II)",
    "section": "Was bisher geschah ‚Ä¶",
    "text": "Was bisher geschah ‚Ä¶\nKurze Wiederholung der wichtigsten Inhalte\nIhr solltet in der Lage sein, die folgenden Fragen zu beantworten:\n\nWas verstehen wir unter Topic Modeling?\nWof√ºr wird Topic Modeling eingesetzt?\nWelche Schritte sind notwendig, um Topic Modeling in R umzusetzen?\n\nHeutige Fokus liegt auf Detailfragen:\n\nWie kann ich mein Themenmodell validieren?\nWie finde ich die optimale Anzahl von Themen?"
  },
  {
    "objectID": "slides/ms-slides-10.html#die-bisherige-transformation-der-daten",
    "href": "slides/ms-slides-10.html#die-bisherige-transformation-der-daten",
    "title": "Unsupervised Machine Learning (II)",
    "section": "Die (bisherige) Transformation der Daten",
    "text": "Die (bisherige) Transformation der Daten\nVon der Subsample bis zum (neuen) Modell: Daten\n\n\n\n\n# Create subsample\nreview_subsample &lt;- review_works %&gt;% \n    # Create additional factor variables\n    mutate(\n        publication_year_fct = as.factor(publication_year), \n        type_fct = as.factor(type)\n        ) %&gt;%\n    # Eingrenzung: Sprache und Typ\n    filter(language == \"en\") %&gt;% \n    filter(type == \"article\") %&gt;%\n    # Datentranformation\n    unnest(topics, names_sep = \"_\") %&gt;%\n    filter(topics_name == \"field\") %&gt;% \n    filter(topics_i == \"1\") %&gt;% \n    # Eingrenzung: Forschungsfeldes\n    filter(\n    topics_display_name == \"Social Sciences\"|\n    topics_display_name == \"Psychology\"\n    ) %&gt;% \n    mutate(\n        field = as.factor(topics_display_name)\n    ) %&gt;% \n    # Eingrenzung: Keine Eintr√§ge ohne Abstract\n    filter(!is.na(ab))\n\n\n\n# Overview\nreview_subsample %&gt;% glimpse  \n\nRows: 36,680\nColumns: 46\n$ id                          &lt;chr&gt; \"https://openalex.org/W4293003987\", \"https‚Ä¶\n$ title                       &lt;chr&gt; \"The WHO-5 Well-Being Index: A Systematic ‚Ä¶\n$ display_name                &lt;chr&gt; \"The WHO-5 Well-Being Index: A Systematic ‚Ä¶\n$ author                      &lt;list&gt; [&lt;data.frame[4 x 12]&gt;], [&lt;data.frame[2 x ‚Ä¶\n$ ab                          &lt;chr&gt; \"The 5-item World Health Organization Well‚Ä¶\n$ publication_date            &lt;chr&gt; \"2015-01-01\", \"2017-08-28\", \"2014-01-01\", ‚Ä¶\n$ relevance_score             &lt;dbl&gt; 938.7603, 752.3500, 591.2553, 576.1210, 56‚Ä¶\n$ so                          &lt;chr&gt; \"Psychotherapy and psychosomatics\", \"Journ‚Ä¶\n$ so_id                       &lt;chr&gt; \"https://openalex.org/S184803288\", \"https:‚Ä¶\n$ host_organization           &lt;chr&gt; \"Karger Publishers\", \"SAGE Publishing\", NA‚Ä¶\n$ issn_l                      &lt;chr&gt; \"0033-3190\", \"0739-456X\", NA, \"2214-7829\",‚Ä¶\n$ url                         &lt;chr&gt; \"https://doi.org/10.1159/000376585\", \"http‚Ä¶\n$ pdf_url                     &lt;chr&gt; \"https://www.karger.com/Article/Pdf/376585‚Ä¶\n$ license                     &lt;chr&gt; \"cc-by-nc\", NA, NA, \"cc-by\", NA, NA, \"cc-b‚Ä¶\n$ version                     &lt;chr&gt; \"publishedVersion\", NA, \"publishedVersion\"‚Ä¶\n$ first_page                  &lt;chr&gt; \"167\", \"93\", NA, \"89\", \"55\", \"2150\", \"e356‚Ä¶\n$ last_page                   &lt;chr&gt; \"176\", \"112\", NA, \"106\", \"64\", \"2159\", \"e3‚Ä¶\n$ volume                      &lt;chr&gt; \"84\", \"39\", NA, \"6\", \"277\", \"32\", \"2\", \"24‚Ä¶\n$ issue                       &lt;chr&gt; \"3\", \"1\", NA, NA, NA, \"19\", \"8\", NA, \"9\", ‚Ä¶\n$ is_oa                       &lt;lgl&gt; TRUE, FALSE, TRUE, TRUE, TRUE, FALSE, TRUE‚Ä¶\n$ is_oa_anywhere              &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE,‚Ä¶\n$ oa_status                   &lt;chr&gt; \"hybrid\", \"green\", \"bronze\", \"gold\", \"bron‚Ä¶\n$ oa_url                      &lt;chr&gt; \"https://www.karger.com/Article/Pdf/376585‚Ä¶\n$ any_repository_has_fulltext &lt;lgl&gt; FALSE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE‚Ä¶\n$ language                    &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", ‚Ä¶\n$ grants                      &lt;list&gt; NA, NA, NA, &lt;\"https://openalex.org/F43203‚Ä¶\n$ cited_by_count              &lt;int&gt; 2657, 1375, 2568, 803, 3664, 1553, 2895, 9‚Ä¶\n$ counts_by_year              &lt;list&gt; [&lt;data.frame[11 x 2]&gt;], [&lt;data.frame[7 x ‚Ä¶\n$ publication_year            &lt;int&gt; 2015, 2017, 2014, 2016, 2020, 2014, 2017, ‚Ä¶\n$ cited_by_api_url            &lt;chr&gt; \"https://api.openalex.org/works?filter=cit‚Ä¶\n$ ids                         &lt;list&gt; &lt;\"https://openalex.org/W4293003987\", \"htt‚Ä¶\n$ doi                         &lt;chr&gt; \"https://doi.org/10.1159/000376585\", \"http‚Ä¶\n$ type                        &lt;chr&gt; \"article\", \"article\", \"article\", \"article\"‚Ä¶\n$ referenced_works            &lt;list&gt; &lt;\"https://openalex.org/W1492518593\", \"htt‚Ä¶\n$ related_works               &lt;list&gt; &lt;\"https://openalex.org/W3020194755\", \"htt‚Ä¶\n$ is_paratext                 &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, ‚Ä¶\n$ is_retracted                &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, ‚Ä¶\n$ concepts                    &lt;list&gt; [&lt;data.frame[7 x 5]&gt;], [&lt;data.frame[18 x ‚Ä¶\n$ topics_i                    &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ‚Ä¶\n$ topics_score                &lt;dbl&gt; 0.9926, 0.9050, 0.9995, 0.9987, 0.9999, 1.‚Ä¶\n$ topics_name                 &lt;chr&gt; \"field\", \"field\", \"field\", \"field\", \"field‚Ä¶\n$ topics_id                   &lt;chr&gt; \"https://openalex.org/fields/32\", \"https:/‚Ä¶\n$ topics_display_name         &lt;chr&gt; \"Psychology\", \"Social Sciences\", \"Psycholo‚Ä¶\n$ publication_year_fct        &lt;fct&gt; 2015, 2017, 2014, 2016, 2020, 2014, 2017, ‚Ä¶\n$ type_fct                    &lt;fct&gt; article, article, article, article, articl‚Ä¶\n$ field                       &lt;fct&gt; Psychology, Social Sciences, Psychology, P‚Ä¶"
  },
  {
    "objectID": "slides/ms-slides-10.html#die-bisherige-transformation-der-daten-1",
    "href": "slides/ms-slides-10.html#die-bisherige-transformation-der-daten-1",
    "title": "Unsupervised Machine Learning (II)",
    "section": "Die (bisherige) Transformation der Daten",
    "text": "Die (bisherige) Transformation der Daten\nVon der Subsample bis zum (neuen) Modell: Document-Term-Matrix\n\n\n\n\n# Create corpus\nquanteda_corpus &lt;- review_subsample %&gt;% \n  quanteda::corpus(\n    docid_field = \"id\", \n    text_field = \"ab\"\n  )\n\n# Tokenize\nquanteda_token &lt;- quanteda_corpus %&gt;% \n  quanteda::tokens(\n    remove_punct = TRUE,\n    remove_symbols = TRUE, \n    remove_numbers = TRUE, \n    remove_url = TRUE, \n  ) %&gt;% \n  quanteda::tokens_tolower() %&gt;% \n  quanteda::tokens_remove(\n    pattern = stopwords(\"en\")\n    )\n\n# Convert to Document-Feature-Matrix (DFM)\nquanteda_dfm &lt;- quanteda_token %&gt;% \n  quanteda::dfm()\n\n# Pruning\nquanteda_dfm_trim &lt;- quanteda_dfm %&gt;% \n  dfm_trim( \n    min_docfreq = 10/nrow(review_subsample),\n    max_docfreq = 0.99, \n    docfreq_type = \"prop\")\n\n# Convert for stm topic modeling\nquanteda_stm &lt;- quanteda_dfm_trim %&gt;% \n   convert(to = \"stm\")\n\n\n\n# Overview\nquanteda_stm %&gt;% summary  \n\n          Length Class      Mode     \ndocuments 36650  -none-     list     \nvocab     14322  -none-     character\nmeta         45  data.frame list"
  },
  {
    "objectID": "slides/ms-slides-10.html#die-bisherige-transformation-der-daten-2",
    "href": "slides/ms-slides-10.html#die-bisherige-transformation-der-daten-2",
    "title": "Unsupervised Machine Learning (II)",
    "section": "Die (bisherige) Transformation der Daten",
    "text": "Die (bisherige) Transformation der Daten\nVon der Subsample bis zum (neuen) Modell: STM\n\n\n\n\n# Estimate model\nstm_mdl_k20 &lt;- stm::stm(\n    documents = quanteda_stm$documents,\n    vocab = quanteda_stm$vocab, \n    prevalence =~ publication_year_fct + field, \n    K = 20, \n    seed = 42,\n    max.em.its = 1000,\n    data = quanteda_stm$meta,\n    init.type = \"Spectral\",\n    verbose = TRUE)\n\n\n\n# Overview\nstm_mdl_k20\n\nA topic model with 20 topics, 36650 documents and a 14322 word dictionary."
  },
  {
    "objectID": "slides/ms-slides-10.html#breaking-down-the-model",
    "href": "slides/ms-slides-10.html#breaking-down-the-model",
    "title": "Unsupervised Machine Learning (II)",
    "section": "Breaking down the model",
    "text": "Breaking down the model\nErweiterte Modellauswertung: Beta-Matrix\n\n# Create tidy beta matrix\ntd_beta &lt;- tidytext::tidy(stm_mdl_k20)\n\n# Output \ntd_beta\n\n# A tibble: 286,440 √ó 3\n   topic term      beta\n   &lt;int&gt; &lt;chr&gt;    &lt;dbl&gt;\n 1     1 #x0d  6.83e-44\n 2     2 #x0d  2.37e-30\n 3     3 #x0d  3.35e-45\n 4     4 #x0d  7.24e- 3\n 5     5 #x0d  1.45e-23\n 6     6 #x0d  5.23e-20\n 7     7 #x0d  2.70e-12\n 8     8 #x0d  3.32e-39\n 9     9 #x0d  8.78e- 4\n10    10 #x0d  2.58e-12\n# ‚Ñπ 286,430 more rows"
  },
  {
    "objectID": "slides/ms-slides-10.html#top-begriffe-nach-thema",
    "href": "slides/ms-slides-10.html#top-begriffe-nach-thema",
    "title": "Unsupervised Machine Learning (II)",
    "section": "Top Begriffe nach Thema",
    "text": "Top Begriffe nach Thema\nErweiterte Modellauswertung: Beta-Matrix\n\n\n# Create top terms\ntop_terms &lt;- td_beta %&gt;%\n  arrange(beta) %&gt;%\n  group_by(topic) %&gt;%\n  top_n(7, beta) %&gt;%\n  arrange(-beta) %&gt;%\n  select(topic, term) %&gt;%\n  summarise(terms = list(term)) %&gt;%\n  mutate(terms = map(terms, paste, collapse = \", \")) %&gt;% \n  unnest(cols = c(terms))\n\n# Output\ntop_terms %&gt;% \n  head(15)\n\n# A tibble: 15 √ó 2\n   topic terms                                                                 \n   &lt;int&gt; &lt;chr&gt;                                                                 \n 1     1 sleep, studies, eating, cognitive, weight, associated, duration       \n 2     2 health, women, gender, cultural, barriers, men, countries             \n 3     3 treatment, disorders, symptoms, disorder, depression, anxiety, therapy\n 4     4 patients, cancer, nurses, patient, music, nursing, pain               \n 5     5 literature, university, author, gt, lt, et, al                        \n 6     6 prevalence, covid-19, suicide, pandemic, studies, among, risk         \n 7     7 articles, review, study, systematic, results, search, science         \n 8     8 physical, activity, disabilities, exercise, cognitive, adults, body   \n 9     9 learning, education, students, educational, skills, teachers, teaching\n10    10 de, la, y, en, los, ÁöÑ, el                                            \n11    11 work, study, development, can, public, management, factors            \n12    12 violence, sexual, use, abuse, risk, youth, h3                         \n13    13 health, mental, care, support, people, social, family                 \n14    14 social, use, media, digital, information, technology, communication   \n15    15 ci, effect, meta-analysis, p, studies, effects, trials"
  },
  {
    "objectID": "slides/ms-slides-10.html#break-down-the-modell-a-little-more",
    "href": "slides/ms-slides-10.html#break-down-the-modell-a-little-more",
    "title": "Unsupervised Machine Learning (II)",
    "section": "Break down the modell (a little more)",
    "text": "Break down the modell (a little more)\nErweiterte Modellauswertung: Gamma-Matrix\n\n# Create tidy beta matrix\ntd_gamma &lt;- tidy(\n  stm_mdl_k20, \n  matrix = \"gamma\", \n  document_names = names(quanteda_stm$documents)\n  )\n\n# Output \ntd_gamma\n\n# A tibble: 733,000 √ó 3\n   document                         topic    gamma\n   &lt;chr&gt;                            &lt;int&gt;    &lt;dbl&gt;\n 1 https://openalex.org/W4293003987     1 0.00473 \n 2 https://openalex.org/W2750168540     1 0.000270\n 3 https://openalex.org/W1998933811     1 0.00368 \n 4 https://openalex.org/W2547134104     1 0.00340 \n 5 https://openalex.org/W3047898105     1 0.00263 \n 6 https://openalex.org/W2149640470     1 0.00230 \n 7 https://openalex.org/W2740726397     1 0.0374  \n 8 https://openalex.org/W2974087526     1 0.00339 \n 9 https://openalex.org/W2195703978     1 0.00287 \n10 https://openalex.org/W2093916237     1 0.170   \n# ‚Ñπ 732,990 more rows"
  },
  {
    "objectID": "slides/ms-slides-10.html#h√§ufigkeit-und-top-begriffe-der-themen",
    "href": "slides/ms-slides-10.html#h√§ufigkeit-und-top-begriffe-der-themen",
    "title": "Unsupervised Machine Learning (II)",
    "section": "H√§ufigkeit und Top Begriffe der Themen",
    "text": "H√§ufigkeit und Top Begriffe der Themen\nErweiterte Modellauswertung: Gamma-Matrix\n\n\nExpand for full code\n# Create data\nprevalence &lt;- td_gamma %&gt;%\n  group_by(topic) %&gt;%\n  summarise(gamma = mean(gamma)) %&gt;%\n  arrange(desc(gamma)) %&gt;%\n  left_join(top_terms, by = \"topic\") %&gt;%\n  mutate(topic = paste0(\"Topic \",sprintf(\"%02d\", topic)),\n         topic = reorder(topic, gamma))\n\n# Create output\nprevalence %&gt;% \n  gt() %&gt;% \n  gt::tab_options(\n      table.width = gt::pct(80), \n      table.font.size = \"10px\", \n      data_row.padding = gt::px(1)\n  ) %&gt;% \n  fmt_number(\n    columns = c(gamma), \n    decimals = 2) %&gt;% \n  gtExtras::gt_theme_538()\n\n\n\n\n\n\n\n\ntopic\ngamma\nterms\n\n\n\n\nTopic 16\n0.12\nresearch, literature, review, future, findings, systematic, paper\n\n\nTopic 19\n0.09\nstudies, interventions, included, evidence, review, systematic, outcomes\n\n\nTopic 07\n0.08\narticles, review, study, systematic, results, search, science\n\n\nTopic 11\n0.07\nwork, study, development, can, public, management, factors\n\n\nTopic 09\n0.06\nlearning, education, students, educational, skills, teachers, teaching\n\n\nTopic 17\n0.06\nchildren, studies, factors, relationship, adolescents, review, associated\n\n\nTopic 13\n0.06\nhealth, mental, care, support, people, social, family\n\n\nTopic 14\n0.05\nsocial, use, media, digital, information, technology, communication\n\n\nTopic 15\n0.05\nci, effect, meta-analysis, p, studies, effects, trials\n\n\nTopic 03\n0.04\ntreatment, disorders, symptoms, disorder, depression, anxiety, therapy\n\n\nTopic 06\n0.04\nprevalence, covid-19, suicide, pandemic, studies, among, risk\n\n\nTopic 18\n0.04\nmeasures, assessment, used, studies, tools, measurement, quality\n\n\nTopic 20\n0.04\nprograms, school, training, interventions, outcomes, review, intervention\n\n\nTopic 02\n0.04\nhealth, women, gender, cultural, barriers, men, countries\n\n\nTopic 01\n0.03\nsleep, studies, eating, cognitive, weight, associated, duration\n\n\nTopic 05\n0.03\nliterature, university, author, gt, lt, et, al\n\n\nTopic 12\n0.03\nviolence, sexual, use, abuse, risk, youth, h3\n\n\nTopic 04\n0.03\npatients, cancer, nurses, patient, music, nursing, pain\n\n\nTopic 08\n0.03\nphysical, activity, disabilities, exercise, cognitive, adults, body\n\n\nTopic 10\n0.00\nde, la, y, en, los, ÁöÑ, el"
  },
  {
    "objectID": "slides/ms-slides-10.html#visualisierung-des-stm-modells",
    "href": "slides/ms-slides-10.html#visualisierung-des-stm-modells",
    "title": "Unsupervised Machine Learning (II)",
    "section": "Visualisierung des STM-Modells",
    "text": "Visualisierung des STM-Modells\nKombination von Beta- und Gamma-Matrix\n\n\nExpand for full code\nprevalence %&gt;%\n  ggplot(aes(topic, gamma, label = terms, fill = topic)) +\n  geom_col(show.legend = FALSE) +\n  geom_text(hjust = 0, nudge_y = 0.0005, size = 3) +\n  coord_flip() +\n  scale_y_continuous(\n    expand = c(0,0),\n    limits = c(0, 0.2)) +\n  theme_pubr() +\n  theme(\n    plot.title = element_text(size = 16),\n    plot.subtitle = element_text(size = 13)) +\n  labs(\n    x = NULL, y = expression(gamma),\n    title = \"Topic Prevalence in the OpenAlex Corpus\",\n    subtitle = \"With the top seven words that contribute to each topic\")"
  },
  {
    "objectID": "slides/ms-slides-10.html#einfluss-von-publikationsjahr-und-forschungsfeld",
    "href": "slides/ms-slides-10.html#einfluss-von-publikationsjahr-und-forschungsfeld",
    "title": "Unsupervised Machine Learning (II)",
    "section": "Einfluss von Publikationsjahr und Forschungsfeld",
    "text": "Einfluss von Publikationsjahr und Forschungsfeld\nBer√ºcksichtigung der Meta-Daten (Kovariaten)\n\n\n# Create estimation\neffects &lt;- estimateEffect(\n  1:20 ~ publication_year_fct + field, \n  stm_mdl_k20, \n  meta = quanteda_stm$meta)\n\n\n\n\n\n# Effects of covariates on Topic 16\neffects %&gt;% summary(topics = 16)\n\n\nCall:\nestimateEffect(formula = 1:20 ~ publication_year_fct + field, \n    stmobj = stm_mdl_k20, metadata = quanteda_stm$meta)\n\n\nTopic 16:\n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               0.063825   0.005213  12.244   &lt;2e-16 ***\npublication_year_fct2014  0.005980   0.006957   0.860   0.3900    \npublication_year_fct2015  0.013380   0.006781   1.973   0.0485 *  \npublication_year_fct2016  0.004308   0.006272   0.687   0.4922    \npublication_year_fct2017  0.007367   0.006214   1.186   0.2358    \npublication_year_fct2018  0.001396   0.006027   0.232   0.8169    \npublication_year_fct2019 -0.002034   0.006308  -0.322   0.7471    \npublication_year_fct2020  0.002527   0.006010   0.421   0.6741    \npublication_year_fct2021  0.002185   0.005577   0.392   0.6952    \npublication_year_fct2022 -0.001232   0.005442  -0.226   0.8209    \npublication_year_fct2023  0.008228   0.005571   1.477   0.1397    \nfieldSocial Sciences      0.086378   0.001804  47.869   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n# Effects of covariates on Topic 6\neffects %&gt;% summary(topics = 6) \n\n\nCall:\nestimateEffect(formula = 1:20 ~ publication_year_fct + field, \n    stmobj = stm_mdl_k20, metadata = quanteda_stm$meta)\n\n\nTopic 6:\n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               0.040416   0.004335   9.323  &lt; 2e-16 ***\npublication_year_fct2014 -0.002838   0.005661  -0.501   0.6162    \npublication_year_fct2015 -0.002249   0.005418  -0.415   0.6780    \npublication_year_fct2016 -0.003768   0.005157  -0.731   0.4650    \npublication_year_fct2017 -0.003795   0.005223  -0.727   0.4675    \npublication_year_fct2018 -0.005061   0.005179  -0.977   0.3284    \npublication_year_fct2019 -0.001944   0.004998  -0.389   0.6972    \npublication_year_fct2020  0.005680   0.004800   1.183   0.2367    \npublication_year_fct2021  0.021697   0.005006   4.334 1.47e-05 ***\npublication_year_fct2022  0.025178   0.004744   5.307 1.12e-07 ***\npublication_year_fct2023  0.012108   0.004774   2.536   0.0112 *  \nfieldSocial Sciences     -0.010559   0.001650  -6.398 1.60e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "slides/ms-slides-10.html#forschungsfeld-im-fokus",
    "href": "slides/ms-slides-10.html#forschungsfeld-im-fokus",
    "title": "Unsupervised Machine Learning (II)",
    "section": "Forschungsfeld im Fokus",
    "text": "Forschungsfeld im Fokus\nEinfluss des Forschungsfeldes auf Themenaufkommen\n\n\nExpand for full code\neffects %&gt;%\n  tidy() %&gt;% \n  filter(\n    term != \"(Intercept)\",\n    term == \"fieldSocial Sciences\") %&gt;% \n    select(-term) %&gt;% \n  gt() %&gt;% \n    fmt_number(\n      columns = -c(topic),\n      decimals = 3\n    ) %&gt;% \n  # Color social science topics \"blue\"\n  data_color(\n    columns = topic,\n    rows = estimate &gt; 0,\n    method = \"numeric\",\n    palette = c(\"#04316A\"),\n    alpha = 0.4\n  ) %&gt;% \n  # Color psychology topics \"yellow\"\n  data_color(\n    columns = topic,\n    rows = estimate &lt; 0,\n    method = \"numeric\",\n    palette = c(\"#D3A518\"),\n    alpha = 0.4\n  ) %&gt;% \n  # Color effect size for estimation\n  data_color(\n    columns = estimate,\n    method = \"numeric\",\n    palette = \"viridis\"\n  ) %&gt;% \n  # Color insignificant p-values\n  data_color(\n    rows = p.value &gt; 0.05,\n    method = \"numeric\",\n    palette = c(\"#C50F3C\")\n  ) %&gt;%\n  gtExtras::gt_theme_538() \n\n\n\n\n\n\n\n\ntopic\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n1\n‚àí0.056\n0.001\n‚àí40.636\n0.000\n\n\n2\n0.014\n0.001\n11.076\n0.000\n\n\n3\n‚àí0.069\n0.001\n‚àí48.356\n0.000\n\n\n4\n‚àí0.014\n0.001\n‚àí11.712\n0.000\n\n\n5\n0.027\n0.001\n18.275\n0.000\n\n\n6\n‚àí0.011\n0.002\n‚àí6.439\n0.000\n\n\n7\n0.014\n0.001\n10.838\n0.000\n\n\n8\n‚àí0.015\n0.001\n‚àí13.151\n0.000\n\n\n9\n0.052\n0.002\n28.954\n0.000\n\n\n10\n‚àí0.001\n0.001\n‚àí2.633\n0.008\n\n\n11\n0.091\n0.002\n53.421\n0.000\n\n\n12\n0.003\n0.001\n1.846\n0.065\n\n\n13\n‚àí0.024\n0.001\n‚àí18.958\n0.000\n\n\n14\n0.044\n0.001\n31.572\n0.000\n\n\n15\n‚àí0.059\n0.002\n‚àí37.951\n0.000\n\n\n16\n0.086\n0.002\n48.100\n0.000\n\n\n17\n‚àí0.033\n0.001\n‚àí23.377\n0.000\n\n\n18\n‚àí0.012\n0.001\n‚àí11.833\n0.000\n\n\n19\n‚àí0.044\n0.001\n‚àí31.866\n0.000\n\n\n20\n0.008\n0.001\n7.404\n0.000"
  },
  {
    "objectID": "slides/ms-slides-10.html#zusammenf√ºhrung-der-daten",
    "href": "slides/ms-slides-10.html#zusammenf√ºhrung-der-daten",
    "title": "Unsupervised Machine Learning (II)",
    "section": "Zusammenf√ºhrung der Daten",
    "text": "Zusammenf√ºhrung der Daten\nMatch Topic Modeling Ergebnisse mit OpenAlex-Daten\n\n\n\nExpand for full code\ngamma_export &lt;- stm_mdl_k20 %&gt;% \n  tidytext::tidy(\n    matrix = \"gamma\", \n    document_names = names(quanteda_stm$documents)) %&gt;%\n  dplyr::group_by(document) %&gt;% \n  dplyr::slice_max(gamma) %&gt;% \n  dplyr::mutate(main_topic = ifelse(gamma &gt; 0.5, topic, NA)) %&gt;% \n  dplyr::ungroup() %&gt;% \n  dplyr::left_join(review_subsample, by = c(\"document\" = \"id\")) %&gt;% \n  dplyr::rename(id = document) %&gt;% \n  dplyr::mutate(\n    stm_topic = as.factor(paste(\"Topic\", sprintf(\"%02d\", topic)))\n  )\n\n# Output\ngamma_export %&gt;% glimpse()\n\n\nRows: 36,650\nColumns: 50\n$ id                          &lt;chr&gt; \"https://openalex.org/W1000529773\", \"https‚Ä¶\n$ topic                       &lt;int&gt; 9, 16, 14, 14, 4, 16, 3, 5, 6, 17, 17, 12,‚Ä¶\n$ gamma                       &lt;dbl&gt; 0.6641380, 0.2635474, 0.4931637, 0.3960384‚Ä¶\n$ main_topic                  &lt;int&gt; 9, NA, NA, NA, NA, 16, 3, 5, NA, NA, NA, N‚Ä¶\n$ title                       &lt;chr&gt; \"A critical evaluation of the teaching of ‚Ä¶\n$ display_name                &lt;chr&gt; \"A critical evaluation of the teaching of ‚Ä¶\n$ author                      &lt;list&gt; [&lt;data.frame[1 x 12]&gt;], [&lt;data.frame[2 x ‚Ä¶\n$ ab                          &lt;chr&gt; \"A Critical Evaluation of the Teaching of ‚Ä¶\n$ publication_date            &lt;chr&gt; \"2014-01-16\", \"2015-05-26\", \"2014-05-22\", ‚Ä¶\n$ relevance_score             &lt;dbl&gt; 4.012791, 27.896568, 32.074608, 23.670475,‚Ä¶\n$ so                          &lt;chr&gt; NA, \"Proceedings of the annual conference ‚Ä¶\n$ so_id                       &lt;chr&gt; NA, \"https://openalex.org/S4306523984\", \"h‚Ä¶\n$ host_organization           &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, \"Taylor & ‚Ä¶\n$ issn_l                      &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, \"1381-1118‚Ä¶\n$ url                         &lt;chr&gt; \"https://uwispace.sta.uwi.edu/dspace/bitst‚Ä¶\n$ pdf_url                     &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"h‚Ä¶\n$ license                     &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"c‚Ä¶\n$ version                     &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"p‚Ä¶\n$ first_page                  &lt;chr&gt; NA, \"76\", NA, NA, NA, NA, NA, NA, \"1\", \"11‚Ä¶\n$ last_page                   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, \"21\", \"122‚Ä¶\n$ volume                      &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, \"20\", \"78\"‚Ä¶\n$ issue                       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, \"1\", NA, \"‚Ä¶\n$ is_oa                       &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, ‚Ä¶\n$ is_oa_anywhere              &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, ‚Ä¶\n$ oa_status                   &lt;chr&gt; \"closed\", \"closed\", \"closed\", \"closed\", \"c‚Ä¶\n$ oa_url                      &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, \"https://e‚Ä¶\n$ any_repository_has_fulltext &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, ‚Ä¶\n$ language                    &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", ‚Ä¶\n$ grants                      &lt;list&gt; NA, NA, NA, NA, NA, NA, NA, NA, &lt;\"https:/‚Ä¶\n$ cited_by_count              &lt;int&gt; 0, 1, 1, 1, 1, 0, 2, 0, 226, 159, 122, 31,‚Ä¶\n$ counts_by_year              &lt;list&gt; NA, [&lt;data.frame[1 x 2]&gt;], [&lt;data.frame[1‚Ä¶\n$ publication_year            &lt;int&gt; 2014, 2015, 2014, 2013, 2013, 2015, 2015, ‚Ä¶\n$ cited_by_api_url            &lt;chr&gt; \"https://api.openalex.org/works?filter=cit‚Ä¶\n$ ids                         &lt;list&gt; &lt;\"https://openalex.org/W1000529773\", \"100‚Ä¶\n$ doi                         &lt;chr&gt; NA, \"https://doi.org/10.5555/2814058.28141‚Ä¶\n$ type                        &lt;chr&gt; \"article\", \"article\", \"article\", \"article\"‚Ä¶\n$ referenced_works            &lt;list&gt; NA, &lt;\"https://openalex.org/W1526029332\", ‚Ä¶\n$ related_works               &lt;list&gt; &lt;\"https://openalex.org/W958254955\", \"http‚Ä¶\n$ is_paratext                 &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, ‚Ä¶\n$ is_retracted                &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, ‚Ä¶\n$ concepts                    &lt;list&gt; [&lt;data.frame[7 x 5]&gt;], [&lt;data.frame[16 x ‚Ä¶\n$ topics_i                    &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ‚Ä¶\n$ topics_score                &lt;dbl&gt; 0.9566, 0.9812, 0.9894, 0.9999, 0.9752, 0.‚Ä¶\n$ topics_name                 &lt;chr&gt; \"field\", \"field\", \"field\", \"field\", \"field‚Ä¶\n$ topics_id                   &lt;chr&gt; \"https://openalex.org/fields/32\", \"https:/‚Ä¶\n$ topics_display_name         &lt;chr&gt; \"Psychology\", \"Social Sciences\", \"Psycholo‚Ä¶\n$ publication_year_fct        &lt;fct&gt; 2014, 2015, 2014, 2013, 2013, 2015, 2015, ‚Ä¶\n$ type_fct                    &lt;fct&gt; article, article, article, article, articl‚Ä¶\n$ field                       &lt;fct&gt; Psychology, Social Sciences, Psychology, S‚Ä¶\n$ stm_topic                   &lt;fct&gt; Topic 09, Topic 16, Topic 14, Topic 14, To‚Ä¶"
  },
  {
    "objectID": "slides/ms-slides-10.html#themenh√§ufig-abstracth√§ufigkeit",
    "href": "slides/ms-slides-10.html#themenh√§ufig-abstracth√§ufigkeit",
    "title": "Unsupervised Machine Learning (II)",
    "section": "Themenh√§ufig ‚â† Abstracth√§ufigkeit",
    "text": "Themenh√§ufig ‚â† Abstracth√§ufigkeit\n√úberblick √ºber Anzahl der Abstracts nach Thema\n\ngamma_export %&gt;% \n  ggplot(aes(x = fct_rev(fct_infreq(stm_topic)))) +\n  geom_bar() +\n  coord_flip() +\n  theme_pubr()"
  },
  {
    "objectID": "slides/ms-slides-10.html#verschiedene-schwerpunkte-in-verschiedenen-feldern",
    "href": "slides/ms-slides-10.html#verschiedene-schwerpunkte-in-verschiedenen-feldern",
    "title": "Unsupervised Machine Learning (II)",
    "section": "Verschiedene Schwerpunkte in verschiedenen Feldern",
    "text": "Verschiedene Schwerpunkte in verschiedenen Feldern\n√úberblick √ºber die Anzahl der Abstracts nach Thema und Feld\n\ngamma_export %&gt;% \n  gtsummary::tbl_cross(\n    row = stm_topic, \n    col = field,\n    percent = \"row\",\n    )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfield\nTotal\n\n\nPsychology\nSocial Sciences\n\n\n\n\nstm_topic\n\n\n\n\n\n\n\n\n¬†¬†¬†¬†Topic 01\n1,470 (94%)\n87 (5.6%)\n1,557 (100%)\n\n\n¬†¬†¬†¬†Topic 02\n649 (50%)\n652 (50%)\n1,301 (100%)\n\n\n¬†¬†¬†¬†Topic 03\n1,481 (95%)\n71 (4.6%)\n1,552 (100%)\n\n\n¬†¬†¬†¬†Topic 04\n459 (60%)\n306 (40%)\n765 (100%)\n\n\n¬†¬†¬†¬†Topic 05\n301 (29%)\n734 (71%)\n1,035 (100%)\n\n\n¬†¬†¬†¬†Topic 06\n1,116 (60%)\n745 (40%)\n1,861 (100%)\n\n\n¬†¬†¬†¬†Topic 07\n825 (44%)\n1,051 (56%)\n1,876 (100%)\n\n\n¬†¬†¬†¬†Topic 08\n540 (70%)\n227 (30%)\n767 (100%)\n\n\n¬†¬†¬†¬†Topic 09\n821 (27%)\n2,260 (73%)\n3,081 (100%)\n\n\n¬†¬†¬†¬†Topic 10\n114 (67%)\n57 (33%)\n171 (100%)\n\n\n¬†¬†¬†¬†Topic 11\n303 (12%)\n2,244 (88%)\n2,547 (100%)\n\n\n¬†¬†¬†¬†Topic 12\n439 (41%)\n630 (59%)\n1,069 (100%)\n\n\n¬†¬†¬†¬†Topic 13\n1,477 (67%)\n722 (33%)\n2,199 (100%)\n\n\n¬†¬†¬†¬†Topic 14\n509 (28%)\n1,277 (72%)\n1,786 (100%)\n\n\n¬†¬†¬†¬†Topic 15\n1,815 (86%)\n299 (14%)\n2,114 (100%)\n\n\n¬†¬†¬†¬†Topic 16\n1,369 (27%)\n3,742 (73%)\n5,111 (100%)\n\n\n¬†¬†¬†¬†Topic 17\n1,672 (75%)\n566 (25%)\n2,238 (100%)\n\n\n¬†¬†¬†¬†Topic 18\n737 (65%)\n396 (35%)\n1,133 (100%)\n\n\n¬†¬†¬†¬†Topic 19\n2,552 (70%)\n1,118 (30%)\n3,670 (100%)\n\n\n¬†¬†¬†¬†Topic 20\n342 (42%)\n475 (58%)\n817 (100%)\n\n\nTotal\n18,991 (52%)\n17,659 (48%)\n36,650 (100%)"
  },
  {
    "objectID": "slides/ms-slides-10.html#a-closer-look",
    "href": "slides/ms-slides-10.html#a-closer-look",
    "title": "Unsupervised Machine Learning (II)",
    "section": "A closer look",
    "text": "A closer look\nFokus auf die Top-Abstracts von Thema 16\n\n\nExpand for full code\ngamma_export %&gt;% \n  filter(stm_topic == \"Topic 16\") %&gt;%\n  arrange(-gamma) %&gt;%\n  select(title, so, gamma, type, ab) %&gt;%\n  slice_head(n = 3) %&gt;% \n  gt() %&gt;% \n  fmt_number(\n    columns = vars(gamma), \n    decimals = 2) %&gt;%\n  gtExtras::gt_theme_538()\n\n\n\n\n\n\n\n\ntitle\nso\ngamma\ntype\nab\n\n\n\n\nTheory of Knowledge for Literature Reviews: An Epistemological Model, Taxonomy and Empirical Analysis of IS Literature\nNA\n0.93\narticle\nLiterature reviews play an important role in the development of knowledge. Yet, we observe a lack of theoretical underpinning of and epistemological insights into how literature reviews can contribute to knowledge creation and have actually contributed in the IS discipline. To address these theoretical and empirical research gaps, we suggest a novel epistemological model of literature reviews. This model allows us to align different contributions of literature reviews with their underlying knowledge conversions - thereby building a bridge between the previously largely unconnected fields of literature reviews and epistemology. We evaluate the appropriateness of the model by conducting an empirical analysis of 173 IS literature reviews which were published in 39 pertinent IS journals between 2000 and 2014. Based on this analysis, we derive an epistemological taxonomy of IS literature reviews, which complements previously suggested typologies.\n\n\nTheory of Knowledge for Literature Reviews: An Epistemological Model, Taxonomy and Empirical Analysis of IS Literature Completed Research Paper\nNA\n0.93\narticle\nLiterature reviews play an important role in the development of knowledge. Yet, we observe a lack of theoretical underpinning of and epistemological insights into how literature reviews can contribute to knowledge creation and have actually contributed in the IS discipline. To address these theoretical and empirical research gaps, we suggest a novel epistemological model of literature reviews. This model allows us to align different contributions of literature reviews with their underlying knowledge conversions - thereby building a bridge between the previously largely unconnected fields of literature reviews and epistemology. We evaluate the appropriateness of the model by conducting an empirical analysis of 173 IS literature reviews which were published in 39 pertinent IS journals between 2000 and 2014. Based on this analysis, we derive an epistemological taxonomy of IS literature reviews, which complements previously suggested typologies.\n\n\nRelationality in negotiations: a systematic review and propositions for future research\nÀúThe ≈ìinternational journal of conflict management/International journal of conflict management\n0.93\narticle\nPurpose The purpose of this paper is to systematically review and analyze the important, yet under-researched, topic of relationality in negotiations and propose new directions for future negotiation research. Design/methodology/approach This paper conducts a systematic review of negotiation literature related to relationality from multiple disciplines. Thirty-nine leading and topical academic journals are selected and 574 papers on negotiation are reviewed from 1990 to 2014. Based on the systematic review, propositions regarding the rationales for relationality in negotiations are developed and future research avenues in this area are discussed. Findings Of 574 papers on negotiations published in 39 peer-reviewed journals between 1990 and 2014, only 18 papers have studied and discussed relationality in negotiations. This suggests that relationality as a theoretical theme has long been under-researched in negotiation research. For future research, this paper proposes to incorporate the dynamic, cultural and mechanism perspectives, and to use a qualitative approach to study relationality in negotiations. Originality/value This paper presents the first systematic review of the negotiation literature on relationality, and identifies new research topics on relationality in negotiations. In so doing, this research opens new avenues for future negotiation research on relationality."
  },
  {
    "objectID": "slides/ms-slides-10.html#validieren-validieren-validieren",
    "href": "slides/ms-slides-10.html#validieren-validieren-validieren",
    "title": "Unsupervised Machine Learning (II)",
    "section": "Validieren, Validieren, Validieren",
    "text": "Validieren, Validieren, Validieren\nFokus auf die Top-Abstracts von Thema 6\n\n\nExpand for full code\ngamma_export %&gt;% \n  filter(stm_topic == \"Topic 06\") %&gt;%\n  arrange(-gamma) %&gt;%\n  select(title, so, gamma, type, ab) %&gt;%\n  slice_head(n = 3) %&gt;% \n  gt() %&gt;% \n  fmt_number(\n    columns = vars(gamma), \n    decimals = 2) %&gt;%\n  gtExtras::gt_theme_538()\n\n\n\n\n\n\n\n\ntitle\nso\ngamma\ntype\nab\n\n\n\n\nThe Acceptance of COVID-19 Vaccine: A Global Rapid Systematic Review and Meta-Analysis\nSocial Science Research Network\n0.89\narticle\nBackground: Vaccination seems to be the most effective way to prevent and control the spread of COVID-19, a disease that has been playing havoc with the lives of over 7 billion people across the globe. Vaccine hesitancy is probably the most common problem worldwide. This study aims to inspect the COVID-19 vaccine acceptance rates worldwide among the general population and healthcare workers. In addition, it compares the vaccine acceptance rates between the pre-and post-vaccine approval periods.Method: A systematic search was conducted on April 25, 2021, through PubMed, MEDLINE, Web of Science, and GOOGLE SCHOLAR databases using PRISMA and MOOSE statements. Q-test, and statistics were used to search for heterogeneity, and Eggers's test and funnel plot were applied to assess the publication bias. The random-effects model was used to estimate the pooled acceptance rates of the COVID-19 vaccines.Results: The combined COVID-19 vaccine acceptance rate among the general population and healthcare workers (n=1,581,562) was estimated at 61.74%. The vaccine acceptance rate among the general population was 62.66% and the rate among healthcare workers was 57.89%. The acceptance rate decreased from 67.21% to 53.44% among the general population and remained constant among healthcare workers during the pre and post-vaccine approval periods. The acceptance rates also vary in different regions of the world. The highest acceptance rate was found in Western Pacific Region (67.85%) and the lowest was found in African Region (39.51%).Conclusion: Low COVID-19 vaccine acceptance rate might be a massive barrier to getting rid of the pandemic. More researches are needed to address the responsible factors influencing the global rate of COVID-19 vaccine acceptance. Integrated global efforts are required to remove the barriers.\n\n\nPrevalence of Suicidal Behavior Among Students in South-East Asia: A Systematic Review and Meta-Analysis\nArchives of Suicide Research\n0.88\narticle\nEstimation of rates of suicidal behaviors (ideation, plan, and attempt) would help to understand the burden and prioritize prevention strategies. However, no attempt to assess suicidal behavior among students was identified in South-East Asia (SEA). We aimed to assess the prevalence of suicidal behavior (ideation, plan, and attempt) among students in SEA.We followed PRISMA 2020 guidelines and registered the protocol in PROSPERO (CRD42022353438). We searched in Medline, Embase, and PsycINFO and performed meta-analyses to pool the lifetime, 1-year, and point prevalence rates for suicidal ideation, plans, and attempts. We considered the duration of a month for point prevalence.The search identified 40 separate populations from which 46 were included in the analyses, as some studies included samples from multiple countries. The pooled prevalence of suicidal ideation was 17.4% (confidence interval [95% CI], 12.4%-23.9%) for lifetime, 9.33% (95% CI, 7.2%-12%) for the past year, and 4.8% (95% CI, 3.6%-6.4%) for the present time. The pooled prevalence of suicide plans was 9% (95% CI, 6.2%-12.9%) for lifetime, 7.3% (95% CI, 5.1%-10.3%) for the past year, and 2.3% (95% CI, 0.8%-6.7%) for the present time. The pooled prevalence of suicide attempts was 5.2% (95% CI, 3.5%-7.8%) for lifetime and 4.5% (95% CI, 3.4%-5.8%) for the past year. Higher rates of suicide attempts in the lifetime were noted in Nepal (10%) and Bangladesh (9%), while lower rates were reported in India (4%) and Indonesia (5%).Suicidal behaviors are a common phenomenon among students in the SEA region. These findings call for integrated, multisectoral efforts to prevent suicidal behaviors in this group.\n\n\nFirst COVID-19 Booster Dose in the General Population: A Systematic Review and Meta-Analysis of Willingness and Its Predictors\nVaccines\n0.88\narticle\nThe emergence of breakthrough infections and new highly contagious variants of SARS-CoV-2 threaten the immunization in individuals who had completed the primary COVID-19 vaccination. This systematic review and meta-analysis investigated, for the first time, acceptance of the first COVID-19 booster dose and its associated factors among fully vaccinated individuals. We followed the PRISMA guidelines. We searched Scopus, Web of Science, Medline, PubMed, ProQuest, CINAHL and medrxiv from inception to 21 May 2022. We found 14 studies including 104,047 fully vaccinated individuals. The prevalence of individuals who intend to accept a booster was 79.0%, while the prevalence of unsure individuals was 12.6%, and the prevalence of individuals that intend to refuse a booster was 14.3%. The main predictors of willingness were older age, flu vaccination in the previous season, and confidence in COVID-19 vaccination. The most important reasons for decline were adverse reactions and discomfort experienced after previous COVID-19 vaccine doses and concerns for serious adverse reactions to COVID-19 booster doses. Considering the burden of COVID-19, a high acceptance rate of booster doses could be critical in controlling the pandemic. Our findings are innovative and could help policymakers to design and implement specific COVID-19 vaccination programs in order to decrease booster vaccine hesitancy."
  },
  {
    "objectID": "slides/ms-slides-10.html#die-suche-nach-dem-optimalen-k",
    "href": "slides/ms-slides-10.html#die-suche-nach-dem-optimalen-k",
    "title": "Unsupervised Machine Learning (II)",
    "section": "Die Suche nach dem optimalen k",
    "text": "Die Suche nach dem optimalen k\nDie wichtigste Frage bei der Modellauswahl\n\nDie Wahl von K (ob das Modell 5, 15 oder 100 Themen identifizieren soll), hat einen erheblichen Einfluss auf die Ergebnisse:\n\nje kleiner K, desto feink√∂rniger und in der Regel exklusiver die Themen;\nje gr√∂√üer K, desto deutlicher identifizieren die Themen einzelne Ereignisse oder Themen.\n\nDas Paket stm (Roberts et al., 2019) verf√ºgt √ºber zwei eingebaute L√∂sungen, um das optimale K zu finden\n\nsearchK()-Funktion\nEinstellung von K = 0 bei der Sch√§tzung des Modells\n\nEmpfehlung f√ºr stm: (Manuelles) Training und Auswertung!"
  },
  {
    "objectID": "slides/ms-slides-10.html#training-und-evaluation-der-modelle",
    "href": "slides/ms-slides-10.html#training-und-evaluation-der-modelle",
    "title": "Unsupervised Machine Learning (II)",
    "section": "Training und Evaluation der Modelle",
    "text": "Training und Evaluation der Modelle\nDie bessere Version von searchK(): Manuelle Exploration\n\n\n\n\n# Define parameters\nfuture::plan(future::multisession()) # use multiple sessions\ntopic_range &lt;- seq(from = 10, to = 100, by = 10) \n\n# Estimate models\nstm_search  &lt;- tibble(k = topic_range) %&gt;%\n  mutate(\n    mdl = furrr::future_map(\n      k, \n      ~stm::stm(\n        documents = quanteda_stm$documents,\n        vocab = quanteda_stm$vocab, \n        prevalence =~ publication_year_fct + field,\n        K = ., \n        seed = 42,\n        max.em.its = 1000,\n        data = quanteda_stm$meta,\n        init.type = \"Spectral\",\n        verbose = FALSE),\n      .options = furrr::furrr_options(seed = 42)\n      )\n    )\n\n\n\n# Overview\nstm_search$mdl\n\n[[1]]\nA topic model with 10 topics, 36650 documents and a 14322 word dictionary.\n\n[[2]]\nA topic model with 20 topics, 36650 documents and a 14322 word dictionary.\n\n[[3]]\nA topic model with 30 topics, 36650 documents and a 14322 word dictionary.\n\n[[4]]\nA topic model with 40 topics, 36650 documents and a 14322 word dictionary.\n\n[[5]]\nA topic model with 50 topics, 36650 documents and a 14322 word dictionary.\n\n[[6]]\nA topic model with 60 topics, 36650 documents and a 14322 word dictionary.\n\n[[7]]\nA topic model with 70 topics, 36650 documents and a 14322 word dictionary.\n\n[[8]]\nA topic model with 80 topics, 36650 documents and a 14322 word dictionary.\n\n[[9]]\nA topic model with 90 topics, 36650 documents and a 14322 word dictionary.\n\n[[10]]\nA topic model with 100 topics, 36650 documents and a 14322 word dictionary."
  },
  {
    "objectID": "slides/ms-slides-10.html#trainings--und-validierungsdatensatz",
    "href": "slides/ms-slides-10.html#trainings--und-validierungsdatensatz",
    "title": "Unsupervised Machine Learning (II)",
    "section": "Trainings- und Validierungsdatensatz",
    "text": "Trainings- und Validierungsdatensatz\nVergleich der verschiedenen Modelle anhand verschiedener Metriken\n\nheldout &lt;- make.heldout(\n  documents = quanteda_stm$documents,\n  vocab = quanteda_stm$vocab,\n  seed = 42)\n\n\nstm_search$results &lt;- stm_search %&gt;%\n  mutate(\n    exclusivity = map(mdl, exclusivity),\n    semantic_coherence = map(mdl, semanticCoherence, quanteda_stm$documents),\n    eval_heldout = map(mdl, eval.heldout, heldout$missing),\n    residual = map(mdl, checkResiduals, quanteda_stm$documents),\n    bound =  map_dbl(mdl, function(x) max(x$convergence$bound)),\n    lfact = map_dbl(mdl, function(x) lfactorial(x$settings$dim$K)),\n    lbound = bound + lfact,\n    iterations = map_dbl(mdl, function(x) length(x$convergence$bound)))"
  },
  {
    "objectID": "slides/ms-slides-10.html#kurzer-crashkurs",
    "href": "slides/ms-slides-10.html#kurzer-crashkurs",
    "title": "Unsupervised Machine Learning (II)",
    "section": "Kurzer Crashkurs",
    "text": "Kurzer Crashkurs\n√úberblick √ºber die verschiedenen Evaluationskritierien\n\nHeld-Out Likelihood misst, wie gut ein Modell ungesehene Daten vorhersagt (ABER: kein allgemeing√ºltiger Schwellenwert, nur Vergleich identischer Daten). H√∂here Werte weisen auf eine bessere Vorhersageleistung hin.\nLower bound ist eine Ann√§herung an die Log-Likelihood des Modells. Ein h√∂herer Wert deutet auf eine bessere Anpassung an die Daten hin.\nResiduen geben die Differenz zwischen den beobachteten und den vorhergesagten Werten an. Kleinere Residuen deuten auf eine bessere Modellanpassung hin. Im Idealfall sollten die Residuen so klein wie m√∂glich sein.\nSemantische Koh√§renz misst, wie semantisch verwandt die wichtigsten W√∂rter eines Themas sind, wobei h√∂here Werte auf koh√§rentere Themen hinweisen."
  },
  {
    "objectID": "slides/ms-slides-10.html#the-best-of-the-not-so-optimal-models",
    "href": "slides/ms-slides-10.html#the-best-of-the-not-so-optimal-models",
    "title": "Unsupervised Machine Learning (II)",
    "section": "The best of the not so optimal models",
    "text": "The best of the not so optimal models\n√úberblick √ºber die verschiedenen Evaluationskritierien\n\n\nExpand for full code\nstm_search$results %&gt;% \n  # Create data for graph\n  transmute(\n    k, \n    `Lower bound` = lbound,\n    Residuals = map_dbl(residual, \"dispersion\"),\n    `Semantic coherence` = map_dbl(semantic_coherence, mean),\n    `Held-out likelihood` = map_dbl(eval_heldout, \"expected.heldout\")\n    ) %&gt;%   \n  gather(Metric, Value, -k) %&gt;%\n  # Create graph\n  ggplot(aes(k, Value, color = Metric)) +\n    geom_line(linewidth = 1.5, alpha = 0.7, show.legend = FALSE) +\n    geom_point(size = 3) +\n    # Add marker\n    geom_vline(aes(xintercept = 20), color = \"#C77CFF\", alpha = .5) +\n    geom_vline(aes(xintercept = 40), color = \"#00BFC4\", alpha = .5) +\n    geom_vline(aes(xintercept = 60), color = \"#C77CFF\", alpha = .5) +\n    geom_vline(aes(xintercept = 70), color = \"#00BFC4\", alpha = .5) +  \n    scale_x_continuous(breaks = seq(from = 10, to = 100, by = 10)) +\n    facet_wrap(~Metric, scales = \"free_y\") +\n    labs(x = \"K (number of topics)\",\n        y = NULL,\n        title = \"Model diagnostics by number of topics\"\n        ) +\n    theme_pubr()\n\n\n\n\nFigure¬†1"
  },
  {
    "objectID": "slides/ms-slides-10.html#koh√§renz-nur-mit-exklusivit√§t",
    "href": "slides/ms-slides-10.html#koh√§renz-nur-mit-exklusivit√§t",
    "title": "Unsupervised Machine Learning (II)",
    "section": "Koh√§renz nur mit Exklusivit√§t",
    "text": "Koh√§renz nur mit Exklusivit√§t\nVergleich verschiedener potentieller ‚Äúoptimaler‚Äù Modelle\n\n\nExpand for full code\nstm_search$results %&gt;% \n  select(k, exclusivity, semantic_coherence) %&gt;% \n  filter(k %in% c(20, 40, 70)) %&gt;%\n  unnest(cols = c(exclusivity, semantic_coherence)) %&gt;%\n  mutate(k = as.factor(k)) %&gt;%\n  ggplot(aes(semantic_coherence, exclusivity, color = k)) +\n  geom_point(size = 2, alpha = 0.7) +\n  labs(x = \"Semantic coherence\",\n       y = \"Exclusivity\",\n       title = \"Comparing exclusivity and semantic coherence\",\n       subtitle = \"Models with fewer topics have higher semantic coherence for more topics, but lower exclusivity\"\n       ) +\n  theme_minimal() \n\n\n\n\nFigure¬†2"
  },
  {
    "objectID": "slides/ms-slides-10.html#and-now-you-textanalyse-mit-r",
    "href": "slides/ms-slides-10.html#and-now-you-textanalyse-mit-r",
    "title": "Unsupervised Machine Learning (II)",
    "section": "üß™ And now ‚Ä¶ you: Textanalyse mit R",
    "text": "üß™ And now ‚Ä¶ you: Textanalyse mit R\nNext Steps: Wiederholung der Inhalte\n\nLaden Sie die auf StudOn bereitgestellten Dateien f√ºr die Sitzungen herunter\nLaden Sie die .zip-Datei in Ihren RStudio Workspace\nNavigieren Sie zu dem Ordner, in dem die Datei ps_24_binder.Rproj liegt. √ñffnen Sie diese Datei mit einem Doppelklick. Nur dadurch ist gew√§hrleistet, dass alle Dependencies korrekt funktionieren.\n√ñffnen Sie die Datei exercise-10.qmd im Ordner exercises und lesen Sie sich gr√ºndlich die Anweisungen durch.\nTipp: Sie finden alle in den Folien verwendeten Code-Bausteine in der Datei showcase.qmd (f√ºr den ‚Äúrohen‚Äù Code) oder showcase.html (mit gerenderten Ausgaben)."
  },
  {
    "objectID": "slides/ms-slides-10.html#references",
    "href": "slides/ms-slides-10.html#references",
    "title": "Unsupervised Machine Learning (II)",
    "section": "References",
    "text": "References\n\n\nRoberts, M. E., Stewart, B. M., & Tingley, D. (2019). stm: An R Package for Structural Topic Models. Journal of Statistical Software, 91(1), 1‚Äì40. https://doi.org/10.18637/jss.v091.i02"
  },
  {
    "objectID": "slides/ms-slides-08.html#seminarplan",
    "href": "slides/ms-slides-08.html#seminarplan",
    "title": "Text processing",
    "section": "Seminarplan",
    "text": "Seminarplan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSitzung\nDatum\nThema (synchron)\n√úbung (asynchron)\nDozent:in\n\n\n\n\n1\n18.04.2024\nEinf√ºhrung & √úberblick\n\nAM & CA\n\n\n\nüìö\nTeil 1: Systematic Review\n\n\n\n\n2\n25.04.2024\nEinf√ºhrung in Systematic Reviews I\nR-Einf√ºhrung\nAM\n\n\n3\n02.05.2024\nEinf√ºhrung in Systematic Reviews II\nR-Einf√ºhrung\nAM\n\n\n\n09.05.2024\nüèñÔ∏è Feiertag\nR-Einf√ºhrung\n\n\n\n4\n16.05.2024\nAutomatisierung von SRs & KI-Tools\nR-Einf√ºhrung\nAM\n\n\n\n23.05.2024\nüçª WiSo-Projekt-Woche\nR-Einf√ºhrung\n\n\n\n5\n04.06.2024\nüçï Gastvortrag: Prof.¬†Dr.¬†Emese Domahidi\nR-Einf√ºhrung\nED\n\n\n6\n06.06.2024\nAutomatisierung von SRs & KI-Tools\nR-Einf√ºhrung\nAM\n\n\n\nüíª\nTeil 2: Text as Data & Unsupervised Machine Learning\n\n\n\n\n7\n13.06.2024\nIntroduction to Text as Data\nzur Sitzung\nCA\n\n\n8\n20.06.2024\nText processing\nzur Sitzung\nCA\n\n\n9\n27.06.2024\nUnsupervised Machine Learning I\nzur Sitzung\nCA\n\n\n10\n04.07.2024\nUnsupervised Machine Learning II\nzur Sitzung\nCA & AM\n\n\n11\n11.07.2024\nRecap & Ausblick\nzur Sitzung\nCA & AM\n\n\n12\n18.07.2024\nüèÅ Semesterabschluss\nzur Sitzung\nCA & AM"
  },
  {
    "objectID": "slides/ms-slides-08.html#building-a-shared-vocabulary",
    "href": "slides/ms-slides-08.html#building-a-shared-vocabulary",
    "title": "Text processing",
    "section": "Building a shared vocabulary",
    "text": "Building a shared vocabulary\nWichtige Begriffe und Konzepte\n\n by Analytics Vidhya\nToken: A token is a string with a known meaning, and a token may be a word, number or just characters like punctuation. ‚ÄúHello‚Äù, ‚Äú123‚Äù, and ‚Äú-‚Äù are some examples of tokens.\nSentence: A sentence is a group of tokens that is complete in meaning. ‚ÄúThe weather looks good‚Äù is an example of a sentence, and the tokens of the sentence are [‚ÄúThe‚Äù, ‚Äúweather‚Äù, ‚Äúlooks‚Äù, ‚Äúgood].\nParagraph: A paragraph is a collection of sentences or phrases, and a sentence can alternatively be viewed as a token of a paragraph.\nDocuments: A document might be a sentence, a paragraph, or a set of paragraphs. A text message sent to an individual is an example of a document.\nCorpus: A corpus is typically an extensive collection of documents as a Bag-of-words. A corpus comprises each word‚Äôs id and frequency count in each record. An example of a corpus is a collection of emails or text messages sent to a particular person."
  },
  {
    "objectID": "slides/ms-slides-08.html#a-bag-of-words",
    "href": "slides/ms-slides-08.html#a-bag-of-words",
    "title": "Text processing",
    "section": "A ‚Äúbag of words‚Äù",
    "text": "A ‚Äúbag of words‚Äù\nEinfache Technik im Natural Language Processing (NLP)\n\n by Shubham Gandhi\na collection of words, disregarding grammar, word order, and context."
  },
  {
    "objectID": "slides/ms-slides-08.html#digitales-w√∂rterbuch-der-deutschen-sprache",
    "href": "slides/ms-slides-08.html#digitales-w√∂rterbuch-der-deutschen-sprache",
    "title": "Text processing",
    "section": "Digitales W√∂rterbuch der deutschen Sprache",
    "text": "Digitales W√∂rterbuch der deutschen Sprache\nGro√ües, frei verf√ºgbares & deutschsprachiges Textkorpora"
  },
  {
    "objectID": "slides/ms-slides-08.html#vom-korpus-zum-token",
    "href": "slides/ms-slides-08.html#vom-korpus-zum-token",
    "title": "Text processing",
    "section": "Vom Korpus zum Token",
    "text": "Vom Korpus zum Token\nEinfaches Beispiel zur Darstellung der verschiedenen Konzepte\n\n by Mina Ghashami"
  },
  {
    "objectID": "slides/ms-slides-08.html#vom-korpus-zum-token-zum-model",
    "href": "slides/ms-slides-08.html#vom-korpus-zum-token-zum-model",
    "title": "Text processing",
    "section": "Vom Korpus zum Token zum Model",
    "text": "Vom Korpus zum Token zum Model\nKomplexer Prozess der Textverarbeitung\n\n by Jiawei Hu"
  },
  {
    "objectID": "slides/ms-slides-08.html#s√§tze-token-lemma-pos",
    "href": "slides/ms-slides-08.html#s√§tze-token-lemma-pos",
    "title": "Text processing",
    "section": "S√§tze ‚ûú Token ‚ûú Lemma ‚ûú POS",
    "text": "S√§tze ‚ûú Token ‚ûú Lemma ‚ûú POS\nBeispielhafte Darstellung des Text Preprocessing\n\n\n\n\n\n\n\n1. Satzerkennung\n\n\nWas gibt‚Äôs in New York zu sehen?\n\n\n\n\n\n\n\n\n\n2. Tokenisierung\n\n\nwas; gibt; `s; in; new; york; zu; sehen; ?\n\n\n\n\n\n\n\n\n\n3. Lemmatisierung\n\n\nwas; geben; `s; in; new; york; zu; sehen; ?\n\n\n\n\n\n\n\n\n\n4. Part-Of-Speech (POS) Tagging\n\n\n&gt;Was/PWS &gt;gibt/VVFIN &gt;‚Äôs/PPER &gt;in/APPR &gt;New/NE &gt;York/NE &gt;zu/PTKZU &gt;sehen/VVINF\n\n\n\n\nSatzerkennung: Aufl√∂sung der Satzstruktur; Aber: Probleme mit Datumsangaben, Uhrzeit, Abk√ºrzungen, URLS\nTokenisierung: Zerteilung in kleinste Einheiten, Abtrennung von Satzzeichen; Fragen: Umgang mit Zeichen, Symbolen, Zahlen, N-Gramme ‚Ä¶\nDefinition Lemmatisierung: Grundform eines Worters, als diejenige Form, unter dem an einen Begriff in einem Nachschlagewerk findet / R√ºckf√ºhrung auf die ‚ÄûVollfrom‚Äù\nDefinition POS: Zuordnung von W√∂rtern und Satzzeichen eines Textes zu Wortarten"
  },
  {
    "objectID": "slides/ms-slides-08.html#von-bow-zu-dfm",
    "href": "slides/ms-slides-08.html#von-bow-zu-dfm",
    "title": "Text processing",
    "section": "Von BoW zu DFM",
    "text": "Von BoW zu DFM\nTransformation des Bag-of-Words (BOW) zur Document-Feature-Matrix (DFM)\n\n by OpenClassrooms\nBag-of-Words-Modell: es z√§hlt lediglich die Worth√§ufigkeit je Dokument, die syntaktischen und grammatikalischen Zusammenh√§nge zwischen einzelnen W√∂rtern werden ignoriert."
  },
  {
    "objectID": "slides/ms-slides-08.html#what-we-did-so-far",
    "href": "slides/ms-slides-08.html#what-we-did-so-far",
    "title": "Text processing",
    "section": "What we did so far",
    "text": "What we did so far\nInformationen zur Datengrundlage und -quelle\n\nSuche nach Literatur zur (Sytematischen) Literatur√ºberblicken auf OpenAlex\nDownload von knapp 100.000 Literaturverweisen via API mit openalexR (Aria et al., 2024)\nDeskriptive Auswertung der Daten (‚ÄúRekonstruktion‚Äù des OpenAlex Web-Dashboards) mit R\n\nHeutige Ziele:\n\nEingrenzung der Datenbasis f√ºr weiterf√ºhrende Analysen\nAnwendung einfacher Textanalyseverfahren zur Untersuchung der Abstracts"
  },
  {
    "objectID": "slides/ms-slides-08.html#euer-input-ist-gefragt",
    "href": "slides/ms-slides-08.html#euer-input-ist-gefragt",
    "title": "Text processing",
    "section": "Euer Input ist gefragt!",
    "text": "Euer Input ist gefragt!\nWie sollen die Daten weiter eingegrenzt werden?\n\n\n\nBitte scannt den QR-Code oder nutzt den folgenden Link f√ºr die Teilnahme an einer kurzen Umfrage:\n\nhttps://www.menti.com/albpi1xur7et\nTemporary Access Code: 3332 2971\n\n\n\n\n \n\n    \n\n\n\n\n‚àí+\n01:00"
  },
  {
    "objectID": "slides/ms-slides-08.html#ergebnis",
    "href": "slides/ms-slides-08.html#ergebnis",
    "title": "Text processing",
    "section": "Ergebnis",
    "text": "Ergebnis"
  },
  {
    "objectID": "slides/ms-slides-08.html#build-the-subsample",
    "href": "slides/ms-slides-08.html#build-the-subsample",
    "title": "Text processing",
    "section": "Build the subsample",
    "text": "Build the subsample\nFokus auf englische Artikel aus den Sozialwissenschaften und der Psychologie\n\n\n\nreview_subsample &lt;- review_works_correct %&gt;% \n  # Eingrenzung: Sprache und Typ\n  filter(language == \"en\") %&gt;% \n  filter(type == \"article\") %&gt;%\n  # Datentranformation\n  unnest(topics, names_sep = \"_\") %&gt;%\n  filter(topics_name == \"field\") %&gt;% \n  filter(topics_i == \"1\") %&gt;% \n  # Eingrenzung: Forschungsfeldes\n  filter(\n   topics_display_name == \"Social Sciences\"|\n   topics_display_name == \"Psychology\"\n    )\n\n\n\n\nExpand for full code\nreview_works_correct %&gt;% \n  mutate(\n    included = ifelse(id %in% review_subsample$id, \"Ja\", \"Nein\"),\n    included = factor(included, levels = c(\"Nein\", \"Ja\"))\n    ) %&gt;%\n  ggplot(aes(x = publication_year_fct, fill = included)) +\n    geom_bar() +\n    labs(\n      x = \"\",\n      y = \"Anzahl der Eintr√§ge\", \n      fill = \"In Subsample enthalten?\"\n     ) +\n    scale_fill_manual(values = c(\"#A0ACBD50\", \"#FF707F\")) +\n    theme_pubr()"
  },
  {
    "objectID": "slides/ms-slides-08.html#explore-abstracts",
    "href": "slides/ms-slides-08.html#explore-abstracts",
    "title": "Text processing",
    "section": "Explore abstracts",
    "text": "Explore abstracts\nTidy data principles als Grundlage f√ºr Analyse-Workflow\n\n(Silge & Robinson, 2017)\nTidy data struture (each variable is column, each observation a row, each value is a cell, each type of observaional unit is a table) results in a table with one-token-per-row (Silge & Robinson, 2017)."
  },
  {
    "objectID": "slides/ms-slides-08.html#tokenization-der-abstracts",
    "href": "slides/ms-slides-08.html#tokenization-der-abstracts",
    "title": "Text processing",
    "section": "Tokenization der Abstracts",
    "text": "Tokenization der Abstracts\nTransform data to tidy text\n\n# Create tidy data\nreview_tidy &lt;- review_subsample %&gt;% \n    # Tokenization\n    tidytext::unnest_tokens(\"text\", ab) %&gt;% \n    # Remove stopwords\n    filter(!text %in% tidytext::stop_words$word)\n\n# Preview\nreview_tidy %&gt;% \n  select(id, text) %&gt;% \n  print(n = 10)\n\n# A tibble: 4,880,965 √ó 2\n   id                               text          \n   &lt;chr&gt;                            &lt;chr&gt;         \n 1 https://openalex.org/W4293003987 5             \n 2 https://openalex.org/W4293003987 item          \n 3 https://openalex.org/W4293003987 world         \n 4 https://openalex.org/W4293003987 health        \n 5 https://openalex.org/W4293003987 organization  \n 6 https://openalex.org/W4293003987 index         \n 7 https://openalex.org/W4293003987 5             \n 8 https://openalex.org/W4293003987 widely        \n 9 https://openalex.org/W4293003987 questionnaires\n10 https://openalex.org/W4293003987 assessing     \n# ‚Ñπ 4,880,955 more rows"
  },
  {
    "objectID": "slides/ms-slides-08.html#before-and-after-the-transformation",
    "href": "slides/ms-slides-08.html#before-and-after-the-transformation",
    "title": "Text processing",
    "section": "Before and after the transformation",
    "text": "Before and after the transformation\nVergleich eines Abstraktes in Rohform und nach Tokenisierung\n\n\n\nreview_subsample$ab[[1]]\n\n[1] \"The 5-item World Health Organization Well-Being Index (WHO-5) is among the most widely used questionnaires assessing subjective psychological well-being. Since its first publication in 1998, the WHO-5 has been translated into more than 30 languages and has been used in research studies all over the world. We now provide a systematic review of the literature on the WHO-5.We conducted a systematic search for literature on the WHO-5 in PubMed and PsycINFO in accordance with the PRISMA guidelines. In our review of the identified articles, we focused particularly on the following aspects: (1) the clinimetric validity of the WHO-5; (2) the responsiveness/sensitivity of the WHO-5 in controlled clinical trials; (3) the potential of the WHO-5 as a screening tool for depression, and (4) the applicability of the WHO-5 across study fields.A total of 213 articles met the predefined criteria for inclusion in the review. The review demonstrated that the WHO-5 has high clinimetric validity, can be used as an outcome measure balancing the wanted and unwanted effects of treatments, is a sensitive and specific screening tool for depression and its applicability across study fields is very high.The WHO-5 is a short questionnaire consisting of 5 simple and non-invasive questions, which tap into the subjective well-being of the respondents. The scale has adequate validity both as a screening tool for depression and as an outcome measure in clinical trials and has been applied successfully across a wide range of study fields.\"\n\n\n\n\nreview_tidy %&gt;% \n  filter(id == \"https://openalex.org/W4293003987\") %&gt;% \n  pull(text) %&gt;% \n  paste(collapse = \" \")\n\n[1] \"5 item world health organization index 5 widely questionnaires assessing subjective psychological publication 1998 5 translated 30 languages research studies world provide systematic review literature 5 conducted systematic search literature 5 pubmed psycinfo accordance prisma guidelines review identified articles focused aspects 1 clinimetric validity 5 2 responsiveness sensitivity 5 controlled clinical trials 3 potential 5 screening tool depression 4 applicability 5 study fields.a total 213 articles met predefined criteria inclusion review review demonstrated 5 clinimetric validity outcome measure balancing unwanted effects treatments sensitive specific screening tool depression applicability study fields high.the 5 short questionnaire consisting 5 simple invasive questions tap subjective respondents scale adequate validity screening tool depression outcome measure clinical trials applied successfully wide range study fields\""
  },
  {
    "objectID": "slides/ms-slides-08.html#count-token-frequency",
    "href": "slides/ms-slides-08.html#count-token-frequency",
    "title": "Text processing",
    "section": "Count token frequency",
    "text": "Count token frequency\nSummarize all tokens over all tweets\n\n\n# Create summarized data\nreview_summarized &lt;- review_tidy %&gt;% \n  count(text, sort = TRUE) \n\n# Preview Top 15 token\nreview_summarized %&gt;% \n    print(n = 15)\n\n\n# A tibble: 122,148 √ó 2\n   text              n\n   &lt;chr&gt;         &lt;int&gt;\n 1 studies       73398\n 2 review        57878\n 3 research      42689\n 4 health        35108\n 5 systematic    32431\n 6 literature    31374\n 7 study         29012\n 8 interventions 22731\n 9 included      21987\n10 social        21528\n11 articles      20631\n12 results       20166\n13 analysis      19624\n14 based         18929\n15 evidence      18545\n# ‚Ñπ 122,133 more rows"
  },
  {
    "objectID": "slides/ms-slides-08.html#the-unavoidable-word-cloud",
    "href": "slides/ms-slides-08.html#the-unavoidable-word-cloud",
    "title": "Text processing",
    "section": "The (Unavoidable) Word Cloud",
    "text": "The (Unavoidable) Word Cloud\nVisualization of Top 50 token\n\nreview_summarized %&gt;% \n    top_n(50) %&gt;% \n    ggplot(aes(label = text, size = n)) +\n    ggwordcloud::geom_text_wordcloud() +\n    scale_size_area(max_size = 20) +\n    theme_minimal()"
  },
  {
    "objectID": "slides/ms-slides-08.html#mehr-als-nur-ein-wort",
    "href": "slides/ms-slides-08.html#mehr-als-nur-ein-wort",
    "title": "Text processing",
    "section": "Mehr als nur ein Wort",
    "text": "Mehr als nur ein Wort\nModellierung von Wortzusammenh√§ngen: n-grams and correlations\n\n\nViele der wirklich interessanten Ergebnisse von Textanalysen basieren auf den Beziehungen zwischen W√∂rtern, z.B.\n\nwelche W√∂rter dazu ‚Äúneigen‚Äù, unmittelbar auf einander zu folgen (n-grams),\noder innerhalb desselben Dokuments gemeinsam aufzutreten (Korrelation)\n\n\n\n\n\n(Silge & Robinson, 2017)"
  },
  {
    "objectID": "slides/ms-slides-08.html#h√§ufige-wortpaare",
    "href": "slides/ms-slides-08.html#h√§ufige-wortpaare",
    "title": "Text processing",
    "section": "H√§ufige Wortpaare",
    "text": "H√§ufige Wortpaare\nWortkombinationen (n-grams) im FokusH\n\n\n# Create word paris\nreview_word_pairs &lt;- review_tidy %&gt;% \n    widyr::pairwise_count(\n        text,\n        id,\n        sort = TRUE)\n\n# Preview\nreview_word_pairs %&gt;% \n    print(n = 14)\n\n\n# A tibble: 114,446,724 √ó 3\n   item1      item2          n\n   &lt;chr&gt;      &lt;chr&gt;      &lt;dbl&gt;\n 1 review     studies    20494\n 2 studies    review     20494\n 3 review     systematic 20266\n 4 systematic review     20266\n 5 review     research   16902\n 6 research   review     16902\n 7 literature review     16754\n 8 review     literature 16754\n 9 systematic studies    16097\n10 studies    systematic 16097\n11 study      review     13391\n12 review     study      13391\n13 studies    research   13173\n14 research   studies    13173\n# ‚Ñπ 114,446,710 more rows"
  },
  {
    "objectID": "slides/ms-slides-08.html#h√§ufig-zusammen-selten-allein",
    "href": "slides/ms-slides-08.html#h√§ufig-zusammen-selten-allein",
    "title": "Text processing",
    "section": "H√§ufig zusammen, selten allein",
    "text": "H√§ufig zusammen, selten allein\nWortkorrelationen im Fokus\n\n\n# Create word correlation\nreview_pairs_corr &lt;- review_tidy %&gt;% \n    group_by(text) %&gt;% \n    filter(n() &gt;= 300) %&gt;% \n    pairwise_cor(\n        text, \n        id, \n        sort = TRUE)\n\n# Preview\nreview_pairs_corr %&gt;% \n    print(n = 15)\n\n\n# A tibble: 5,529,552 √ó 3\n   item1      item2      correlation\n   &lt;chr&gt;      &lt;chr&gt;            &lt;dbl&gt;\n 1 ottawa     newcastle        0.977\n 2 newcastle  ottawa           0.977\n 3 briggs     joanna           0.967\n 4 joanna     briggs           0.967\n 5 scholar    google           0.938\n 6 google     scholar          0.938\n 7 obsessive  compulsive       0.929\n 8 compulsive obsessive        0.929\n 9 nervosa    anorexia         0.893\n10 anorexia   nervosa          0.893\n11 ci         95               0.887\n12 95         ci               0.887\n13 las        los              0.886\n14 los        las              0.886\n15 gay        bisexual         0.861\n# ‚Ñπ 5,529,537 more rows"
  },
  {
    "objectID": "slides/ms-slides-08.html#spezifische-partner-in-spezifischen-umgebungen",
    "href": "slides/ms-slides-08.html#spezifische-partner-in-spezifischen-umgebungen",
    "title": "Text processing",
    "section": "Spezifische ‚ÄúPartner‚Äù in spezifischen Umgebungen",
    "text": "Spezifische ‚ÄúPartner‚Äù in spezifischen Umgebungen\nH√§ufig auftretenden W√∂rter in der Umgebung von review, literature, systematic\n\n\nreview_pairs_corr %&gt;% \n  filter(\n    item1 %in% c(\n      \"review\",\n      \"literature\",\n      \"systematic\")\n    ) %&gt;% \n  group_by(item1) %&gt;% \n  slice_max(correlation, n = 5) %&gt;% \n  ungroup() %&gt;% \n  mutate(\n    item2 = reorder(item2, correlation)\n    ) %&gt;% \n  ggplot(\n    aes(item2, correlation, fill = item1)\n    ) +\n  geom_bar(stat = \"identity\") +\n  facet_wrap(~ item1, scales = \"free_y\") +\n  coord_flip() +\n  scale_fill_manual(\n    values = c(\n      \"#04316A\",\n      \"#C50F3C\",\n      \"#00B2D1\")) +\n  theme_pubr()"
  },
  {
    "objectID": "slides/ms-slides-08.html#lets-talk-about-sentiments",
    "href": "slides/ms-slides-08.html#lets-talk-about-sentiments",
    "title": "Text processing",
    "section": "Let‚Äôs talk about sentiments",
    "text": "Let‚Äôs talk about sentiments\nDictionary based approach of text analysis\n\n(Silge & Robinson, 2017)\n\n\n\n\n\nAtteveldt et al. (2021) argue that sentiment, in fact, are quite a complex concepts that are often hard to capture with dictionaries."
  },
  {
    "objectID": "slides/ms-slides-08.html#√ºber-die-bedeutung-von-positivnegativ",
    "href": "slides/ms-slides-08.html#√ºber-die-bedeutung-von-positivnegativ",
    "title": "Text processing",
    "section": "√úber die Bedeutung von ‚Äúpositiv;negativ‚Äù",
    "text": "√úber die Bedeutung von ‚Äúpositiv;negativ‚Äù\nDie h√§ufigsten ‚Äúpositiven‚Äù und ‚Äúnegativen‚Äù W√∂rter in den Abstracts\n\n\nreview_sentiment_count &lt;- review_tidy %&gt;% \n  inner_join(\n     get_sentiments(\"bing\"),\n     by = c(\"text\" = \"word\"),\n     relationship = \"many-to-many\") %&gt;% \n  count(text, sentiment)\n  \n# Preview\nreview_sentiment_count %&gt;% \n  group_by(sentiment) %&gt;%\n  slice_max(n, n = 10) %&gt;% \n  ungroup() %&gt;% \n  mutate(text = reorder(text, n)) %&gt;%\n  ggplot(aes(n, text, fill = sentiment)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(\n    ~sentiment, scales = \"free_y\") +\n  labs(x = \"Contribution to sentiment\",\n       y = NULL) +\n  scale_fill_manual(\n    values = c(\"#C50F3C\", \"#007900\")) +\n  theme_pubr()"
  },
  {
    "objectID": "slides/ms-slides-08.html#anreicherung-der-daten",
    "href": "slides/ms-slides-08.html#anreicherung-der-daten",
    "title": "Text processing",
    "section": "Anreicherung der Daten",
    "text": "Anreicherung der Daten\nVerkn√ºpfung des Sentiemnt (Scores) mit den Abstracts\n\nreview_sentiment &lt;- review_tidy %&gt;% \n  inner_join(\n     get_sentiments(\"bing\"),\n     by = c(\"text\" = \"word\"),\n     relationship = \"many-to-many\") %&gt;% \n  count(id, sentiment) %&gt;% \n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %&gt;% \n  mutate(sentiment = positive - negative)\n  \n# Check\nreview_sentiment \n\n# A tibble: 35,710 √ó 4\n   id                               negative positive sentiment\n   &lt;chr&gt;                               &lt;int&gt;    &lt;int&gt;     &lt;int&gt;\n 1 https://openalex.org/W1000529773        2        2         0\n 2 https://openalex.org/W1006561082        0        1         1\n 3 https://openalex.org/W100685805         4       15        11\n 4 https://openalex.org/W1007410967        0        7         7\n 5 https://openalex.org/W1008209175        8        1        -7\n 6 https://openalex.org/W1009104829        2        4         2\n 7 https://openalex.org/W1009607471       15        8        -7\n 8 https://openalex.org/W1031503832       13        6        -7\n 9 https://openalex.org/W1035654938       10        5        -5\n10 https://openalex.org/W1044055445        5        0        -5\n# ‚Ñπ 35,700 more rows"
  },
  {
    "objectID": "slides/ms-slides-08.html#neutral-mit-einem-leicht-negativen-unterton",
    "href": "slides/ms-slides-08.html#neutral-mit-einem-leicht-negativen-unterton",
    "title": "Text processing",
    "section": "Neutral, mit einem leicht ‚Äúnegativen‚Äù Unterton",
    "text": "Neutral, mit einem leicht ‚Äúnegativen‚Äù Unterton\nVerteilung des Sentiment (Scores) in den Abstracts\n\n\n[1] 0.4858737\n\n\n\nreview_sentiment %&gt;% \n  ggplot(aes(sentiment)) +\n  geom_histogram(binwidth = 0.5, fill = \"#FF707F\") +\n  labs(\n    x = \"Sentiment (Score) des Abstracts\", \n    y = \"Anzahl der Eintr√§ge\"\n  ) +\n  theme_pubr() +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))"
  },
  {
    "objectID": "slides/ms-slides-08.html#keep-it-neutral",
    "href": "slides/ms-slides-08.html#keep-it-neutral",
    "title": "Text processing",
    "section": "Keep it neutral",
    "text": "Keep it neutral\nEntwicklung des Sentiment (Scores) der Abstracts im Zeitverlauf\n\n\nExpand for full code\n# Create first graph\ng1 &lt;- review_works_correct %&gt;% \n  filter(id %in% review_sentiment$id) %&gt;% \n  left_join(review_sentiment, by = join_by(id)) %&gt;% \n  sjmisc::rec(\n    sentiment,\n    rec = \"min:-2=negative; -1:1=neutral; 2:max=positive\") %&gt;% \n  ggplot(aes(x = publication_year_fct, fill = as.factor(sentiment_r))) +\n    geom_bar() +\n    labs(\n      x = \"\",\n      y = \"Anzahl der Eintr√§ge\", \n      fill = \"Sentiment (Score)\") +\n    scale_fill_manual(values = c(\"#C50F3C\", \"#90A0AF\", \"#007900\")) +\n    theme_pubr() \n    #theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n# Create second graph\ng2 &lt;- review_works_correct %&gt;% \n  filter(id %in% review_sentiment$id) %&gt;% \n  left_join(review_sentiment, by = join_by(id)) %&gt;% \n  sjmisc::rec(\n    sentiment,\n    rec = \"min:-2=negative; -1:1=neutral; 2:max=positive\") %&gt;% \n  ggplot(aes(x = publication_year_fct, fill = as.factor(sentiment_r))) +\n    geom_bar(position = \"fill\") +\n    labs(\n      x = \"\",\n      y = \"Anteil der Eintr√§ge\", \n      fill = \"Sentiment (Score)\") +\n    scale_fill_manual(values = c(\"#C50F3C\", \"#90A0AF\", \"#007D29\")) +\n    theme_pubr() \n\n# COMBINE GRPAHS\nggarrange(g1, g2,\n          nrow = 1, ncol = 2, \n          align = \"hv\",\n          common.legend = TRUE)"
  },
  {
    "objectID": "slides/ms-slides-08.html#and-now-you-wiederholung",
    "href": "slides/ms-slides-08.html#and-now-you-wiederholung",
    "title": "Text processing",
    "section": "üß™ And now ‚Ä¶ you: Wiederholung",
    "text": "üß™ And now ‚Ä¶ you: Wiederholung\nNext Steps: Wiederholung der R-Grundlagen an OpenAlex-Daten\n\nLaden Sie die auf StudOn bereitgestellten Dateien f√ºr die Sitzungen herunter\nLaden Sie die .zip-Datei in Ihren RStudio Workspace\nNavigieren Sie zu dem Ordner, in dem die Datei ps_24_binder.Rproj liegt. √ñffnen Sie diese Datei mit einem Doppelklick. Nur dadurch ist gew√§hrleistet, dass alle Dependencies korrekt funktionieren.\n√ñffnen Sie die Datei exercise-08.qmd im Ordner exercises und lesen Sie sich gr√ºndlich die Anweisungen durch.\nTipp: Sie finden alle in den Folien verwendeten Code-Bausteine in der Datei showcase.qmd (f√ºr den ‚Äúrohen‚Äù Code) oder showcase.html (mit gerenderten Ausgaben)."
  },
  {
    "objectID": "slides/ms-slides-08.html#literatur",
    "href": "slides/ms-slides-08.html#literatur",
    "title": "Text processing",
    "section": "Literatur",
    "text": "Literatur\n\n\nAria, M., Le, T., Cuccurullo, C., Belfiore, A., & Choe, J. (2024). openalexR: An R-Tool for Collecting Bibliometric Data from OpenAlex. The R Journal, 15(4), 167‚Äì180. https://doi.org/10.32614/rj-2023-089\n\n\nAtteveldt, W. van, Trilling, D., & Arc√≠la, C. (2021). Computational analysis of communication: A practical introduction to the analysis of texts, networks, and images with code examples in python and r. John Wiley & Sons.\n\n\nSilge, J., & Robinson, D. (2017). Text mining with r: A tidy approach (First edition). O‚ÄôReilly."
  },
  {
    "objectID": "sessions/ms-session-09.html",
    "href": "sessions/ms-session-09.html",
    "title": "Unsupervised Machine Learning I",
    "section": "",
    "text": "üñ•Ô∏è Slides\nüìã Showcase"
  },
  {
    "objectID": "sessions/ms-session-09.html#participate",
    "href": "sessions/ms-session-09.html#participate",
    "title": "Unsupervised Machine Learning I",
    "section": "",
    "text": "üñ•Ô∏è Slides\nüìã Showcase"
  },
  {
    "objectID": "sessions/ms-session-09.html#practice",
    "href": "sessions/ms-session-09.html#practice",
    "title": "Unsupervised Machine Learning I",
    "section": "Practice",
    "text": "Practice\n‚úçÔ∏è Exercise"
  },
  {
    "objectID": "sessions/ms-session-09.html#suggested-readings",
    "href": "sessions/ms-session-09.html#suggested-readings",
    "title": "Unsupervised Machine Learning I",
    "section": "Suggested readings",
    "text": "Suggested readings\n\nTopic Modeling\n\nChang, J., Boyd-Graber, J., Gerrish, S., Wang, C., & Blei, D. (2009). Reading tea leaves: How humans interpret topic models. 32, 288‚Äì296.\nChen, Y., Peng, Z., Kim, S.-H., & Choi, C. W. (2023). What We Can Do and Cannot Do with Topic Modeling: A Systematic Review. Communication Methods and Measures, 17(2), 111‚Äì130. https://doi.org/10.1080/19312458.2023.2167965\nEgger, R., & Yu, J. (2022). A topic modeling comparison between LDA, NMF, Top2Vec, and BERTopic to demystify twitter posts. Frontiers in Sociology, 7, 886498. https://doi.org/10.3389/fsoc.2022.886498\nFriemel, T. N. (2017). Social Network Analysis (J. Matthes, C. S. Davis, & R. F. Potter, Eds.; 1st ed., pp. 1‚Äì14). Wiley. https://onlinelibrary.wiley.com/doi/10.1002/9781118901731.iecrm0235\nHase, V. (2023). Automated Content Analysis (F. Oehmer-Pedrazzi, S. H. Kessler, E. Humprecht, K. Sommer, & L. Castro, Eds.; pp. 23‚Äì36). Springer Fachmedien Wiesbaden. https://link.springer.com/10.1007/978-3-658-36179-2_3\nHimelboim, I. (2017). Social Network Analysis (Social Media) (J. Matthes, C. S. Davis, & R. F. Potter, Eds.; 1st ed., pp. 1‚Äì15). Wiley. https://onlinelibrary.wiley.com/doi/10.1002/9781118901731.iecrm0236\nMaier, D., Waldherr, A., Miltner, P., Wiedemann, G., Niekler, A., Keinert, A., Pfetsch, B., Heyer, G., Reber, U., H√§ussler, T., Schmid-Petri, H., & Adam, S. (2018). Applying LDA Topic Modeling in Communication Research: Toward a Valid and Reliable Methodology. Communication Methods and Measures, 12(2-3), 93‚Äì118. https://doi.org/10.1080/19312458.2018.1430754\n\n\n\nTopic modeling in R\n\nAtteveldt, W. van, Trilling, D., & Arc√≠la, C. (2021). Computational analysis of communication: A practical introduction to the analysis of texts, networks, and images with code examples in python and r. John Wiley & Sons.\nSilge, J., & Hvitfeldt, E. (n.d.). Supervised machine learning for text analysis in r. https://smltar.com/\nSilge, J., & Robinson, D. (2017). Text mining with r: A tidy approach (First edition). O‚ÄôReilly.\nWelbers, K., Van Atteveldt, W., & Benoit, K. (2017). Text Analysis in R. Communication Methods and Measures, 11(4), 245‚Äì265. https://doi.org/10.1080/19312458.2017.1387238"
  },
  {
    "objectID": "sessions/ms-session-09.html#useful-resources",
    "href": "sessions/ms-session-09.html#useful-resources",
    "title": "Unsupervised Machine Learning I",
    "section": "Useful resources",
    "text": "Useful resources\n\nTutorials des CCS-Amsterdam zu ‚Äúquanteda-based‚Äù Textanalyse:\n\nüìñ Text analysis (including üé• video tutorial)\nüìñ Lexical sentiment analysis (including üé• video tutorial)\nüìñ LDA Topic Modeling (including üé• video tutorial)\nüìñ Structural Topic Modeling (including üé• video tutorial)\n\n\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "sessions/ms-session-10.html",
    "href": "sessions/ms-session-10.html",
    "title": "Unsupervised Machine Learning II",
    "section": "",
    "text": "üñ•Ô∏è Slides\nüìã Showcase"
  },
  {
    "objectID": "sessions/ms-session-10.html#participate",
    "href": "sessions/ms-session-10.html#participate",
    "title": "Unsupervised Machine Learning II",
    "section": "",
    "text": "üñ•Ô∏è Slides\nüìã Showcase"
  },
  {
    "objectID": "sessions/ms-session-10.html#practice",
    "href": "sessions/ms-session-10.html#practice",
    "title": "Unsupervised Machine Learning II",
    "section": "Practice",
    "text": "Practice\n‚úçÔ∏è Exercise"
  },
  {
    "objectID": "sessions/ms-session-10.html#suggested-readings",
    "href": "sessions/ms-session-10.html#suggested-readings",
    "title": "Unsupervised Machine Learning II",
    "section": "Suggested readings",
    "text": "Suggested readings\n\nTopic Modeling\n\nChang, J., Boyd-Graber, J., Gerrish, S., Wang, C., & Blei, D. (2009). Reading tea leaves: How humans interpret topic models. 32, 288‚Äì296.\nChen, Y., Peng, Z., Kim, S.-H., & Choi, C. W. (2023). What We Can Do and Cannot Do with Topic Modeling: A Systematic Review. Communication Methods and Measures, 17(2), 111‚Äì130. https://doi.org/10.1080/19312458.2023.2167965\nEgger, R., & Yu, J. (2022). A topic modeling comparison between LDA, NMF, Top2Vec, and BERTopic to demystify twitter posts. Frontiers in Sociology, 7, 886498. https://doi.org/10.3389/fsoc.2022.886498\nFriemel, T. N. (2017). Social Network Analysis (J. Matthes, C. S. Davis, & R. F. Potter, Eds.; 1st ed., pp. 1‚Äì14). Wiley. https://onlinelibrary.wiley.com/doi/10.1002/9781118901731.iecrm0235\nHase, V. (2023). Automated Content Analysis (F. Oehmer-Pedrazzi, S. H. Kessler, E. Humprecht, K. Sommer, & L. Castro, Eds.; pp. 23‚Äì36). Springer Fachmedien Wiesbaden. https://link.springer.com/10.1007/978-3-658-36179-2_3\nHimelboim, I. (2017). Social Network Analysis (Social Media) (J. Matthes, C. S. Davis, & R. F. Potter, Eds.; 1st ed., pp. 1‚Äì15). Wiley. https://onlinelibrary.wiley.com/doi/10.1002/9781118901731.iecrm0236\nMaier, D., Waldherr, A., Miltner, P., Wiedemann, G., Niekler, A., Keinert, A., Pfetsch, B., Heyer, G., Reber, U., H√§ussler, T., Schmid-Petri, H., & Adam, S. (2018). Applying LDA Topic Modeling in Communication Research: Toward a Valid and Reliable Methodology. Communication Methods and Measures, 12(2-3), 93‚Äì118. https://doi.org/10.1080/19312458.2018.1430754\n\n\n\nTopic modeling in R\n\nAtteveldt, W. van, Trilling, D., & Arc√≠la, C. (2021). Computational analysis of communication: A practical introduction to the analysis of texts, networks, and images with code examples in python and r. John Wiley & Sons.\nSilge, J., & Hvitfeldt, E. (n.d.). Supervised machine learning for text analysis in r. https://smltar.com/\nSilge, J., & Robinson, D. (2017). Text mining with r: A tidy approach (First edition). O‚ÄôReilly.\nWelbers, K., Van Atteveldt, W., & Benoit, K. (2017). Text Analysis in R. Communication Methods and Measures, 11(4), 245‚Äì265. https://doi.org/10.1080/19312458.2017.1387238"
  },
  {
    "objectID": "sessions/ms-session-10.html#useful-resources",
    "href": "sessions/ms-session-10.html#useful-resources",
    "title": "Unsupervised Machine Learning II",
    "section": "Useful resources",
    "text": "Useful resources\n\nTutorials des CCS-Amsterdam zu ‚Äúquanteda-based‚Äù Textanalyse:\n\nüìñ Text analysis (including üé• video tutorial)\nüìñ Lexical sentiment analysis (including üé• video tutorial)\nüìñ LDA Topic Modeling (including üé• video tutorial)\nüìñ Structural Topic Modeling (including üé• video tutorial)\n\n\n\n\nBack to course schedule ‚èé"
  },
  {
    "objectID": "exercises/ms-exercise-10_solution.html",
    "href": "exercises/ms-exercise-10_solution.html",
    "title": "Unsupervised Machine Learning II",
    "section": "",
    "text": "Link to source file"
  },
  {
    "objectID": "exercises/ms-exercise-10_solution.html#background",
    "href": "exercises/ms-exercise-10_solution.html#background",
    "title": "Unsupervised Machine Learning II",
    "section": "Background",
    "text": "Background\n\n\n\n\n\n\nTodays‚Äôs data basis: OpenAlex\n\n\n\n\nVia API bzw. openalexR (Aria et al. 2024) gesammelte ‚Äúworks‚Äù der Datenbank OpenAlex mit Bezug zu Literaturriews in den Sozialwissenschaften zwischen 2013 und 2023\nDetaillierte Informationen und Ergebnisse zur Suchquery finden Sie hier."
  },
  {
    "objectID": "exercises/ms-exercise-10_solution.html#preparation",
    "href": "exercises/ms-exercise-10_solution.html#preparation",
    "title": "Unsupervised Machine Learning II",
    "section": "Preparation",
    "text": "Preparation\n\n\n\n\n\n\nWichtige Information\n\n\n\n\nBitte stellen Sie sicher, dass Sie das jeweilige R-Studio Projekt zur √úbung ge√∂ffnet haben. Nur so funktionieren alle Dependencies korrekt.\nUm den einwandfreien Ablauf der √úbung zu gew√§hrleisten, wird f√ºr die Aufgaben auf eine eigenst√§ndige Datenerhebung verzichtet und ein √úbungsdatensatz zu verf√ºgung gestelt.\n\n\n\n\nPackages\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n    here, qs, # file management\n    magrittr, janitor, # data wrangling\n    easystats, sjmisc, # data analysis\n    gt, gtExtras, # table visualization\n    ggpubr, ggwordcloud, # visualization\n    # text analysis    \n    tidytext, widyr, # based on tidytext\n    quanteda, # based on quanteda\n    quanteda.textmodels, quanteda.textplots, quanteda.textstats, \n    stm, # structural topic modeling\n    openalexR, pushoverr, tictoc, \n    tidyverse # load last to avoid masking issues\n  )\n\n\n\nImport und Vorverarbeitung der Daten\n\nreview_works &lt;- qs::qread(here(\"data/session-07/openalex-review_works-2013_2023.qs\"))\n\n# Create correct data\nreview_subsample &lt;- review_works %&gt;% \n    # Create additional factor variables\n    mutate(\n        publication_year_fct = as.factor(publication_year), \n        type_fct = as.factor(type)\n        ) %&gt;%\n    # Eingrenzung: Sprache und Typ\n    filter(language == \"en\") %&gt;% \n    filter(type == \"article\") %&gt;%\n    # Datentranformation\n    unnest(topics, names_sep = \"_\") %&gt;%\n    filter(topics_name == \"field\") %&gt;% \n    filter(topics_i == \"1\") %&gt;% \n    # Eingrenzung: Forschungsfeldes\n    filter(\n    topics_display_name == \"Social Sciences\"|\n    topics_display_name == \"Psychology\"\n    ) %&gt;% \n    mutate(\n        field = as.factor(topics_display_name)\n    ) %&gt;% \n    # Eingrenzung: Keine Eintr√§ge ohne Abstract\n    filter(!is.na(ab))\n\n\n# Create corpus\nquanteda_corpus &lt;- review_subsample %&gt;% \n  quanteda::corpus(\n    docid_field = \"id\", \n    text_field = \"ab\"\n  )\n\n# Tokenize\nquanteda_token &lt;- quanteda_corpus %&gt;% \n  quanteda::tokens(\n    remove_punct = TRUE,\n    remove_symbols = TRUE, \n    remove_numbers = TRUE, \n    remove_url = TRUE, \n    split_tags = FALSE # keep hashtags and mentions\n  ) %&gt;% \n  quanteda::tokens_tolower() %&gt;% \n  quanteda::tokens_remove(\n    pattern = stopwords(\"en\")\n    )\n\n# Convert to Document-Feature-Matrix (DFM)\nquanteda_dfm &lt;- quanteda_token %&gt;% \n  quanteda::dfm()\n\n# Pruning\nquanteda_dfm_trim &lt;- quanteda_dfm %&gt;% \n  dfm_trim( \n    min_docfreq = 10/nrow(review_subsample),\n    max_docfreq = 0.99, \n    docfreq_type = \"prop\")\n\n# Convert for stm topic modeling\nquanteda_stm &lt;- quanteda_dfm_trim %&gt;% \n   convert(to = \"stm\")"
  },
  {
    "objectID": "exercises/ms-exercise-10_solution.html#praktische-anwendung",
    "href": "exercises/ms-exercise-10_solution.html#praktische-anwendung",
    "title": "Unsupervised Machine Learning II",
    "section": "üõ†Ô∏è Praktische Anwendung",
    "text": "üõ†Ô∏è Praktische Anwendung\n\n\n\n\n\n\nAchtung, bitte lesen!\n\n\n\n\nBevor Sie mit der Arbeit an den folgenden üìã Exercises beginnen, stellen Sie bitte sicher, dass Sie alle Chunks des Abschnitts Preparation gerendert haben. Das k√∂nnen Sie tun, indem Sie den ‚ÄúRun all chunks above‚Äù-Knopf des n√§chsten Chunks benutzen.\nBei Fragen zum Code lohnt sich ein Blick in den Showcase (.qmd oder .html). Beim Showcase handelt es sich um eine kompakte Darstellung des in der Pr√§sentation verwenden R-Codes. Sie k√∂nnen das Showcase also nutzen, um sich die Code-Bausteine anzusehen, die f√ºr die R-Outputs auf den Slides benutzt wurden.\n\n\n\n\nüìã Exercise 1: Visualisierung der Themenpr√§valenz\n\n1.1. Auswahl des passenden Models\n\nErstelen Sie einen neuen Datensatz stm_mdl_k40\n\nbasierend auf dem Datensatz stm_serach\n\nVerwenden Sie filter(k == 40), um das Modell mit 40 Themen zu auszuw√§hlen.\nVerwenden Sie pull(mdl) %&gt;% .[[1]] um die Spalte und das Element zu extrahieren, die das Modell enth√§lt.\nSpeichern Sie diese Umwandlung, indem Sie einen neuen Datensatz mit dem Namen stm_mdl_k40 erstellen.\n\n\n√úberpr√ºfen Sie die Transformation indem Sie stm_mdl_k40 in die Konsole eingeben.\n\n\n\nL√∂sung anzeigen\n# Pull tpm with 40 topics\nstm_mdl_k40 &lt;- stm_search %&gt;% \n  filter(k == 40) %&gt;% \n  pull(mdl) %&gt;% \n  .[[1]]\n\n# Check\nstm_mdl_k40\n\n\nA topic model with 40 topics, 36650 documents and a 14322 word dictionary.\n\n\n\n\n1.2. Identifikation der Top-Terms f√ºr jedes Thema\n\nErstellen Sie einen neuen Datensatz td_beta\n\nbasierend auf dem Datensatz stm_mdl_k40,\nVerwenden Sie tidy(method = \"frex\"), um die Beta-Matrix zu erstellen.\n\nErstellen Sie einen neuen Datensatz top_terms\n\nbasierend auf dem Datenastz td_beta,\n\nVerwenden Sie arrange(beta), um die Begriffe nach Beta zu sortieren.\nGruppieren Sie die Begriffe nach topic mit group_by(topic).\nExtrahieren Sie die 7 h√§ufigsten Begriffe mit top_n(7, beta).\nSortieren Sie die Begriffe absteigend mit arrange(-beta).\nW√§hlen Sie die Variablen topic und term mit select(topic, term) aus.\nExtrahieren Sie die Top-Begriffe pro Thema mit summarise(terms = list(term)).\nTransformieren Sie die extrahierten Begriffe pro Thema mit map(terms, paste, collapse = \", \") zu einem String.\n‚ÄúEntpacken‚Äù Sie die Begriffe aus der Liste (unnesten) mit unnest(cols = c(terms)).\n\n\n√úberpr√ºfen Sie die Transformation indem Sie top_terms in die Konsole eingeben.\n\n\n\nL√∂sung anzeigen\n# Create tidy beta matrix\ntd_beta &lt;- tidy(stm_mdl_k40, method = \"frex\")\n\n# Create top terms\ntop_terms &lt;- td_beta %&gt;%\n  arrange(beta) %&gt;%\n  group_by(topic) %&gt;%\n  top_n(7, beta) %&gt;%\n  arrange(-beta) %&gt;%\n  select(topic, term) %&gt;%\n  summarise(terms = list(term)) %&gt;%\n  mutate(terms = map(terms, paste, collapse = \", \")) %&gt;% \n  unnest(cols = c(terms))\n\n# Output\ntop_terms\n\n\n# A tibble: 40 √ó 2\n   topic terms                                                                  \n   &lt;int&gt; &lt;chr&gt;                                                                  \n 1     1 care, nursing, healthcare, nurses, professionals, patients, patient    \n 2     2 students, school, academic, education, educational, schools, literacy  \n 3     3 ÁöÑ, Á†îÁ©∂, Âíå, rs, Âú®, ‰∫Ü, ÊÄß                                           \n 4     4 elderly, #x0d, can, review, literature, google, keywords               \n 5     5 article, journal, decision, describes, aids, pressure, section         \n 6     6 prevalence, countries, among, studies, rates, population, higher       \n 7     7 depression, anxiety, psychological, stress, life, symptoms, cancer     \n 8     8 people, services, community, service, barriers, participation, support \n 9     9 factors, relationship, positive, studies, associated, behavior, negati‚Ä¶\n10    10 b, et, al, r, s, c, d                                                  \n# ‚Ñπ 30 more rows\n\n\n\n\n1.3 Erstellung der Pr√§valenz-Tabelle f√ºr die Themen\n\nErstellen Sie einen neuen Datensatz td_gamma\n\nbasierend auf dem Datensatz stm_mdl_k40,\nVerwenden Sie tidy(), um die Gamma-Matrix zu erstellen.\nVerwenden Sie document_names = names(quanteda_stm$documents) um die Dokumentennamen zu speichern\n\nErstellen Sie einen neuen Datensatz prevalence\n\nbasierend auf dem Datensatz td_gamma,\n\nGruppieren Sie die Themen nach topic mit group_by(topic).\nBerechnen Sie den Durchschnitt der gamma-Werte pro Thema mit summarise(gamma = mean(gamma)).\nSortieren Sie die Themen (absteigend) nach gamma mit arrange(desc(gamma)).\nVerkn√ºpfen Sie die Top-Begriffe mit den Themen mit left_join(top_terms, by = \"topic\").\n√úberarbeiten Sie die Variable topic mit dem mutate-Befehl:\n\nErstellen Sie eine neue Variable topic mit paste0(\"Topic \",sprintf(\"%02d\", topic)).\nOrdnen Sie die Themen nach gamma mit reorder(topic, gamma).\n\n\n\nErstellung Sie eine Tabelle als Output\n\nbasierend auf dem Datenastz prevalence,\n\nVerwenden Sie gt() um eine Tabelle zu erstellen.\nFormatieren Sie die Spalte gamma mit fmt_number(columns = vars(gamma), decimals = 2) um nur zwei Nachkommastellen anzuzeigen.\nVerwenden Sie gtExtras::gt_theme_538() um das Design der Tabelle anzupassen.\n\n\n‚úçÔ∏è Auf Basis des Outputs von prevalence Notieren Sie, welche Themen Sie als problematisch sehen und warum.\n\n\n\nL√∂sung anzeigen\n# Create tidy gamma matrix\ntd_gamma &lt;- tidy(\n  stm_mdl_k40, \n  matrix = \"gamma\", \n  document_names = names(quanteda_stm$documents)\n  )\n\n# Create prevalence\nprevalence &lt;- td_gamma %&gt;%\n  group_by(topic) %&gt;%\n  summarise(gamma = mean(gamma)) %&gt;%\n  arrange(desc(gamma)) %&gt;%\n  left_join(top_terms, by = \"topic\") %&gt;%\n  mutate(topic = paste0(\"Topic \",sprintf(\"%02d\", topic)),\n         topic = reorder(topic, gamma))\n\n# Output\nprevalence %&gt;% \n  gt() %&gt;% \n  fmt_number(\n    columns = vars(gamma), \n    decimals = 2) %&gt;% \n  gtExtras::gt_theme_538()\n\n\n\n\n\n\n\n\ntopic\ngamma\nterms\n\n\n\n\nTopic 16\n0.09\nresearch, review, literature, future, findings, systematic, studies\n\n\nTopic 19\n0.07\nstudies, included, review, quality, evidence, data, systematic\n\n\nTopic 38\n0.05\narticles, search, science, databases, review, systematic, criteria\n\n\nTopic 39\n0.04\nstudy, research, literature, analysis, used, results, method\n\n\nTopic 09\n0.04\nfactors, relationship, positive, studies, associated, behavior, negative\n\n\nTopic 25\n0.04\nlearning, education, students, teaching, teachers, skills, higher\n\n\nTopic 29\n0.04\ncultural, change, policy, political, human, identity, different\n\n\nTopic 35\n0.04\ninterventions, intervention, effectiveness, n, outcomes, effective, studies\n\n\nTopic 20\n0.03\nmanagement, tourism, development, public, paper, economic, marketing\n\n\nTopic 14\n0.03\ndigital, use, information, technology, online, communication, technologies\n\n\nTopic 34\n0.03\neffect, effects, meta-analysis, p, ptsd, ci, significant\n\n\nTopic 30\n0.03\ndisorders, disorder, suicide, eating, risk, suicidal, psychiatric\n\n\nTopic 33\n0.03\nphysical, cognitive, activity, studies, body, exercise, weight\n\n\nTopic 28\n0.03\nsleep, ci, meta-analysis, risk, studies, pooled, p\n\n\nTopic 21\n0.03\ntreatment, therapy, trials, patients, music, pain, controlled\n\n\nTopic 18\n0.03\nmeasures, assessment, used, tools, measurement, instruments, measure\n\n\nTopic 32\n0.03\nhealth, mental, stigma, problems, wellbeing, outcomes, review\n\n\nTopic 06\n0.02\nprevalence, countries, among, studies, rates, population, higher\n\n\nTopic 08\n0.02\npeople, services, community, service, barriers, participation, support\n\n\nTopic 15\n0.02\nreviews, outcomes, reporting, systematic, outcome, items, preferred\n\n\nTopic 13\n0.02\nfamily, support, resilience, experiences, caregivers, parents, parental\n\n\nTopic 07\n0.02\ndepression, anxiety, psychological, stress, life, symptoms, cancer\n\n\nTopic 17\n0.02\nchildren, adolescents, language, development, early, child, skills\n\n\nTopic 01\n0.02\ncare, nursing, healthcare, nurses, professionals, patients, patient\n\n\nTopic 23\n0.02\nsocial, media, older, adults, use, loneliness, people\n\n\nTopic 37\n0.02\ntraining, programs, work, program, professional, skills, workplace\n\n\nTopic 36\n0.02\nliterature, history, american, black, book, literary, historical\n\n\nTopic 24\n0.02\nviolence, women, abuse, sexual, ipv, child, trauma\n\n\nTopic 26\n0.02\ncovid-19, pandemic, vaccine, vaccination, health, acceptance, disease\n\n\nTopic 27\n0.02\nuse, gender, sexual, substance, alcohol, sex, men\n\n\nTopic 02\n0.02\nstudents, school, academic, education, educational, schools, literacy\n\n\nTopic 31\n0.02\nenvironment, environmental, urban, travel, physical, transport, safety\n\n\nTopic 40\n0.01\nauthors, interest, information, group, studies, case, term\n\n\nTopic 12\n0.01\ncrime, review, police, et, al, studies, may\n\n\nTopic 11\n0.01\nuniversity, author, papers, college, search, review, share\n\n\nTopic 04\n0.01\nelderly, #x0d, can, review, literature, google, keywords\n\n\nTopic 10\n0.01\nb, et, al, r, s, c, d\n\n\nTopic 05\n0.00\narticle, journal, decision, describes, aids, pressure, section\n\n\nTopic 22\n0.00\nde, la, y, en, los, el, se\n\n\nTopic 03\n0.00\nÁöÑ, Á†îÁ©∂, Âíå, rs, Âú®, ‰∫Ü, ÊÄß\n\n\n\n\n\n\n\n\n\n\nüìã Exercise 2: Einfluss der Metadaten\n\n2.1. Sch√§tzung der Meta-Effekte\n\nErstellen Sie einen neuen Datensatz effects:\n\nVerwenden Sie die Funktion estimateEffect(), um die Effekte zu sch√§tzen.\nVerwenden Sie f√ºr das formular-Argument 1:40 ~ publication_year_fct + field, um die Effekte der Ver√∂ffentlichungsjahre und Fachbereiche zu sch√§tzen.\nVerwenden Sie stm_mdl_40 als das zu analysierende Modell.\nVerwenden Sie meta = quanteda_stm$meta, um die Metadaten f√ºr die Sch√§tzung zu verwenden.\n\n\n\n\nL√∂sung anzeigen\n# Create data\neffects &lt;- estimateEffect(\n    1:40 ~ publication_year_fct + field,\n    stm_mdl_k40,\n    meta = quanteda_stm$meta)\n\n\n\n\n2.2. Untersuchung der Effekte\n\nErstellen Sie einen neuen Datensatz effects_tidy eine bereinigte Tabelle der Effekte:\n\nBasierend auf dem Datensatz effects\n\n\nVerwenden Sie die tidy() Funktion, um die Effekte in ein aufbereitetes Format zu bringen.\nFiltern Sie die Daten:\n\nEntfernen Sie Zeilen, bei denen term den Wert (Intercept) hat.\nBehalten Sie nur die Zeilen, bei denen term == \"fieldSocial Sciences\" ist.\n\nEntfernen Sie die Spalte term mit select(-term)\n\nErstellen Sie ein Tabelle zur √úberpr√ºfung der Effekte\n\nBasierend auf dem Datensatz effects_tidy:\n\n\nVerwenden Sie die Funktion gt(), um eine Tabelle zu erstellen.\nFormatieren Sie alle numerischen Variablen mit fmt_number(columns = -c(topic), decimals = 3), um lediglich drei Dezimalstellen darzustellen.\nVerwenden Sie data_color(columns = estimate, method = \"numeric\", palette = \"viridis\"), um die Sch√§tzwerte farblich zu kennzeichnen.\nWenden Sie das Design gtExtras::gt_theme_538() an.\n\n‚úçÔ∏è Notieren Sie, welches Thema am st√§rksten im Forschungsfeld ‚ÄúSocial Science‚Äù vertreten ist.\n\n\n\nL√∂sung anzeigen\n# Filter effect data\neffects_tidy &lt;- effects %&gt;% \n  tidy() %&gt;% \n  filter(\n    term != \"(Intercept)\",\n    term == \"fieldSocial Sciences\") %&gt;% \n    select(-term)\n\n\n# Explore effects (table outpu)\neffects_tidy %&gt;% \n    gt() %&gt;% \n    fmt_number(\n      columns = -c(topic),\n      decimals = 3\n    ) %&gt;% \n    data_color(\n       columns = estimate,\n    method = \"numeric\",\n    palette = \"viridis\"\n  ) %&gt;% \n  gtExtras::gt_theme_538()\n\n\n\n\n\n\n\n\ntopic\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n1\n‚àí0.008\n0.001\n‚àí8.425\n0.000\n\n\n2\n0.011\n0.001\n14.607\n0.000\n\n\n3\n0.000\n0.000\n‚àí1.528\n0.126\n\n\n4\n0.005\n0.001\n6.421\n0.000\n\n\n5\n0.003\n0.000\n13.002\n0.000\n\n\n6\n0.005\n0.001\n6.302\n0.000\n\n\n7\n‚àí0.020\n0.001\n‚àí27.268\n0.000\n\n\n8\n0.012\n0.001\n15.394\n0.000\n\n\n9\n‚àí0.015\n0.001\n‚àí16.304\n0.000\n\n\n10\n0.000\n0.000\n‚àí1.062\n0.288\n\n\n11\n0.006\n0.001\n9.255\n0.000\n\n\n12\n0.008\n0.001\n10.585\n0.000\n\n\n13\n‚àí0.009\n0.001\n‚àí10.631\n0.000\n\n\n14\n0.021\n0.001\n19.583\n0.000\n\n\n15\n0.001\n0.001\n1.848\n0.065\n\n\n16\n0.036\n0.001\n28.312\n0.000\n\n\n17\n‚àí0.011\n0.001\n‚àí13.594\n0.000\n\n\n18\n‚àí0.011\n0.001\n‚àí13.544\n0.000\n\n\n19\n‚àí0.026\n0.001\n‚àí26.514\n0.000\n\n\n20\n0.052\n0.001\n41.114\n0.000\n\n\n21\n‚àí0.041\n0.001\n‚àí35.883\n0.000\n\n\n22\n‚àí0.001\n0.000\n‚àí2.142\n0.032\n\n\n23\n0.013\n0.001\n18.664\n0.000\n\n\n24\n0.002\n0.001\n1.766\n0.077\n\n\n25\n0.036\n0.001\n25.463\n0.000\n\n\n26\n0.008\n0.001\n6.707\n0.000\n\n\n27\n‚àí0.008\n0.001\n‚àí8.447\n0.000\n\n\n28\n‚àí0.028\n0.001\n‚àí21.311\n0.000\n\n\n29\n0.039\n0.001\n39.942\n0.000\n\n\n30\n‚àí0.048\n0.001\n‚àí41.114\n0.000\n\n\n31\n0.007\n0.001\n7.647\n0.000\n\n\n32\n‚àí0.013\n0.001\n‚àí17.915\n0.000\n\n\n33\n‚àí0.034\n0.001\n‚àí31.266\n0.000\n\n\n34\n‚àí0.039\n0.001\n‚àí32.195\n0.000\n\n\n35\n‚àí0.025\n0.001\n‚àí25.614\n0.000\n\n\n36\n0.022\n0.001\n19.319\n0.000\n\n\n37\n0.012\n0.001\n14.407\n0.000\n\n\n38\n‚àí0.004\n0.001\n‚àí5.478\n0.000\n\n\n39\n0.042\n0.001\n41.523\n0.000\n\n\n40\n0.001\n0.000\n2.500\n0.012\n\n\n\n\n\n\n\nL√∂sung anzeigen\n#### Notes:\n# \n\n\n\n\n\nüìã Exercise 3: Einzelthema im Fokus\n\n3.1. Benennung des Themas k = 20\n\nBenennen Sie das Thema k = 20 aus dem Modell stm_mdl_40:\n\nVerwenden Sie die Funktion labelTopics().\nGeben Sie das Thema 20 als Parameter mit topic = 20 an.\n\n‚úçÔ∏è Notieren Sie die Themennamen. Begr√ºnden Sie kurz Ihre Entscheidung.\n\n\n\nL√∂sung anzeigen\n# Create topic label\nstm_mdl_k40 %&gt;% labelTopics(topic = 20)\n\n\nTopic 20 Top Words:\n     Highest Prob: management, tourism, development, public, paper, economic, marketing \n     FREX: tourism, marketing, consumer, halal, sustainable, disaster, economy \n     Lift: hotel, mega-events, tourism, post-disaster, smes, tourist, b2b \n     Score: tourism, marketing, halal, business, governance, sustainable, disaster \n\n\nL√∂sung anzeigen\n# Themenname:\n\n\n\n\n3.2. Zusammenf√ºhrung mit OpenAlex-Daten\n\nErstellen Sie einen neuen Datensatz gamma_export\n\nbasierend auf dem Datensatz stm_mdl_k40:\n\nVerwenden Sie tidy() um die Gamma-Matrix zu erstellen. Geben Sie matrix = \"gamma\" und document_names = names(quanteda_stm$documents) als Parameter an.\nGruppieren Sie die Dokumente nach document mit group_by(document).\nW√§hlen Sie die Dokumente mit dem h√∂chsten gamma-Wert mit slice_max(gamma).\nL√∂sen Sie die Gruppierung mit dplyr::ungroup().\nVerkn√ºpfen Sie die Daten mit review_subsample mittels left_join(review_subsample, by = c(\"document\" = \"id\")).\n\nBenennen Sie die Spalte document in id um mit dplyr::rename(id = document)\nErstellen Sie eine neue Variable stm_topic mit Hilfe des mutate()-Befehls. Verwenden Sie as.factor(paste(\"Topic\", sprintf(\"%02d\", topic))) um die Themen zu benennen und als Faktor zu speichern.\n\n√úberpr√ºfen Sie Transformation mit Hilfe der glimpse()-Funktion, um sicherzustellen, dass die Daten korrekt erstellt wurden.\n\n\n\nL√∂sung anzeigen\n# Create gamma export\ngamma_export &lt;- stm_mdl_k40 %&gt;% \n  tidytext::tidy(\n    matrix = \"gamma\", \n    document_names = names(quanteda_stm$documents)) %&gt;%\n  dplyr::group_by(document) %&gt;% \n  dplyr::slice_max(gamma) %&gt;% \n  dplyr::ungroup() %&gt;% \n  dplyr::left_join(review_subsample, by = c(\"document\" = \"id\")) %&gt;% \n  dplyr::rename(id = document) %&gt;% \n  dplyr::mutate(\n    stm_topic = as.factor(paste(\"Topic\", sprintf(\"%02d\", topic)))\n  )\n\n# Check\nglimpse(gamma_export)\n\n\nRows: 36,650\nColumns: 49\n$ id                          &lt;chr&gt; \"https://openalex.org/W1000529773\", \"https‚Ä¶\n$ topic                       &lt;int&gt; 25, 14, 14, 14, 7, 16, 34, 10, 30, 19, 9, ‚Ä¶\n$ gamma                       &lt;dbl&gt; 0.5042240, 0.2308276, 0.4425161, 0.3935272‚Ä¶\n$ title                       &lt;chr&gt; \"A critical evaluation of the teaching of ‚Ä¶\n$ display_name                &lt;chr&gt; \"A critical evaluation of the teaching of ‚Ä¶\n$ author                      &lt;list&gt; [&lt;data.frame[1 x 12]&gt;], [&lt;data.frame[2 x ‚Ä¶\n$ ab                          &lt;chr&gt; \"A Critical Evaluation of the Teaching of ‚Ä¶\n$ publication_date            &lt;chr&gt; \"2014-01-16\", \"2015-05-26\", \"2014-05-22\", ‚Ä¶\n$ relevance_score             &lt;dbl&gt; 4.012791, 27.896568, 32.074608, 23.670475,‚Ä¶\n$ so                          &lt;chr&gt; NA, \"Proceedings of the annual conference ‚Ä¶\n$ so_id                       &lt;chr&gt; NA, \"https://openalex.org/S4306523984\", \"h‚Ä¶\n$ host_organization           &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, \"Taylor & ‚Ä¶\n$ issn_l                      &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, \"1381-1118‚Ä¶\n$ url                         &lt;chr&gt; \"https://uwispace.sta.uwi.edu/dspace/bitst‚Ä¶\n$ pdf_url                     &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"h‚Ä¶\n$ license                     &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"c‚Ä¶\n$ version                     &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"p‚Ä¶\n$ first_page                  &lt;chr&gt; NA, \"76\", NA, NA, NA, NA, NA, NA, \"1\", \"11‚Ä¶\n$ last_page                   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, \"21\", \"122‚Ä¶\n$ volume                      &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, \"20\", \"78\"‚Ä¶\n$ issue                       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, \"1\", NA, \"‚Ä¶\n$ is_oa                       &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, ‚Ä¶\n$ is_oa_anywhere              &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, ‚Ä¶\n$ oa_status                   &lt;chr&gt; \"closed\", \"closed\", \"closed\", \"closed\", \"c‚Ä¶\n$ oa_url                      &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, \"https://e‚Ä¶\n$ any_repository_has_fulltext &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, ‚Ä¶\n$ language                    &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", ‚Ä¶\n$ grants                      &lt;list&gt; NA, NA, NA, NA, NA, NA, NA, NA, &lt;\"https:/‚Ä¶\n$ cited_by_count              &lt;int&gt; 0, 1, 1, 1, 1, 0, 2, 0, 226, 159, 122, 31,‚Ä¶\n$ counts_by_year              &lt;list&gt; NA, [&lt;data.frame[1 x 2]&gt;], [&lt;data.frame[1‚Ä¶\n$ publication_year            &lt;int&gt; 2014, 2015, 2014, 2013, 2013, 2015, 2015, ‚Ä¶\n$ cited_by_api_url            &lt;chr&gt; \"https://api.openalex.org/works?filter=cit‚Ä¶\n$ ids                         &lt;list&gt; &lt;\"https://openalex.org/W1000529773\", \"100‚Ä¶\n$ doi                         &lt;chr&gt; NA, \"https://doi.org/10.5555/2814058.28141‚Ä¶\n$ type                        &lt;chr&gt; \"article\", \"article\", \"article\", \"article\"‚Ä¶\n$ referenced_works            &lt;list&gt; NA, &lt;\"https://openalex.org/W1526029332\", ‚Ä¶\n$ related_works               &lt;list&gt; &lt;\"https://openalex.org/W958254955\", \"http‚Ä¶\n$ is_paratext                 &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, ‚Ä¶\n$ is_retracted                &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, ‚Ä¶\n$ concepts                    &lt;list&gt; [&lt;data.frame[7 x 5]&gt;], [&lt;data.frame[16 x ‚Ä¶\n$ topics_i                    &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ‚Ä¶\n$ topics_score                &lt;dbl&gt; 0.9566, 0.9812, 0.9894, 0.9999, 0.9752, 0.‚Ä¶\n$ topics_name                 &lt;chr&gt; \"field\", \"field\", \"field\", \"field\", \"field‚Ä¶\n$ topics_id                   &lt;chr&gt; \"https://openalex.org/fields/32\", \"https:/‚Ä¶\n$ topics_display_name         &lt;chr&gt; \"Psychology\", \"Social Sciences\", \"Psycholo‚Ä¶\n$ publication_year_fct        &lt;fct&gt; 2014, 2015, 2014, 2013, 2013, 2015, 2015, ‚Ä¶\n$ type_fct                    &lt;fct&gt; article, article, article, article, articl‚Ä¶\n$ field                       &lt;fct&gt; Psychology, Social Sciences, Psychology, S‚Ä¶\n$ stm_topic                   &lt;fct&gt; Topic 25, Topic 14, Topic 14, Topic 14, To‚Ä¶\n\n\n\n\n3.3 Verteilungsparameter von Thema 20\n\nErstellung eines Outputs zur √úberpr√ºfung der Lageparameter\n\nBasierend auf dem Datensatz gamma_export:\n\nFiltern Sie die Daten nach topic == 20.\nW√§hlen Sie mit Hilfe der select()-Funktion die Variablen gamma, relevance_score und cited_by_count aus.\nVerwenden Sie die Funktion datawizard::describe_distribution() um die Verteilungsparameter zu berechnen.\n\n\n‚úçÔ∏è Identifizieren und notieren Sie folgende Informationen:\n\nWie viele Abstracts haben Thema 20 als Hauptthema?\nWie hoch ist der durschnittliche Relevance Score?\nWie viele Zitationen haben die Dokumente im Durchschnitt?\nWie viel Zitate hat das hochzitierteste Dokument?\n\n\n\n\nL√∂sung anzeigen\n# Create distribution parameters\ngamma_export %&gt;% \n  filter(topic == 20) %&gt;%\n  select(gamma, relevance_score, cited_by_count) %&gt;% \n  datawizard::describe_distribution()\n\n\nVariable        |  Mean |    SD |   IQR |          Range | Skewness | Kurtosis |    n | n_Missing\n-------------------------------------------------------------------------------------------------\ngamma           |  0.37 |  0.13 |  0.17 |   [0.12, 0.87] |     0.74 |     0.43 | 1477 |         0\nrelevance_score | 32.55 | 40.35 | 36.74 | [2.01, 402.59] |     3.07 |    14.76 | 1477 |         0\ncited_by_count  | 13.54 | 50.51 |  7.00 | [0.00, 948.00] |    10.41 |   143.55 | 1477 |         0\n\n\nL√∂sung anzeigen\n#### Notes\n# Anzahl der Abstrats von Thema 20\n# Durchschnittlicher Relevace Score: \n# Durchschnittliche Zitationen:\n# Anzahl der Zitationen des am meisten zitierten Dokuments:\n\n\n\n\n3.4. Top-Dokumente des Themas\n\nIdentifizierung der Top-Dokumente\n\nBasierend auf dem Datensatz gamma_export:\n\nFiltern Sie den Datensatz nach stm_topic == \"Topic 20\".\nSortieren Sie die Daten absteigend nach gamma mit arrange(-gamma).\nW√§hlen Sie die Variablen title, so, gamma, type, und ab mit select() aus.\nW√§hlen Sie die obersten 5 Zeilen mit slice_head(n = 5).\n\n\nErstellung eines Outputs zur √úberpr√ºfung der Top-Dokumente\n\nBasierend auf dem Datensatz top_docs_k20:\n\nVerwenden Sie gt() um eine Tabelle zu erstellen.\nFormatieren Sie die Spalte gamma mit fmt_number(columns = vars(gamma), decimals = 2) um nur zwei Nachkommastellen anzuzeigen.\nVerwenden Sie gtExtras::gt_theme_538() um das Design der Tabelle anzupassen.\n\n\n\n‚úçÔ∏è Basierend auf den den Abstracts und den Titeln der Top-Dokumente:\n\nWelche Themenbereiche decken die Dokumente ab?\nW√ºrden Sie den im Abschnitt 3.1. gew√§hlten Themennamen beibehalten oder ab√§ndern?\n\n\n\n\nL√∂sung anzeigen\n# Identify top documents for topic 20\ntop_docs_k20 &lt;- gamma_export %&gt;% \n  filter(stm_topic == \"Topic 20\") %&gt;%\n  arrange(-gamma) %&gt;%\n  select(title, so, gamma, type, ab) %&gt;%\n  slice_head(n = 5) \n\n# Creae output\ntop_docs_k20 %&gt;% \n  gt() %&gt;% \n  fmt_number(\n    columns = c(gamma), \n    decimals = 2) %&gt;% \n  gtExtras::gt_theme_538()\n\n\n\n\n\n\n\n\ntitle\nso\ngamma\ntype\nab\n\n\n\n\nLiterature Review of Overseas Tourism Destination Brand Research\nJournal of Chongqing Technology and Business University\n0.87\narticle\nApplying brand theory to the study on tourism destination has been always hot issues for overseas scholars since 1990s.Tourism destination branding management is a significant marketing tool which can bring about effective identification internally,achieving differentiation with external competitors.Systematically reviewing and analyzing recent overseas tourism destination brand literatures,this paper makes conclusion and evaluation of the tourism destination brand construction,branding,brand stakeholders,brand operation,branding performance evaluation to provide reference for domestic tourism destination brand research and management.\n\n\nNatural Disasters in Colombia and Their Impact on the Food Security of the Affected Population. a Quick Review of the Literature.\nSocial Science Research Network\n0.85\narticle\nNatural disasters in Colombia significantly impact multiple domains of the affected population, including food security. Those events can cause food production and distribution interruptions, leading to scarcity and increased prices. Additionally, they can damage infrastructure and limit the communities' ability to access food. Food assistance during disasters is crucial to ensuring food security. The former is crucial for human survival and development, and entities responsible for risk management and food assistance play a fundamental role in protecting populations affected by natural disasters. Entities responsible for risk management in Colombia, such as the National Unit for Disaster Risk Management (UNGRD) and the Departmental and Municipal Risk Management Councils, coordinate efforts to provide food assistance and other basic needs. The Colombia Food Bank also plays an essential role in responding to food emergencies as a first responder. In this article, we investigate some of the leading natural disasters that Colombia has suffered in recent years and how these events have affected different communities. Likewise, we explore how the response has been made from the risk management framework, highlighting food assistance.\n\n\nGovernment Responsibility as the Main Stakeholder in Tourism Development With Collaboration Approach: Literature Review on Heritage Tourism\nNA\n0.84\narticle\ncurrently, the economic growth of some countries is a contribution by the vast development of the tourism sector, and one of the potential destinations is heritage motivation.More than fifty-three of previous study founds five elements that associated with Heritage Tourism Development, which are four elements influencing directly and one element is impact after development as an outcome.The study is focusing on stakeholders responsibility which led by government to do the development of heritage tourism.Barriers of policies and low attention to strategic plants and policies are an influencer to the obstacles because of less attention from the leader.Collaboration approach helped government to control the system in heritage tourism process.\n\n\nBusiness Strategy in Management Perspective: A Literature Review\nIndonesian Journal of Economic & Management Sciences\n0.83\narticle\nBusiness development in the world has entered the era of free markets and broad competition, not only in small areas but also in large areas. Efforts made by a company to win the market are by providing competitive advantages, analyzing competitors, and implementing effective and efficient marketing strategies\n\n\nA Literature Review on Structural Reform of Agricultural Supply Side\nNA\n0.82\narticle\nThe structural reform of the agricultural supply side is the major deployment of the \"No. 1 document\" on agriculture, not only for the direction of agricultural industry development, but also for the agricultural industry structure optimization adjustment to play a needle \"tonic\", also engaged in agricultural economic research experts and scholars Correct future and long-term research direction to play a \"heading\" role.This paper summarizes the policy of \"structural reform of agricultural supply side\", which is related to the optimization and upgrading of agricultural industry structure, the cultivation of agricultural enterprises, the integration of agriculture, the development strategy of agricultural brand, the innovation of agricultural technology, \"Rural land management system reform\", \"agricultural development policy\" and other research results."
  },
  {
    "objectID": "exercises/ms-showcase-09.html",
    "href": "exercises/ms-showcase-09.html",
    "title": "Unsupervised Machine Learning I",
    "section": "",
    "text": "Link to slides"
  },
  {
    "objectID": "exercises/ms-showcase-09.html#preparation",
    "href": "exercises/ms-showcase-09.html#preparation",
    "title": "Unsupervised Machine Learning I",
    "section": "Preparation",
    "text": "Preparation\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n    here, qs, # file management\n    magrittr, janitor, # data wrangling\n    easystats, sjmisc, # data analysis\n    gt, gtExtras, # table visualization\n    ggpubr, ggwordcloud, # visualization\n    # text analysis    \n    tidytext, widyr, # based on tidytext\n    quanteda, # based on quanteda\n    quanteda.textmodels, quanteda.textplots, quanteda.textstats, \n    stm, # structural topic modeling\n    openalexR, \n    tidyverse # load last to avoid masking issues\n  )"
  },
  {
    "objectID": "exercises/ms-showcase-09.html#codechunks-aus-der-sitzung",
    "href": "exercises/ms-showcase-09.html#codechunks-aus-der-sitzung",
    "title": "Unsupervised Machine Learning I",
    "section": "Codechunks aus der Sitzung",
    "text": "Codechunks aus der Sitzung\n\nErstellung und Bearbeitung der Subsample\n\nreview_subsample &lt;- review_works_correct %&gt;% \n  # Eingrenzung: Sprache und Typ\n  filter(language == \"en\") %&gt;% \n  filter(type == \"article\") %&gt;%\n  # Datentranformation\n  unnest(topics, names_sep = \"_\") %&gt;%\n  filter(topics_name == \"field\") %&gt;% \n  filter(topics_i == \"1\") %&gt;% \n  # Eingrenzung: Forschungsfeldes\n  filter(\n   topics_display_name == \"Social Sciences\"|\n   topics_display_name == \"Psychology\"\n    )\n\n# √úberblick \nreview_subsample %&gt;% glimpse\n\nRows: 45,221\nColumns: 45\n$ id                          &lt;chr&gt; \"https://openalex.org/W4293003987\", \"https‚Ä¶\n$ title                       &lt;chr&gt; \"The WHO-5 Well-Being Index: A Systematic ‚Ä¶\n$ display_name                &lt;chr&gt; \"The WHO-5 Well-Being Index: A Systematic ‚Ä¶\n$ author                      &lt;list&gt; [&lt;data.frame[4 x 12]&gt;], [&lt;data.frame[2 x ‚Ä¶\n$ ab                          &lt;chr&gt; \"The 5-item World Health Organization Well‚Ä¶\n$ publication_date            &lt;chr&gt; \"2015-01-01\", \"2017-08-28\", \"2014-01-01\", ‚Ä¶\n$ relevance_score             &lt;dbl&gt; 938.7603, 752.3500, 591.2553, 576.1210, 56‚Ä¶\n$ so                          &lt;chr&gt; \"Psychotherapy and psychosomatics\", \"Journ‚Ä¶\n$ so_id                       &lt;chr&gt; \"https://openalex.org/S184803288\", \"https:‚Ä¶\n$ host_organization           &lt;chr&gt; \"Karger Publishers\", \"SAGE Publishing\", NA‚Ä¶\n$ issn_l                      &lt;chr&gt; \"0033-3190\", \"0739-456X\", NA, \"2214-7829\",‚Ä¶\n$ url                         &lt;chr&gt; \"https://doi.org/10.1159/000376585\", \"http‚Ä¶\n$ pdf_url                     &lt;chr&gt; \"https://www.karger.com/Article/Pdf/376585‚Ä¶\n$ license                     &lt;chr&gt; \"cc-by-nc\", NA, NA, \"cc-by\", NA, NA, \"cc-b‚Ä¶\n$ version                     &lt;chr&gt; \"publishedVersion\", NA, \"publishedVersion\"‚Ä¶\n$ first_page                  &lt;chr&gt; \"167\", \"93\", NA, \"89\", \"55\", \"2150\", \"e356‚Ä¶\n$ last_page                   &lt;chr&gt; \"176\", \"112\", NA, \"106\", \"64\", \"2159\", \"e3‚Ä¶\n$ volume                      &lt;chr&gt; \"84\", \"39\", NA, \"6\", \"277\", \"32\", \"2\", \"24‚Ä¶\n$ issue                       &lt;chr&gt; \"3\", \"1\", NA, NA, NA, \"19\", \"8\", NA, \"9\", ‚Ä¶\n$ is_oa                       &lt;lgl&gt; TRUE, FALSE, TRUE, TRUE, TRUE, FALSE, TRUE‚Ä¶\n$ is_oa_anywhere              &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE,‚Ä¶\n$ oa_status                   &lt;chr&gt; \"hybrid\", \"green\", \"bronze\", \"gold\", \"bron‚Ä¶\n$ oa_url                      &lt;chr&gt; \"https://www.karger.com/Article/Pdf/376585‚Ä¶\n$ any_repository_has_fulltext &lt;lgl&gt; FALSE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE‚Ä¶\n$ language                    &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", ‚Ä¶\n$ grants                      &lt;list&gt; NA, NA, NA, &lt;\"https://openalex.org/F43203‚Ä¶\n$ cited_by_count              &lt;int&gt; 2657, 1375, 2568, 803, 3664, 1553, 2895, 9‚Ä¶\n$ counts_by_year              &lt;list&gt; [&lt;data.frame[11 x 2]&gt;], [&lt;data.frame[7 x ‚Ä¶\n$ publication_year            &lt;int&gt; 2015, 2017, 2014, 2016, 2020, 2014, 2017, ‚Ä¶\n$ cited_by_api_url            &lt;chr&gt; \"https://api.openalex.org/works?filter=cit‚Ä¶\n$ ids                         &lt;list&gt; &lt;\"https://openalex.org/W4293003987\", \"htt‚Ä¶\n$ doi                         &lt;chr&gt; \"https://doi.org/10.1159/000376585\", \"http‚Ä¶\n$ type                        &lt;chr&gt; \"article\", \"article\", \"article\", \"article\"‚Ä¶\n$ referenced_works            &lt;list&gt; &lt;\"https://openalex.org/W1492518593\", \"htt‚Ä¶\n$ related_works               &lt;list&gt; &lt;\"https://openalex.org/W3020194755\", \"htt‚Ä¶\n$ is_paratext                 &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, ‚Ä¶\n$ is_retracted                &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, ‚Ä¶\n$ concepts                    &lt;list&gt; [&lt;data.frame[7 x 5]&gt;], [&lt;data.frame[18 x ‚Ä¶\n$ topics_i                    &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ‚Ä¶\n$ topics_score                &lt;dbl&gt; 0.9926, 0.9050, 0.9995, 0.9987, 0.9999, 1.‚Ä¶\n$ topics_name                 &lt;chr&gt; \"field\", \"field\", \"field\", \"field\", \"field‚Ä¶\n$ topics_id                   &lt;chr&gt; \"https://openalex.org/fields/32\", \"https:/‚Ä¶\n$ topics_display_name         &lt;chr&gt; \"Psychology\", \"Social Sciences\", \"Psycholo‚Ä¶\n$ publication_year_fct        &lt;fct&gt; 2015, 2017, 2014, 2016, 2020, 2014, 2017, ‚Ä¶\n$ type_fct                    &lt;fct&gt; article, article, article, article, articl‚Ä¶\n\n\n\n\nExkurs: Identfikation von fehlenden Werten\n\nvisdat::vis_miss(review_subsample, warn_large_data = FALSE)\n\n\n\n\n\n\n\nFigure¬†1\n\n\n\n\n\n\n\nAnpassung der Subsample\n\nreview_subsample &lt;- review_works_correct %&gt;%\n  # Eingrenzung: Sprache und Typ\n  filter(language == \"en\") %&gt;% \n  filter(type == \"article\") %&gt;%\n  # Datentranformation\n  unnest(topics, names_sep = \"_\") %&gt;%\n  filter(topics_name == \"field\") %&gt;% \n  filter(topics_i == \"1\") %&gt;% \n  # Eingrenzung: Forschungsfeldes\n  filter(\n   topics_display_name == \"Social Sciences\"|\n   topics_display_name == \"Psychology\"\n   ) %&gt;% \n  # Eingrenzung: Keine Eintr√§ge ohne Abstract\n  filter(!is.na(ab))\n\n# √úberblick \nreview_subsample %&gt;% glimpse\n\nRows: 36,680\nColumns: 45\n$ id                          &lt;chr&gt; \"https://openalex.org/W4293003987\", \"https‚Ä¶\n$ title                       &lt;chr&gt; \"The WHO-5 Well-Being Index: A Systematic ‚Ä¶\n$ display_name                &lt;chr&gt; \"The WHO-5 Well-Being Index: A Systematic ‚Ä¶\n$ author                      &lt;list&gt; [&lt;data.frame[4 x 12]&gt;], [&lt;data.frame[2 x ‚Ä¶\n$ ab                          &lt;chr&gt; \"The 5-item World Health Organization Well‚Ä¶\n$ publication_date            &lt;chr&gt; \"2015-01-01\", \"2017-08-28\", \"2014-01-01\", ‚Ä¶\n$ relevance_score             &lt;dbl&gt; 938.7603, 752.3500, 591.2553, 576.1210, 56‚Ä¶\n$ so                          &lt;chr&gt; \"Psychotherapy and psychosomatics\", \"Journ‚Ä¶\n$ so_id                       &lt;chr&gt; \"https://openalex.org/S184803288\", \"https:‚Ä¶\n$ host_organization           &lt;chr&gt; \"Karger Publishers\", \"SAGE Publishing\", NA‚Ä¶\n$ issn_l                      &lt;chr&gt; \"0033-3190\", \"0739-456X\", NA, \"2214-7829\",‚Ä¶\n$ url                         &lt;chr&gt; \"https://doi.org/10.1159/000376585\", \"http‚Ä¶\n$ pdf_url                     &lt;chr&gt; \"https://www.karger.com/Article/Pdf/376585‚Ä¶\n$ license                     &lt;chr&gt; \"cc-by-nc\", NA, NA, \"cc-by\", NA, NA, \"cc-b‚Ä¶\n$ version                     &lt;chr&gt; \"publishedVersion\", NA, \"publishedVersion\"‚Ä¶\n$ first_page                  &lt;chr&gt; \"167\", \"93\", NA, \"89\", \"55\", \"2150\", \"e356‚Ä¶\n$ last_page                   &lt;chr&gt; \"176\", \"112\", NA, \"106\", \"64\", \"2159\", \"e3‚Ä¶\n$ volume                      &lt;chr&gt; \"84\", \"39\", NA, \"6\", \"277\", \"32\", \"2\", \"24‚Ä¶\n$ issue                       &lt;chr&gt; \"3\", \"1\", NA, NA, NA, \"19\", \"8\", NA, \"9\", ‚Ä¶\n$ is_oa                       &lt;lgl&gt; TRUE, FALSE, TRUE, TRUE, TRUE, FALSE, TRUE‚Ä¶\n$ is_oa_anywhere              &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE,‚Ä¶\n$ oa_status                   &lt;chr&gt; \"hybrid\", \"green\", \"bronze\", \"gold\", \"bron‚Ä¶\n$ oa_url                      &lt;chr&gt; \"https://www.karger.com/Article/Pdf/376585‚Ä¶\n$ any_repository_has_fulltext &lt;lgl&gt; FALSE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE‚Ä¶\n$ language                    &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", ‚Ä¶\n$ grants                      &lt;list&gt; NA, NA, NA, &lt;\"https://openalex.org/F43203‚Ä¶\n$ cited_by_count              &lt;int&gt; 2657, 1375, 2568, 803, 3664, 1553, 2895, 9‚Ä¶\n$ counts_by_year              &lt;list&gt; [&lt;data.frame[11 x 2]&gt;], [&lt;data.frame[7 x ‚Ä¶\n$ publication_year            &lt;int&gt; 2015, 2017, 2014, 2016, 2020, 2014, 2017, ‚Ä¶\n$ cited_by_api_url            &lt;chr&gt; \"https://api.openalex.org/works?filter=cit‚Ä¶\n$ ids                         &lt;list&gt; &lt;\"https://openalex.org/W4293003987\", \"htt‚Ä¶\n$ doi                         &lt;chr&gt; \"https://doi.org/10.1159/000376585\", \"http‚Ä¶\n$ type                        &lt;chr&gt; \"article\", \"article\", \"article\", \"article\"‚Ä¶\n$ referenced_works            &lt;list&gt; &lt;\"https://openalex.org/W1492518593\", \"htt‚Ä¶\n$ related_works               &lt;list&gt; &lt;\"https://openalex.org/W3020194755\", \"htt‚Ä¶\n$ is_paratext                 &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, ‚Ä¶\n$ is_retracted                &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, ‚Ä¶\n$ concepts                    &lt;list&gt; [&lt;data.frame[7 x 5]&gt;], [&lt;data.frame[18 x ‚Ä¶\n$ topics_i                    &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ‚Ä¶\n$ topics_score                &lt;dbl&gt; 0.9926, 0.9050, 0.9995, 0.9987, 0.9999, 1.‚Ä¶\n$ topics_name                 &lt;chr&gt; \"field\", \"field\", \"field\", \"field\", \"field‚Ä¶\n$ topics_id                   &lt;chr&gt; \"https://openalex.org/fields/32\", \"https:/‚Ä¶\n$ topics_display_name         &lt;chr&gt; \"Psychology\", \"Social Sciences\", \"Psycholo‚Ä¶\n$ publication_year_fct        &lt;fct&gt; 2015, 2017, 2014, 2016, 2020, 2014, 2017, ‚Ä¶\n$ type_fct                    &lt;fct&gt; article, article, article, article, articl‚Ä¶\n\n\n\n\nDTM/DFM Erstellung\n\n`tidytext``\n\n# Create tidy data\nsubsample_tidy &lt;- review_subsample %&gt;% \n    tidytext::unnest_tokens(\"text\", ab) %&gt;% \n    filter(!text %in% tidytext::stop_words$word)\n\n# Create summarized data\nsubsample_summarized &lt;- subsample_tidy %&gt;% \n  count(id, text) \n\n# Create DTM\nsubsample_dtm &lt;- subsample_summarized %&gt;% \n  cast_dtm(id, text, n)\n\n# Preview\nsubsample_dtm\n\n&lt;&lt;DocumentTermMatrix (documents: 36654, terms: 122147)&gt;&gt;\nNon-/sparse entries: 3280664/4473895474\nSparsity           : 100%\nMaximal term length: 188\nWeighting          : term frequency (tf)\n\n\n\n\nquanteda\n\n# Create corpus\nquanteda_corpus &lt;- review_subsample %&gt;% \n  quanteda::corpus(\n    docid_field = \"id\", \n    text_field = \"ab\"\n  )\n\n# Tokenize\nquanteda_token &lt;- quanteda_corpus %&gt;% \n  quanteda::tokens(\n    remove_punct = TRUE,\n    remove_symbols = TRUE, \n    remove_numbers = TRUE, \n    remove_url = TRUE, \n    split_tags = FALSE # keep hashtags and mentions\n  ) %&gt;% \n  quanteda::tokens_tolower() %&gt;% \n  quanteda::tokens_remove(\n    pattern = stopwords(\"en\")\n    )\n\n# Convert to Document-Feature-Matrix (DFM)\nquanteda_dfm &lt;- quanteda_token %&gt;% \n  quanteda::dfm()\n\n# Preview\nquanteda_dfm\n\n\n\n\nNetzwerk der Top-Begriffe\n\ntidytext\n\n# Extract most common hashtags\ntop50_features_tidy &lt;- subsample_tidy %&gt;% \n  count(text, sort = TRUE) %&gt;%\n  slice_head(n = 50) %&gt;% \n  pull(text)\n\n# Visualize\nsubsample_tidy %&gt;% \n  count(id, text, sort = TRUE) %&gt;% \n  filter(!is.na(text)) %&gt;% \n  cast_dfm(id, text, n) %&gt;% \n  quanteda::fcm() %&gt;% \n  quanteda::fcm_select(\n    pattern = top50_features_tidy,\n    case_insensitive = FALSE\n  ) %&gt;%  \n  quanteda.textplots::textplot_network(\n    edge_color = \"#04316A\"\n  )\n\n\n\n\n\n\n\n\n\n\nquanteda\n\n# Extract most common features \ntop50_features_quanteda &lt;- quanteda_dfm %&gt;% \n  topfeatures(50) %&gt;% \n  names()\n\n# Construct feature-occurrence matrix of features\nquanteda_dfm %&gt;% \n  fcm() %&gt;% \n  fcm_select(pattern = top50_features_quanteda) %&gt;% \n  textplot_network(\n    edge_color = \"#C50F3C\"\n  ) \n\n\n\n\n\n\n\n\n\n\n\nPreparation for STM\n\n# Pruning\nquanteda_dfm_trim &lt;- quanteda_dfm %&gt;% \n  dfm_trim( \n    min_docfreq = 10/nrow(review_subsample),\n    max_docfreq = 0.99, \n    docfreq_type = \"prop\")\n\n# Convert for stm topic modeling\nquanteda_stm &lt;- quanteda_dfm_trim %&gt;% \n   convert(to = \"stm\")\n\n\n\nStructural Topic Model\n\nSch√§tzung\n\ntictoc::tic()\nstm_mdl &lt;- stm::stm(\n  documents = quanteda_stm$documents,\n  vocab = quanteda_stm$vocab, \n  K = 20, \n  seed = 42,\n  max.em.its = 10,\n  init.type = \"Spectral\",\n  verbose = TRUE)\ntictoc::toc(log = TRUE)\n\n\n\nModelinformationen\n\n# √úberblick √ºber STM\nstm_mdl\n\nA topic model with 20 topics, 36650 documents and a 14322 word dictionary.\n\n\n\n\n√úberblick √ºber die Themen\n\n# Simple\nplot(stm_mdl, type = \"summary\")\n\n\n\n\n\n\n\n\n\n# Komplex\ntop_gamma &lt;- stm_mdl %&gt;%\n  tidy(matrix = \"gamma\") %&gt;% \n  dplyr::group_by(topic) %&gt;%\n  dplyr::summarise(gamma = mean(gamma), .groups = \"drop\") %&gt;%\n  dplyr::arrange(desc(gamma))\n\ntop_beta &lt;- stm_mdl %&gt;%\n  tidytext::tidy(.) %&gt;% \n  dplyr::group_by(topic) %&gt;%\n  dplyr::arrange(-beta) %&gt;%\n  dplyr::top_n(10, wt = beta) %&gt;% \n  dplyr::select(topic, term) %&gt;%\n  dplyr::summarise(terms_beta = toString(term), .groups = \"drop\")\n\ntop_topics_terms &lt;- top_beta %&gt;% \n  dplyr::left_join(top_gamma, by = \"topic\") %&gt;%\n  dplyr::mutate(\n          topic = reorder(topic, gamma)\n      )\n\n# Preview\ntop_topics_terms %&gt;%\n  mutate(across(gamma, ~round(.,3))) %&gt;% \n  dplyr::arrange(-gamma) %&gt;% \n  gt() %&gt;% \n  cols_label(\n    topic = \"Topic\", \n    terms_beta = \"Top Terms (based on beta)\",\n    gamma = \"Gamma\"\n  ) %&gt;% \n  gtExtras::gt_theme_538()\n\n\n\n\n\n\n\nTopic\nTop Terms (based on beta)\nGamma\n\n\n\n\n16\nresearch, literature, review, paper, systematic, future, study, analysis, findings, knowledge\n0.142\n\n\n19\nstudies, interventions, review, systematic, evidence, outcomes, included, quality, intervention, health\n0.096\n\n\n13\nhealth, mental, care, review, support, family, children, social, factors, studies\n0.079\n\n\n9\nlearning, students, education, school, review, skills, educational, teachers, teaching, study\n0.072\n\n\n11\nstudy, literature, research, can, work, development, review, also, human, economic\n0.068\n\n\n15\nstudies, ci, effect, meta-analysis, depression, p, trials, anxiety, effects, interventions\n0.061\n\n\n14\nsocial, media, information, use, digital, data, technology, communication, review, research\n0.060\n\n\n17\nstudies, children, relationship, review, language, variables, research, factors, results, effects\n0.052\n\n\n6\nprevalence, studies, covid-19, suicide, among, risk, pandemic, countries, ci, vaccine\n0.050\n\n\n1\nsleep, studies, eating, cognitive, review, associated, weight, body, may, association\n0.044\n\n\n2\nhealth, studies, women, gender, review, care, social, services, cultural, access\n0.043\n\n\n18\nstudies, health, used, measures, review, tools, instruments, assessment, training, n\n0.039\n\n\n3\ntreatment, disorder, disorders, patients, ptsd, symptoms, clinical, studies, therapy, anxiety\n0.036\n\n\n7\narticles, review, adolescents, studies, literature, search, use, results, systematic, databases\n0.036\n\n\n4\npatients, articles, review, music, therapy, cancer, can, study, pain, life\n0.032\n\n\n12\nviolence, studies, use, sexual, risk, h3, ipv, substance, alcohol, review\n0.031\n\n\n5\net, al, university, review, gt, literature, lt, author, p, search\n0.030\n\n\n8\nphysical, training, studies, disability, exercise, disabilities, employment, ed, review, strength\n0.015\n\n\n20\nattachment, studies, scholar, google, science, review, social, styles, welfare, research\n0.009\n\n\n10\nde, la, y, en, los, e, ÁöÑ, el, se, que\n0.004\n\n\n\n\n\n\n\n\n\nThemenkorrelation\n\nstm_corr &lt;- stm::topicCorr(stm_mdl)\nplot(stm_corr)\n\n\n\n\n\n\n\n\n\n\nFokus auf einzele Themen\n\nProminente W√∂rter\n\n# Fokus auf Themas 16\nstm::labelTopics(stm_mdl, topic=16)\n\nTopic 16 Top Words:\n     Highest Prob: research, literature, review, paper, systematic, future, study \n     FREX: tourism, sustainable, sustainability, originality, innovation, conceptual, agenda \n     Lift: paradigmatic, hrd, edlm, positivist, internationalisation, tccm, wom \n     Score: tourism, research, literature, paper, leadership, themes, sustainable \n\n# Fokus auf Thema 10\nstm::labelTopics(stm_mdl, topic=10)\n\nTopic 10 Top Words:\n     Highest Prob: de, la, y, en, los, e, ÁöÑ \n     FREX: resultados, foram, que, sobre, intervenciones, riesgo, uma \n     Lift: criterios, efecto, as√≠, comparaci√≥n, cumplieron, debido, depresi√≥n \n     Score: de, la, ÁöÑ, en, los, y, que"
  },
  {
    "objectID": "exercises/ms-exercise-09.html",
    "href": "exercises/ms-exercise-09.html",
    "title": "Unsupervised Machine Learning I",
    "section": "",
    "text": "Link to source file"
  },
  {
    "objectID": "exercises/ms-exercise-09.html#background",
    "href": "exercises/ms-exercise-09.html#background",
    "title": "Unsupervised Machine Learning I",
    "section": "Background",
    "text": "Background\n\n\n\n\n\n\nTodays‚Äôs data basis: OpenAlex\n\n\n\n\nVia API bzw. openalexR (Aria et al. 2024) gesammelte ‚Äúworks‚Äù der Datenbank OpenAlex mit Bezug zu Literaturriews in den Sozialwissenschaften zwischen 2013 und 2023\nDetaillierte Informationen und Ergebnisse zur Suchquery finden Sie hier."
  },
  {
    "objectID": "exercises/ms-exercise-09.html#preparation",
    "href": "exercises/ms-exercise-09.html#preparation",
    "title": "Unsupervised Machine Learning I",
    "section": "Preparation",
    "text": "Preparation\n\n\n\n\n\n\nWichtige Information\n\n\n\n\nBitte stellen Sie sicher, dass Sie das jeweilige R-Studio Projekt zur √úbung ge√∂ffnet haben. Nur so funktionieren alle Dependencies korrekt.\nUm den einwandfreien Ablauf der √úbung zu gew√§hrleisten, wird f√ºr die Aufgaben auf eine eigenst√§ndige Datenerhebung verzichtet und ein √úbungsdatensatz zu verf√ºgung gestelt.\n\n\n\n\nPackages\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n    here, qs, # file management\n    magrittr, janitor, # data wrangling\n    easystats, sjmisc, # data analysis\n    gt, gtExtras, # table visualization\n    ggpubr, ggwordcloud, # visualization\n    # text analysis    \n    tidytext, widyr, # based on tidytext\n    quanteda, # based on quanteda\n    quanteda.textmodels, quanteda.textplots, quanteda.textstats, \n    openalexR, \n    tidyverse # load last to avoid masking issues\n  )\n\n\n\nImport und Vorverarbeitung der Daten\n\n# Import from local\nreview_works &lt;- qs::qread(here(\"data/session-07/openalex-review_works-2013_2023.qs\"))\n\n# Create correct data\nreview_works_correct &lt;- review_works %&gt;% \n    mutate(\n        # Create additional factor variables\n        publication_year_fct = as.factor(publication_year), \n        type_fct = as.factor(type)\n        )\n\n# Create subsample\nreview_subsample &lt;- review_works_correct %&gt;%\n    # Eingrenzung: Sprache und Typ\n    filter(language == \"en\") %&gt;% \n    filter(type == \"article\") %&gt;%\n    # Eingrenzung: Keine Eintr√§ge ohne Abstract\n    filter(!is.na(ab)) %&gt;% \n    # Datentranformation\n    unnest(topics, names_sep = \"_\") %&gt;%\n    filter(topics_name == \"field\" ) %&gt;% \n    filter(topics_i == \"1\") %&gt;% \n    # Eingrenzung: Forschungsfeldes\n    filter(\n    topics_display_name == \"Social Sciences\"|\n    topics_display_name == \"Psychology\"\n    ) %&gt;% \n    # Eingrenzung: Keine Eintr√§ge ohne Abstract\n    filter(!is.na(ab))   \n\n\n\nErstellung Korpus & DFM\n\n\nL√∂sung anzeigen\n# Create corpus\nquanteda_corpus &lt;- review_subsample %&gt;% \n  quanteda::corpus(\n    docid_field = \"id\", \n    text_field = \"ab\"\n  )\n\n# Tokenize\nquanteda_token &lt;- quanteda_corpus %&gt;% \n  quanteda::tokens(\n    remove_punct = TRUE,\n    remove_symbols = TRUE, \n    remove_numbers = TRUE, \n    remove_url = TRUE, \n    split_tags = FALSE # keep hashtags and mentions\n  ) %&gt;% \n  quanteda::tokens_tolower() %&gt;% \n  quanteda::tokens_remove(\n    pattern = stopwords(\"en\")\n    )\n\n# Convert to Document-Feature-Matrix (DFM)\nquanteda_dfm &lt;- quanteda_token %&gt;% \n  quanteda::dfm()"
  },
  {
    "objectID": "exercises/ms-exercise-09.html#praktische-anwendung",
    "href": "exercises/ms-exercise-09.html#praktische-anwendung",
    "title": "Unsupervised Machine Learning I",
    "section": "üõ†Ô∏è Praktische Anwendung",
    "text": "üõ†Ô∏è Praktische Anwendung\n\n\n\n\n\n\nAchtung, bitte lesen!\n\n\n\n\nBevor Sie mit der Arbeit an den folgenden üìã Exercises beginnen, stellen Sie bitte sicher, dass Sie alle Chunks des Abschnitts Preparation gerendert haben. Das k√∂nnen Sie tun, indem Sie den ‚ÄúRun all chunks above‚Äù-Knopf des n√§chsten Chunks benutzen.\nBei Fragen zum Code lohnt sich ein Blick in den Showcase (.qmd oder .html). Beim Showcase handelt es sich um eine kompakte Darstellung des in der Pr√§sentation verwenden R-Codes. Sie k√∂nnen das Showcase also nutzen, um sich die Code-Bausteine anzusehen, die f√ºr die R-Outputs auf den Slides benutzt wurden.\n\n\n\n\nüìã Exercise 1: Cleaned DFM\n\nErstelen Sie einen neuen Datensatz quanteda_dfm_cleaned\n\nbasierend auf dem Datensatz quanteda_dfm\n\nVerwenden Sie quanteda::dfm_remove(pattern = c(\"systematic\", \"literature\", \"review\"), um die Suchquery zu entfernen.\nSpeichern Sie diese Umwandlung, indem Sie einen neuen Datensatz mit dem Namen quanteda_dfm_cleaned erstellen.\n\n\n√úberpr√ºfen Sie die Transformation indem Sie quanteda_dfm_cleaned in die Konsole eingeben.\n‚úçÔ∏è Notieren Sie, wie viele Dokumente & Features in quanteda_dfm_cleaned enthalten sind.\n\n\n\nL√∂sung anzeigen\n# `quanteda_dfm_cleaned` erstellen\n\n# √úberpr√ºfung\n\n# Notiz:\n# `quanteda_dfm_cleaned` enth√§lt 36680 Dokumente und 135074 Features.\n\n\n\n\nüìã Exercise 2: Neues Netzwerk der Top-Begriffe\n\nNeues Dataset top_features_quanteda erstellen\n\nBasierend auf dem Dataset quanteda_dfm_cleaned,\nVerwenden Sie quanteda::topfeatures(20), um die 20 h√§ufigsten Begriffe zu extrahieren.\nVerwenden Sie names(), um nur die Namen (nicht die Werte) zu speichern.\nSpeichern Sie diese Transformation, indem Sie einen neuen Datensatz mit dem Namen top_features_quanteda erstellen.\n\nVisualisierung des Netzwerks an Top-Begriffen\n\nBasierend auf dem Dataset quanteda_dfm_cleaned,\nTransformieren Sie die Daten mit quanteda::fcm() in eine Feature-Co-Occurrence-Matrix [FCM].\nAuswahl relevanter Hashtags mit quanteda::fcm_select(pattern = top_features_quanteda, case_insensitive = FALSE).\nVisualisierung mit quanteda.textplots::textplot_network().\n\nErgebnisse interpretieren und vergleichen\n\nAnalysieren Sie die Beziehungen zwischen den Top-Begriffen.\nVergleichen Sie die Ergebnisse mit den Auswertungen der Folien. Welche Unterschiede gibt es?\n\n\n\n\nL√∂sung anzeigen\n# Create top features\n\n# Visualisierung des Netzwerks an Top-Begriffen"
  },
  {
    "objectID": "exercises/ms-exercise-09_solution.html",
    "href": "exercises/ms-exercise-09_solution.html",
    "title": "Unsupervised Machine Learning I",
    "section": "",
    "text": "Link to source file"
  },
  {
    "objectID": "exercises/ms-exercise-09_solution.html#background",
    "href": "exercises/ms-exercise-09_solution.html#background",
    "title": "Unsupervised Machine Learning I",
    "section": "Background",
    "text": "Background\n\n\n\n\n\n\nTodays‚Äôs data basis: OpenAlex\n\n\n\n\nVia API bzw. openalexR (Aria et al. 2024) gesammelte ‚Äúworks‚Äù der Datenbank OpenAlex mit Bezug zu Literaturriews in den Sozialwissenschaften zwischen 2013 und 2023\nDetaillierte Informationen und Ergebnisse zur Suchquery finden Sie hier."
  },
  {
    "objectID": "exercises/ms-exercise-09_solution.html#preparation",
    "href": "exercises/ms-exercise-09_solution.html#preparation",
    "title": "Unsupervised Machine Learning I",
    "section": "Preparation",
    "text": "Preparation\n\n\n\n\n\n\nWichtige Information\n\n\n\n\nBitte stellen Sie sicher, dass Sie das jeweilige R-Studio Projekt zur √úbung ge√∂ffnet haben. Nur so funktionieren alle Dependencies korrekt.\nUm den einwandfreien Ablauf der √úbung zu gew√§hrleisten, wird f√ºr die Aufgaben auf eine eigenst√§ndige Datenerhebung verzichtet und ein √úbungsdatensatz zu verf√ºgung gestelt.\n\n\n\n\nPackages\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n    here, qs, # file management\n    magrittr, janitor, # data wrangling\n    easystats, sjmisc, # data analysis\n    gt, gtExtras, # table visualization\n    ggpubr, ggwordcloud, # visualization\n    # text analysis    \n    tidytext, widyr, # based on tidytext\n    quanteda, # based on quanteda\n    quanteda.textmodels, quanteda.textplots, quanteda.textstats, \n    openalexR, \n    tidyverse # load last to avoid masking issues\n  )\n\n\n\nImport und Vorverarbeitung der Daten\n\n# Import from local\nreview_works &lt;- qs::qread(here(\"data/session-07/openalex-review_works-2013_2023.qs\"))\n\n# Create correct data\nreview_works_correct &lt;- review_works %&gt;% \n    mutate(\n        # Create additional factor variables\n        publication_year_fct = as.factor(publication_year), \n        type_fct = as.factor(type)\n        )\n\n# Create subsample\nreview_subsample &lt;- review_works_correct %&gt;%\n    # Eingrenzung: Sprache und Typ\n    filter(language == \"en\") %&gt;% \n    filter(type == \"article\") %&gt;%\n    # Eingrenzung: Keine Eintr√§ge ohne Abstract\n    filter(!is.na(ab)) %&gt;% \n    # Datentranformation\n    unnest(topics, names_sep = \"_\") %&gt;%\n    filter(topics_name == \"field\" ) %&gt;% \n    filter(topics_i == \"1\") %&gt;% \n    # Eingrenzung: Forschungsfeldes\n    filter(\n    topics_display_name == \"Social Sciences\"|\n    topics_display_name == \"Psychology\"\n    ) %&gt;% \n    # Eingrenzung: Keine Eintr√§ge ohne Abstract\n    filter(!is.na(ab))   \n\n\n\nErstellung Korpus & DFM\n\n\nL√∂sung anzeigen\n# Create corpus\nquanteda_corpus &lt;- review_subsample %&gt;% \n  quanteda::corpus(\n    docid_field = \"id\", \n    text_field = \"ab\"\n  )\n\n# Tokenize\nquanteda_token &lt;- quanteda_corpus %&gt;% \n  quanteda::tokens(\n    remove_punct = TRUE,\n    remove_symbols = TRUE, \n    remove_numbers = TRUE, \n    remove_url = TRUE, \n    split_tags = FALSE # keep hashtags and mentions\n  ) %&gt;% \n  quanteda::tokens_tolower() %&gt;% \n  quanteda::tokens_remove(\n    pattern = stopwords(\"en\")\n    )\n\n# Convert to Document-Feature-Matrix (DFM)\nquanteda_dfm &lt;- quanteda_token %&gt;% \n  quanteda::dfm()"
  },
  {
    "objectID": "exercises/ms-exercise-09_solution.html#praktische-anwendung",
    "href": "exercises/ms-exercise-09_solution.html#praktische-anwendung",
    "title": "Unsupervised Machine Learning I",
    "section": "üõ†Ô∏è Praktische Anwendung",
    "text": "üõ†Ô∏è Praktische Anwendung\n\n\n\n\n\n\nAchtung, bitte lesen!\n\n\n\n\nBevor Sie mit der Arbeit an den folgenden üìã Exercises beginnen, stellen Sie bitte sicher, dass Sie alle Chunks des Abschnitts Preparation gerendert haben. Das k√∂nnen Sie tun, indem Sie den ‚ÄúRun all chunks above‚Äù-Knopf des n√§chsten Chunks benutzen.\nBei Fragen zum Code lohnt sich ein Blick in den Showcase (.qmd oder .html). Beim Showcase handelt es sich um eine kompakte Darstellung des in der Pr√§sentation verwenden R-Codes. Sie k√∂nnen das Showcase also nutzen, um sich die Code-Bausteine anzusehen, die f√ºr die R-Outputs auf den Slides benutzt wurden.\n\n\n\n\nüìã Exercise 1: Cleaned DFM\n\nErstelen Sie einen neuen Datensatz quanteda_dfm_cleaned\n\nbasierend auf dem Datensatz quanteda_dfm\n\nVerwenden Sie quanteda::dfm_remove(pattern = c(\"systematic\", \"literature\", \"review\"), um die Suchquery zu entfernen.\nSpeichern Sie diese Umwandlung, indem Sie einen neuen Datensatz mit dem Namen quanteda_dfm_cleaned erstellen.\n\n\n√úberpr√ºfen Sie die Transformation indem Sie quanteda_dfm_cleaned in die Konsole eingeben.\n‚úçÔ∏è Notieren Sie, wie viele Dokumente & Features in quanteda_dfm_cleaned enthalten sind.\n\n\n\nL√∂sung anzeigen\n# `quanteda_dfm_cleaned` erstellen\nquanteda_dfm_cleaned &lt;- quanteda_dfm %&gt;% \n  quanteda::dfm_remove(pattern = c(\"systematic\", \"literature\", \"review\"))\n\n# √úberpr√ºfung\nquanteda_dfm_cleaned\n\n\nDocument-feature matrix of: 36,680 documents, 135,074 features (99.93% sparse) and 43 docvars.\n                                  features\ndocs                               5-item world health organization well-being\n  https://openalex.org/W4293003987      1     2      1            1          3\n  https://openalex.org/W2750168540      0     0      0            0          0\n  https://openalex.org/W1998933811      0     0      0            0          0\n  https://openalex.org/W2547134104      0     0      7            0          4\n  https://openalex.org/W3047898105      0     0      4            0          0\n  https://openalex.org/W2149640470      0     0      0            0          0\n                                  features\ndocs                               index who-5 among widely used\n  https://openalex.org/W4293003987     1    10     1      1    3\n  https://openalex.org/W2750168540     0     0     0      0    0\n  https://openalex.org/W1998933811     0     0     0      0    0\n  https://openalex.org/W2547134104     0     0     0      0    1\n  https://openalex.org/W3047898105     0     0     0      0    0\n  https://openalex.org/W2149640470     0     0     0      0    1\n[ reached max_ndoc ... 36,674 more documents, reached max_nfeat ... 135,064 more features ]\n\n\nL√∂sung anzeigen\n# Notiz:\n# `quanteda_dfm_cleaned` enth√§lt 36680 Dokumente und 135074 Features.\n\n\n\n\nüìã Exercise 2: Neues Netzwerk der Top-Begriffe\n\nNeues Dataset top_features_quanteda erstellen\n\nBasierend auf dem Dataset quanteda_dfm_cleaned,\nVerwenden Sie quanteda::topfeatures(20), um die 20 h√§ufigsten Begriffe zu extrahieren.\nVerwenden Sie names(), um nur die Namen (nicht die Werte) zu speichern.\nSpeichern Sie diese Transformation, indem Sie einen neuen Datensatz mit dem Namen top_features_quanteda erstellen.\n\nVisualisierung des Netzwerks an Top-Begriffen\n\nBasierend auf dem Dataset quanteda_dfm_cleaned,\nTransformieren Sie die Daten mit quanteda::fcm() in eine Feature-Co-Occurrence-Matrix [FCM].\nAuswahl relevanter Hashtags mit quanteda::fcm_select(pattern = top_features_quanteda, case_insensitive = FALSE).\nVisualisierung mit quanteda.textplots::textplot_network().\n\nErgebnisse interpretieren und vergleichen\n\nAnalysieren Sie die Beziehungen zwischen den Top-Begriffen.\nVergleichen Sie die Ergebnisse mit den Auswertungen der Folien. Welche Unterschiede gibt es?\n\n\n\n\nL√∂sung anzeigen\n# Create top features\ntop_features_quanteda &lt;- quanteda_dfm_cleaned %&gt;% \n  topfeatures(20) %&gt;% \n  names()\n\n# Construct feature-occurrence matrix of features\nquanteda_dfm_cleaned %&gt;% \n  fcm() %&gt;% \n  fcm_select(pattern = top_features_quanteda) %&gt;% \n  textplot_network(\n    edge_color = \"#C50F3C\"\n  )"
  },
  {
    "objectID": "exercises/ms-exercise-10.html",
    "href": "exercises/ms-exercise-10.html",
    "title": "Unsupervised Machine Learning II",
    "section": "",
    "text": "Link to source file"
  },
  {
    "objectID": "exercises/ms-exercise-10.html#background",
    "href": "exercises/ms-exercise-10.html#background",
    "title": "Unsupervised Machine Learning II",
    "section": "Background",
    "text": "Background\n\n\n\n\n\n\nTodays‚Äôs data basis: OpenAlex\n\n\n\n\nVia API bzw. openalexR (Aria et al. 2024) gesammelte ‚Äúworks‚Äù der Datenbank OpenAlex mit Bezug zu Literaturriews in den Sozialwissenschaften zwischen 2013 und 2023\nDetaillierte Informationen und Ergebnisse zur Suchquery finden Sie hier."
  },
  {
    "objectID": "exercises/ms-exercise-10.html#preparation",
    "href": "exercises/ms-exercise-10.html#preparation",
    "title": "Unsupervised Machine Learning II",
    "section": "Preparation",
    "text": "Preparation\n\n\n\n\n\n\nWichtige Information\n\n\n\n\nBitte stellen Sie sicher, dass Sie das jeweilige R-Studio Projekt zur √úbung ge√∂ffnet haben. Nur so funktionieren alle Dependencies korrekt.\nUm den einwandfreien Ablauf der √úbung zu gew√§hrleisten, wird f√ºr die Aufgaben auf eine eigenst√§ndige Datenerhebung verzichtet und ein √úbungsdatensatz zu verf√ºgung gestelt.\n\n\n\n\nPackages\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n    here, qs, # file management\n    magrittr, janitor, # data wrangling\n    easystats, sjmisc, # data analysis\n    gt, gtExtras, # table visualization\n    ggpubr, ggwordcloud, # visualization\n    # text analysis    \n    tidytext, widyr, # based on tidytext\n    quanteda, # based on quanteda\n    quanteda.textmodels, quanteda.textplots, quanteda.textstats, \n    stm, # structural topic modeling\n    openalexR, pushoverr, tictoc, \n    tidyverse # load last to avoid masking issues\n  )\n\n\n\nImport und Vorverarbeitung der Daten\n\nreview_works &lt;- qs::qread(here(\"data/session-07/openalex-review_works-2013_2023.qs\"))\n\n# Create correct data\nreview_subsample &lt;- review_works %&gt;% \n    # Create additional factor variables\n    mutate(\n        publication_year_fct = as.factor(publication_year), \n        type_fct = as.factor(type)\n        ) %&gt;%\n    # Eingrenzung: Sprache und Typ\n    filter(language == \"en\") %&gt;% \n    filter(type == \"article\") %&gt;%\n    # Datentranformation\n    unnest(topics, names_sep = \"_\") %&gt;%\n    filter(topics_name == \"field\") %&gt;% \n    filter(topics_i == \"1\") %&gt;% \n    # Eingrenzung: Forschungsfeldes\n    filter(\n    topics_display_name == \"Social Sciences\"|\n    topics_display_name == \"Psychology\"\n    ) %&gt;% \n    mutate(\n        field = as.factor(topics_display_name)\n    ) %&gt;% \n    # Eingrenzung: Keine Eintr√§ge ohne Abstract\n    filter(!is.na(ab))\n\n\n# Create corpus\nquanteda_corpus &lt;- review_subsample %&gt;% \n  quanteda::corpus(\n    docid_field = \"id\", \n    text_field = \"ab\"\n  )\n\n# Tokenize\nquanteda_token &lt;- quanteda_corpus %&gt;% \n  quanteda::tokens(\n    remove_punct = TRUE,\n    remove_symbols = TRUE, \n    remove_numbers = TRUE, \n    remove_url = TRUE, \n    split_tags = FALSE # keep hashtags and mentions\n  ) %&gt;% \n  quanteda::tokens_tolower() %&gt;% \n  quanteda::tokens_remove(\n    pattern = stopwords(\"en\")\n    )\n\n# Convert to Document-Feature-Matrix (DFM)\nquanteda_dfm &lt;- quanteda_token %&gt;% \n  quanteda::dfm()\n\n# Pruning\nquanteda_dfm_trim &lt;- quanteda_dfm %&gt;% \n  dfm_trim( \n    min_docfreq = 10/nrow(review_subsample),\n    max_docfreq = 0.99, \n    docfreq_type = \"prop\")\n\n# Convert for stm topic modeling\nquanteda_stm &lt;- quanteda_dfm_trim %&gt;% \n   convert(to = \"stm\")\n\n\n# Define parameters\nfuture::plan(future::multisession()) # use multiple sessions\ntopic_range &lt;- seq(from = 10, to = 100, by = 10) \n\n# Estimate models\nstm_search  &lt;- tibble(k = topic_range) %&gt;%\n  mutate(\n    mdl = furrr::future_map(\n      k, \n      ~stm::stm(\n        documents = quanteda_stm$documents,\n        vocab = quanteda_stm$vocab, \n        prevalence =~ publication_year_fct + field,\n        K = ., \n        seed = 42,\n        max.em.its = 1000,\n        data = quanteda_stm$meta,\n        init.type = \"Spectral\",\n        verbose = FALSE),\n      .options = furrr::furrr_options(seed = 42)\n      )\n    )"
  },
  {
    "objectID": "exercises/ms-exercise-10.html#praktische-anwendung",
    "href": "exercises/ms-exercise-10.html#praktische-anwendung",
    "title": "Unsupervised Machine Learning II",
    "section": "üõ†Ô∏è Praktische Anwendung",
    "text": "üõ†Ô∏è Praktische Anwendung\n\n\n\n\n\n\nAchtung, bitte lesen!\n\n\n\n\nBevor Sie mit der Arbeit an den folgenden üìã Exercises beginnen, stellen Sie bitte sicher, dass Sie alle Chunks des Abschnitts Preparation gerendert haben. Das k√∂nnen Sie tun, indem Sie den ‚ÄúRun all chunks above‚Äù-Knopf des n√§chsten Chunks benutzen.\nBei Fragen zum Code lohnt sich ein Blick in den Showcase (.qmd oder .html). Beim Showcase handelt es sich um eine kompakte Darstellung des in der Pr√§sentation verwenden R-Codes. Sie k√∂nnen das Showcase also nutzen, um sich die Code-Bausteine anzusehen, die f√ºr die R-Outputs auf den Slides benutzt wurden.\n\n\n\n\nüìã Exercise 1: Visualisierung der Themenpr√§valenz\n\n1.1. Auswahl des passenden Models\n\nErstelen Sie einen neuen Datensatz stm_mdl_k40\n\nbasierend auf dem Datensatz stm_search\n\nVerwenden Sie filter(k == 40), um das Modell mit 40 Themen zu auszuw√§hlen.\nVerwenden Sie pull(mdl) %&gt;% .[[1]] um die Spalte und das Element zu extrahieren, die das Modell enth√§lt.\nSpeichern Sie diese Umwandlung, indem Sie einen neuen Datensatz mit dem Namen stm_mdl_k40 erstellen.\n\n\n√úberpr√ºfen Sie die Transformation indem Sie stm_mdl_k40 in die Konsole eingeben.\n\n\n# Pull tpm with 40 topics\n\n# Check\n\n\n\n1.2. Identifikation der Top-Terms f√ºr jedes Thema\n\nErstellen Sie einen neuen Datensatz td_beta\n\nbasierend auf dem Datensatz stm_mdl_k40,\nVerwenden Sie tidy(method = \"frex\"), um die Beta-Matrix zu erstellen.\n\nErstellen Sie einen neuen Datensatz top_terms\n\nbasierend auf dem Datenastz td_beta,\n\nVerwenden Sie arrange(beta), um die Begriffe nach Beta zu sortieren.\nGruppieren Sie die Begriffe nach topic mit group_by(topic).\nExtrahieren Sie die 7 h√§ufigsten Begriffe mit top_n(7, beta).\nSortieren Sie die Begriffe absteigend mit arrange(-beta).\nW√§hlen Sie die Variablen topic und term mit select(topic, term) aus.\nExtrahieren Sie die Top-Begriffe pro Thema mit summarise(terms = list(term)).\nTransformieren Sie die extrahierten Begriffe pro Thema mit map(terms, paste, collapse = \", \") zu einem String.\n‚ÄúEntpacken‚Äù Sie die Begriffe aus der Liste (unnesten) mit unnest(cols = c(terms)).\n\n\n√úberpr√ºfen Sie die Transformation indem Sie top_terms in die Konsole eingeben.\n\n\n# Create tidy beta matrix\n\n# Create top terms\n\n# Output\n\n\n\n1.3 Erstellung der Pr√§valenz-Tabelle f√ºr die Themen\n\nErstellen Sie einen neuen Datensatz td_gamma\n\nbasierend auf dem Datensatz stm_mdl_k40,\nVerwenden Sie tidy(), um die Gamma-Matrix zu erstellen.\nVerwenden Sie document_names = names(quanteda_stm$documents) um die Dokumentennamen zu speichern\n\nErstellen Sie einen neuen Datensatz prevalence\n\nbasierend auf dem Datensatz td_gamma,\n\nGruppieren Sie die Themen nach topic mit group_by(topic).\nBerechnen Sie den Durchschnitt der gamma-Werte pro Thema mit summarise(gamma = mean(gamma)).\nSortieren Sie die Themen (absteigend) nach gamma mit arrange(desc(gamma)).\nVerkn√ºpfen Sie die Top-Begriffe mit den Themen mit left_join(top_terms, by = \"topic\").\n√úberarbeiten Sie die Variable topic mit dem mutate-Befehl:\n\nErstellen Sie eine neue Variable topic mit paste0(\"Topic \",sprintf(\"%02d\", topic)).\nOrdnen Sie die Themen nach gamma mit reorder(topic, gamma).\n\n\n\nErstellung Sie eine Tabelle als Output\n\nbasierend auf dem Datenastz prevalence,\n\nVerwenden Sie gt() um eine Tabelle zu erstellen.\nFormatieren Sie die Spalte gamma mit fmt_number(columns = vars(gamma), decimals = 2) um nur zwei Nachkommastellen anzuzeigen.\nVerwenden Sie gtExtras::gt_theme_538() um das Design der Tabelle anzupassen.\n\n\n‚úçÔ∏è Auf Basis des Outputs von prevalence Notieren Sie, welche Themen Sie als problematisch sehen und warum.\n\n\n# Create tidy gamma matrix\n\n# Create prevalence\n\n# Output\n\n\n\n\nüìã Exercise 2: Einfluss der Metadaten\n\n2.1. Sch√§tzung der Meta-Effekte\n\nErstellen Sie einen neuen Datensatz effects:\n\nVerwenden Sie die Funktion estimateEffect(), um die Effekte zu sch√§tzen.\nVerwenden Sie f√ºr das formular-Argument 1:40 ~ publication_year_fct + field, um die Effekte der Ver√∂ffentlichungsjahre und Fachbereiche zu sch√§tzen.\nVerwenden Sie stm_mdl_40 als das zu analysierende Modell.\nVerwenden Sie meta = quanteda_stm$meta, um die Metadaten f√ºr die Sch√§tzung zu verwenden.\n\n\n\n# Create data\n\n\n\n2.2. Untersuchung der Effekte\n\nErstellen Sie einen neuen Datensatz effects_tidy eine bereinigte Tabelle der Effekte:\n\nBasierend auf dem Datensatz effects\n\n\nVerwenden Sie die tidy() Funktion, um die Effekte in ein aufbereitetes Format zu bringen.\nFiltern Sie die Daten:\n\nEntfernen Sie Zeilen, bei denen term den Wert (Intercept) hat.\nBehalten Sie nur die Zeilen, bei denen term == \"fieldSocial Sciences\" ist.\n\nEntfernen Sie die Spalte term mit select(-term)\n\nErstellen Sie ein Tabelle zur √úberpr√ºfung der Effekte\n\nBasierend auf dem Datensatz effects_tidy:\n\n\nVerwenden Sie die Funktion gt(), um eine Tabelle zu erstellen.\nFormatieren Sie alle numerischen Variablen mit fmt_number(columns = -c(topic), decimals = 3), um lediglich drei Dezimalstellen darzustellen.\nVerwenden Sie data_color(columns = estimate, method = \"numeric\", palette = \"viridis\"), um die Sch√§tzwerte farblich zu kennzeichnen.\nWenden Sie das Design gtExtras::gt_theme_538() an.\n\n‚úçÔ∏è Notieren Sie, welches Thema am st√§rksten im Forschungsfeld ‚ÄúSocial Science‚Äù vertreten ist.\n\n\n# Filter effect tidy data\n\n# Explore effects (table outpu)\n\n#### Notes:\n# \n\n\n\n\nüìã Exercise 3: Einzelthema im Fokus\n\n3.1. Benennung des Themas k = 20\n\nBenennen Sie das Thema k = 20 aus dem Modell stm_mdl_40:\n\nVerwenden Sie die Funktion labelTopics().\nGeben Sie das Thema 20 als Parameter mit topic = 20 an.\n\n‚úçÔ∏è Notieren Sie die Themennamen. Begr√ºnden Sie kurz Ihre Entscheidung.\n\n\n# Create topic label\n\n# Themenname:\n\n\n\n3.2. Zusammenf√ºhrung mit OpenAlex-Daten\n\nErstellen Sie einen neuen Datensatz gamma_export\n\nbasierend auf dem Datensatz stm_mdl_k40:\n\nVerwenden Sie tidy() um die Gamma-Matrix zu erstellen. Geben Sie matrix = \"gamma\" und document_names = names(quanteda_stm$documents) als Parameter an.\nGruppieren Sie die Dokumente nach document mit group_by(document).\nW√§hlen Sie die Dokumente mit dem h√∂chsten gamma-Wert mit slice_max(gamma).\nL√∂sen Sie die Gruppierung mit dplyr::ungroup().\nVerkn√ºpfen Sie die Daten mit review_subsample mittels left_join(review_subsample, by = c(\"document\" = \"id\")).\n\nBenennen Sie die Spalte document in id um mit dplyr::rename(id = document)\nErstellen Sie eine neue Variable stm_topic mit Hilfe des mutate()-Befehls. Verwenden Sie as.factor(paste(\"Topic\", sprintf(\"%02d\", topic))) um die Themen zu benennen und als Faktor zu speichern.\n\n√úberpr√ºfen Sie Transformation mit Hilfe der glimpse()-Funktion, um sicherzustellen, dass die Daten korrekt erstellt wurden.\n\n\n# Create gamma export\n\n# Check\n\n\n\n3.3 Verteilungsparameter von Thema 20\n\nErstellung eines Outputs zur √úberpr√ºfung der Lageparameter\n\nBasierend auf dem Datensatz gamma_export:\n\nFiltern Sie die Daten nach topic == 20.\nW√§hlen Sie mit Hilfe der select()-Funktion die Variablen gamma, relevance_score und cited_by_count aus.\nVerwenden Sie die Funktion datawizard::describe_distribution() um die Verteilungsparameter zu berechnen.\n\n\n‚úçÔ∏è Identifizieren und notieren Sie folgende Informationen:\n\nWie viele Abstracts haben Thema 20 als Hauptthema?\nWie hoch ist der durschnittliche Relevance Score?\nWie viele Zitationen haben die Dokumente im Durchschnitt?\nWie viel Zitate hat das hochzitierteste Dokument?\n\n\n# Create distribution parameters\ngamma_export %&gt;% \n  filter(topic == 20) %&gt;%\n  select(gamma, relevance_score, cited_by_count) %&gt;% \n  datawizard::describe_distribution()\n\n#### Notes\n# Anzahl der Abstrats von Thema 20\n# Durchschnittlicher Relevace Score: \n# Durchschnittliche Zitationen:\n# Anzahl der Zitationen des am meisten zitierten Dokuments:\n\n\n\n\n3.4. Top-Dokumente des Themas\n\nIdentifizierung der Top-Dokumente\n\nBasierend auf dem Datensatz gamma_export:\n\nFiltern Sie den Datensatz nach stm_topic == \"Topic 20\".\nSortieren Sie die Daten absteigend nach gamma mit arrange(-gamma).\nW√§hlen Sie die Variablen title, so, gamma, type, und ab mit select() aus.\nW√§hlen Sie die obersten 5 Zeilen mit slice_head(n = 5).\n\n\nErstellung eines Outputs zur √úberpr√ºfung der Top-Dokumente\n\nBasierend auf dem Datensatz top_docs_k20:\n\nVerwenden Sie gt() um eine Tabelle zu erstellen.\nFormatieren Sie die Spalte gamma mit fmt_number(columns = vars(gamma), decimals = 2) um nur zwei Nachkommastellen anzuzeigen.\nVerwenden Sie gtExtras::gt_theme_538() um das Design der Tabelle anzupassen.\n\n\n\n‚úçÔ∏è Basierend auf den den Abstracts und den Titeln der Top-Dokumente:\n\nWelche Themenbereiche decken die Dokumente ab?\nW√ºrden Sie den im Abschnitt 3.1. gew√§hlten Themennamen beibehalten oder ab√§ndern?\n\n\n\n# Identify top documents for topic 20\n\n# Creae output"
  },
  {
    "objectID": "exercises/ms-showcase-07.html",
    "href": "exercises/ms-showcase-07.html",
    "title": "API mining and data wrangling with R",
    "section": "",
    "text": "Link to slides"
  },
  {
    "objectID": "exercises/ms-showcase-07.html#packages",
    "href": "exercises/ms-showcase-07.html#packages",
    "title": "API mining and data wrangling with R",
    "section": "Packages",
    "text": "Packages\n\nZum Laden der Pakete wird das Paket pacman::pload() genutzt, dass gegen√ºber der herk√∂mmlichen Methode mit library() eine Reihe an Vorteile hat:\n\nPr√§gnante Syntax\nAutomatische Installation (wenn Paket noch nicht vorhanden)\nLaden mehrerer Pakete auf einmal\nAutomatische Suche nach dependencies\n\n\n\npacman::p_load(\n  here, qs, \n  magrittr, janitor,\n  easystats, sjmisc,\n  ggpubr, \n  openalexR, \n  tidyverse\n)"
  },
  {
    "objectID": "exercises/ms-showcase-07.html#codechunks-aus-der-sitzung",
    "href": "exercises/ms-showcase-07.html#codechunks-aus-der-sitzung",
    "title": "API mining and data wrangling with R",
    "section": "Codechunks aus der Sitzung",
    "text": "Codechunks aus der Sitzung\n\nDatenerhebung via API\n\n# Set openalexR.mailto option so that your requests go to the polite pool for faster response times\noptions(openalexR.mailto = \"christoph.adrian@fau.de\")\n\n\n# Download data via API\nreview_works &lt;- openalexR::oa_fetch(\n  entity = \"works\",\n  title.search = \"(literature OR systematic) AND review\",\n  primary_topic.domain.id = \"domains/2\", # Social Science\n  publication_year = \"2013 - 2023\",\n  verbose = TRUE\n)\n\n\n# Overview\nreview_works \n\n# A tibble: 93,655 √ó 39\n   id     title display_name author ab    publication_date relevance_score so   \n   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;        &lt;list&gt; &lt;chr&gt; &lt;chr&gt;                      &lt;dbl&gt; &lt;chr&gt;\n 1 https‚Ä¶ The ‚Ä¶ The PRISMA ‚Ä¶ &lt;df&gt;   The ‚Ä¶ 2021-03-29                 1625. BMJ  \n 2 https‚Ä¶ Pref‚Ä¶ Preferred r‚Ä¶ &lt;df&gt;   Syst‚Ä¶ 2015-01-01                 1340. Syst‚Ä¶\n 3 https‚Ä¶ Rayy‚Ä¶ Rayyan‚Äîa we‚Ä¶ &lt;df&gt;   Synt‚Ä¶ 2016-12-01                 1314. Syst‚Ä¶\n 4 https‚Ä¶ Syst‚Ä¶ Systematic ‚Ä¶ &lt;df&gt;   Scop‚Ä¶ 2018-11-19                  990. BMC ‚Ä¶\n 5 https‚Ä¶ Upda‚Ä¶ Updated gui‚Ä¶ &lt;df&gt;   On a‚Ä¶ 2019-10-03                  963. Coch‚Ä¶\n 6 https‚Ä¶ The ‚Ä¶ The WHO-5 W‚Ä¶ &lt;df&gt;   The ‚Ä¶ 2015-01-01                  939. Psyc‚Ä¶\n 7 https‚Ä¶ Pref‚Ä¶ Preferred r‚Ä¶ &lt;df&gt;   Prot‚Ä¶ 2015-01-02                  914. BMJ  \n 8 https‚Ä¶ Guid‚Ä¶ Guidance on‚Ä¶ &lt;df&gt;   Lite‚Ä¶ 2017-08-28                  752. Jour‚Ä¶\n 9 https‚Ä¶ Lite‚Ä¶ Literature ‚Ä¶ &lt;df&gt;   Know‚Ä¶ 2019-11-01                  705. Jour‚Ä¶\n10 https‚Ä¶ The ‚Ä¶ The PRISMA ‚Ä¶ &lt;df&gt;   The ‚Ä¶ 2021-04-01                  653. Inte‚Ä¶\n# ‚Ñπ 93,645 more rows\n# ‚Ñπ 31 more variables: so_id &lt;chr&gt;, host_organization &lt;chr&gt;, issn_l &lt;chr&gt;,\n#   url &lt;chr&gt;, pdf_url &lt;chr&gt;, license &lt;chr&gt;, version &lt;chr&gt;, first_page &lt;chr&gt;,\n#   last_page &lt;chr&gt;, volume &lt;chr&gt;, issue &lt;chr&gt;, is_oa &lt;lgl&gt;,\n#   is_oa_anywhere &lt;lgl&gt;, oa_status &lt;chr&gt;, oa_url &lt;chr&gt;,\n#   any_repository_has_fulltext &lt;lgl&gt;, language &lt;chr&gt;, grants &lt;list&gt;,\n#   cited_by_count &lt;int&gt;, counts_by_year &lt;list&gt;, publication_year &lt;int&gt;, ‚Ä¶\n\n\n\n\nInitiale Sichtung und √úberpr√ºfung der Datem\n\n\n\n\n\n\nTypische Bestandteile\n\n\n\n\nWie viele F√§lle sind enthalten? Wie viele Variablen? Sind die Variablennamen aussagekr√§ftig?\nWelchen Typ hat jede Variable, z. B. numerisch, kategorisch, logisch?\n\n\n\n\nreview_works %&gt;% glimpse()\n\nRows: 93,655\nColumns: 39\n$ id                          &lt;chr&gt; \"https://openalex.org/W3118615836\", \"https‚Ä¶\n$ title                       &lt;chr&gt; \"The PRISMA 2020 statement: an updated gui‚Ä¶\n$ display_name                &lt;chr&gt; \"The PRISMA 2020 statement: an updated gui‚Ä¶\n$ author                      &lt;list&gt; [&lt;data.frame[26 x 12]&gt;], [&lt;data.frame[8 x‚Ä¶\n$ ab                          &lt;chr&gt; \"The Preferred Reporting Items for Systema‚Ä¶\n$ publication_date            &lt;chr&gt; \"2021-03-29\", \"2015-01-01\", \"2016-12-01\", ‚Ä¶\n$ relevance_score             &lt;dbl&gt; 1625.1708, 1340.1902, 1314.3904, 990.4521,‚Ä¶\n$ so                          &lt;chr&gt; \"BMJ\", \"Systematic reviews\", \"Systematic r‚Ä¶\n$ so_id                       &lt;chr&gt; \"https://openalex.org/S4393917726\", \"https‚Ä¶\n$ host_organization           &lt;chr&gt; NA, \"BioMed Central\", \"BioMed Central\", \"B‚Ä¶\n$ issn_l                      &lt;chr&gt; \"1756-1833\", \"2046-4053\", \"2046-4053\", \"14‚Ä¶\n$ url                         &lt;chr&gt; \"https://doi.org/10.1136/bmj.n71\", \"https:‚Ä¶\n$ pdf_url                     &lt;chr&gt; \"https://www.bmj.com/content/bmj/372/bmj.n‚Ä¶\n$ license                     &lt;chr&gt; \"cc-by\", \"cc-by\", \"cc-by\", \"cc-by\", NA, \"c‚Ä¶\n$ version                     &lt;chr&gt; \"publishedVersion\", \"publishedVersion\", \"p‚Ä¶\n$ first_page                  &lt;chr&gt; \"n71\", NA, NA, NA, NA, \"167\", \"g7647\", \"93‚Ä¶\n$ last_page                   &lt;chr&gt; \"n71\", NA, NA, NA, NA, \"176\", \"g7647\", \"11‚Ä¶\n$ volume                      &lt;chr&gt; NA, \"4\", \"5\", \"18\", NA, \"84\", \"349\", \"39\",‚Ä¶\n$ issue                       &lt;chr&gt; NA, \"1\", \"1\", \"1\", NA, \"3\", \"jan02 1\", \"1\"‚Ä¶\n$ is_oa                       &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, FALSE, TRUE, TRUE,‚Ä¶\n$ is_oa_anywhere              &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, ‚Ä¶\n$ oa_status                   &lt;chr&gt; \"hybrid\", \"gold\", \"gold\", \"gold\", \"green\",‚Ä¶\n$ oa_url                      &lt;chr&gt; \"https://www.bmj.com/content/bmj/372/bmj.n‚Ä¶\n$ any_repository_has_fulltext &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE,‚Ä¶\n$ language                    &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", ‚Ä¶\n$ grants                      &lt;list&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, &lt;\"htt‚Ä¶\n$ cited_by_count              &lt;int&gt; 30303, 17347, 10540, 5298, 5664, 2657, 909‚Ä¶\n$ counts_by_year              &lt;list&gt; [&lt;data.frame[5 x 2]&gt;], [&lt;data.frame[11 x ‚Ä¶\n$ publication_year            &lt;int&gt; 2021, 2015, 2016, 2018, 2019, 2015, 2015, ‚Ä¶\n$ cited_by_api_url            &lt;chr&gt; \"https://api.openalex.org/works?filter=cit‚Ä¶\n$ ids                         &lt;list&gt; &lt;\"https://openalex.org/W3118615836\", \"htt‚Ä¶\n$ doi                         &lt;chr&gt; \"https://doi.org/10.1136/bmj.n71\", \"https:‚Ä¶\n$ type                        &lt;chr&gt; \"article\", \"article\", \"article\", \"article\"‚Ä¶\n$ referenced_works            &lt;list&gt; &lt;\"https://openalex.org/W1528251861\", \"htt‚Ä¶\n$ related_works               &lt;list&gt; &lt;\"https://openalex.org/W4234875088\", \"htt‚Ä¶\n$ is_paratext                 &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, ‚Ä¶\n$ is_retracted                &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, ‚Ä¶\n$ concepts                    &lt;list&gt; [&lt;data.frame[20 x 5]&gt;], [&lt;data.frame[18 x‚Ä¶\n$ topics                      &lt;list&gt; [&lt;tbl_df[12 x 5]&gt;], [&lt;tbl_df[12 x 5]&gt;], [‚Ä¶\n\n\n\n\nDatentransformationen\n\nKorrektur der Rohdaten\n\nreview_works_correct &lt;- review_works %&gt;% \n    mutate(\n        # Create additional factor variables\n        publication_year_fct = as.factor(publication_year), \n        type_fct = as.factor(type)\n        )\n\n\n\nUnnest topics\n\nreview_works_correct %&gt;% \n    unnest(topics, names_sep = \"_\") %&gt;%\n    glimpse()\n\nRows: 942,560\nColumns: 45\n$ id                          &lt;chr&gt; \"https://openalex.org/W3118615836\", \"https‚Ä¶\n$ title                       &lt;chr&gt; \"The PRISMA 2020 statement: an updated gui‚Ä¶\n$ display_name                &lt;chr&gt; \"The PRISMA 2020 statement: an updated gui‚Ä¶\n$ author                      &lt;list&gt; [&lt;data.frame[26 x 12]&gt;], [&lt;data.frame[26 ‚Ä¶\n$ ab                          &lt;chr&gt; \"The Preferred Reporting Items for Systema‚Ä¶\n$ publication_date            &lt;chr&gt; \"2021-03-29\", \"2021-03-29\", \"2021-03-29\", ‚Ä¶\n$ relevance_score             &lt;dbl&gt; 1625.171, 1625.171, 1625.171, 1625.171, 16‚Ä¶\n$ so                          &lt;chr&gt; \"BMJ\", \"BMJ\", \"BMJ\", \"BMJ\", \"BMJ\", \"BMJ\", ‚Ä¶\n$ so_id                       &lt;chr&gt; \"https://openalex.org/S4393917726\", \"https‚Ä¶\n$ host_organization           &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA‚Ä¶\n$ issn_l                      &lt;chr&gt; \"1756-1833\", \"1756-1833\", \"1756-1833\", \"17‚Ä¶\n$ url                         &lt;chr&gt; \"https://doi.org/10.1136/bmj.n71\", \"https:‚Ä¶\n$ pdf_url                     &lt;chr&gt; \"https://www.bmj.com/content/bmj/372/bmj.n‚Ä¶\n$ license                     &lt;chr&gt; \"cc-by\", \"cc-by\", \"cc-by\", \"cc-by\", \"cc-by‚Ä¶\n$ version                     &lt;chr&gt; \"publishedVersion\", \"publishedVersion\", \"p‚Ä¶\n$ first_page                  &lt;chr&gt; \"n71\", \"n71\", \"n71\", \"n71\", \"n71\", \"n71\", ‚Ä¶\n$ last_page                   &lt;chr&gt; \"n71\", \"n71\", \"n71\", \"n71\", \"n71\", \"n71\", ‚Ä¶\n$ volume                      &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA‚Ä¶\n$ issue                       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA‚Ä¶\n$ is_oa                       &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, ‚Ä¶\n$ is_oa_anywhere              &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, ‚Ä¶\n$ oa_status                   &lt;chr&gt; \"hybrid\", \"hybrid\", \"hybrid\", \"hybrid\", \"h‚Ä¶\n$ oa_url                      &lt;chr&gt; \"https://www.bmj.com/content/bmj/372/bmj.n‚Ä¶\n$ any_repository_has_fulltext &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, ‚Ä¶\n$ language                    &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", ‚Ä¶\n$ grants                      &lt;list&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n$ cited_by_count              &lt;int&gt; 30303, 30303, 30303, 30303, 30303, 30303, ‚Ä¶\n$ counts_by_year              &lt;list&gt; [&lt;data.frame[5 x 2]&gt;], [&lt;data.frame[5 x 2‚Ä¶\n$ publication_year            &lt;int&gt; 2021, 2021, 2021, 2021, 2021, 2021, 2021, ‚Ä¶\n$ cited_by_api_url            &lt;chr&gt; \"https://api.openalex.org/works?filter=cit‚Ä¶\n$ ids                         &lt;list&gt; &lt;\"https://openalex.org/W3118615836\", \"htt‚Ä¶\n$ doi                         &lt;chr&gt; \"https://doi.org/10.1136/bmj.n71\", \"https:‚Ä¶\n$ type                        &lt;chr&gt; \"article\", \"article\", \"article\", \"article\"‚Ä¶\n$ referenced_works            &lt;list&gt; &lt;\"https://openalex.org/W1528251861\", \"htt‚Ä¶\n$ related_works               &lt;list&gt; &lt;\"https://openalex.org/W4234875088\", \"htt‚Ä¶\n$ is_paratext                 &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, ‚Ä¶\n$ is_retracted                &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, ‚Ä¶\n$ concepts                    &lt;list&gt; [&lt;data.frame[20 x 5]&gt;], [&lt;data.frame[20 x‚Ä¶\n$ topics_i                    &lt;int&gt; 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 1, 1, ‚Ä¶\n$ topics_score                &lt;dbl&gt; 0.9993, 0.9993, 0.9993, 0.9993, 0.9832, 0.‚Ä¶\n$ topics_name                 &lt;chr&gt; \"topic\", \"subfield\", \"field\", \"domain\", \"t‚Ä¶\n$ topics_id                   &lt;chr&gt; \"https://openalex.org/T10206\", \"https://op‚Ä¶\n$ topics_display_name         &lt;chr&gt; \"Methods for Evidence Synthesis in Researc‚Ä¶\n$ publication_year_fct        &lt;fct&gt; 2021, 2021, 2021, 2021, 2021, 2021, 2021, ‚Ä¶\n$ type_fct                    &lt;fct&gt; article, article, article, article, articl‚Ä¶\n\n\n\n\n\nRekonstruktion OpexAlex Dashboard\n\nPublikationen im Zeitverlauf\n\nreview_works_correct %&gt;% \n    ggplot(aes(publication_year)) +\n    geom_bar() +\n    theme_pubr()\n\n\n\n\n\n\n\n\n\n\nH√§ufigkeit Forschungsfelder\n\nreview_works_correct %&gt;% \n    unnest(topics, names_sep = \"_\") %&gt;% \n    filter(topics_name == \"field\") %&gt;% \n    filter(topics_i == 1) %&gt;% \n    sjmisc::frq(topics_display_name, sort.frq = \"desc\")\n\ntopics_display_name &lt;character&gt; \n# total N=93655 valid N=93655 mean=4.41 sd=1.62\n\nValue                               |     N | Raw % | Valid % | Cum. %\n----------------------------------------------------------------------\nSocial Sciences                     | 30580 | 32.65 |   32.65 |  32.65\nPsychology                          | 29054 | 31.02 |   31.02 |  63.67\nBusiness, Management and Accounting | 15558 | 16.61 |   16.61 |  80.29\nDecision Sciences                   |  7261 |  7.75 |    7.75 |  88.04\nEconomics, Econometrics and Finance |  6796 |  7.26 |    7.26 |  95.30\nArts and Humanities                 |  4406 |  4.70 |    4.70 | 100.00\n&lt;NA&gt;                                |     0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\n\n\n\nRelevanteste Publikationen\n\nreview_works_correct %&gt;% \n    arrange(desc(relevance_score)) %&gt;%\n    select(publication_year_fct, relevance_score, title) %&gt;% \n    head(5) %&gt;% \n    gt::gt()\n\n\n\n\n\n\n\npublication_year_fct\nrelevance_score\ntitle\n\n\n\n\n2021\n1625.1708\nThe PRISMA 2020 statement: an updated guideline for reporting systematic reviews\n\n\n2015\n1340.1902\nPreferred reporting items for systematic review and meta-analysis protocols (PRISMA-P) 2015 statement\n\n\n2016\n1314.3904\nRayyan‚Äîa web and mobile app for systematic reviews\n\n\n2018\n990.4521\nSystematic review or scoping review? Guidance for authors when choosing between a systematic or scoping review approach\n\n\n2019\n962.6738\nUpdated guidance for trusted systematic reviews: a new edition of the Cochrane Handbook for Systematic Reviews of Interventions\n\n\n\n\n\n\n\n\n\nLageparameter\n\nreview_works_correct %&gt;% \n  select(where(is.numeric)) %&gt;% \n  datawizard::describe_distribution() %&gt;% \n  print_html()\n\n\n\n\n\n\n\nVariable\nMean\nSD\nIQR\nMin\nMax\nSkewness\nKurtosis\nn\nn_Missing\n\n\n\n\nrelevance_score\n31.73\n42.51\n36.48\n1.17\n1625.17\n4.75\n67.87\n93655\n0\n\n\ncited_by_count\n18.33\n146.36\n10.00\n0.00\n30303.00\n123.44\n22236.55\n93655\n0\n\n\npublication_year\n2019.40\n2.97\n5.00\n2013.00\n2023.00\n-0.58\n-0.77\n93655\n0"
  },
  {
    "objectID": "computing/computing-useful_links.html",
    "href": "computing/computing-useful_links.html",
    "title": "Useful sources",
    "section": "",
    "text": "This is selection of useful R sources:\n\n Quarto tutorials by Andy Field\n Automatisierte Inhaltsanalyse mit R by Kornelius Puschmann",
    "crumbs": [
      "Computing",
      "Useful R sources"
    ]
  },
  {
    "objectID": "computing/computing-textbooks.html",
    "href": "computing/computing-textbooks.html",
    "title": "R textbooks",
    "section": "",
    "text": "While there is no official R textbook for the course, there are a few to look at:\n\nüîó R for Data Science, 2nd Edition\nüîó Data Visualization: A Practical Introduction\nüîó Tidy modeling with R\nüîó Text Mining with R",
    "crumbs": [
      "Computing",
      "R Textbooks"
    ]
  }
]