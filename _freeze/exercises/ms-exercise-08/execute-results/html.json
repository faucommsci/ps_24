{
  "hash": "898258cb1ac3099cf56fc244ac5a51d5",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Text processing with R\"\nsubtitle: \"Session 08 - Exercise\"\ndate: last-modified\ndate-format: \"DD.MM.YYYY\"\n---\n\n\n::: {.callout-tip icon=\"false\"}\n[![Quarto Document](https://raw.githubusercontent.com/faucommsci/teaching_materials/main/images/badges/badge-quarto_document.svg)](https://github.com/faucommsci/ps_24/blob/main/exercises/ms-exercise-08.qmd) Link to source file\n:::\n\n::: callout-note\n## Ziel der Anwendung: Textanalyse in R kennenlernen\n\n-   Auffrischung der Grundkenntnisse im Umgang mit R, tidyverse und ggplot2\n-   Typische Schritte der Textanalyse mit `tidytext` kennenlernen, von der Tokenisierung bis zur Visualisierung.\n:::\n\n## Background\n\n::: callout-tip\n## Todays's data basis: [OpenAlex](https://openalex.org/)\n\n-   Via API bzw. openalexR [@aria2024] gesammelte \"works\" der Datenbank [OpenAlex](https://openalex.org/) mit Bezug zu Literaturriews in den Sozialwissenschaften zwischen 2013 und 2023\n\n-   Detaillierte Informationen und Ergebnisse zur Suchquery finden Sie [hier](https://openalex.org/works?page=1&filter=display_name.search%3A%28literature%20OR%20systematic%29%20AND%20review,primary_topic.domain.id%3Adomains%2F2,publication_year%3A2014%20-%202024&group_by=publication_year,open_access.is_oa,primary_topic.field.id).\n:::\n\n## Preparation\n\n::: callout-important\n## Wichtige Information\n\n-   Bitte stellen Sie sicher, dass Sie das jeweilige R-Studio Projekt zur √úbung ge√∂ffnet haben. Nur so funktionieren alle Dependencies korrekt.\n-   Um den einwandfreien Ablauf der √úbung zu gew√§hrleisten, wird f√ºr die Aufgaben auf eine eigenst√§ndige Datenerhebung verzichtet und ein √úbungsdatensatz zu verf√ºgung gestelt.\n:::\n\n### Packages\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n    here, qs, # file management\n    magrittr, janitor, # data wrangling\n    easystats, sjmisc, # data analysis\n    gt, gtExtras, # table visualization\n    ggpubr, ggwordcloud, # visualization\n    tidytext, widyr, # text analysis    \n    openalexR, \n    tidyverse # load last to avoid masking issues\n  )\n```\n:::\n\n\n### Import und Vorverarbeitung der Daten\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# Import from local\nreview_works <- qs::qread(here(\"data/session-07/openalex-review_works-2013_2023.qs\"))\n\n# Create correct data\nreview_works_correct <- review_works %>% \n    mutate(\n        # Create additional factor variables\n        publication_year_fct = as.factor(publication_year), \n        type_fct = as.factor(type)\n        )\n```\n:::\n\n\n## üõ†Ô∏è Praktische Anwendung\n\n::: callout-important\n## Achtung, bitte lesen!\n\n-   Bevor Sie mit der Arbeit an den folgenden üìã **Exercises** beginnen, stellen Sie bitte sicher, dass Sie alle Chunks des Abschnitts [Preparation] gerendert haben. Das k√∂nnen Sie tun, indem Sie den \"*Run all chunks above*\"-Knopf ![](/img/rstudio-button-render_all_chunks_above.png)des n√§chsten Chunks benutzen.\n-   Bei Fragen zum Code lohnt sich ein Blick in den **Showcase** (.qmd oder .html). Beim Showcase handelt es sich um eine kompakte Darstellung des in der Pr√§sentation verwenden R-Codes. Sie k√∂nnen das Showcase also nutzen, um sich die Code-Bausteine anzusehen, die f√ºr die R-Outputs auf den Slides benutzt wurden.\n:::\n\n### üìã Exercise 1: Neues Subsample\n\n::: callout-caution\n## Ziel der Aufgabe\n\n-   Erstellung eines neuen Datensatzes `review_subsample_new`, der sich auf *englischsprachig* *B√ºcher bzw. Buchrartikel* beschr√§nkt.\n:::\n\n1.  Erstellen Sie einen neuen Datensatz `review_subsample_new`\n    -   Basierend auf dem Datensatzes `review_works_correct`:\n        1.  Nutzen Sie die `filter()`-Funktion, um\n            -   nur englischsprachige (`language`),\n            -   B√ºcher und Buchkapitel (`type`) herauszufiltern.\n        2.  Speichern Sie diese Umwandlung in einem neuen Datensatz mit dem Namen `review_subsample_new`\n2.  √úberpr√ºfen Sie die Transformation mit Hilfe der `glimpse()`-Funktion.\n3.  ‚úçÔ∏è Notieren Sie, wie viele Artikel im neuen Subsample enthalten sind.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Erstellung Subsample\n\n# √úberpr√ºfung\n\n# Notiz:\n# Subsample enth√§lt ... Eintr√§ge\n```\n:::\n\n\n### üìã Exercise 2: Umwandlung zu 'tidy text'\n\n1.  Erstellen Sie einen neuen Datensatz `subsample_new_tidy`,\n    -   Basierend auf dem Datensatz `review_subsample_new`, mit folgenden Schritten:\n        1.  Tokenisierung der Abstracts (`ab`) mit der Funktion `unnest_tokens`.\n        2.  Ausschluss von Stoppw√∂rter mit `filter` und `stopwords$words` heraus.\n        3.  Speichern Sie diese Umwandlung, indem Sie einen neuen Datensatz mit dem Namen `subsample_new_tidy` erstellen.\n2.  Pr√ºfen Sie, ob die Umwandlung erfolgreich war (z.B. mit der Funktion `glimpse()`)\n3.  ‚úçÔ∏è Notieren Sie, wie viele Token im neuen Datensatz `subsample_new_tidy` enthalten sind.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Erstellung des neuen Datensatzes `subsample_new_tidy`\n\n# √úberpr√ºfung\n\n# Notiz:\n# Der neue Datensatz enth√§lt ... Token. \n```\n:::\n\n\n### üìã Exercise 3: Auswertung der Token\n\n1.  Erstellen Sie einen neuen Datensatz `subsample_new_summarized`,\n    -   Fassen Sie auf der Grundlage des Datensatzes `subsample_new_tidy` die H√§ufigkeit der einzelnen Token zusammen, indem Sie die Funktion `count()` auf die Variable `text` anwenden. Verwenden Sie das Argument `sort = TRUE`, um den Datensatz nach absteigender H√§ufigkeit der Token zu sortieren.\n    -   Speichern Sie diese Transformation, indem Sie einen neuen Datensatz mit dem Namen `subsample_new_summarized` erstellen.\n2.  Pr√ºfen Sie, ob die Umwandlung erfolgreich war, indem Sie die Funktion `print()` verwenden.\n    -   Verwenden Sie das Argument `n = 50`, um die 50 wichtigsten Token anzuzeigen (nur m√∂glich, wenn das Argument `sort = TRUE` bei der Ausf√ºhrung der Funktion `count()` verwendet wurde)\n3.  Verteilung der Token pr√ºfen\n    -   Verwenden Sie die Funktion `datawizard::describe_distribution()`, um verschiedene Verteilungsparameter des neuen Datensatzes zu √ºberpr√ºfen\n    -   ‚úçÔ∏è Notieren Sie, wie viele Token ein Abstract durchschnittlich enth√§lt.\n\n-   *Optional:* Ergebnisse mit einer Wortwolke √ºberpr√ºfen\n    -   Basierend auf dem sortierten Datensatz `subsample_new_summarized`\n        1.  Auswahl der 50 h√§ufigsten Token mit Hilfe der Funktion `top_n()`\n        2.  Erstellen Sie eine `ggplot()`-Basis mit `label = text` und `size = n` als `aes()` und\n        3.  Benutze ggwordcloud::geom_text_wordclout() um die Wortwolke zu erstellen.\n        4.  Verwenden Sie scale_size_are(), um die Skalierung der Wortwolke zu √ºbernehmen.\n        5.  Verwenden Sie `theme_minimal()` f√ºr eine saubere Visualisierung.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Erstellung des neuen Datensatzes `subsample_new_summmarized`\n\n# Preview Top 50 token\n\n# Check distribution parameters \n\n# Notiz:\n# Ein Absatz enth√§lt durchschnittlich ... Token. \n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Optional: Check results with a wordcloud\n```\n:::\n\n\n### üìã Exercise 4: Wortbeziehungen im Fokus\n\n#### 4.1 Couting word pairs\n\n1.  Z√§hlen von h√§ufigen Wortpaaren\n    -   Z√§hlen Sie auf der Grundlage des Datensatzes `subsample_new_tidy` Wortpaare mit `widyr::pairwise_count()`, mit den Argumenten `item = text`, `feature = id` und `sort = TRUE.`\n    -   Speichern Sie diese Transformation, indem Sie einen neuen Datensatz mit dem Namen `subsample_new_word_pairs` erstellen.\n2.  Pr√ºfen Sie, ob die Umwandlung erfolgreich war, indem Sie die Funktion `print()` verwenden.\n    -   Verwenden Sie das Argument `n = 50`, um die 50 wichtigsten Token anzuzeigen (nur m√∂glich, wenn bei der Ausf√ºhrung der Funktion `count()` das Argument `sort = TRUE` verwendet wurde)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Couting word pairs among sections\n\n# √úberpr√ºfung \n```\n:::\n\n\n#### 4.2 Pairwise correlation\n\n1.  Ermittlung der paarweisen Korrelation\n    -   Basierend auf dem Datensatz `subsample_new_tidy`,\n    -   gruppieren Sie die Daten mit der Funktion `group_by()` nach der Variable `text` und\n    -   verwenden Sie `filter(n() >= X)`, um nur Token zu verwenden, die mindestens in einer bestimmte Anzahl (`X`) vorkommen; Sie k√∂nnen f√ºr `X` einen Wert Ihrer Wahl w√§hlen, ich w√ºrde jedoch dringend empfehlen, ein `X > 100` zu w√§hlen, da die folgende Funktion sonst m√∂glicherweise nicht in der Lage ist, die Berechnung durchzuf√ºhren.\n    -   Erstellen Sie Wortkorrelationen mit `widyr::pairwise_cor()`, mit den Argumenten `item = text`,`feature = id` und `sort = TRUE`.\n    -   Speichern Sie diese Transformation, indem Sie einen neuen Datensatz mit dem Namen `subsample_new_corr` erstellen.\n2.  Pr√ºfen Sie die Paare mit der h√∂chsten Korrelation mit der Funktion `print().`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Getting pairwise correlation \n\n# Check pairs with highest correlation\n```\n:::\n\n\n### üìã Exercise 5: Inhaltlicher Vergleich\n\n-   Vergleichen Sie die Ergebnisse der √úbung mit den Auswertungen der Folien:\n    -   Wie unterscheiden sich die Ergebnisse?\n    -   W√ºrden Sie die B√ºcher bzw. Buchabschnitte mit in die Untersuchung integrieren?",
    "supporting": [
      "ms-exercise-08_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}