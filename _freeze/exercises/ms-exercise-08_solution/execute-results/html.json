{
  "hash": "cef1ebd219d47d48604e19913270892d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Text processing with R\"\nsubtitle: \"Session 08 - Exercise\"\ndate: last-modified\ndate-format: \"DD.MM.YYYY\"\nformat:\n    html: \n        code-fold: true\n        code-summary: \"L√∂sung anzeigen\"\n---\n\n\n\n::: {.callout-tip icon=\"false\"}\n[![Quarto Document](https://raw.githubusercontent.com/faucommsci/teaching_materials/main/images/badges/badge-quarto_document.svg)](https://github.com/faucommsci/ps_24/blob/main/exercises/ms-exercise-08_solution.qmd) Link to source file\n:::\n\n::: callout-note\n## Ziel der Anwendung: Textanalyse in R kennenlernen\n\n-   Auffrischung der Grundkenntnisse im Umgang mit R, tidyverse und ggplot2\n-   Typische Schritte der Textanalyse mit `tidytext` kennenlernen, von der Tokenisierung bis zur Visualisierung.\n:::\n\n## Background\n\n::: callout-tip\n## Todays's data basis: [OpenAlex](https://openalex.org/)\n\n-   Via API bzw. openalexR [@aria2024] gesammelte \"works\" der Datenbank [OpenAlex](https://openalex.org/) mit Bezug zu Literaturriews in den Sozialwissenschaften zwischen 2013 und 2023\n\n-   Detaillierte Informationen und Ergebnisse zur Suchquery finden Sie [hier](https://openalex.org/works?page=1&filter=display_name.search%3A%28literature%20OR%20systematic%29%20AND%20review,primary_topic.domain.id%3Adomains%2F2,publication_year%3A2014%20-%202024&group_by=publication_year,open_access.is_oa,primary_topic.field.id).\n:::\n\n## Preparation\n\n::: callout-important\n## Wichtige Information\n\n-   Bitte stellen Sie sicher, dass Sie das jeweilige R-Studio Projekt zur √úbung ge√∂ffnet haben. Nur so funktionieren alle Dependencies korrekt.\n-   Um den einwandfreien Ablauf der √úbung zu gew√§hrleisten, wird f√ºr die Aufgaben auf eine eigenst√§ndige Datenerhebung verzichtet und ein √úbungsdatensatz zu verf√ºgung gestelt.\n:::\n\n### Packages\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n    here, qs, # file management\n    magrittr, janitor, # data wrangling\n    easystats, sjmisc, # data analysis\n    gt, gtExtras, # table visualization\n    ggpubr, ggwordcloud, # visualization\n    tidytext, widyr, # text analysis    \n    openalexR, \n    tidyverse # load last to avoid masking issues\n  )\n```\n:::\n\n\n\n### Import und Vorverarbeitung der Daten\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# Import from local\nreview_works <- qs::qread(here(\"data/session-07/openalex-review_works-2013_2023.qs\"))\n\n# Create correct data\nreview_works_correct <- review_works %>% \n    mutate(\n        # Create additional factor variables\n        publication_year_fct = as.factor(publication_year), \n        type_fct = as.factor(type)\n        )\n```\n:::\n\n\n\n## üõ†Ô∏è Praktische Anwendung\n\n::: callout-important\n## Achtung, bitte lesen!\n\n-   Bevor Sie mit der Arbeit an den folgenden üìã **Exercises** beginnen, stellen Sie bitte sicher, dass Sie alle Chunks des Abschnitts [Preparation] gerendert haben. Das k√∂nnen Sie tun, indem Sie den \"*Run all chunks above*\"-Knopf ![](/img/rstudio-button-render_all_chunks_above.png)des n√§chsten Chunks benutzen.\n-   Bei Fragen zum Code lohnt sich ein Blick in den **Showcase** (.qmd oder .html). Beim Showcase handelt es sich um eine kompakte Darstellung des in der Pr√§sentation verwenden R-Codes. Sie k√∂nnen das Showcase also nutzen, um sich die Code-Bausteine anzusehen, die f√ºr die R-Outputs auf den Slides benutzt wurden.\n:::\n\n### üìã Exercise 1: Neues Subsample\n\n::: callout-caution\n## Ziel der Aufgabe\n\n-   Erstellung eines neuen Datensatzes `review_subsample_new`, der sich auf *englischsprachig* *B√ºcher bzw. Buchrartikel* beschr√§nkt.\n:::\n\n1.  Erstellen Sie einen neuen Datensatz `review_subsample_new`\n    -   Basierend auf dem Datensatzes `review_works_correct`:\n        1.  Nutzen Sie die `filter()`-Funktion, um\n            -   nur englischsprachige (`language`),\n            -   B√ºcher und Buchkapitel (`type`) herauszufiltern.\n        2.  Speichern Sie diese Umwandlung in einem neuen Datensatz mit dem Namen `review_subsample_new`\n2.  √úberpr√ºfen Sie die Transformation mit Hilfe der `glimpse()`-Funktion.\n3.  ‚úçÔ∏è Notieren Sie, wie viele Artikel im neuen Subsample enthalten sind.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Erstellung Subsample\nreview_subsample_new <- review_works_correct %>% \n  # Eingrenzung: Sprache und Typ\n  filter(language == \"en\") %>% \n  filter(type == \"book\" | type == \"book-chapter\")\n\n# √úberpr√ºfung\nreview_subsample_new %>% glimpse\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 2,994\nColumns: 41\n$ id                          <chr> \"https://openalex.org/W2899962821\", \"https‚Ä¶\n$ title                       <chr> \"Introduction to systematic reviews\", \"Cha‚Ä¶\n$ display_name                <chr> \"Introduction to systematic reviews\", \"Cha‚Ä¶\n$ author                      <list> [<data.frame[2 x 12]>], NA, [<data.frame[‚Ä¶\n$ ab                          <chr> \"A systematic review is a vital part of th‚Ä¶\n$ publication_date            <chr> \"2018-10-01\", \"2020-01-01\", \"2019-01-01\", ‚Ä¶\n$ relevance_score             <dbl> 300.1405, 255.6323, 248.6059, 231.0026, 22‚Ä¶\n$ so                          <chr> \"Manchester University Press eBooks\", \"JBI‚Ä¶\n$ so_id                       <chr> \"https://openalex.org/S4306463591\", \"https‚Ä¶\n$ host_organization           <chr> \"Winchester University Press\", NA, NA, NA,‚Ä¶\n$ issn_l                      <chr> NA, NA, NA, NA, \"0302-9743\", NA, NA, NA, N‚Ä¶\n$ url                         <chr> \"https://doi.org/10.7765/9781526136527.000‚Ä¶\n$ pdf_url                     <chr> \"https://www.manchesteropenhive.com/downlo‚Ä¶\n$ license                     <chr> \"cc-by-nc-nd\", NA, NA, NA, NA, NA, NA, \"cc‚Ä¶\n$ version                     <chr> \"publishedVersion\", \"publishedVersion\", \"p‚Ä¶\n$ first_page                  <chr> NA, NA, NA, NA, \"214\", \"48\", NA, \"3\", \"85\"‚Ä¶\n$ last_page                   <chr> NA, NA, NA, NA, \"227\", \"78\", NA, \"22\", \"94‚Ä¶\n$ volume                      <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA‚Ä¶\n$ issue                       <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA‚Ä¶\n$ is_oa                       <lgl> TRUE, TRUE, TRUE, FALSE, FALSE, FALSE, TRU‚Ä¶\n$ is_oa_anywhere              <lgl> TRUE, TRUE, TRUE, FALSE, FALSE, FALSE, TRU‚Ä¶\n$ oa_status                   <chr> \"hybrid\", \"bronze\", \"bronze\", \"closed\", \"c‚Ä¶\n$ oa_url                      <chr> \"https://www.manchesteropenhive.com/downlo‚Ä¶\n$ any_repository_has_fulltext <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, ‚Ä¶\n$ language                    <chr> \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", ‚Ä¶\n$ grants                      <list> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n$ cited_by_count              <int> 423, 496, 421, 547, 274, 64, 247, 189, 136‚Ä¶\n$ counts_by_year              <list> [<data.frame[8 x 2]>], [<data.frame[6 x 2‚Ä¶\n$ publication_year            <int> 2018, 2020, 2019, 2013, 2014, 2015, 2020, ‚Ä¶\n$ cited_by_api_url            <chr> \"https://api.openalex.org/works?filter=cit‚Ä¶\n$ ids                         <list> <\"https://openalex.org/W2899962821\", \"htt‚Ä¶\n$ doi                         <chr> \"https://doi.org/10.7765/9781526136527.000‚Ä¶\n$ type                        <chr> \"book-chapter\", \"book-chapter\", \"book-chap‚Ä¶\n$ referenced_works            <list> NA, NA, NA, <\"https://openalex.org/W11488‚Ä¶\n$ related_works               <list> <\"https://openalex.org/W4385987771\", \"htt‚Ä¶\n$ is_paratext                 <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, ‚Ä¶\n$ is_retracted                <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, ‚Ä¶\n$ concepts                    <list> [<data.frame[16 x 5]>], [<data.frame[7 x ‚Ä¶\n$ topics                      <list> [<tbl_df[4 x 5]>], [<tbl_df[4 x 5]>], [<t‚Ä¶\n$ publication_year_fct        <fct> 2018, 2020, 2019, 2013, 2014, 2015, 2020, ‚Ä¶\n$ type_fct                    <fct> book-chapter, book-chapter, book-chapter, ‚Ä¶\n```\n\n\n:::\n\n```{.r .cell-code}\n# Notiz:\n# Subsample enth√§lt 2994 Eintr√§ge\n```\n:::\n\n\n\n### üìã Exercise 2: Umwandlung zu 'tidy text'\n\n1.  Erstellen Sie einen neuen Datensatz `subsample_new_tidy`,\n    -   Basierend auf dem Datensatz `review_subsample_new`, mit folgenden Schritten:\n        1.  Tokenisierung der Abstracts (`ab`) mit der Funktion `unnest_tokens`.\n        2.  Ausschluss von Stoppw√∂rter mit `filter` und `stopwords$words` heraus.\n        3.  Speichern Sie diese Umwandlung, indem Sie einen neuen Datensatz mit dem Namen `subsample_new_tidy` erstellen.\n2.  Pr√ºfen Sie, ob die Umwandlung erfolgreich war (z.B. mit der Funktion `glimpse()`)\n3.  ‚úçÔ∏è Notieren Sie, wie viele Token im neuen Datensatz `subsample_new_tidy` enthalten sind.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Erstellung des neuen Datensatzes `subsample_new_tidy`\nsubsample_new_tidy <- review_subsample_new %>% \n  tidytext::unnest_tokens(\"text\", ab) %>% \n   filter(!text %in% tidytext::stop_words$word)\n\n# √úberpr√ºfung\nsubsample_new_tidy %>% print()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 182,776 √ó 41\n   id     title display_name author publication_date relevance_score so    so_id\n   <chr>  <chr> <chr>        <list> <chr>                      <dbl> <chr> <chr>\n 1 https‚Ä¶ Intr‚Ä¶ Introductio‚Ä¶ <df>   2018-10-01                  300. Manc‚Ä¶ http‚Ä¶\n 2 https‚Ä¶ Intr‚Ä¶ Introductio‚Ä¶ <df>   2018-10-01                  300. Manc‚Ä¶ http‚Ä¶\n 3 https‚Ä¶ Intr‚Ä¶ Introductio‚Ä¶ <df>   2018-10-01                  300. Manc‚Ä¶ http‚Ä¶\n 4 https‚Ä¶ Intr‚Ä¶ Introductio‚Ä¶ <df>   2018-10-01                  300. Manc‚Ä¶ http‚Ä¶\n 5 https‚Ä¶ Intr‚Ä¶ Introductio‚Ä¶ <df>   2018-10-01                  300. Manc‚Ä¶ http‚Ä¶\n 6 https‚Ä¶ Intr‚Ä¶ Introductio‚Ä¶ <df>   2018-10-01                  300. Manc‚Ä¶ http‚Ä¶\n 7 https‚Ä¶ Intr‚Ä¶ Introductio‚Ä¶ <df>   2018-10-01                  300. Manc‚Ä¶ http‚Ä¶\n 8 https‚Ä¶ Intr‚Ä¶ Introductio‚Ä¶ <df>   2018-10-01                  300. Manc‚Ä¶ http‚Ä¶\n 9 https‚Ä¶ Intr‚Ä¶ Introductio‚Ä¶ <df>   2018-10-01                  300. Manc‚Ä¶ http‚Ä¶\n10 https‚Ä¶ Intr‚Ä¶ Introductio‚Ä¶ <df>   2018-10-01                  300. Manc‚Ä¶ http‚Ä¶\n# ‚Ñπ 182,766 more rows\n# ‚Ñπ 33 more variables: host_organization <chr>, issn_l <chr>, url <chr>,\n#   pdf_url <chr>, license <chr>, version <chr>, first_page <chr>,\n#   last_page <chr>, volume <chr>, issue <chr>, is_oa <lgl>,\n#   is_oa_anywhere <lgl>, oa_status <chr>, oa_url <chr>,\n#   any_repository_has_fulltext <lgl>, language <chr>, grants <list>,\n#   cited_by_count <int>, counts_by_year <list>, publication_year <int>, ‚Ä¶\n```\n\n\n:::\n\n```{.r .cell-code}\n# Notiz:\n# Der neue Datensatz enth√§lt 182776 Token. \n```\n:::\n\n\n\n### üìã Exercise 3: Auswertung der Token\n\n1.  Erstellen Sie einen neuen Datensatz `subsample_new_summarized`,\n    -   Fassen Sie auf der Grundlage des Datensatzes `subsample_new_tidy` die H√§ufigkeit der einzelnen Token zusammen, indem Sie die Funktion `count()` auf die Variable `text` anwenden. Verwenden Sie das Argument `sort = TRUE`, um den Datensatz nach absteigender H√§ufigkeit der Token zu sortieren.\n    -   Speichern Sie diese Transformation, indem Sie einen neuen Datensatz mit dem Namen `subsample_new_summarized` erstellen.\n2.  Pr√ºfen Sie, ob die Umwandlung erfolgreich war, indem Sie die Funktion `print()` verwenden.\n    -   Verwenden Sie das Argument `n = 50`, um die 50 wichtigsten Token anzuzeigen (nur m√∂glich, wenn das Argument `sort = TRUE` bei der Ausf√ºhrung der Funktion `count()` verwendet wurde)\n3.  Verteilung der Token pr√ºfen\n    -   Verwenden Sie die Funktion `datawizard::describe_distribution()`, um verschiedene Verteilungsparameter des neuen Datensatzes zu √ºberpr√ºfen\n    -   ‚úçÔ∏è Notieren Sie, wie viele Token ein Abstract durchschnittlich enth√§lt.\n\n-   *Optional:* Ergebnisse mit einer Wortwolke √ºberpr√ºfen\n    -   Basierend auf dem sortierten Datensatz `subsample_new_summarized`\n        1.  Auswahl der 50 h√§ufigsten Token mit Hilfe der Funktion `top_n()`\n        2.  Erstellen Sie eine `ggplot()`-Basis mit `label = text` und `size = n` als `aes()` und\n        3.  Benutze ggwordcloud::geom_text_wordclout() um die Wortwolke zu erstellen.\n        4.  Verwenden Sie scale_size_are(), um die Skalierung der Wortwolke zu √ºbernehmen.\n        5.  Verwenden Sie `theme_minimal()` f√ºr eine saubere Visualisierung.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Erstellung des neuen Datensatzes `subsample_new_summmarized`\nsubsample_new_summmarized <- subsample_new_tidy %>% \n  count(text, sort = TRUE) \n\n# Preview Top 50 token\nsubsample_new_summmarized %>% \n    print(n = 50)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 14,959 √ó 2\n   text            n\n   <chr>       <int>\n 1 research     2761\n 2 literature   2539\n 3 review       2466\n 4 studies      1478\n 5 systematic   1231\n 6 study        1206\n 7 <NA>          936\n 8 social        903\n 9 analysis      881\n10 chapter       851\n11 data          824\n12 management    788\n13 based         786\n14 paper         695\n15 articles      686\n16 learning      666\n17 development   627\n18 future        622\n19 results       596\n20 information   581\n21 knowledge     572\n22 reviews       545\n23 education     518\n24 factors       516\n25 process       498\n26 related       491\n27 business      483\n28 health        454\n29 technology    452\n30 findings      449\n31 digital       437\n32 methods       424\n33 main          421\n34 design        417\n35 provide       410\n36 field         400\n37 quality       396\n38 context       384\n39 identify      382\n40 current       380\n41 performance   369\n42 published     367\n43 approach      356\n44 identified    356\n45 impact        347\n46 relevant      344\n47 systems       334\n48 role          331\n49 model         328\n50 conducted     327\n# ‚Ñπ 14,909 more rows\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check distribution parameters \nsubsample_new_summmarized %>%\n  datawizard::describe_distribution()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nVariable |  Mean |    SD | IQR |           Range | Skewness | Kurtosis |     n | n_Missing\n------------------------------------------------------------------------------------------\nn        | 12.22 | 57.24 |   5 | [1.00, 2761.00] |    24.32 |   925.44 | 14959 |         0\n```\n\n\n:::\n\n```{.r .cell-code}\n# Notiz:\n# Ein Absatz enth√§lt durchschnittlich 22 Token. \n\n# Optional: Check results with a wordcloud\nsubsample_new_summmarized %>% \n    top_n(50) %>% \n    ggplot(aes(label = text, size = n)) +\n    ggwordcloud::geom_text_wordcloud() +\n    scale_size_area(max_size = 15) +\n    theme_minimal()\n```\n\n::: {.cell-output-display}\n![](ms-exercise-08_solution_files/figure-html/exercise-3-solution-1.png){width=672}\n:::\n:::\n\n\n\n### üìã Exercise 4: Wortbeziehungen im Fokus\n\n#### 4.1 Couting word pairs\n\n1.  Z√§hlen von h√§ufigen Wortpaaren\n    -   Z√§hlen Sie auf der Grundlage des Datensatzes `subsample_new_tidy` Wortpaare mit `widyr::pairwise_count()`, mit den Argumenten `item = text`, `feature = id` und `sort = TRUE.`\n    -   Speichern Sie diese Transformation, indem Sie einen neuen Datensatz mit dem Namen `subsample_new_word_pairs` erstellen.\n2.  Pr√ºfen Sie, ob die Umwandlung erfolgreich war, indem Sie die Funktion `print()` verwenden.\n    -   Verwenden Sie das Argument `n = 50`, um die 50 wichtigsten Token anzuzeigen (nur m√∂glich, wenn bei der Ausf√ºhrung der Funktion `count()` das Argument `sort = TRUE` verwendet wurde)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Couting word pairs among sections\nsubsample_new_word_pairs <- subsample_new_tidy %>% \n  widyr::pairwise_count(\n    item = text,\n    feature = id,\n    sort = TRUE)\n\n# Check \nsubsample_new_word_pairs %>% print(n = 50)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 7,286,586 √ó 3\n   item1      item2          n\n   <chr>      <chr>      <dbl>\n 1 literature review      1160\n 2 review     literature  1160\n 3 research   review       947\n 4 review     research     947\n 5 literature research     915\n 6 research   literature   915\n 7 review     systematic   795\n 8 systematic review       795\n 9 literature systematic   642\n10 systematic literature   642\n11 studies    review       588\n12 review     studies      588\n13 research   systematic   561\n14 systematic research     561\n15 study      literature   555\n16 literature study        555\n17 study      review       549\n18 review     study        549\n19 literature studies      510\n20 studies    literature   510\n21 studies    research     508\n22 research   studies      508\n23 study      research     505\n24 research   study        505\n25 based      review       420\n26 review     based        420\n27 future     research     417\n28 research   future       417\n29 analysis   review       408\n30 review     analysis     408\n31 literature chapter      404\n32 chapter    literature   404\n33 paper      review       394\n34 review     paper        394\n35 future     review       391\n36 review     future       391\n37 results    review       385\n38 review     results      385\n39 literature paper        382\n40 paper      literature   382\n41 future     literature   381\n42 literature future       381\n43 literature analysis     379\n44 analysis   literature   379\n45 based      literature   379\n46 literature based        379\n47 chapter    review       377\n48 review     chapter      377\n49 studies    systematic   371\n50 systematic studies      371\n# ‚Ñπ 7,286,536 more rows\n```\n\n\n:::\n:::\n\n\n\n#### 4.2 Pairwise correlation\n\n1.  Ermittlung der paarweisen Korrelation\n    -   Basierend auf dem Datensatz `subsample_new_tidy`,\n    -   gruppieren Sie die Daten mit der Funktion `group_by()` nach der Variable `text` und\n    -   verwenden Sie `filter(n() >= X)`, um nur Token zu verwenden, die mindestens in einer bestimmte Anzahl (`X`) vorkommen; Sie k√∂nnen f√ºr `X` einen Wert Ihrer Wahl w√§hlen, ich w√ºrde jedoch dringend empfehlen, ein `X > 100` zu w√§hlen, da die folgende Funktion sonst m√∂glicherweise nicht in der Lage ist, die Berechnung durchzuf√ºhren.\n    -   Erstellen Sie Wortkorrelationen mit `widyr::pairwise_cor()`, mit den Argumenten `item = text`,`feature = id` und `sort = TRUE`.\n    -   Speichern Sie diese Transformation, indem Sie einen neuen Datensatz mit dem Namen `subsample_new_corr` erstellen.\n2.  Pr√ºfen Sie die Paare mit der h√∂chsten Korrelation mit der Funktion `print().`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Getting pairwise correlation \nsubsample_new_corr <- subsample_new_tidy %>% \n  group_by(text) %>% \n  filter(n() >= 250) %>% \n  pairwise_cor(text, id, sort = TRUE)\n\n# Check pairs with highest correlation\nsubsample_new_corr %>% print(n = 50)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 8,372 √ó 3\n   item1          item2          correlation\n   <chr>          <chr>                <dbl>\n 1 chain          supply               0.886\n 2 supply         chain                0.886\n 3 literature     review               0.614\n 4 review         literature           0.614\n 5 review         systematic           0.561\n 6 systematic     review               0.561\n 7 sustainability sustainable          0.467\n 8 sustainable    sustainability       0.467\n 9 research       review               0.462\n10 review         research             0.462\n11 literature     research             0.446\n12 research       literature           0.446\n13 future         research             0.373\n14 research       future               0.373\n15 media          social               0.368\n16 social         media                0.368\n17 literature     systematic           0.354\n18 systematic     literature           0.354\n19 learning       education            0.330\n20 education      learning             0.330\n21 studies        review               0.330\n22 review         studies              0.330\n23 research       systematic           0.319\n24 systematic     research             0.319\n25 study          research             0.309\n26 research       study                0.309\n27 study          literature           0.307\n28 literature     study                0.307\n29 studies        research             0.306\n30 research       studies              0.306\n31 articles       published            0.297\n32 published      articles             0.297\n33 articles       systematic           0.280\n34 systematic     articles             0.280\n35 models         model                0.280\n36 model          models               0.280\n37 study          review               0.279\n38 review         study                0.279\n39 results        review               0.278\n40 review         results              0.278\n41 results        systematic           0.272\n42 systematic     results              0.272\n43 based          review               0.269\n44 review         based                0.269\n45 articles       review               0.268\n46 review         articles             0.268\n47 studies        systematic           0.266\n48 systematic     studies              0.266\n49 theory         theoretical          0.266\n50 theoretical    theory               0.266\n# ‚Ñπ 8,322 more rows\n```\n\n\n:::\n:::\n\n\n\n### üìã Exercise 5: Inhaltlicher Vergleich\n\n-   Vergleichen Sie die Ergebnisse der √úbung mit den Auswertungen der Folien:\n    -   Wie unterscheiden sich die Ergebnisse?\n    -   W√ºrden Sie die B√ºcher bzw. Buchabschnitte mit in die Untersuchung integrieren?",
    "supporting": [
      "ms-exercise-08_solution_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}