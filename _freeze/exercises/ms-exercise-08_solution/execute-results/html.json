{
  "hash": "a1206cfbc8e57eb3d2b60dbbf480289a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Text processing with R\"\nsubtitle: \"Session 08 - Exercise\"\ndate: last-modified\ndate-format: \"DD.MM.YYYY\"\nformat:\n    html: \n        code-fold: true\n        code-summary: \"L√∂sung anzeigen\"\n---\n\n\n::: {.callout-tip icon=\"false\"}\n[![Quarto Document](https://raw.githubusercontent.com/faucommsci/teaching_materials/main/images/badges/badge-quarto_document.svg)](https://github.com/faucommsci/ps_24/blob/main/exercises/ms-exercise-08_solution.qmd) Link to source file\n:::\n\n::: callout-note\n## Ziel der Anwendung: Textanalyse in R kennenlernen\n\n-   Auffrischung der Grundkenntnisse im Umgang mit R, tidyverse und ggplot2\n-   Typische Schritte der Textanalyse mit `tidytext` kennenlernen, von der Tokenisierung bis zur Visualisierung.\n:::\n\n## Background\n\n::: callout-tip\n## Todays's data basis: [OpenAlex](https://openalex.org/)\n\n-   Via API bzw. openalexR [@aria2024] gesammelte \"works\" der Datenbank [OpenAlex](https://openalex.org/) mit Bezug zu Literaturriews in den Sozialwissenschaften zwischen 2013 und 2023\n\n-   Detaillierte Informationen und Ergebnisse zur Suchquery finden Sie [hier](https://openalex.org/works?page=1&filter=display_name.search%3A%28literature%20OR%20systematic%29%20AND%20review,primary_topic.domain.id%3Adomains%2F2,publication_year%3A2014%20-%202024&group_by=publication_year,open_access.is_oa,primary_topic.field.id).\n:::\n\n## Preparation\n\n::: callout-important\n## Wichtige Information\n\n-   Bitte stellen Sie sicher, dass Sie das jeweilige R-Studio Projekt zur √úbung ge√∂ffnet haben. Nur so funktionieren alle Dependencies korrekt.\n-   Um den einwandfreien Ablauf der √úbung zu gew√§hrleisten, wird f√ºr die Aufgaben auf eine eigenst√§ndige Datenerhebung verzichtet und ein √úbungsdatensatz zu verf√ºgung gestelt.\n:::\n\n### Packages\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n    here, qs, # file management\n    magrittr, janitor, # data wrangling\n    easystats, sjmisc, # data analysis\n    gt, gtExtras, # table visualization\n    ggpubr, ggwordcloud, # visualization\n    tidytext, widyr, # text analysis    \n    openalexR, \n    tidyverse # load last to avoid masking issues\n  )\n```\n:::\n\n\n### Import und Vorverarbeitung der Daten\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# Import from local\nreview_works <- qs::qread(here(\"data/session-07/openalex-review_works-2013_2023.qs\"))\n\n# Create correct data\nreview_works_correct <- review_works %>% \n    mutate(\n        # Create additional factor variables\n        publication_year_fct = as.factor(publication_year), \n        type_fct = as.factor(type)\n        )\n```\n:::\n\n\n## üõ†Ô∏è Praktische Anwendung\n\n::: callout-important\n## Achtung, bitte lesen!\n\n-   Bevor Sie mit der Arbeit an den folgenden üìã **Exercises** beginnen, stellen Sie bitte sicher, dass Sie alle Chunks des Abschnitts [Preparation] gerendert haben. Das k√∂nnen Sie tun, indem Sie den \"*Run all chunks above*\"-Knopf ![](/img/rstudio-button-render_all_chunks_above.png)des n√§chsten Chunks benutzen.\n-   Bei Fragen zum Code lohnt sich ein Blick in den **Showcase** (.qmd oder .html). Beim Showcase handelt es sich um eine kompakte Darstellung des in der Pr√§sentation verwenden R-Codes. Sie k√∂nnen das Showcase also nutzen, um sich die Code-Bausteine anzusehen, die f√ºr die R-Outputs auf den Slides benutzt wurden.\n:::\n\n### üìã Exercise 1: Neues Subsample\n\n::: callout-caution\n## Ziel der Aufgabe\n\n-   Erstellung eines neuen Datensatzes `review_subsample_new`, der sich auf *englischsprachig* *B√ºcher bzw. Buchrartikel* beschr√§nkt.\n:::\n\n1.  Erstellen Sie einen neuen Datensatz `review_subsample_new`\n    -   Basierend auf dem Datensatzes `review_works_correct`:\n        1.  Nutzen Sie die `filter()`-Funktion, um\n            -   nur englischsprachige (`language`),\n            -   B√ºcher und Buchkapitel (`type`) herauszufiltern.\n        2.  Speichern Sie diese Umwandlung in einem neuen Datensatz mit dem Namen `review_subsample_new`\n2.  √úberpr√ºfen Sie die Transformation mit Hilfe der `glimpse()`-Funktion.\n3.  ‚úçÔ∏è Notieren Sie, wie viele Artikel im neuen Subsample enthalten sind.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Erstellung Subsample\nreview_subsample_new <- review_works_correct %>% \n  # Eingrenzung: Sprache und Typ\n  filter(language == \"en\") %>% \n  filter(type == \"preprint\")\n\n# √úberpr√ºfung\nreview_subsample_new %>% glimpse\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 3,547\nColumns: 41\n$ id                          <chr> \"https://openalex.org/W4236476849\", \"https‚Ä¶\n$ title                       <chr> \"The PRISMA 2020 statement: an updated gui‚Ä¶\n$ display_name                <chr> \"The PRISMA 2020 statement: an updated gui‚Ä¶\n$ author                      <list> [<data.frame[26 x 12]>], [<data.frame[3 x‚Ä¶\n$ ab                          <chr> \"Background: The Preferred Reporting Items‚Ä¶\n$ publication_date            <chr> \"2020-09-14\", \"2019-07-01\", \"2017-04-01\", ‚Ä¶\n$ relevance_score             <dbl> 584.98030, 253.79811, 214.51546, 199.14885‚Ä¶\n$ so                          <chr> NA, \"Technological forecasting & social ch‚Ä¶\n$ so_id                       <chr> NA, \"https://openalex.org/S39307421\", \"htt‚Ä¶\n$ host_organization           <chr> NA, \"Elsevier BV\", \"Elsevier BV\", \"Faculty‚Ä¶\n$ issn_l                      <chr> NA, \"0040-1625\", \"0959-6526\", \"2046-1402\",‚Ä¶\n$ url                         <chr> \"https://doi.org/10.31222/osf.io/v7gm2\", \"‚Ä¶\n$ pdf_url                     <chr> \"https://osf.io/v7gm2/download\", NA, NA, \"‚Ä¶\n$ license                     <chr> NA, NA, NA, \"cc-by\", NA, NA, \"cc-by\", NA, ‚Ä¶\n$ version                     <chr> \"submittedVersion\", NA, NA, \"publishedVers‚Ä¶\n$ first_page                  <chr> NA, \"251\", \"1278\", \"588\", \"281\", \"113113\",‚Ä¶\n$ last_page                   <chr> NA, \"269\", \"1302\", \"588\", \"312\", \"113113\",‚Ä¶\n$ volume                      <chr> NA, \"144\", \"149\", \"6\", \"32\", \"125\", \"6\", N‚Ä¶\n$ issue                       <chr> NA, NA, NA, NA, \"3-4\", NA, NA, NA, NA, NA,‚Ä¶\n$ is_oa                       <lgl> TRUE, FALSE, FALSE, TRUE, FALSE, FALSE, TR‚Ä¶\n$ is_oa_anywhere              <lgl> TRUE, FALSE, FALSE, TRUE, FALSE, FALSE, TR‚Ä¶\n$ oa_status                   <chr> \"green\", \"closed\", \"closed\", \"gold\", \"clos‚Ä¶\n$ oa_url                      <chr> \"https://osf.io/v7gm2/download\", NA, NA, \"‚Ä¶\n$ any_repository_has_fulltext <lgl> TRUE, FALSE, FALSE, TRUE, FALSE, FALSE, TR‚Ä¶\n$ language                    <chr> \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", ‚Ä¶\n$ grants                      <list> NA, NA, <\"https://openalex.org/F432032109‚Ä¶\n$ cited_by_count              <int> 3320, 222, 153, 190, 105, 140, 132, 57, 95‚Ä¶\n$ counts_by_year              <list> [<data.frame[5 x 2]>], [<data.frame[7 x 2‚Ä¶\n$ publication_year            <int> 2020, 2019, 2017, 2017, 2019, 2019, 2017, ‚Ä¶\n$ cited_by_api_url            <chr> \"https://api.openalex.org/works?filter=cit‚Ä¶\n$ ids                         <list> <\"https://openalex.org/W4236476849\", \"htt‚Ä¶\n$ doi                         <chr> \"https://doi.org/10.31222/osf.io/v7gm2\", \"‚Ä¶\n$ type                        <chr> \"preprint\", \"preprint\", \"preprint\", \"prepr‚Ä¶\n$ referenced_works            <list> <\"https://openalex.org/W2022190222\", \"htt‚Ä¶\n$ related_works               <list> <\"https://openalex.org/W2921208823\", \"htt‚Ä¶\n$ is_paratext                 <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, ‚Ä¶\n$ is_retracted                <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, ‚Ä¶\n$ concepts                    <list> [<data.frame[20 x 5]>], [<data.frame[20 x‚Ä¶\n$ topics                      <list> [<tbl_df[4 x 5]>], [<tbl_df[12 x 5]>], [<‚Ä¶\n$ publication_year_fct        <fct> 2020, 2019, 2017, 2017, 2019, 2019, 2017, ‚Ä¶\n$ type_fct                    <fct> preprint, preprint, preprint, preprint, pr‚Ä¶\n```\n\n\n:::\n\n```{.r .cell-code}\n# Notiz:\n# Subsample enth√§lt 3547 Eintr√§ge\n```\n:::\n\n\n### üìã Exercise 2: Umwandlung zu 'tidy text'\n\n1.  Erstellen Sie einen neuen Datensatz `subsample_new_tidy`,\n    -   Basierend auf dem Datensatz `review_subsample_new`, mit folgenden Schritten:\n        1.  Tokenisierung der Abstracts (`ab`) mit der Funktion `unnest_tokens`.\n        2.  Ausschluss von Stoppw√∂rter mit `filter` und `stopwords$words` heraus.\n        3.  Speichern Sie diese Umwandlung, indem Sie einen neuen Datensatz mit dem Namen `subsample_new_tidy` erstellen.\n2.  Pr√ºfen Sie, ob die Umwandlung erfolgreich war (z.B. mit der Funktion `glimpse()`)\n3.  ‚úçÔ∏è Notieren Sie, wie viele Token im neuen Datensatz `subsample_new_tidy` enthalten sind.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Erstellung des neuen Datensatzes `subsample_new_tidy`\nsubsample_new_tidy <- review_subsample_new %>% \n  tidytext::unnest_tokens(\"text\", ab) %>% \n   filter(!text %in% tidytext::stop_words$word)\n\n# √úberpr√ºfung\nsubsample_new_tidy %>% print()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 498,535 √ó 41\n   id     title display_name author publication_date relevance_score so    so_id\n   <chr>  <chr> <chr>        <list> <chr>                      <dbl> <chr> <chr>\n 1 https‚Ä¶ The ‚Ä¶ The PRISMA ‚Ä¶ <df>   2020-09-14                  585. <NA>  <NA> \n 2 https‚Ä¶ The ‚Ä¶ The PRISMA ‚Ä¶ <df>   2020-09-14                  585. <NA>  <NA> \n 3 https‚Ä¶ The ‚Ä¶ The PRISMA ‚Ä¶ <df>   2020-09-14                  585. <NA>  <NA> \n 4 https‚Ä¶ The ‚Ä¶ The PRISMA ‚Ä¶ <df>   2020-09-14                  585. <NA>  <NA> \n 5 https‚Ä¶ The ‚Ä¶ The PRISMA ‚Ä¶ <df>   2020-09-14                  585. <NA>  <NA> \n 6 https‚Ä¶ The ‚Ä¶ The PRISMA ‚Ä¶ <df>   2020-09-14                  585. <NA>  <NA> \n 7 https‚Ä¶ The ‚Ä¶ The PRISMA ‚Ä¶ <df>   2020-09-14                  585. <NA>  <NA> \n 8 https‚Ä¶ The ‚Ä¶ The PRISMA ‚Ä¶ <df>   2020-09-14                  585. <NA>  <NA> \n 9 https‚Ä¶ The ‚Ä¶ The PRISMA ‚Ä¶ <df>   2020-09-14                  585. <NA>  <NA> \n10 https‚Ä¶ The ‚Ä¶ The PRISMA ‚Ä¶ <df>   2020-09-14                  585. <NA>  <NA> \n# ‚Ñπ 498,525 more rows\n# ‚Ñπ 33 more variables: host_organization <chr>, issn_l <chr>, url <chr>,\n#   pdf_url <chr>, license <chr>, version <chr>, first_page <chr>,\n#   last_page <chr>, volume <chr>, issue <chr>, is_oa <lgl>,\n#   is_oa_anywhere <lgl>, oa_status <chr>, oa_url <chr>,\n#   any_repository_has_fulltext <lgl>, language <chr>, grants <list>,\n#   cited_by_count <int>, counts_by_year <list>, publication_year <int>, ‚Ä¶\n```\n\n\n:::\n\n```{.r .cell-code}\n# Notiz:\n# Der neue Datensatz enth√§lt 498535 Token. \n```\n:::\n\n\n### üìã Exercise 3: Auswertung der Token\n\n1.  Erstellen Sie einen neuen Datensatz `subsample_new_summarized`,\n    -   Fassen Sie auf der Grundlage des Datensatzes `subsample_new_tidy` die H√§ufigkeit der einzelnen Token zusammen, indem Sie die Funktion `count()` auf die Variable `text` anwenden. Verwenden Sie das Argument `sort = TRUE`, um den Datensatz nach absteigender H√§ufigkeit der Token zu sortieren.\n    -   Speichern Sie diese Transformation, indem Sie einen neuen Datensatz mit dem Namen `subsample_new_summarized` erstellen.\n2.  Pr√ºfen Sie, ob die Umwandlung erfolgreich war, indem Sie die Funktion `print()` verwenden.\n    -   Verwenden Sie das Argument `n = 50`, um die 50 wichtigsten Token anzuzeigen (nur m√∂glich, wenn das Argument `sort = TRUE` bei der Ausf√ºhrung der Funktion `count()` verwendet wurde)\n3.  Verteilung der Token pr√ºfen\n    -   Verwenden Sie die Funktion `datawizard::describe_distribution()`, um verschiedene Verteilungsparameter des neuen Datensatzes zu √ºberpr√ºfen\n    -   ‚úçÔ∏è Notieren Sie, wie viele Token ein Abstract durchschnittlich enth√§lt.\n\n-   *Optional:* Ergebnisse mit einer Wortwolke √ºberpr√ºfen\n    -   Basierend auf dem sortierten Datensatz `subsample_new_summarized`\n        1.  Auswahl der 50 h√§ufigsten Token mit Hilfe der Funktion `top_n()`\n        2.  Erstellen Sie eine `ggplot()`-Basis mit `label = text` und `size = n` als `aes()` und\n        3.  Benutze ggwordcloud::geom_text_wordclout() um die Wortwolke zu erstellen.\n        4.  Verwenden Sie scale_size_are(), um die Skalierung der Wortwolke zu √ºbernehmen.\n        5.  Verwenden Sie `theme_minimal()` f√ºr eine saubere Visualisierung.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Erstellung des neuen Datensatzes `subsample_new_summmarized`\nsubsample_new_summmarized <- subsample_new_tidy %>% \n  count(text, sort = TRUE) \n\n# Preview Top 50 token\nsubsample_new_summmarized %>% \n    print(n = 50)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 22,001 √ó 2\n   text              n\n   <chr>         <int>\n 1 studies        7115\n 2 review         5857\n 3 title          5223\n 4 sec            5133\n 5 health         4591\n 6 systematic     4197\n 7 research       3597\n 8 results        2981\n 9 study          2949\n10 literature     2881\n11 data           2764\n12 analysis       2600\n13 interventions  2532\n14 included       2491\n15 methods        2469\n16 evidence       2229\n17 meta           2122\n18 quality        2069\n19 based          2017\n20 mental         1998\n21 reviews        1812\n22 social         1752\n23 articles       1734\n24 risk           1653\n25 search         1540\n26 outcomes       1515\n27 conducted      1498\n28 lt             1481\n29 background     1434\n30 19             1426\n31 identified     1420\n32 factors        1307\n33 effects        1300\n34 covid          1279\n35 findings       1254\n36 care           1248\n37 95             1247\n38 gt             1245\n39 published      1241\n40 related        1234\n41 abstract       1214\n42 databases      1211\n43 reported       1191\n44 2              1138\n45 conclusions    1133\n46 effect         1116\n47 ci             1111\n48 intervention   1110\n49 criteria       1077\n50 support        1065\n# ‚Ñπ 21,951 more rows\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check distribution parameters \nsubsample_new_summmarized %>%\n  datawizard::describe_distribution()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nVariable |  Mean |     SD | IQR |           Range | Skewness | Kurtosis |     n | n_Missing\n-------------------------------------------------------------------------------------------\nn        | 22.66 | 136.91 |   7 | [1.00, 7115.00] |    24.15 |   858.87 | 22001 |         0\n```\n\n\n:::\n\n```{.r .cell-code}\n# Notiz:\n# Ein Absatz enth√§lt durchschnittlich 22 Token. \n\n# Optional: Check results with a wordcloud\nsubsample_new_summmarized %>% \n    top_n(50) %>% \n    ggplot(aes(label = text, size = n)) +\n    ggwordcloud::geom_text_wordcloud() +\n    scale_size_area(max_size = 15) +\n    theme_minimal()\n```\n\n::: {.cell-output-display}\n![](ms-exercise-08_solution_files/figure-html/exercise-3-solution-1.png){width=672}\n:::\n:::\n\n\n### üìã Exercise 4: Wortbeziehungen im Fokus\n\n#### 4.1 Couting word pairs\n\n1.  Z√§hlen von h√§ufigen Wortpaaren\n    -   Z√§hlen Sie auf der Grundlage des Datensatzes `subsample_new_tidy` Wortpaare mit `widyr::pairwise_count()`, mit den Argumenten `item = text`, `feature = id` und `sort = TRUE.`\n    -   Speichern Sie diese Transformation, indem Sie einen neuen Datensatz mit dem Namen `subsample_new_word_pairs` erstellen.\n2.  Pr√ºfen Sie, ob die Umwandlung erfolgreich war, indem Sie die Funktion `print()` verwenden.\n    -   Verwenden Sie das Argument `n = 50`, um die 50 wichtigsten Token anzuzeigen (nur m√∂glich, wenn bei der Ausf√ºhrung der Funktion `count()` das Argument `sort = TRUE` verwendet wurde)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Couting word pairs among sections\nsubsample_new_word_pairs <- subsample_new_tidy %>% \n  widyr::pairwise_count(\n    item = text,\n    feature = id,\n    sort = TRUE)\n\n# Check \nsubsample_new_word_pairs %>% print(n = 50)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 12,662,794 √ó 3\n   item1      item2          n\n   <chr>      <chr>      <dbl>\n 1 review     systematic  2068\n 2 systematic review      2068\n 3 studies    review      1909\n 4 review     studies     1909\n 5 studies    systematic  1668\n 6 results    review      1668\n 7 review     results     1668\n 8 systematic studies     1668\n 9 studies    results     1502\n10 results    studies     1502\n11 research   review      1483\n12 review     research    1483\n13 results    systematic  1478\n14 systematic results     1478\n15 literature review      1471\n16 review     literature  1471\n17 methods    review      1453\n18 review     methods     1453\n19 methods    results     1387\n20 results    methods     1387\n21 study      review      1336\n22 review     study       1336\n23 methods    systematic  1320\n24 systematic methods     1320\n25 studies    methods     1306\n26 methods    studies     1306\n27 included   review      1236\n28 review     included    1236\n29 review     background  1227\n30 background review      1227\n31 methods    background  1224\n32 background methods     1224\n33 research   systematic  1216\n34 systematic research    1216\n35 results    background  1204\n36 background results     1204\n37 included   studies     1198\n38 studies    included    1198\n39 study      studies     1197\n40 studies    study       1197\n41 study      systematic  1195\n42 systematic study       1195\n43 research   studies     1178\n44 studies    research    1178\n45 health     review      1153\n46 review     health      1153\n47 analysis   review      1152\n48 review     analysis    1152\n49 studies    background  1148\n50 background studies     1148\n# ‚Ñπ 12,662,744 more rows\n```\n\n\n:::\n:::\n\n\n#### 4.2 Pairwise correlation\n\n1.  Ermittlung der paarweisen Korrelation\n    -   Basierend auf dem Datensatz `subsample_new_tidy`,\n    -   gruppieren Sie die Daten mit der Funktion `group_by()` nach der Variable `text` und\n    -   verwenden Sie `filter(n() >= X)`, um nur Token zu verwenden, die mindestens in einer bestimmte Anzahl (`X`) vorkommen; Sie k√∂nnen f√ºr `X` einen Wert Ihrer Wahl w√§hlen, ich w√ºrde jedoch dringend empfehlen, ein `X > 100` zu w√§hlen, da die folgende Funktion sonst m√∂glicherweise nicht in der Lage ist, die Berechnung durchzuf√ºhren.\n    -   Erstellen Sie Wortkorrelationen mit `widyr::pairwise_cor()`, mit den Argumenten `item = text`,`feature = id` und `sort = TRUE`.\n    -   Speichern Sie diese Transformation, indem Sie einen neuen Datensatz mit dem Namen `subsample_new_corr` erstellen.\n2.  Pr√ºfen Sie die Paare mit der h√∂chsten Korrelation mit der Funktion `print().`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Getting pairwise correlation \nsubsample_new_corr <- subsample_new_tidy %>% \n  group_by(text) %>% \n  filter(n() >= 250) %>% \n  pairwise_cor(text, id, sort = TRUE)\n\n# Check pairs with highest correlation\nsubsample_new_corr %>% print(n = 50)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 131,406 √ó 3\n   item1        item2        correlation\n   <chr>        <chr>              <dbl>\n 1 sec          title              0.935\n 2 title        sec                0.935\n 3 ci           95                 0.881\n 4 95           ci                 0.881\n 5 19           covid              0.844\n 6 covid        19                 0.844\n 7 items        preferred          0.835\n 8 preferred    items              0.835\n 9 pandemic     covid              0.812\n10 covid        pandemic           0.812\n11 vaccination  vaccine            0.796\n12 vaccine      vaccination        0.796\n13 trials       controlled         0.763\n14 controlled   trials             0.763\n15 web          science            0.755\n16 science      web                0.755\n17 sec          objective          0.715\n18 objective    sec                0.715\n19 srs          sr                 0.710\n20 sr           srs                0.710\n21 randomized   controlled         0.708\n22 controlled   randomized         0.708\n23 pandemic     19                 0.695\n24 19           pandemic           0.695\n25 objective    title              0.669\n26 title        objective          0.669\n27 gt           lt                 0.662\n28 lt           gt                 0.662\n29 trials       randomized         0.658\n30 randomized   trials             0.658\n31 depression   anxiety            0.650\n32 anxiety      depression         0.650\n33 bold         ns4                0.636\n34 ns4          bold               0.636\n35 methods      background         0.613\n36 background   methods            0.613\n37 inclusion    criteria           0.607\n38 criteria     inclusion          0.607\n39 reporting    preferred          0.599\n40 preferred    reporting          0.599\n41 items        reporting          0.599\n42 reporting    items              0.599\n43 peer         reviewed           0.592\n44 reviewed     peer               0.592\n45 1            2                  0.585\n46 2            1                  0.585\n47 sec          conclusions        0.577\n48 conclusions  sec                0.577\n49 registration prospero           0.574\n50 prospero     registration       0.574\n# ‚Ñπ 131,356 more rows\n```\n\n\n:::\n:::\n\n\n### üìã Exercise 5: Inhaltlicher Vergleich\n\n-   Vergleichen Sie die Ergebnisse der √úbung mit den Auswertungen der Folien:\n    -   Wie unterscheiden sich die Ergebnisse?\n    -   W√ºrden Sie die B√ºcher bzw. Buchabschnitte mit in die Untersuchung integrieren?",
    "supporting": [
      "ms-exercise-08_solution_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}