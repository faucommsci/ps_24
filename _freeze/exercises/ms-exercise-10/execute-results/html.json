{
  "hash": "af4b5c4259025249dae4fca199bfc2a1",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Unsupervised Machine Learning II\"\nsubtitle: \"Session 10 - Exercise\"\ndate: last-modified\ndate-format: \"DD.MM.YYYY\"\nexecute: \n  eval: false\n---\n\n\n\n::: {.callout-tip icon=\"false\"}\n[![Quarto Document](https://raw.githubusercontent.com/faucommsci/teaching_materials/main/images/badges/badge-quarto_document.svg)](https://github.com/faucommsci/ps_24/blob/main/exercises/ms-exercise-10.qmd) Link to source file\n:::\n\n::: callout-note\n## Fokus der √úbung: stm-Topicmodeling mit R kennenlernen\n\n-   Typische Schritte der Auswertung eines `stm`-Topicmodels mit Hilfe von `tidytext` [@silge2016] reproduzieren\n-   Verst√§ndnis f√ºr die Interpretation von Themenmodellen sch√§rfen.\n-   Einfluss von Metadaten untersuchen und interpretieren.\n:::\n\n## Background\n\n::: callout-tip\n## Todays's data basis: [OpenAlex](https://openalex.org/)\n\n-   Via API bzw. openalexR [@aria2024] gesammelte \"works\" der Datenbank [OpenAlex](https://openalex.org/) mit Bezug zu Literaturriews in den Sozialwissenschaften zwischen 2013 und 2023\n\n-   Detaillierte Informationen und Ergebnisse zur Suchquery finden Sie [hier](https://openalex.org/works?page=1&filter=display_name.search%3A%28literature%20OR%20systematic%29%20AND%20review,primary_topic.domain.id%3Adomains%2F2,publication_year%3A2014%20-%202024&group_by=publication_year,open_access.is_oa,primary_topic.field.id).\n:::\n\n## Preparation\n\n::: callout-important\n## Wichtige Information\n\n-   Bitte stellen Sie sicher, dass Sie das jeweilige R-Studio Projekt zur √úbung ge√∂ffnet haben. Nur so funktionieren alle Dependencies korrekt.\n-   Um den einwandfreien Ablauf der √úbung zu gew√§hrleisten, wird f√ºr die Aufgaben auf eine eigenst√§ndige Datenerhebung verzichtet und ein √úbungsdatensatz zu verf√ºgung gestelt.\n:::\n\n### Packages\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n    here, qs, # file management\n    magrittr, janitor, # data wrangling\n    easystats, sjmisc, # data analysis\n    gt, gtExtras, # table visualization\n    ggpubr, ggwordcloud, # visualization\n    # text analysis    \n    tidytext, widyr, # based on tidytext\n    quanteda, # based on quanteda\n    quanteda.textmodels, quanteda.textplots, quanteda.textstats, \n    stm, # structural topic modeling\n    openalexR, pushoverr, tictoc, \n    tidyverse # load last to avoid masking issues\n  )\n```\n:::\n\n\n\n### Import und Vorverarbeitung der Daten\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreview_works <- qs::qread(here(\"data/session-07/openalex-review_works-2013_2023.qs\"))\n\n# Create correct data\nreview_subsample <- review_works %>% \n    # Create additional factor variables\n    mutate(\n        publication_year_fct = as.factor(publication_year), \n        type_fct = as.factor(type)\n        ) %>%\n    # Eingrenzung: Sprache und Typ\n    filter(language == \"en\") %>% \n    filter(type == \"article\") %>%\n    # Datentranformation\n    unnest(topics, names_sep = \"_\") %>%\n    filter(topics_name == \"field\") %>% \n    filter(topics_i == \"1\") %>% \n    # Eingrenzung: Forschungsfeldes\n    filter(\n    topics_display_name == \"Social Sciences\"|\n    topics_display_name == \"Psychology\"\n    ) %>% \n    mutate(\n        field = as.factor(topics_display_name)\n    ) %>% \n    # Eingrenzung: Keine Eintr√§ge ohne Abstract\n    filter(!is.na(ab))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create corpus\nquanteda_corpus <- review_subsample %>% \n  quanteda::corpus(\n    docid_field = \"id\", \n    text_field = \"ab\"\n  )\n\n# Tokenize\nquanteda_token <- quanteda_corpus %>% \n  quanteda::tokens(\n    remove_punct = TRUE,\n    remove_symbols = TRUE, \n    remove_numbers = TRUE, \n    remove_url = TRUE, \n    split_tags = FALSE # keep hashtags and mentions\n  ) %>% \n  quanteda::tokens_tolower() %>% \n  quanteda::tokens_remove(\n    pattern = stopwords(\"en\")\n    )\n\n# Convert to Document-Feature-Matrix (DFM)\nquanteda_dfm <- quanteda_token %>% \n  quanteda::dfm()\n\n# Pruning\nquanteda_dfm_trim <- quanteda_dfm %>% \n  dfm_trim( \n    min_docfreq = 10/nrow(review_subsample),\n    max_docfreq = 0.99, \n    docfreq_type = \"prop\")\n\n# Convert for stm topic modeling\nquanteda_stm <- quanteda_dfm_trim %>% \n   convert(to = \"stm\")\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n\n## üõ†Ô∏è Praktische Anwendung\n\n::: callout-important\n## Achtung, bitte lesen!\n\n-   Bevor Sie mit der Arbeit an den folgenden üìã **Exercises** beginnen, stellen Sie bitte sicher, dass Sie alle Chunks des Abschnitts [Preparation] gerendert haben. Das k√∂nnen Sie tun, indem Sie den \"*Run all chunks above*\"-Knopf ![](https://raw.githubusercontent.com/faucommsci/teaching_materials/main/images/buttons/rstudio-button-render_all_chunks_above.png)des n√§chsten Chunks benutzen.\n-   Bei Fragen zum Code lohnt sich ein Blick in den **Showcase** (.qmd oder .html). Beim Showcase handelt es sich um eine kompakte Darstellung des in der Pr√§sentation verwenden R-Codes. Sie k√∂nnen das Showcase also nutzen, um sich die Code-Bausteine anzusehen, die f√ºr die R-Outputs auf den Slides benutzt wurden.\n:::\n\n### üìã Exercise 1: Visualisierung der Themenpr√§valenz\n\n#### 1.1. Auswahl des passenden Models\n\n1.  Erstelen Sie einen neuen Datensatz `stm_mdl_k40`\n    -   basierend auf dem Datensatz `stm_serach`\n        1.  Verwenden Sie `filter(k == 40)`, um das Modell mit 40 Themen zu auszuw√§hlen.\n        2.  Verwenden Sie `pull(mdl) %>% .[[1]]` um die Spalte und das Element zu extrahieren, die das Modell enth√§lt.\n        3.  Speichern Sie diese Umwandlung, indem Sie einen neuen Datensatz mit dem Namen `stm_mdl_k40` erstellen.\n2.  √úberpr√ºfen Sie die Transformation indem Sie `stm_mdl_k40` in die Konsole eingeben.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Pull tpm with 40 topics\n\n# Check\n```\n:::\n\n\n\n#### 1.2. Identifikation der Top-Terms f√ºr jedes Thema\n\n1.  Erstellen Sie einen neuen Datensatz `td_beta`\n    -   basierend auf dem Datensatz `stm_mdl_k40`,\n    -   Verwenden Sie `tidy(method = \"frex\")`, um die Beta-Matrix zu erstellen.\n2.  Erstellen Sie einen neuen Datensatz `top_terms`\n    -   basierend auf dem Datenastz `td_beta`,\n        1.  Verwenden Sie `arrange(beta)`, um die Begriffe nach Beta zu sortieren.\n        2.  Gruppieren Sie die Begriffe nach `topic` mit `group_by(topic)`.\n        3.  Extrahieren Sie die 7 h√§ufigsten Begriffe mit `top_n(7, beta)`.\n        4.  Sortieren Sie die Begriffe absteigend mit `arrange(-beta)`.\n        5.  W√§hlen Sie die Variablen `topic` und `term` mit `select(topic, term)` aus.\n        6.  Extrahieren Sie die Top-Begriffe pro Thema mit `summarise(terms = list(term))`.\n        7.  Transformieren Sie die extrahierten Begriffe pro Thema mit `map(terms, paste, collapse = \", \")` zu einem String.\n        8.  \"Entpacken\" Sie die Begriffe aus der Liste (unnesten) mit `unnest(cols = c(terms))`.\n3.  √úberpr√ºfen Sie die Transformation indem Sie `top_terms` in die Konsole eingeben.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create tidy beta matrix\n\n# Create top terms\n\n# Output\n```\n:::\n\n\n\n#### 1.3 Erstellung der Pr√§valenz-Tabelle f√ºr die Themen\n\n1.  Erstellen Sie einen neuen Datensatz `td_gamma`\n    -   basierend auf dem Datensatz `stm_mdl_k40`,\n    -   Verwenden Sie `tidy()`, um die Gamma-Matrix zu erstellen.\n    -   Verwenden Sie `document_names = names(quanteda_stm$documents)` um die Dokumentennamen zu speichern\n2.  Erstellen Sie einen neuen Datensatz `prevalence`\n    -   basierend auf dem Datensatz `td_gamma`,\n        1.  Gruppieren Sie die Themen nach `topic` mit `group_by(topic)`.\n        2.  Berechnen Sie den Durchschnitt der `gamma`-Werte pro Thema mit `summarise(gamma = mean(gamma))`.\n        3.  Sortieren Sie die Themen (absteigend) nach `gamma` mit `arrange(desc(gamma))`.\n        4.  Verkn√ºpfen Sie die Top-Begriffe mit den Themen mit `left_join(top_terms, by = \"topic\")`.\n        5.  √úberarbeiten Sie die Variable `topic` mit dem `mutate`-Befehl:\n            1.  Erstellen Sie eine neue Variable `topic` mit `paste0(\"Topic \",sprintf(\"%02d\", topic))`.\n            2.  Ordnen Sie die Themen nach `gamma` mit `reorder(topic, gamma)`.\n3.  Erstellung Sie eine Tabelle als Output\n    -   basierend auf dem Datenastz `prevalence`,\n        1.  Verwenden Sie `gt()` um eine Tabelle zu erstellen.\n        2.  Formatieren Sie die Spalte `gamma` mit `fmt_number(columns = vars(gamma), decimals = 2)` um nur zwei Nachkommastellen anzuzeigen.\n        3.  Verwenden Sie `gtExtras::gt_theme_538()` um das Design der Tabelle anzupassen.\n4.  ‚úçÔ∏è Auf Basis des Outputs von `prevalence` Notieren Sie, welche Themen Sie als problematisch sehen und warum.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create tidy gamma matrix\n\n# Create prevalence\n\n# Output\n```\n:::\n\n\n\n### üìã Exercise 2: Einfluss der Metadaten\n\n#### 2.1. Sch√§tzung der Meta-Effekte\n\n1.  Erstellen Sie einen neuen Datensatz `effects`:\n    -   Verwenden Sie die Funktion `estimateEffect()`, um die Effekte zu sch√§tzen.\n    -   Verwenden Sie f√ºr das `formular`-Argument `1:40 ~ publication_year_fct + field`, um die Effekte der Ver√∂ffentlichungsjahre und Fachbereiche zu sch√§tzen.\n    -   Verwenden Sie `stm_mdl_40` als das zu analysierende Modell.\n    -   Verwenden Sie `meta = quanteda_stm$meta`, um die Metadaten f√ºr die Sch√§tzung zu verwenden.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create data\n```\n:::\n\n\n\n#### 2.2. Untersuchung der Effekte\n\n1.  Erstellen Sie einen neuen Datensatz `effects_tidy` eine bereinigte Tabelle der Effekte:\n\n    -   Basierend auf dem Datensatz `effects`\n\n    1.  Verwenden Sie die `tidy()` Funktion, um die Effekte in ein aufbereitetes Format zu bringen.\n    2.  Filtern Sie die Daten:\n        1.  Entfernen Sie Zeilen, bei denen term den Wert `(Intercept)` hat.\n        2.  Behalten Sie nur die Zeilen, bei denen `term == \"fieldSocial Sciences\"` ist.\n    3.  Entfernen Sie die Spalte term mit `select(-term)`\n\n2.  Erstellen Sie ein Tabelle zur √úberpr√ºfung der Effekte\n\n    -   Basierend auf dem Datensatz `effects_tidy`:\n\n    1.  Verwenden Sie die Funktion `gt()`, um eine Tabelle zu erstellen.\n    2.  Formatieren Sie alle numerischen Variablen mit `fmt_number(columns = -c(topic), decimals = 3)`, um lediglich drei Dezimalstellen darzustellen.\n    3.  Verwenden Sie `data_color(columns = estimate, method = \"numeric\", palette = \"viridis\")`, um die Sch√§tzwerte farblich zu kennzeichnen.\n    4.  Wenden Sie das Design `gtExtras::gt_theme_538()` an.\n\n3.  ‚úçÔ∏è Notieren Sie, welches Thema am st√§rksten im Forschungsfeld \"Social Science\" vertreten ist.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Filter effect tidy data\n\n# Explore effects (table outpu)\n\n#### Notes:\n# \n```\n:::\n\n\n\n### üìã Exercise 3: Einzelthema im Fokus\n\n#### 3.1. Benennung des Themas k = 20\n\n1.  Benennen Sie das Thema `k = 20` aus dem Modell `stm_mdl_40`:\n    -   Verwenden Sie die Funktion `labelTopics()`.\n    -   Geben Sie das Thema 20 als Parameter mit `topic = 20` an.\n2.  ‚úçÔ∏è Notieren Sie die Themennamen. Begr√ºnden Sie kurz Ihre Entscheidung.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create topic label\n\n# Themenname:\n```\n:::\n\n\n\n#### 3.2. Zusammenf√ºhrung mit OpenAlex-Daten\n\n1.  Erstellen Sie einen neuen Datensatz `gamma_export`\n    -   basierend auf dem Datensatz `stm_mdl_k40`:\n        1.  Verwenden Sie `tidy()` um die Gamma-Matrix zu erstellen. Geben Sie `matrix = \"gamma\"` und `document_names = names(quanteda_stm$documents)` als Parameter an.\n        2.  Gruppieren Sie die Dokumente nach `document` mit `group_by(document)`.\n        3.  W√§hlen Sie die Dokumente mit dem h√∂chsten `gamma`-Wert mit `slice_max(gamma)`.\n        4.  L√∂sen Sie die Gruppierung mit `dplyr::ungroup()`.\n        5.  Verkn√ºpfen Sie die Daten mit `review_subsample` mittels `left_join(review_subsample, by = c(\"document\" = \"id\"))`.\n    -   Benennen Sie die Spalte `document` in `id` um mit `dplyr::rename(id = document)`\n    -   Erstellen Sie eine neue Variable `stm_topic` mit Hilfe des `mutate()`-Befehls. Verwenden Sie `as.factor(paste(\"Topic\", sprintf(\"%02d\", topic)))` um die Themen zu benennen und als Faktor zu speichern.\n2.  √úberpr√ºfen Sie Transformation mit Hilfe der `glimpse()`-Funktion, um sicherzustellen, dass die Daten korrekt erstellt wurden.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create gamma export\n\n# Check\n```\n:::\n\n\n\n#### 3.3 Verteilungsparameter von Thema 20\n\n1.  Erstellung eines Outputs zur √úberpr√ºfung der Lageparameter\n\n    -   Basierend auf dem Datensatz `gamma_export`:\n        1.  Filtern Sie die Daten nach `topic == 20`.\n        2.  W√§hlen Sie mit Hilfe der select()-Funktion die Variablen `gamma`, `relevance_score` und `cited_by_count` aus.\n        3.  Verwenden Sie die Funktion `datawizard::describe_distribution()` um die Verteilungsparameter zu berechnen.\n\n2.  ‚úçÔ∏è Identifizieren und notieren Sie folgende Informationen:\n\n    -   Wie viele Abstracts haben Thema 20 als Hauptthema?\n    -   Wie hoch ist der durschnittliche Relevance Score?\n    -   Wie viele Zitationen haben die Dokumente im Durchschnitt?\n    -   Wie viel Zitate hat das hochzitierteste Dokument?\n\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    # Create distribution parameters\n    gamma_export %>% \n      filter(topic == 20) %>%\n      select(gamma, relevance_score, cited_by_count) %>% \n      datawizard::describe_distribution()\n    \n    #### Notes\n    # Anzahl der Abstrats von Thema 20\n    # Durchschnittlicher Relevace Score: \n    # Durchschnittliche Zitationen:\n    # Anzahl der Zitationen des am meisten zitierten Dokuments:\n    ```\n    :::\n\n\n\n#### 3.4. Top-Dokumente des Themas\n\n1.  Identifizierung der Top-Dokumente\n    -   Basierend auf dem Datensatz `gamma_export`:\n        1.  Filtern Sie den Datensatz nach `stm_topic == \"Topic 20\"`.\n        2.  Sortieren Sie die Daten absteigend nach `gamma` mit `arrange(-gamma)`.\n        3.  W√§hlen Sie die Variablen `title`, `so`, `gamma`, `type`, und `ab` mit `select()` aus.\n        4.  W√§hlen Sie die obersten 5 Zeilen mit `slice_head(n = 5)`.\n2.  Erstellung eines Outputs zur √úberpr√ºfung der Top-Dokumente\n    -   Basierend auf dem Datensatz `top_docs_k20`:\n        1.  Verwenden Sie `gt()` um eine Tabelle zu erstellen.\n        2.  Formatieren Sie die Spalte `gamma` mit `fmt_number(columns = vars(gamma), decimals = 2)` um nur zwei Nachkommastellen anzuzeigen.\n        3.  Verwenden Sie `gtExtras::gt_theme_538()` um das Design der Tabelle anzupassen.\\\n3.  ‚úçÔ∏è Basierend auf den den Abstracts und den Titeln der Top-Dokumente:\n    -   Welche Themenbereiche decken die Dokumente ab?\n    -   W√ºrden Sie den im Abschnitt 3.1. gew√§hlten Themennamen beibehalten oder ab√§ndern?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Identify top documents for topic 20\n\n# Creae output\n```\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}